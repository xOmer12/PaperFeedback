{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data = pd.read_csv('/home/student/FinalProject/PaperFeedback/Datasets/acm_citation_network_v8_labeled.csv')\n",
    "# print(network_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = network_data['references'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42                              5390877920f70186a0d2ce74\n",
       "203    5390877920f70186a0d2cdc1;5390877f20f70186a0d30...\n",
       "Name: references, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(network_data['references'][network_data['references'].notna()].count())\n",
    "# print(network_data['index'].count() - network_data['references'][network_data['references'].notna()].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_dataset(data, relation_type='citation'):\n",
    "    edges = {}\n",
    "    nodes = set()\n",
    "\n",
    "    if relation_type == 'citation':\n",
    "        for _, row in data.iterrows():\n",
    "            paper_index = row['index']\n",
    "            refs = str(row['references']).split(';')\n",
    "            if refs:\n",
    "                nodes.update([paper_index]+refs)\n",
    "                for cited_paper in refs:\n",
    "                    edges[(paper_index, cited_paper)] = 1\n",
    "            else:\n",
    "                continue        \n",
    "\n",
    "    elif relation_type == 'co-citation': \n",
    "        for _, row in data.iterrows():\n",
    "            paper_index = row['index']\n",
    "            refs = str(row['references']).split(';')\n",
    "            if refs:\n",
    "                nodes.update([paper_index]+refs)\n",
    "                if len(refs) > 1:\n",
    "                    combs = itertools.combinations(refs, 2)\n",
    "                    for paper1, paper2 in combs:\n",
    "                        if (paper1, paper2) in edges.keys():\n",
    "                            edges[(paper1, paper2)] += 1\n",
    "                        else:\n",
    "                            edges[(paper1, paper2)] = 1\n",
    "            else:\n",
    "                continue  \n",
    "\n",
    "    elif relation_type == 'bibliographic_coupling':\n",
    "        over_threshold = 200\n",
    "        paper_references = defaultdict(set)\n",
    "        reference_counts = Counter()\n",
    "\n",
    "        # The new graph with references that have more than one citation\n",
    "        nodes = set()  # Include all papers as nodes\n",
    "        edges = defaultdict(int)  # Dictionary to store directed edges with incremented weights\n",
    "\n",
    "        # Step 1: Populate `paper_references` dictionary and count references\n",
    "        for _, row in data.iterrows():\n",
    "            paper_index = row['index']\n",
    "            nodes.add(paper_index)\n",
    "            refs = str(row['references']).split(', ')\n",
    "            for cited_paper in refs:\n",
    "                paper_references[cited_paper].add(paper_index)\n",
    "                reference_counts[cited_paper] += 1\n",
    "\n",
    "        # Identify references that have more than one citation\n",
    "        references_with_multiple_cites = {ref for ref, count in reference_counts.items() if count > 1}\n",
    "        \n",
    "        # Generate edges for references with more than one citation\n",
    "        for cited_paper in references_with_multiple_cites:  # Iterate over references with more than one citation\n",
    "            citing_papers = paper_references.get(cited_paper, set())\n",
    "            \n",
    "            # Only process references with fewer citing papers than the threshold\n",
    "            if len(citing_papers) < over_threshold:\n",
    "                # Generate directed combinations between citing papers\n",
    "                for paper1, paper2 in itertools.permutations(citing_papers, 2):\n",
    "                    edges[(paper1, paper2)] += 1  # Increment the weight by 1 each time\n",
    "\n",
    "        return nodes, edges\n",
    "\n",
    "    elif relation_type == 'author_collaboration':\n",
    "        author_papers = defaultdict(set)  # Store which papers each author wrote\n",
    "        edges = defaultdict(int)  # Store edges with their weights, initialized to 0\n",
    "        max_paper_threshold = 80  # Define the max number of papers an author can have before collaboration is considered\n",
    "        collaboration_threshold = 2  # Define a minimum threshold for how many papers are needed for collaboration\n",
    "        \n",
    "        # Step 1: Build the map from authors to papers they wrote\n",
    "        for _, row in data.iterrows():\n",
    "            paper_index = row['index']\n",
    "            authors = str(row['authors']).split(', ')\n",
    "            for author in authors:\n",
    "                author_papers[author].add(paper_index)\n",
    "\n",
    "        # Step 2: Process collaborations only for authors with fewer papers than the threshold\n",
    "        for author, written_papers in author_papers.items():\n",
    "            # Proceed only if the author has written fewer than the maximum paper threshold and at least 2 papers\n",
    "            if len(written_papers) >= collaboration_threshold and len(written_papers) <= max_paper_threshold:\n",
    "                nodes.update(written_papers)  # Add papers to nodes\n",
    "\n",
    "                # Generate all combinations of papers written by the same author\n",
    "                for paper1, paper2 in itertools.combinations(written_papers, 2):\n",
    "                    if paper1 != paper2:  # Avoid self-loops (paper citing itself)\n",
    "                        # Add the edge and increment the weight by 1\n",
    "                        edges[(paper1, paper2)] += 1  # paper1 -> paper2\n",
    "                        edges[(paper2, paper1)] += 1  # paper2 -> paper1\n",
    "\n",
    "        return nodes, edges\n",
    "\n",
    "    else:\n",
    "        raise Exception('Illegal relation type')           \n",
    "\n",
    "    \n",
    "    return data['index'], edges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # elif relation_type == 'bibliographic_coupling':\n",
    "    #     paper_references = {}  # Dictionary to store which papers cite which references\n",
    "        \n",
    "    #     # First, build a map from references to the papers that cite them\n",
    "    #     for _, row in data.iterrows():\n",
    "    #         paper_index = row['index']\n",
    "    #         refs = str(row['references']).split(', ')\n",
    "    #         if refs:\n",
    "    #             for cited_paper in refs:\n",
    "    #                 if cited_paper in paper_references.keys():\n",
    "    #                     paper_references[cited_paper].append([paper_index])\n",
    "    #                 else:\n",
    "    #                     paper_references[cited_paper] = [paper_index]\n",
    "    #         else:\n",
    "    #             continue        \n",
    "        \n",
    "    #     # Now, connect papers that cite the same references\n",
    "    #     for refs, citing_papers in paper_references.items():\n",
    "    #         nodes.update(citing_papers)\n",
    "    #         combs = itertools.combinations(citing_papers, 2)\n",
    "    #         for paper1, paper2 in combs:\n",
    "    #             if (paper1, paper2) in edges.keys():\n",
    "    #                 edges[(paper1, paper2)] += 1\n",
    "    #             else:\n",
    "    #                 edges[(paper1, paper2)] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # elif relation_type == 'author_collaboration':\n",
    "        # author_papers = {}  # Dictionary to store which papers wrote each authors\n",
    "        \n",
    "        # # First, build a map from authors to the papers he wrote\n",
    "        # for _, row in data.iterrows():\n",
    "        #     paper_index = row['index']\n",
    "        #     authors = str(row['authors']).split(', ')\n",
    "        #     if authors:\n",
    "        #         for author in authors:\n",
    "        #             if author in author_papers.keys():\n",
    "        #                 author_papers[author].append([paper_index])\n",
    "        #             else:\n",
    "        #                 author_papers[author] = [paper_index]\n",
    "        #     else:\n",
    "        #         continue        \n",
    "        \n",
    "        # # Now, connect papers that cite the same references\n",
    "        # for author, written_papers in author_papers.items():\n",
    "        #     nodes.update(citing_papers)\n",
    "        #     combs = itertools.combinations(written_papers, 2)\n",
    "        #     for paper1, paper2 in combs:\n",
    "        #         if (paper1, paper2) in edges.keys():\n",
    "        #             edges[(paper1, paper2)] += 1\n",
    "        #         else:\n",
    "        #             edges[(paper1, paper2)] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dictionary, file_name):\n",
    "    # Convert tuples to lists for JSON serialization\n",
    "    dict_with_str_keys = {key[0]+';'+key[1]: value for key, value in dictionary.items()}\n",
    "    \n",
    "    # Save the converted dictionary to a file in JSON format\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(dict_with_str_keys, file, indent=4)\n",
    "\n",
    "def save_set_to_file(my_set, file_name):\n",
    "    # Convert the set to a list for serialization\n",
    "    list_representation = list(my_set)\n",
    "    \n",
    "    # Write the list to a file in JSON format\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(list_representation, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_type = 'co-citation'\n",
    "nodes, edges = build_graph_dataset(data=network_data, relation_type=relation_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_with_str_keys = {key[0]+';'+key[1]: value for key, value in edges.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{relation_type}_edge_index', 'w') as file:\n",
    "        json.dump(dict_with_str_keys, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_set_to_file(nodes ,f'{relation_type}_node_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct references (R): 884063\n",
      "Average number of papers citing each reference (Average M): 2.694010494727186\n",
      "Max papers citing a single reference: 1464772\n",
      "Min papers citing a single reference: 1\n",
      "Distribution of citing counts (M):\n",
      "  References cited by 1 papers: 865905\n",
      "  References cited by 2 papers: 13973\n",
      "  References cited by 3 papers: 2102\n",
      "  References cited by 4 papers: 714\n",
      "  References cited by 5 papers: 385\n",
      "  References cited by 6 papers: 254\n",
      "  References cited by 7 papers: 148\n",
      "  References cited by 8 papers: 102\n",
      "  References cited by 9 papers: 71\n",
      "  References cited by 10 papers: 52\n",
      "  References cited by 11 papers: 46\n",
      "  References cited by 12 papers: 35\n",
      "  References cited by 13 papers: 30\n",
      "  References cited by 14 papers: 30\n",
      "  References cited by 15 papers: 23\n",
      "  References cited by 16 papers: 23\n",
      "  References cited by 17 papers: 19\n",
      "  References cited by 18 papers: 12\n",
      "  References cited by 19 papers: 14\n",
      "  References cited by 20 papers: 17\n",
      "  References cited by 21 papers: 12\n",
      "  References cited by 22 papers: 8\n",
      "  References cited by 23 papers: 7\n",
      "  References cited by 24 papers: 3\n",
      "  References cited by 25 papers: 4\n",
      "  References cited by 26 papers: 13\n",
      "  References cited by 27 papers: 3\n",
      "  References cited by 28 papers: 3\n",
      "  References cited by 29 papers: 2\n",
      "  References cited by 30 papers: 4\n",
      "  References cited by 31 papers: 2\n",
      "  References cited by 32 papers: 2\n",
      "  References cited by 33 papers: 3\n",
      "  References cited by 34 papers: 8\n",
      "  References cited by 35 papers: 3\n",
      "  References cited by 36 papers: 1\n",
      "  References cited by 37 papers: 3\n",
      "  References cited by 39 papers: 1\n",
      "  References cited by 40 papers: 2\n",
      "  References cited by 41 papers: 2\n",
      "  References cited by 42 papers: 1\n",
      "  References cited by 44 papers: 2\n",
      "  References cited by 45 papers: 1\n",
      "  References cited by 46 papers: 2\n",
      "  References cited by 47 papers: 2\n",
      "  References cited by 48 papers: 1\n",
      "  References cited by 52 papers: 1\n",
      "  References cited by 56 papers: 1\n",
      "  References cited by 58 papers: 1\n",
      "  References cited by 60 papers: 1\n",
      "  References cited by 61 papers: 1\n",
      "  References cited by 66 papers: 1\n",
      "  References cited by 70 papers: 1\n",
      "  References cited by 77 papers: 1\n",
      "  References cited by 87 papers: 1\n",
      "  References cited by 132 papers: 1\n",
      "  References cited by 154 papers: 1\n",
      "  References cited by 162 papers: 1\n",
      "  References cited by 1464772 papers: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_reference_data(data):\n",
    "    paper_references = defaultdict(set)\n",
    "    \n",
    "    # Populate the `paper_references` dictionary\n",
    "    for _, row in data.iterrows():\n",
    "        paper_index = row['index']\n",
    "        refs = str(row['references']).split(', ')\n",
    "        for cited_paper in refs:\n",
    "            paper_references[cited_paper].add(paper_index)\n",
    "        \n",
    "\n",
    "    # Calculate R and M metrics\n",
    "    R = len(paper_references)  # Total number of distinct references\n",
    "    M_values = [len(citing_papers) for citing_papers in paper_references.values()]  # List of citing counts per reference\n",
    "    M_average = sum(M_values) / R if R > 0 else 0  # Average citing count per reference\n",
    "    \n",
    "    # Summary of M distribution\n",
    "    M_distribution = Counter(M_values)\n",
    "    max_citations = max(M_values) if M_values else 0\n",
    "    min_citations = min(M_values) if M_values else 0\n",
    "\n",
    "    print(\"Total distinct references (R):\", R)\n",
    "    print(\"Average number of papers citing each reference (Average M):\", M_average)\n",
    "    print(\"Max papers citing a single reference:\", max_citations)\n",
    "    print(\"Min papers citing a single reference:\", min_citations)\n",
    "    print(\"Distribution of citing counts (M):\")\n",
    "    for count, num_references in sorted(M_distribution.items()):\n",
    "        print(f\"  References cited by {count} papers: {num_references}\")\n",
    "\n",
    "analyze_reference_data(data=network_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_type = 'bibliographic_coupling'\n",
    "nodes, edges = build_graph_dataset(data=network_data, relation_type=relation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_file(edges, f'{relation_type}_edge_index')\n",
    "save_set_to_file(nodes ,f'{relation_type}_node_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited paper: 5390880720f70186a0d789af\n",
      "Papers citing it: ['53908b6c20f70186a0dbd87a', '5390972920f70186a0dfa5a1', '5390a2e920f70186a0e67686']\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your dataframe\n",
    "def check_reference_data(data):\n",
    "    paper_references = defaultdict(set)\n",
    "    \n",
    "    # Step 1: Populate the `paper_references` dictionary\n",
    "    for _, row in data.iterrows():\n",
    "        paper_index = row['index']\n",
    "        refs = str(row['references']).split(', ')\n",
    "        for cited_paper in refs:\n",
    "            paper_references[cited_paper].add(paper_index)\n",
    "\n",
    "    # Step 2: Find a cited paper that is cited by exactly 3 papers\n",
    "    for cited_paper, citing_papers in paper_references.items():\n",
    "        if len(citing_papers) == 3:\n",
    "            print(f\"Cited paper: {cited_paper}\")\n",
    "            print(f\"Papers citing it: {list(citing_papers)}\")\n",
    "            break  # Only get the first one found, remove if you want to find all\n",
    "\n",
    "\n",
    "check_reference_data(network_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Node: 53908b6c20f70186a0dbd87a, Right Node: 5390972920f70186a0dfa5a1, Weight: 1\n",
      "Left Node: 53908b6c20f70186a0dbd87a, Right Node: 5390a2e920f70186a0e67686, Weight: 1\n",
      "Left Node: 5390972920f70186a0dfa5a1, Right Node: 53908b6c20f70186a0dbd87a, Weight: 1\n",
      "Left Node: 5390972920f70186a0dfa5a1, Right Node: 5390a2e920f70186a0e67686, Weight: 1\n",
      "Left Node: 5390a2e920f70186a0e67686, Right Node: 53908b6c20f70186a0dbd87a, Weight: 1\n",
      "Left Node: 5390a2e920f70186a0e67686, Right Node: 5390972920f70186a0dfa5a1, Weight: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# List of papers citing the target paper\n",
    "citing_papers = ['53908b6c20f70186a0dbd87a', '5390972920f70186a0dfa5a1', '5390a2e920f70186a0e67686']\n",
    "\n",
    "# Load the edges from the JSON file\n",
    "with open(f'{relation_type}_edge_index', 'r') as file:\n",
    "    edges = json.load(file)\n",
    "\n",
    "# Filter edges where one of the citing papers is the left node (source)\n",
    "filtered_edges = [\n",
    "    (left_node, right_node, weight) \n",
    "    for edge, weight in edges.items()\n",
    "    for left_node, right_node in [edge.split(';')]  # Split string keys into left and right nodes\n",
    "    if left_node in citing_papers\n",
    "]\n",
    "\n",
    "# Print the filtered edges\n",
    "for edge in filtered_edges:\n",
    "    print(f\"Left Node: {edge[0]}, Right Node: {edge[1]}, Weight: {edge[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total collaborations (edges): 3618456\n",
      "Average number of collaborations per paper pair: 1.08448548220567\n",
      "Max collaborations between two papers: 77\n",
      "Min collaborations between two papers: 1\n",
      "Distribution of collaboration counts:\n",
      "  Paper pairs with 1 collaborations: 3376357\n",
      "  Paper pairs with 2 collaborations: 197784\n",
      "  Paper pairs with 3 collaborations: 32806\n",
      "  Paper pairs with 4 collaborations: 7654\n",
      "  Paper pairs with 5 collaborations: 2246\n",
      "  Paper pairs with 6 collaborations: 799\n",
      "  Paper pairs with 7 collaborations: 359\n",
      "  Paper pairs with 8 collaborations: 201\n",
      "  Paper pairs with 9 collaborations: 88\n",
      "  Paper pairs with 10 collaborations: 53\n",
      "  Paper pairs with 11 collaborations: 31\n",
      "  Paper pairs with 12 collaborations: 17\n",
      "  Paper pairs with 13 collaborations: 18\n",
      "  Paper pairs with 14 collaborations: 15\n",
      "  Paper pairs with 15 collaborations: 6\n",
      "  Paper pairs with 16 collaborations: 3\n",
      "  Paper pairs with 17 collaborations: 1\n",
      "  Paper pairs with 18 collaborations: 3\n",
      "  Paper pairs with 19 collaborations: 2\n",
      "  Paper pairs with 20 collaborations: 1\n",
      "  Paper pairs with 21 collaborations: 1\n",
      "  Paper pairs with 22 collaborations: 1\n",
      "  Paper pairs with 23 collaborations: 1\n",
      "  Paper pairs with 25 collaborations: 1\n",
      "  Paper pairs with 26 collaborations: 1\n",
      "  Paper pairs with 27 collaborations: 1\n",
      "  Paper pairs with 31 collaborations: 1\n",
      "  Paper pairs with 41 collaborations: 1\n",
      "  Paper pairs with 49 collaborations: 1\n",
      "  Paper pairs with 69 collaborations: 1\n",
      "  Paper pairs with 70 collaborations: 1\n",
      "  Paper pairs with 77 collaborations: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_author_collaboration(data, paper_threshold=2, max_papers=10):\n",
    "    author_papers = defaultdict(set)  # Store which papers each author wrote\n",
    "    collaboration_counts = defaultdict(int)  # Store number of collaborations between authors\n",
    "\n",
    "    # Step 1: Build the map from authors to papers they wrote\n",
    "    for _, row in data.iterrows():\n",
    "        paper_index = row['index']\n",
    "        authors = str(row['authors']).split(', ')\n",
    "        for author in authors:\n",
    "            author_papers[author].add(paper_index)\n",
    "\n",
    "    # Step 2: Analyze collaboration only for authors with papers within the threshold range\n",
    "    for author, written_papers in author_papers.items():\n",
    "        # Filter authors who have at least 'paper_threshold' papers and no more than 'max_papers'\n",
    "        if len(written_papers) >= paper_threshold and len(written_papers) <= max_papers:\n",
    "            # Create collaborations (pairs of co-authored papers)\n",
    "            for paper1, paper2 in itertools.combinations(written_papers, 2):\n",
    "                # Increment collaboration count (collaboration is bidirectional)\n",
    "                collaboration_counts[(paper1, paper2)] += 1\n",
    "\n",
    "    # Step 3: Calculate metrics\n",
    "    total_collaborations = len(collaboration_counts)  # Total collaborations (edges)\n",
    "    collaboration_values = list(collaboration_counts.values())  # Collaboration counts per pair\n",
    "    avg_collaboration_count = sum(collaboration_values) / total_collaborations if total_collaborations > 0 else 0  # Average collaboration count\n",
    "    \n",
    "    # Distribution of collaborations\n",
    "    collaboration_distribution = Counter(collaboration_values)\n",
    "    max_collaborations = max(collaboration_values) if collaboration_values else 0\n",
    "    min_collaborations = min(collaboration_values) if collaboration_values else 0\n",
    "\n",
    "    # Print summary\n",
    "    print(\"Total collaborations (edges):\", total_collaborations)\n",
    "    print(\"Average number of collaborations per paper pair:\", avg_collaboration_count)\n",
    "    print(\"Max collaborations between two papers:\", max_collaborations)\n",
    "    print(\"Min collaborations between two papers:\", min_collaborations)\n",
    "    print(\"Distribution of collaboration counts:\")\n",
    "    for count, num_collaborations in sorted(collaboration_distribution.items()):\n",
    "        print(f\"  Paper pairs with {count} collaborations: {num_collaborations}\")\n",
    "\n",
    "# Example of calling the function with your data\n",
    "analyze_author_collaboration(data=network_data, paper_threshold=2, max_papers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_type = 'author_collaboration'\n",
    "nodes, edges = build_graph_dataset(data=network_data, relation_type=relation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_file(edges, f'{relation_type}_edge_index')\n",
    "save_set_to_file(nodes ,f'{relation_type}_node_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Keith Brian Gallagher\n",
      "Papers written by this author: ['53908cde20f70186a0dcd63e', '539087a520f70186a0d4811f', '5390879920f70186a0d422ab']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Assuming 'data' is your dataframe\n",
    "def check_author_collaboration(data):\n",
    "    author_papers = defaultdict(set)\n",
    "    \n",
    "    # Step 1: Populate the `author_papers` dictionary\n",
    "    for _, row in data.iterrows():\n",
    "        paper_index = row['index']\n",
    "        authors = str(row['authors']).split(', ')\n",
    "        for author in authors:\n",
    "            author_papers[author].add(paper_index)\n",
    "\n",
    "    # Step 2: Find authors who have written exactly 3 papers\n",
    "    for author, written_papers in author_papers.items():\n",
    "        if len(written_papers) == 3:  # Author has written exactly 3 papers\n",
    "            print(f\"Author: {author}\")\n",
    "            print(f\"Papers written by this author: {list(written_papers)}\")\n",
    "            break  # Stop after finding the first author with exactly 3 papers\n",
    "\n",
    "# Example call to the function\n",
    "check_author_collaboration(network_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Node: 53908cde20f70186a0dcd63e, Right Node: 539087a520f70186a0d4811f, Weight: 1\n",
      "Left Node: 53908cde20f70186a0dcd63e, Right Node: 5390879920f70186a0d422ab, Weight: 1\n",
      "Left Node: 539087a520f70186a0d4811f, Right Node: 5390879920f70186a0d422ab, Weight: 1\n",
      "Left Node: 539087e120f70186a0d66c5e, Right Node: 539087a520f70186a0d4811f, Weight: 1\n",
      "Left Node: 5390b61e20f70186a0f14cbf, Right Node: 539087a520f70186a0d4811f, Weight: 1\n",
      "Left Node: 5390b61e20f70186a0f14bfe, Right Node: 539087a520f70186a0d4811f, Weight: 1\n",
      "Left Node: 5390b61e20f70186a0f14c1d, Right Node: 539087a520f70186a0d4811f, Weight: 1\n",
      "Left Node: 5390b5df20f70186a0f0b4a8, Right Node: 539087a520f70186a0d4811f, Weight: 1\n",
      "Left Node: 539087a520f70186a0d4811f, Right Node: 5390879220f70186a0d3d486, Weight: 1\n"
     ]
    }
   ],
   "source": [
    "# List of papers written by the target author\n",
    "papers_written_by_author = ['53908cde20f70186a0dcd63e', '539087a520f70186a0d4811f', '5390879920f70186a0d422ab']\n",
    "\n",
    "# Load the edges from the JSON file\n",
    "with open(f'{relation_type}_edge_index', 'r') as file:\n",
    "    edges = json.load(file)\n",
    "\n",
    "# Filter edges where one of the papers written by the author is the left node (source)\n",
    "filtered_edges = [\n",
    "    (left_node, right_node, weight) \n",
    "    for edge, weight in edges.items()\n",
    "    for left_node, right_node in [edge.split(';')]  # Split string keys into left and right nodes\n",
    "    if left_node in papers_written_by_author or right_node in papers_written_by_author\n",
    "]\n",
    "\n",
    "# Print the filtered edges\n",
    "for edge in filtered_edges:\n",
    "    print(f\"Left Node: {edge[0]}, Right Node: {edge[1]}, Weight: {edge[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
