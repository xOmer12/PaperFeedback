{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Retrieval - Initial Retrieval for Pseudo-Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and VectorDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '703c7c8a-2b8f-46bc-b2f7-ede6b037b3fa'\n",
    "index_name = 'ann-embeddings'\n",
    "TOP_N=10\n",
    "TOP_K=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=API_KEY)\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('using GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('using CPUG')\n",
    "    device = 'cpu'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'Algorithms for image processing and computer vision'\n",
    "query = 'Computer vision, the automatic construction of scene descriptions from image input data, has just entered its second decade. Approaches have varied widely, especially in the amounts of symbolic, domain-dependent knowledge and inference that are incorporated into the vision process. Much current research addresses the extraction of physical properties of the scene (depth, surface orientation, reflectance) from images by using only a few general assumptions about the scene domain. Extraction of physical parameters is part of a hierarchy of operations needed to transform image input data to symbolic descriptions. Two other processes that serve as examples are stereo fusion and the partitioning of image phenomena into related groups. Computer vision research is influencing theories of animal perception as well as the design of computing architectures for artificial intelligence.'\n",
    "embedded_query = model.encode(query, convert_to_tensor=True).cpu().detach().numpy()\n",
    "embedded_query_list = embedded_query.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top K Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=embedded_query_list,\n",
    "    top_k=TOP_K,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/home/student/FinalProject/PaperFeedback/Datasets/acm_citation_network_v8_labeled.csv')\n",
    "# data.head(5)\n",
    "# num_nulls = data['abstract'].isnull().sum()\n",
    "# print(num_nulls/len(data['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '5390a6b120f70186a0e8462d',\n",
      "              'metadata': {'abstract': 'For human vision to be explained by a '\n",
      "                                       'computational theory, the first '\n",
      "                                       'question is plain: What are the '\n",
      "                                       'problems the brain solves when we see? '\n",
      "                                       'It is argued that vision is the '\n",
      "                                       'construction of efficient symbolic '\n",
      "                                       'descriptions from images of the world. '\n",
      "                                       'An important aspect of vision is '\n",
      "                                       'therefore the choice of '\n",
      "                                       'representations for the different '\n",
      "                                       'kinds of information in a visual '\n",
      "                                       'scene. An overall framework is '\n",
      "                                       'suggested for extracting shape '\n",
      "                                       'information from images, in which the '\n",
      "                                       'analysis proceeds through three '\n",
      "                                       'representations; (1) the primal '\n",
      "                                       'sketch, which makes explicit the '\n",
      "                                       'intensity changes and local '\n",
      "                                       'two-dimensional geometry of an image, '\n",
      "                                       '(2) the 2 1/2-D sketch, which is a '\n",
      "                                       'viewer-centred representation of the '\n",
      "                                       'depth, orientation and discontinuities '\n",
      "                                       'of the visible surfaces, and (3) the '\n",
      "                                       '3-D model representation, which allows '\n",
      "                                       'an object-centred description of the '\n",
      "                                       'threedimensional structure and '\n",
      "                                       'organization of a viewed shape. The '\n",
      "                                       'critical act in formulating '\n",
      "                                       'computational theories for processes '\n",
      "                                       'capable of constructing these '\n",
      "                                       'representations is the discovery of '\n",
      "                                       'valid constraints on the way the world '\n",
      "                                       'behaves, that provide sufficient '\n",
      "                                       'additional information to allow '\n",
      "                                       'recovery of the desired '\n",
      "                                       'characteristic. Finally, once a '\n",
      "                                       'computational theory for a process has '\n",
      "                                       'been formulated, algorithms for '\n",
      "                                       'implementing it may be designed, and '\n",
      "                                       'their performance compared with that '\n",
      "                                       'of the human visual processor.'},\n",
      "              'score': 0.664827943,\n",
      "              'values': []},\n",
      "             {'id': '5390b2d720f70186a0eeced2',\n",
      "              'metadata': {'abstract': 'Using stereo vision in the field of '\n",
      "                                       'mapping and localization is an '\n",
      "                                       'intuitive idea, as demonstrated by the '\n",
      "                                       'number of animals that have developed '\n",
      "                                       'the ability. Though it seems logical '\n",
      "                                       'to use vision, the problem is a very '\n",
      "                                       'difficult one to solve. It requires '\n",
      "                                       'the ability to identify objects in the '\n",
      "                                       'field of view, and classify their '\n",
      "                                       'relationship to the observer. A '\n",
      "                                       'procedure for extracting and matching '\n",
      "                                       'object data using a stereo vision '\n",
      "                                       'system is introduced, and initial '\n",
      "                                       'results are provided to demonstrate '\n",
      "                                       'the potential of this system.'},\n",
      "              'score': 0.658404469,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda6f9',\n",
      "              'metadata': {'abstract': 'IN THIS PAPER WE CONSIDER SOME OF THE '\n",
      "                                       'PROBLEMS CONFRONTING THE DEVELOP- MENT '\n",
      "                                       'OF GENERAL INTEGRATED COMPUTER VISION '\n",
      "                                       'SYSTEMS, AND THE STATUS OF THE VISIONS '\n",
      "                                       'PROJECT WHICH HAS BECOME AN '\n",
      "                                       'EXPERIMENTAL TESTBED FOR THE CONSTRUC- '\n",
      "                                       'TION OF KNOWLEDGE-BASED IMAGE '\n",
      "                                       'INTERPRETATION SYSTEMS. THE GOAL IS '\n",
      "                                       'THE CONSTRUCTION OF A SYMBOLIC '\n",
      "                                       'REPRESENTATION OF THE '\n",
      "                                       'THREE-DIMENSIONAL WORLD DEPICTED IN A '\n",
      "                                       'TWO-DIMENSIONAL IMAGE, INCLUDING THE '\n",
      "                                       'LABELING OF OBJECTS, THE DETERMINATION '\n",
      "                                       'OF THEIR LOCATION IN SPACE, AND TO THE '\n",
      "                                       'DEGREE POSSIBLE THE CONSTRUCTION OF A '\n",
      "                                       'SURFACE REPRESENTATION OF THE '\n",
      "                                       'ENVIRONMENT. OUR SYSTEM INVOLVES THREE '\n",
      "                                       'LEVELS OF PROCESSING FOR STATIC IMAGE '\n",
      "                                       'INTER- PRETATION. LOW-LEVEL PROCESSES '\n",
      "                                       'MANIPULATE PIXEL DATA AND PRODUCE '\n",
      "                                       'INTERME- DIATE SYMBOLIC EVENTS SUCH AS '\n",
      "                                       'REGIONS AND LINES WITH THEIR '\n",
      "                                       'ATTRIBUTES. HIGH-LEVEL PROCESSES FOCUS '\n",
      "                                       'ATTENTION ON AGGREGATES OF THESE '\n",
      "                                       'EVENTS VIA RULE-BASED OBJECT '\n",
      "                                       'HYPOTHESES IN ORDER TO SELECTIVELY '\n",
      "                                       'INVOKE SCHEMAS, WHICH CONTAIN MORE '\n",
      "                                       'COMPLEX KNOWLEDGE-BASED INTERPRETATION '\n",
      "                                       'STRATEGIES. INTERMED- IATE-LEVEL '\n",
      "                                       'PROCESSES CARRY OUT GROUPING AND '\n",
      "                                       'REORGANIZATION OF THE ERROR- PRONE '\n",
      "                                       'SYMBOLIC REPRESENTATION EXTRACTED FROM '\n",
      "                                       'THE SENSORY DATA, UTILIZING BOTH '\n",
      "                                       '\"TOP-DOWN\" CONTROL OF THE PROCESSING '\n",
      "                                       'BY THE SCHEMA INTERPRETATION '\n",
      "                                       'STRATEGIES AS WELL AS \"BOTTOM-UP\" '\n",
      "                                       'DATA-DIRECTED ORGANIZATION OF '\n",
      "                                       'INTERESTING PERCEPTUAL EVENTS. OUR '\n",
      "                                       'DESIGN IS BEING EXTENDED TO INTEGRATE '\n",
      "                                       'THE RESULTS OF MOTION AND STEREO '\n",
      "                                       'PROCESSING THROUGHOUT THE THREE LEVELS '\n",
      "                                       'OF PROCESSING,'},\n",
      "              'score': 0.655618966,\n",
      "              'values': []},\n",
      "             {'id': '53908bad20f70186a0dc3545',\n",
      "              'metadata': {'abstract': 'Perception is best understood as the '\n",
      "                                       'interpretation of sensory data in '\n",
      "                                       'terms of models of how the world is '\n",
      "                                       'structured and how it behaves; these '\n",
      "                                       'models are exactly those that are most '\n",
      "                                       'useful for generation of computer '\n",
      "                                       'images. By recognizing and exploiting '\n",
      "                                       'this commonality we have been able to '\n",
      "                                       'make surprising progress in both '\n",
      "                                       'fields.'},\n",
      "              'score': 0.647426665,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e8482e',\n",
      "              'metadata': {'abstract': 'Depth reconstruction from the '\n",
      "                                       'two-dimensional image plays an '\n",
      "                                       'important role in certain visual tasks '\n",
      "                                       'and has been a major focus of computer '\n",
      "                                       'vision research. However, in this '\n",
      "                                       'paper we argue that most instances of '\n",
      "                                       'recognition in human and machine '\n",
      "                                       'vision can best be performed without '\n",
      "                                       'the preliminary reconstruction of '\n",
      "                                       'depth. Three other mechanisms are '\n",
      "                                       'described that can be used to bridge '\n",
      "                                       'the gap between the two-dimensional '\n",
      "                                       'image and knowledge of '\n",
      "                                       'three-dimensional objects. First, a '\n",
      "                                       'process of perceptual organization can '\n",
      "                                       'be used to form groupings and '\n",
      "                                       'structures in the image that are '\n",
      "                                       'likely to be invariant over a wide '\n",
      "                                       'range of viewpoints. Secondly, '\n",
      "                                       'evidential reasoning can be used to '\n",
      "                                       'combine evidence from these groupings '\n",
      "                                       'and other sources of information to '\n",
      "                                       'reduce the size of the search-space '\n",
      "                                       'during model-based matching. Finally, '\n",
      "                                       'a process of spatial correspondence '\n",
      "                                       'can be used to bring the projections '\n",
      "                                       'of three-dimensional models into '\n",
      "                                       'direct correspondence with the image '\n",
      "                                       'by solving for unknown viewpoint and '\n",
      "                                       'model parameters. These methods have '\n",
      "                                       'been combined in an experimental '\n",
      "                                       'computer vision system named SCERPO. '\n",
      "                                       'This system has demonstrated the use '\n",
      "                                       'of these methods for the recognition '\n",
      "                                       'of objects from unknown viewpoints in '\n",
      "                                       'single gray-scale images.'},\n",
      "              'score': 0.642689884,\n",
      "              'values': []},\n",
      "             {'id': '53909f2c20f70186a0e3719d',\n",
      "              'metadata': {'abstract': 'This thesis describes an effort to '\n",
      "                                       'construct a scene understanding system '\n",
      "                                       'that is able to analyze the content of '\n",
      "                                       'real images. While constructing the '\n",
      "                                       'system we had to provide solutions to '\n",
      "                                       'many of the fundamental questions that '\n",
      "                                       'every student of object recognition '\n",
      "                                       'deals with daily. These include the '\n",
      "                                       'choice of data set, the choice of '\n",
      "                                       'success measurement, the '\n",
      "                                       'representation of the image content, '\n",
      "                                       'the selection of inference engine, and '\n",
      "                                       'the representation of the relations '\n",
      "                                       'between objects. The main test-bed for '\n",
      "                                       'our system is the CBCL StreetScenes '\n",
      "                                       'data base. It is a carefully labeled '\n",
      "                                       'set of images, much larger than any '\n",
      "                                       'similar data set available at the time '\n",
      "                                       'it was collected. Each image in this '\n",
      "                                       'data set was labeled for 9 common '\n",
      "                                       'classes such as cars, pedestrians, '\n",
      "                                       'roads and trees. Our system represents '\n",
      "                                       'each image using a set of features '\n",
      "                                       'that are based on a model of the human '\n",
      "                                       'visual system constructed in our lab. '\n",
      "                                       'We demonstrate that this biologically '\n",
      "                                       'motivated image representation, along '\n",
      "                                       'with its extensions, constitutes an '\n",
      "                                       'effective representation for object '\n",
      "                                       'detection, facilitating unprecedented '\n",
      "                                       'levels of detection accuracy. '\n",
      "                                       'Similarly to biological vision '\n",
      "                                       'systems, our system uses hierarchical '\n",
      "                                       'representations. We therefore explore '\n",
      "                                       'the possible ways of combining '\n",
      "                                       'information across the hierarchy into '\n",
      "                                       'the final perception. Our system is '\n",
      "                                       'trained using standard machine '\n",
      "                                       'learning machinery, which was first '\n",
      "                                       'applied to computer vision in earlier '\n",
      "                                       'work of Prof. Poggio and others. We '\n",
      "                                       'demonstrate how the same standard '\n",
      "                                       'methods can be used to model relations '\n",
      "                                       'between objects in images as well, '\n",
      "                                       'capturing context information. The '\n",
      "                                       'resulting system detects and '\n",
      "                                       'localizes, using a unified set of '\n",
      "                                       'tools and image representations, '\n",
      "                                       'compact objects such as cars, '\n",
      "                                       'amorphous objects such as trees and '\n",
      "                                       'roads, and the relations between '\n",
      "                                       'objects within the scene. The same '\n",
      "                                       'representation also excels in '\n",
      "                                       'identifying objects in clutter without '\n",
      "                                       'scanning the image. Much of the work '\n",
      "                                       'presented in the thesis was devoted to '\n",
      "                                       'a rigorous comparison of our system to '\n",
      "                                       'alternative object recognition '\n",
      "                                       'systems. The results of these '\n",
      "                                       'experiments support the effectiveness '\n",
      "                                       'of simple feed-forward systems for the '\n",
      "                                       'basic tasks involved in scene '\n",
      "                                       'understanding. We make our results '\n",
      "                                       'fully available to the public by '\n",
      "                                       'publishing our code and data sets in '\n",
      "                                       'hope that others may improve and '\n",
      "                                       'extend our results. (Copies available '\n",
      "                                       'exclusively from MIT Libraries, Rm. '\n",
      "                                       '14-0551, Cambridge, MA 02139-4307. Ph. '\n",
      "                                       '617-253-5668; Fax 617-253-1690.)'},\n",
      "              'score': 0.636233807,\n",
      "              'values': []},\n",
      "             {'id': '53908b6c20f70186a0dbf5c7',\n",
      "              'metadata': {'abstract': 'The image represents an information '\n",
      "                                       'structure of an extreme complexity. We '\n",
      "                                       'present a method for symbolic and '\n",
      "                                       'structured description with several '\n",
      "                                       'levels of abstraction. The application '\n",
      "                                       'area concerns the construction and '\n",
      "                                       'interpretation knowledge on 3D objects '\n",
      "                                       'in a machine perception. The knowledge '\n",
      "                                       'representation and scenes '\n",
      "                                       'interpretation tasks based on 2D image '\n",
      "                                       'used \"Perceived Aspects Tables\" and '\n",
      "                                       '\"Quality Tables\" proposed by our '\n",
      "                                       'vision system. The necessary knowledge '\n",
      "                                       'to this task are formalised. Then, we '\n",
      "                                       'resolve the main interpretation '\n",
      "                                       'problem, the control and the '\n",
      "                                       'identification of objects, thanks to '\n",
      "                                       'the approach called: '\n",
      "                                       '\"Prediction-Checking of Hypotheses\". '\n",
      "                                       'The representation, which is '\n",
      "                                       'suggested, is based on the \"Frames\" '\n",
      "                                       'Model. With this end in view, an '\n",
      "                                       'organisation system of perception and '\n",
      "                                       'scenes interpretation tasks in order '\n",
      "                                       'to maintain a coherent representation '\n",
      "                                       'of a structured and evolutive '\n",
      "                                       'environment is designed. The released '\n",
      "                                       'concepts and the proposed system are '\n",
      "                                       'realised in a LE-LISP Object Oriented '\n",
      "                                       'environment based on the SHIRKA '\n",
      "                                       'knowledge representation system.'},\n",
      "              'score': 0.63575834,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e8537d',\n",
      "              'metadata': {'abstract': 'Understanding machine vision can '\n",
      "                                       'certainly improve our understanding of '\n",
      "                                       'artificial intelligence as vision '\n",
      "                                       'happens to be one of the basic '\n",
      "                                       'intellectual activities of living '\n",
      "                                       'beings. Since the notion of '\n",
      "                                       'computation unifies the concept of a '\n",
      "                                       'machine, computer vision can be '\n",
      "                                       'understood as an application of modern '\n",
      "                                       'approaches for achieving artificial '\n",
      "                                       'intelligence, like machine learning '\n",
      "                                       'and cognitive psychology. Computer '\n",
      "                                       'vision mainly involves processing of '\n",
      "                                       'different types of sensor data '\n",
      "                                       'resulting in ”perception of machines”. '\n",
      "                                       'Perception of machines plays a very '\n",
      "                                       'important role in several artificial '\n",
      "                                       'intelligence applications with '\n",
      "                                       'sensors. There are numerous practical '\n",
      "                                       'situations where we acquire sensor '\n",
      "                                       'data for e.g. from mobile robots, '\n",
      "                                       'security cameras, service and '\n",
      "                                       'recreational robots. Making sense of '\n",
      "                                       'this sensor data is very important so '\n",
      "                                       'that we have increased automation in '\n",
      "                                       'using the data. Tools from image '\n",
      "                                       'processing, shape analysis and '\n",
      "                                       'probabilistic inferences i.e. learning '\n",
      "                                       'theory form the artillery for current '\n",
      "                                       'generation of computer vision '\n",
      "                                       'researchers. In my thesis I will '\n",
      "                                       'address some of the most annoying '\n",
      "                                       'components of two important open '\n",
      "                                       'problems viz. object recognition and '\n",
      "                                       'autonomous navigation that remain '\n",
      "                                       'central in robotic, or in other words '\n",
      "                                       'computational, intelligence. These '\n",
      "                                       'problems are concerned with inducing '\n",
      "                                       'computers, the abilities to recognize '\n",
      "                                       'and navigate similar to those of '\n",
      "                                       'humans. Object boundaries are very '\n",
      "                                       'useful descriptors for recognizing '\n",
      "                                       'objects. Extracting boundaries from '\n",
      "                                       'real images has been a notoriously '\n",
      "                                       'open problem for several decades in '\n",
      "                                       'the vision community. In the first '\n",
      "                                       'part I will present novel techniques '\n",
      "                                       'for extracting object boundaries. The '\n",
      "                                       'techniques are based on practically '\n",
      "                                       'successful state-of-the-art Bayesian '\n",
      "                                       'filtering framework, well founded '\n",
      "                                       'geometric properties relating '\n",
      "                                       'boundaries and skeletons and robust '\n",
      "                                       'high-level shape analyses. Acquiring '\n",
      "                                       'global maps of the environments is '\n",
      "                                       'crucial for robots to localize and be '\n",
      "                                       'able to navigate autonomously. Though '\n",
      "                                       'there has been a lot of progress in '\n",
      "                                       'achieving autonomous mobility, for '\n",
      "                                       'e.g. as in DARPA grand-challenges of '\n",
      "                                       '2005 and 2007, the mapping problem '\n",
      "                                       'itself remains to be unsolved which is '\n",
      "                                       'essential for robust autonomy in hard '\n",
      "                                       'cases like rescue arenas and '\n",
      "                                       'collaborative exploration. In the '\n",
      "                                       'second part I will present techniques '\n",
      "                                       'for merging maps acquired by multiple '\n",
      "                                       'and single robots. We developed '\n",
      "                                       'physics-based energy minimization '\n",
      "                                       'techniques and also shape based '\n",
      "                                       'techniques for scalable merging of '\n",
      "                                       'maps. Our shape based techniques are a '\n",
      "                                       'product of combining of high-level '\n",
      "                                       'vision techniques that exploit '\n",
      "                                       'similarities among maps and strong '\n",
      "                                       'statistical methods that can handle '\n",
      "                                       'uncertainties in Bayesian sense.'},\n",
      "              'score': 0.633365571,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e846a1',\n",
      "              'metadata': {'abstract': 'Among the sources of information that '\n",
      "                                       'can be used to guide the processing of '\n",
      "                                       'visual sensory data are the '\n",
      "                                       'constraints implied by the geometry of '\n",
      "                                       'the objects being viewed. Experiments '\n",
      "                                       'have been performed using '\n",
      "                                       'representations of this type of '\n",
      "                                       'knowledge to control image processing. '\n",
      "                                       'They show how such geometric knowledge '\n",
      "                                       'can be used to aid in the '\n",
      "                                       'identification of object projections '\n",
      "                                       'in images of natural outdoor scenes.'},\n",
      "              'score': 0.631801665,\n",
      "              'values': []},\n",
      "             {'id': '5390b20120f70186a0ee5e5f',\n",
      "              'metadata': {'abstract': 'One of the grand challenges of '\n",
      "                                       'artificial intelligence is to enable '\n",
      "                                       'computers to interpret 3D scenes and '\n",
      "                                       'objects from imagery. This book '\n",
      "                                       'organizes and introduces major '\n",
      "                                       'concepts in 3D scene and object '\n",
      "                                       'representation and inference from '\n",
      "                                       'still images, with a focus on recent '\n",
      "                                       'efforts to fuse models of geometry and '\n",
      "                                       'perspective with statistical machine '\n",
      "                                       'learning. The book is organized into '\n",
      "                                       'three sections: (1) Interpretation of '\n",
      "                                       'Physical Space; (2) Recognition of 3D '\n",
      "                                       'Objects; and (3) Integrated 3D Scene '\n",
      "                                       'Interpretation. The first discusses '\n",
      "                                       'representations of spatial layout and '\n",
      "                                       'techniques to interpret physical '\n",
      "                                       'scenes from images. The second section '\n",
      "                                       'introduces representations for 3D '\n",
      "                                       'object categories that account for the '\n",
      "                                       'intrinsically 3D nature of objects and '\n",
      "                                       'provide robustness to change in '\n",
      "                                       'viewpoints. The third section '\n",
      "                                       'discusses strategies to unite '\n",
      "                                       'inference of scene geometry and object '\n",
      "                                       'pose and identity into a coherent '\n",
      "                                       'scene interpretation. Each section '\n",
      "                                       'broadly surveys important ideas from '\n",
      "                                       'cognitive science and artificial '\n",
      "                                       'intelligence research, organizes and '\n",
      "                                       'discusses key concepts and techniques '\n",
      "                                       'from recent work in computer vision, '\n",
      "                                       'and describes a few sample approaches '\n",
      "                                       'in detail. Newcomers to computer '\n",
      "                                       'vision will benefit from introductions '\n",
      "                                       'to basic concepts, such as single-view '\n",
      "                                       'geometry and image classification, '\n",
      "                                       'while experts and novices alike may '\n",
      "                                       \"find inspiration from the book's \"\n",
      "                                       'organization and discussion of the '\n",
      "                                       'most recent ideas in 3D scene '\n",
      "                                       'understanding and 3D object '\n",
      "                                       'recognition. Specific topics include: '\n",
      "                                       'mathematics of perspective geometry; '\n",
      "                                       'visual elements of the physical scene, '\n",
      "                                       'structural 3D scene representations; '\n",
      "                                       'techniques and features for image and '\n",
      "                                       'region categorization; historical '\n",
      "                                       'perspective, computational models, and '\n",
      "                                       'datasets and machine learning '\n",
      "                                       'techniques for 3D object recognition; '\n",
      "                                       'inferences of geometrical attributes '\n",
      "                                       'of objects, such as size and pose; and '\n",
      "                                       'probabilistic and feature-passing '\n",
      "                                       'approaches for contextual reasoning '\n",
      "                                       'about 3D objects and scenes. Table of '\n",
      "                                       'Contents: Background on 3D Scene '\n",
      "                                       'Models / Single-view Geometry / '\n",
      "                                       'Modeling the Physical Scene / '\n",
      "                                       'Categorizing Images and Regions / '\n",
      "                                       'Examples of 3D Scene Interpretation / '\n",
      "                                       'Background on 3D Recognition / '\n",
      "                                       'Modeling 3D Objects / Recognizing and '\n",
      "                                       'Understanding 3D Objects / Examples of '\n",
      "                                       '2D 1/2 Layout Models / Reasoning about '\n",
      "                                       'Objects and Scenes / Cascades of '\n",
      "                                       'Classifiers / Conclusion and Future '\n",
      "                                       'Directions'},\n",
      "              'score': 0.625960231,\n",
      "              'values': []},\n",
      "             {'id': '53908bfb20f70186a0dcb3bf',\n",
      "              'metadata': {'abstract': 'The symbolic level of a dynamic scene '\n",
      "                                       'interpretation system is presented. '\n",
      "                                       'This symbolic level is based on plan '\n",
      "                                       'prototypes represented by Petri nets '\n",
      "                                       'whose interpretation is expressed '\n",
      "                                       'thanks to 1st order constrained cubes, '\n",
      "                                       'and on a reasoning aiming at '\n",
      "                                       'instantiating the plan prototypes with '\n",
      "                                       'objects delivered by the numerical '\n",
      "                                       'processing of sensor data. An example '\n",
      "                                       'on real world data is given.'},\n",
      "              'score': 0.624751329,\n",
      "              'values': []},\n",
      "             {'id': '539089d220f70186a0d9ad54',\n",
      "              'metadata': {'abstract': 'It is generally agreed that individual '\n",
      "                                       'visual cues are fallible and often '\n",
      "                                       'ambiguous. This has generated a lot of '\n",
      "                                       'interest in design of integrated '\n",
      "                                       'vision systems which are expected to '\n",
      "                                       'give a reliable performance in '\n",
      "                                       'practical situations. The design of '\n",
      "                                       'such systems is challenging since each '\n",
      "                                       'vision module works under a different '\n",
      "                                       'and possibly conflicting set of '\n",
      "                                       'assumptions. We have proposed and '\n",
      "                                       'implemented a multiresolution system '\n",
      "                                       'which integrates perceptual '\n",
      "                                       'organization (grouping), segmentation, '\n",
      "                                       'stereo, shape from shading, and line '\n",
      "                                       'labeling modules. We demonstrate the '\n",
      "                                       'efficacy of our approach using images '\n",
      "                                       'of several different realistic scenes. '\n",
      "                                       'The output of the integrated system is '\n",
      "                                       'shown to be insensitive to the '\n",
      "                                       'constraints imposed by the individual '\n",
      "                                       'modules. The numerical accuracy of the '\n",
      "                                       'recovered depth is assessed in case of '\n",
      "                                       'synthetically generated data. Finally, '\n",
      "                                       'we have qualitatively evaluated our '\n",
      "                                       'approach by reconstructing geons from '\n",
      "                                       'the depth data obtained from the '\n",
      "                                       'integrated system. These results '\n",
      "                                       'indicate that integrated vision '\n",
      "                                       'systems are likely to produce better '\n",
      "                                       'reconstruction of the input scene than '\n",
      "                                       'the individual modules.'},\n",
      "              'score': 0.620958686,\n",
      "              'values': []},\n",
      "             {'id': '5390881820f70186a0d8204b',\n",
      "              'metadata': {'abstract': 'A framework for high-level '\n",
      "                                       'representations in computer '\n",
      "                                       'visionarchitectures is described.The '\n",
      "                                       'framework is based on the notion of '\n",
      "                                       'conceptual space.This approach allows '\n",
      "                                       'us to define a conceptual semantics '\n",
      "                                       'for the symbolic representations of '\n",
      "                                       'the vision system. In this way, the '\n",
      "                                       'semantics of the symbols can be '\n",
      "                                       'grounded to the data coming fromthe '\n",
      "                                       'sensors. In addition, the proposed '\n",
      "                                       'approach generalizesthe most popular '\n",
      "                                       'frameworks adopted in computer '\n",
      "                                       'vision.'},\n",
      "              'score': 0.620689273,\n",
      "              'values': []},\n",
      "             {'id': '53909f2c20f70186a0e3707c',\n",
      "              'metadata': {'abstract': 'The goal of computer vision is to use '\n",
      "                                       'an image to recover the '\n",
      "                                       'characteristics of scene, such as its '\n",
      "                                       'shape or illumination. This is '\n",
      "                                       'difficult because an image is the '\n",
      "                                       'mixture of multiple characteristics. '\n",
      "                                       'For example, an edge in an image could '\n",
      "                                       'be caused by either an edge on a '\n",
      "                                       \"surface or a change in the surface's \"\n",
      "                                       'color. Distinguishing the effects of '\n",
      "                                       'different scene characteristics is an '\n",
      "                                       'important step towards high-level '\n",
      "                                       'analysis of an image. This thesis '\n",
      "                                       'describes how to use machine learning '\n",
      "                                       'to build a system that recovers '\n",
      "                                       'different characteristics of the scene '\n",
      "                                       'from a single, gray-scale image of the '\n",
      "                                       'scene. The goal of the system is to '\n",
      "                                       'use, the observed image to recover '\n",
      "                                       'images, referred to as Intrinsic '\n",
      "                                       'Component Images, that represent the '\n",
      "                                       \"scene's characteristics. The \"\n",
      "                                       'development of the system is focused '\n",
      "                                       'on estimating two important '\n",
      "                                       'characteristics of a scene, its '\n",
      "                                       'shading and reflectance, from a single '\n",
      "                                       'image. From the observed image, the '\n",
      "                                       'system estimates a shading image, '\n",
      "                                       'which captures the interaction of the '\n",
      "                                       'illumination and shape of the scene '\n",
      "                                       'pictured, and an albedo image, which '\n",
      "                                       'represents ow the surfaces in the '\n",
      "                                       'image reflect light. Measured both '\n",
      "                                       'qualitatively and quantitatively, this '\n",
      "                                       'system produces state-of-the-art '\n",
      "                                       'estimates of shading and albedo '\n",
      "                                       'images. This system is also flexible '\n",
      "                                       'enough to be used for the separate '\n",
      "                                       'problem of removing noise from an '\n",
      "                                       'image. Building this system requires '\n",
      "                                       'algorithms for continuous regression '\n",
      "                                       'and learning the parameters of a '\n",
      "                                       'Conditionally Gaussian Markov Random '\n",
      "                                       'Field. Unlike previous work, this '\n",
      "                                       'system is trained using real-world '\n",
      "                                       'surfaces with ground-truth shading and '\n",
      "                                       'albedo images. The learning algorithms '\n",
      "                                       'are designed to accommodate the large '\n",
      "                                       'amount of data in this training set. '\n",
      "                                       '(Copies available exclusively from MIT '\n",
      "                                       'Libraries, Rm. 14-0551, Cambridge, MA '\n",
      "                                       '02139-4307. Ph. 617-253-5668; Fax '\n",
      "                                       '617-253-1690.)'},\n",
      "              'score': 0.619793952,\n",
      "              'values': []},\n",
      "             {'id': '5390981d20f70186a0e04411',\n",
      "              'metadata': {'abstract': 'Ongoing research at Boston University '\n",
      "                                       'has produced computational models of '\n",
      "                                       'biological vision and learning that '\n",
      "                                       'embody a growing corpus of scientific '\n",
      "                                       'data and predictions. Vision models '\n",
      "                                       'perform long-range grouping and '\n",
      "                                       'figure/ground segmentation, and memory '\n",
      "                                       'models create attentionally controlled '\n",
      "                                       'recognition codes that intrinsically '\n",
      "                                       'combine bottom-up activation and '\n",
      "                                       'top-down learned expectations. These '\n",
      "                                       'two streams of research form the '\n",
      "                                       'foundation of novel dynamically '\n",
      "                                       'integrated systems for image '\n",
      "                                       'understanding. Simulations using '\n",
      "                                       'multispectral images illustrate road '\n",
      "                                       'completion across occlusions in a '\n",
      "                                       'cluttered scene and information fusion '\n",
      "                                       'from input labels that are '\n",
      "                                       'simultaneously inconsistent and '\n",
      "                                       'correct. The CNS Vision and Technology '\n",
      "                                       'Labs (cns.bu.edu/visionlab and '\n",
      "                                       'cns.bu.edu/techlab) are further '\n",
      "                                       'integrating science and technology '\n",
      "                                       'through analysis, testing, and '\n",
      "                                       'development of cognitive and neural '\n",
      "                                       'models for large-scale applications, '\n",
      "                                       'complemented by software specification '\n",
      "                                       'and code distribution.'},\n",
      "              'score': 0.615413725,\n",
      "              'values': []},\n",
      "             {'id': '53908e0020f70186a0dd63ba',\n",
      "              'metadata': {'abstract': 'The problems under consideration '\n",
      "                                       'center around the interpretation of '\n",
      "                                       'binocular stereo disparity. In '\n",
      "                                       'particular, the goal is to establish a '\n",
      "                                       'set of mappings from stereo disparity '\n",
      "                                       'to corresponding three-dimensional '\n",
      "                                       'scene geometry. An analysis has been '\n",
      "                                       'developed that shows how disparity '\n",
      "                                       'information can be interpreted in '\n",
      "                                       'terms of three-dimensional scene '\n",
      "                                       'properties, such as surface depth, '\n",
      "                                       'discontinuities, and orientation. '\n",
      "                                       'These theoretical developments have '\n",
      "                                       'been embodied in a set of computer '\n",
      "                                       'algorithms for the recovery of scene '\n",
      "                                       'geometry from input stereo disparity. '\n",
      "                                       'The results of applying these '\n",
      "                                       'algorithms to several disparity maps '\n",
      "                                       'are presented. Comparisons are made to '\n",
      "                                       'the interpretation of stereo disparity '\n",
      "                                       'by biological systems.'},\n",
      "              'score': 0.611579359,\n",
      "              'values': []},\n",
      "             {'id': '53908b9320f70186a0dc0050',\n",
      "              'metadata': {'abstract': 'This paper describes a representation '\n",
      "                                       'for people and animals, called a body '\n",
      "                                       'plan, which is adapted to segmentation '\n",
      "                                       'and to recognition in complex '\n",
      "                                       'environments. The representation is an '\n",
      "                                       'organized collection of grouping hints '\n",
      "                                       'obtained from a combination of '\n",
      "                                       'constraints on color and texture and '\n",
      "                                       'constraints on geometric properties '\n",
      "                                       'such as the structure of individual '\n",
      "                                       'parts and the relationships between '\n",
      "                                       'parts. Body plans can be learned from '\n",
      "                                       'image data, using established '\n",
      "                                       'statistical learning techniques. The '\n",
      "                                       'approach is illustrated with two '\n",
      "                                       'examples of programs that successfully '\n",
      "                                       'use body plans for recognition: one '\n",
      "                                       'example involves determining whether a '\n",
      "                                       'picture contains a scantily clad '\n",
      "                                       'human, using a body plan built by '\n",
      "                                       'hand; the other involves determining '\n",
      "                                       'whether a picture contains a horse, '\n",
      "                                       'using a body plan learned from image '\n",
      "                                       'data. In both cases, the system '\n",
      "                                       'demonstrates excellent performance on '\n",
      "                                       'large, uncontrolled test sets and very '\n",
      "                                       'large and diverse control sets.'},\n",
      "              'score': 0.611554921,\n",
      "              'values': []},\n",
      "             {'id': '53909a0320f70186a0e20b09',\n",
      "              'metadata': {'abstract': 'Understanding objects in an image is a '\n",
      "                                       'current problem in computer vision. '\n",
      "                                       'This paper propose an image '\n",
      "                                       'understanding system that recognizes '\n",
      "                                       'complex objects based on geometric '\n",
      "                                       'shapes and color. This system is based '\n",
      "                                       'on a blackboard architecture that uses '\n",
      "                                       'image processing algorithms as '\n",
      "                                       'Knowledge Sources (KSs) to extract '\n",
      "                                       'features in the image, and then infer '\n",
      "                                       'the presence of certain objects based '\n",
      "                                       'on the these features. The management '\n",
      "                                       'of these KSs will be driven by a '\n",
      "                                       'reasoning system like a belief network '\n",
      "                                       'or a rule-based system.'},\n",
      "              'score': 0.610388935,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda5f2',\n",
      "              'metadata': {'abstract': 'IMAGE INTERPRETATION IS A COMPLEX '\n",
      "                                       'PROCESS THROUGH WHICH A NUMERIC ARRAY, '\n",
      "                                       'REPRESENTING A DIGITIZED VISUAL SCENE, '\n",
      "                                       'CAN BE ANALYZED TO PROVIDE A SEMAN- '\n",
      "                                       'TIC DESCRIPTION OF THE SCENE CONTENT. '\n",
      "                                       'ONE SUBGOAL OF THE INTERPRETATION '\n",
      "                                       'PROCESS IS IMAGE SEGMENTATION, THE '\n",
      "                                       'LOW-LEVEL PROCESS BY WHICH THE '\n",
      "                                       'DIGITIZED IMAGE IS ABSTRACTED INTO A '\n",
      "                                       'SET OF PRIMITIVE ELEMENTS THAT MAY BE '\n",
      "                                       'USED AS THE BASIS FOR THE CONSTRUCTION '\n",
      "                                       'OF AN ABSTRACT SYMBOLIC MODEL OF THE '\n",
      "                                       'ORIGIN- AL SCENE. A COMMONLY ACCEPTED '\n",
      "                                       'VIEW IS THAT IMAGE SEGMENTATION IS '\n",
      "                                       'SIMPLY THE FIRST STAGE OF THE '\n",
      "                                       'INTERPRETATION PROCESS. WE TAKE THE '\n",
      "                                       'VIEW, HOWEVER, THAT SEGMENTATION IS A '\n",
      "                                       'PROCESS WHICH DOES NOT EXIST IN '\n",
      "                                       'ISOLATION, BUT RATHER AS AN INTEGRAL '\n",
      "                                       'PART OF THE OVERALL IMAGE '\n",
      "                                       'INTERPRETATION PROCESS. THIS '\n",
      "                                       \"DISSERTATION PRESENTS `GOLDIE'',AN \"\n",
      "                                       'INTERMEDIATE-LEVEL, GOAL-DIRECTED '\n",
      "                                       'SYSTEM THAT HAS BEEN DEVELOPED WITHIN '\n",
      "                                       'THE VISIONS SYSTEM TO ACCOMPLISH THE '\n",
      "                                       'INTEGRATION OF THE HIGH AND LOW LEVELS '\n",
      "                                       'OF THE IMAGE UNDERSTANDING PROCESS. '\n",
      "                                       'GOLDIE IS A SYSTEM DESIGNED TO OPERATE '\n",
      "                                       'IN EITHER A DATA-DRIVEN OR INTERPRE- '\n",
      "                                       'TATION-DRIVEN MANNER. IN THE '\n",
      "                                       'DATA-DRIVEN MODE, GOLDIE FUNCTIONS AS '\n",
      "                                       'A SEG- MENTATION SYSTEM THAT IS ABLE '\n",
      "                                       'TO CHOOSE BETWEEN A VARIETY OF '\n",
      "                                       'ALGORITHMS AND IMAGE FEATURES BASED ON '\n",
      "                                       'THE CHARACTERISTICS OF THE IMAGE DATA. '\n",
      "                                       'IN THE IN- TERPRETATION-DRIVEN MODE, '\n",
      "                                       'THE SYSTEM PROVIDES THE MECHANISM BY '\n",
      "                                       'WHICH HIGH- LEVEL REQUESTS FOR DATA '\n",
      "                                       '(GOALS) CAN ACTIVATE A SET OF '\n",
      "                                       'APPROPRIATE LOW OR INTERMEDIATE-LEVEL '\n",
      "                                       'PROCESSES THAT MAY BE CAPABLE OF '\n",
      "                                       'PRODUCING THE DESIRED'},\n",
      "              'score': 0.608045876,\n",
      "              'values': []},\n",
      "             {'id': '539087a120f70186a0d475d7',\n",
      "              'metadata': {'abstract': 'Results from an ongoing project '\n",
      "                                       'concerned with recognizing objects in '\n",
      "                                       'complex scene domains, especially in '\n",
      "                                       'the domain that includes the natural '\n",
      "                                       'outdoor world, are described. '\n",
      "                                       'Traditional machine recognition '\n",
      "                                       'paradigms assume either that all '\n",
      "                                       'objects of interest are definable by a '\n",
      "                                       'relatively small number of explicit '\n",
      "                                       'shape models or that all objects of '\n",
      "                                       'interest have characteristic, locally '\n",
      "                                       'measurable features. The failure of '\n",
      "                                       'both assumptions has a dramatic impact '\n",
      "                                       'on the form of an acceptable '\n",
      "                                       'architecture for an object recognition '\n",
      "                                       'system. In this work, the use of the '\n",
      "                                       'contextual information is a central '\n",
      "                                       'issue, and a system is explicitly '\n",
      "                                       'designed to identify and use context '\n",
      "                                       'as an integral part of recognition '\n",
      "                                       'that eliminates the traditional '\n",
      "                                       'dependence on stored geometric models '\n",
      "                                       'and universal image partitioning '\n",
      "                                       'algorithms. This paradigm combines the '\n",
      "                                       'results of many simple procedures that '\n",
      "                                       'analyze monochrome, color, stereo, or '\n",
      "                                       '3D range images. Interpreting the '\n",
      "                                       'results along with relevant contextual '\n",
      "                                       'knowledge makes it possible to achieve '\n",
      "                                       'a reliable recognition result, even '\n",
      "                                       'when using imperfect visual '\n",
      "                                       'procedures. Initial experimentation '\n",
      "                                       'with the system on ground-level '\n",
      "                                       'outdoor imagery has demonstrated '\n",
      "                                       'competence beyond what is attainable '\n",
      "                                       'with other vision systems.'},\n",
      "              'score': 0.606724441,\n",
      "              'values': []},\n",
      "             {'id': '5390a37f20f70186a0e6c6f8',\n",
      "              'metadata': {'abstract': 'Based on the neurobiological and '\n",
      "                                       'cognitive principles of human '\n",
      "                                       'information processing, we develop a '\n",
      "                                       'system for the automatic visual '\n",
      "                                       'identification and exploration of '\n",
      "                                       'scenes. The system architecture '\n",
      "                                       'consists of three layers: a bottom-up '\n",
      "                                       'feature extraction stage, a top-down '\n",
      "                                       'object identification stage and '\n",
      "                                       'knowledge from a domain ontology for '\n",
      "                                       'scene analysis. The uncertainty in the '\n",
      "                                       'latter two stages is managed by '\n",
      "                                       'Dempster-Shafer belief measures. The '\n",
      "                                       'system sequentially selects '\n",
      "                                       \"''informative'' image regions, \"\n",
      "                                       'identifies the local structure in '\n",
      "                                       'these regions, and uses this '\n",
      "                                       'information for drawing efficient '\n",
      "                                       'conclusions about an object in the '\n",
      "                                       'scene. The selection process involves '\n",
      "                                       'low-level, bottom-up processes for '\n",
      "                                       'sensory feature extraction, and '\n",
      "                                       'cognitive top-down processes for the '\n",
      "                                       'generation of active motor commands '\n",
      "                                       'that control the positioning of the '\n",
      "                                       'sensors towards the most informative '\n",
      "                                       'regions. Both processing levels have '\n",
      "                                       'to deal with uncertain data, and have '\n",
      "                                       'to take into account learned '\n",
      "                                       'statistical knowledge. For bottom-up '\n",
      "                                       'feature extraction this is achieved by '\n",
      "                                       'integrating a nonlinear filtering '\n",
      "                                       'stage modeled after the neural '\n",
      "                                       'computations performed in the early '\n",
      "                                       'stages of the visual system. The '\n",
      "                                       'top-down cognitive reasoning strategy '\n",
      "                                       'operates in an adaptive fashion on a '\n",
      "                                       'belief distribution. The resulting '\n",
      "                                       'object hypotheses in combination with '\n",
      "                                       'knowledge from the domain ontology in '\n",
      "                                       'the third layer are used for '\n",
      "                                       'generating a scene hypothesis.'},\n",
      "              'score': 0.606442451,\n",
      "              'values': []},\n",
      "             {'id': '5390985d20f70186a0e088fb',\n",
      "              'metadata': {'abstract': 'Computer vision is the process of '\n",
      "                                       'using computers to extract from images '\n",
      "                                       'useful information about the physical '\n",
      "                                       'world, including meaningful '\n",
      "                                       'descriptions of physical objects. For '\n",
      "                                       'example, if an image sensor, such as a '\n",
      "                                       'digitizing video camera, captured an '\n",
      "                                       'image of a physical scene, and the '\n",
      "                                       'digital image was input to a computer '\n",
      "                                       'vision system, the desired output '\n",
      "                                       'would be a description of the physical '\n",
      "                                       'scene in terms that would be useful '\n",
      "                                       'for the particular task at hand. '\n",
      "                                       'Computer vision has many applications, '\n",
      "                                       'including robotics, industrial '\n",
      "                                       'automation, document processing, '\n",
      "                                       'remote sensing, navigation, '\n",
      "                                       'microscopy, medical imaging, and the '\n",
      "                                       'development of visual prostheses for '\n",
      "                                       'the blind.'},\n",
      "              'score': 0.605695367,\n",
      "              'values': []},\n",
      "             {'id': '5390877920f70186a0d2d883',\n",
      "              'metadata': {'abstract': 'Superimposition of two image data sets '\n",
      "                                       'allows the spatial distribution of one '\n",
      "                                       'to be directly related to that of the '\n",
      "                                       'other. If the two data sets have '\n",
      "                                       'different spatial structures, the '\n",
      "                                       'composite image is generally confusing '\n",
      "                                       'and difficult to interpret. A method '\n",
      "                                       'of representing image data sets in the '\n",
      "                                       'form of naturally occurring variables '\n",
      "                                       'in a realistic apparently '\n",
      "                                       'three-dimensional scene is presented. '\n",
      "                                       'One data set is represented by the '\n",
      "                                       'topography of a surface, depicted by '\n",
      "                                       'shaded-relief methods, while another '\n",
      "                                       'is represented by the color of the '\n",
      "                                       'surface, or by the color of an '\n",
      "                                       'overlaid transparency. Presentation in '\n",
      "                                       'this form exploits the normal scene '\n",
      "                                       'decomposition abilities of the human '\n",
      "                                       'visual system, allowing intuitive '\n",
      "                                       'appreciation and separation of the '\n",
      "                                       'scene, and hence data set, variables. '\n",
      "                                       'The method relies on techniques for '\n",
      "                                       'the modeling of surfaces and surface '\n",
      "                                       'reflectance to render the synthesised '\n",
      "                                       'scenes realistically.'},\n",
      "              'score': 0.605682552,\n",
      "              'values': []},\n",
      "             {'id': '5390958920f70186a0dee255',\n",
      "              'metadata': {'abstract': 'Scientists and experts have explored '\n",
      "                                       'the mechanism of visual systems for '\n",
      "                                       'decades for smart image processing and '\n",
      "                                       'pattern recognition in order to '\n",
      "                                       'satisfy sophisticated engineering '\n",
      "                                       'applications. In this paper we apply '\n",
      "                                       \"independent component analyses' (ICA) \"\n",
      "                                       'unsupervised learning to natural '\n",
      "                                       'images, topography images and other '\n",
      "                                       'special environment images for '\n",
      "                                       \"demonstrating the simple-cell's \"\n",
      "                                       'process in animal vision. Our results '\n",
      "                                       'confirm an early biological experiment '\n",
      "                                       '(Nature 228 (1970) 419) about the '\n",
      "                                       \"growth of simple cells in cat's V1 \"\n",
      "                                       'area. Furthermore, by applying ICA '\n",
      "                                       'methodology and the simplex algorithm, '\n",
      "                                       \"the unsupervised neural synapse's \"\n",
      "                                       'learning can obtain the receptive '\n",
      "                                       'fields in visual cortex and can '\n",
      "                                       'simulate the growth of the visual '\n",
      "                                       'cortex of young animal in the special '\n",
      "                                       'environment. These findings imply that '\n",
      "                                       'an input image can be efficiently '\n",
      "                                       'represented by ICA bases. An '\n",
      "                                       'application of image matching in the '\n",
      "                                       'navigation by ICA is shown that the '\n",
      "                                       'animal visual system method is indeed '\n",
      "                                       'better than those classical methods at '\n",
      "                                       'least more than 5%.'},\n",
      "              'score': 0.605640292,\n",
      "              'values': []},\n",
      "             {'id': '53908a7420f70186a0da2e98',\n",
      "              'metadata': {'abstract': 'The task of visual classification is '\n",
      "                                       'thce recognition of an object in the '\n",
      "                                       'image as belonging to a general class '\n",
      "                                       'of similar objects, such as a face, a '\n",
      "                                       'car, a dog, and the like. This is a '\n",
      "                                       'fundamental and natural task for '\n",
      "                                       'biological visual systems, but it has '\n",
      "                                       'proven difficult to perform visual '\n",
      "                                       'classification by artificial computer '\n",
      "                                       'vision systems. The main reason for '\n",
      "                                       'this difficulty is the variability of '\n",
      "                                       'shape within a class: different '\n",
      "                                       'objects vary widely in appearance, and '\n",
      "                                       'it is difficult to capture the '\n",
      "                                       'essential shape features that '\n",
      "                                       'characterize the members of one '\n",
      "                                       'category and distinguish them from '\n",
      "                                       'another, such as dogs from cats. In '\n",
      "                                       'this paper we describe an approach to '\n",
      "                                       'classification using a fragment-based '\n",
      "                                       'representation. In this approach, '\n",
      "                                       'objects within a class are represented '\n",
      "                                       'in terms of common image fragments '\n",
      "                                       'that are used as building blocks for '\n",
      "                                       'representing a large variety of '\n",
      "                                       'different objects that belong to a '\n",
      "                                       'common class. The fragments are '\n",
      "                                       'selected from a training set of images '\n",
      "                                       'based on a criterion of maximizing the '\n",
      "                                       'mutual information of the fragments '\n",
      "                                       'and the class they represent. For the '\n",
      "                                       'purpose of classification the '\n",
      "                                       'fragments are also organized into '\n",
      "                                       'types, where each type is a collection '\n",
      "                                       'of alternative fragments, such as '\n",
      "                                       'different hairline or eye regions for '\n",
      "                                       'face classification. During '\n",
      "                                       'classification, the algorithm detects '\n",
      "                                       'fragments of the different types, and '\n",
      "                                       'then combines the evidence for the '\n",
      "                                       'detected fragments to reach a final '\n",
      "                                       'decision. Experiments indicate that it '\n",
      "                                       'is possible to trade off the '\n",
      "                                       'complexity of fragments with the '\n",
      "                                       'complexity of the combination and '\n",
      "                                       'decision stage, and this tradeoff is '\n",
      "                                       'discussed. The method is different '\n",
      "                                       'from previous part-based methods in '\n",
      "                                       'using class-specific object fragments '\n",
      "                                       'of varying complexity, the method of '\n",
      "                                       'selecting fragments, and the '\n",
      "                                       'organization into fragment types. '\n",
      "                                       'Experimental results of detecting face '\n",
      "                                       'and car views show that the '\n",
      "                                       'fragment-based approach can generalize '\n",
      "                                       'well to a variety of novel image views '\n",
      "                                       'within a class while maintaining low '\n",
      "                                       'mis-classification error rates. We '\n",
      "                                       'briefly discuss relationships between '\n",
      "                                       'the proposed method and properties of '\n",
      "                                       'parts of the primate visual system '\n",
      "                                       'involved in object perception'},\n",
      "              'score': 0.605498493,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dd9493',\n",
      "              'metadata': {'abstract': 'It is too hard to tell vision systems '\n",
      "                                       'what things look like. It is easier to '\n",
      "                                       'talk about purpose and what things are '\n",
      "                                       'for. Consequently, we want vision '\n",
      "                                       'systems to use functional descriptions '\n",
      "                                       'to identify things, when necessary, '\n",
      "                                       'and we want them to learn physical '\n",
      "                                       'descriptions for themselves, when '\n",
      "                                       'possible. This paper describes a '\n",
      "                                       'theory that explains how to make such '\n",
      "                                       'systems work. The theory is a '\n",
      "                                       'synthesis of two sets of ideas: ideas '\n",
      "                                       'about learning from precedents and '\n",
      "                                       'exercises developed at MIT and ideas '\n",
      "                                       'about physical description developed '\n",
      "                                       'at Stanford. The strength of the '\n",
      "                                       'synthesis is illustrated by way of '\n",
      "                                       'representative experiments. All of '\n",
      "                                       'these experiments have been performed '\n",
      "                                       'with an implementation system.'},\n",
      "              'score': 0.605486572,\n",
      "              'values': []},\n",
      "             {'id': '5390a93b20f70186a0ea02a3',\n",
      "              'metadata': {'abstract': 'Image parsing continues to be a '\n",
      "                                       'challenging research task in the field '\n",
      "                                       'of computer vision. In this '\n",
      "                                       'dissertation, we have developed a '\n",
      "                                       'hybrid image parser which accounts for '\n",
      "                                       'different vision-related phenomena (i) '\n",
      "                                       'the perception of objects, their parts '\n",
      "                                       'and the relationships between them; '\n",
      "                                       '(ii) the use of semantic, spoken '\n",
      "                                       'language to describe attributes of '\n",
      "                                       'objects in images to be parsed and a '\n",
      "                                       'heterogeneous computational model for '\n",
      "                                       'object/part recognition; (iii) an '\n",
      "                                       'image segmentation process which uses '\n",
      "                                       'multiple visual cues and (iv) an '\n",
      "                                       'optimization technique which reduces '\n",
      "                                       'the solution space for scene '\n",
      "                                       'identification. The parser is built on '\n",
      "                                       'an image grammar-based framework. '\n",
      "                                       'Because the patterns to be analyzed '\n",
      "                                       'are often multifarious, with one '\n",
      "                                       'element having numerous diverse parts '\n",
      "                                       'to it, we have developed a general '\n",
      "                                       'symbol-based ontology paradigm that '\n",
      "                                       'describes complex image patterns in '\n",
      "                                       'terms of a hierarchical composition of '\n",
      "                                       'simpler subpatterns. The relationships '\n",
      "                                       'between objects, and with their parts, '\n",
      "                                       'are presented using first order formal '\n",
      "                                       'logic. In order to perform rapid scene '\n",
      "                                       'parsing and identification, the input '\n",
      "                                       'image is over-segmented to yield '\n",
      "                                       '\"superpixels\", which are a locally, '\n",
      "                                       'coherent grouping of pixels that '\n",
      "                                       'preserve the structure necessary for '\n",
      "                                       'image parsing. Their usage greatly '\n",
      "                                       'reduces the computational complexity '\n",
      "                                       'of the parser.In the thesis, we also '\n",
      "                                       'present an algorithm for performing a '\n",
      "                                       '(near) global Markov Random Field '\n",
      "                                       '(MRF) optimization for labeling '\n",
      "                                       'segmented images. Our previous results '\n",
      "                                       'of labeling images using the pairwise '\n",
      "                                       'and local interactions only, are also '\n",
      "                                       'presented. The labeling results are '\n",
      "                                       'produced primarily using the semantic '\n",
      "                                       'attributes of objects, such as, blue '\n",
      "                                       'skies, green vegetation etc. Because '\n",
      "                                       'semantic attributes alone are not '\n",
      "                                       'sufficient to fully describe objects '\n",
      "                                       'and their parts, we develop/use '\n",
      "                                       'different computational models such as '\n",
      "                                       'a human skin color model, the output '\n",
      "                                       'of a probabilistic classifier for '\n",
      "                                       'detecting the presence of buildings, a '\n",
      "                                       'face detector etc. These different '\n",
      "                                       'models result in the hybrid nature of '\n",
      "                                       'the image parser. Going forward, we '\n",
      "                                       'intend to include the use of more '\n",
      "                                       'visual perception cues such as depth '\n",
      "                                       'and motion to further constrain the '\n",
      "                                       'labeling process. Also, although the '\n",
      "                                       'MRF optimization algorithm greatly '\n",
      "                                       'reduces the search space for a (near) '\n",
      "                                       'global solution, it still runs in an '\n",
      "                                       'order exponential in the number of '\n",
      "                                       'nodes present in the graph. We can '\n",
      "                                       'further investigate methods to reduce '\n",
      "                                       'its computational complexity. Lastly, '\n",
      "                                       'this method will be useful for '\n",
      "                                       '3-dimensional data labeling, where '\n",
      "                                       'many different spatial constraints can '\n",
      "                                       'be better enforced than in '\n",
      "                                       '2-dimensional images. We therefore '\n",
      "                                       'intend to apply this labeling '\n",
      "                                       'technique to 3-D medical data in the '\n",
      "                                       'future.The symbol-based ontology was '\n",
      "                                       'developed for the natural images '\n",
      "                                       'domain, specifically for outdoor '\n",
      "                                       'images. Several segmentation '\n",
      "                                       'algorithms including our in-house '\n",
      "                                       'technique were compared using image '\n",
      "                                       'benchmark data found in the Berkeley '\n",
      "                                       'Database System (BDS). The image '\n",
      "                                       'parser was tested on natural images '\n",
      "                                       'from the BDS, from the Lotus Hill '\n",
      "                                       'dataset and on natural photographs '\n",
      "                                       'from flickr.com.'},\n",
      "              'score': 0.604416,\n",
      "              'values': []},\n",
      "             {'id': '5390bed320f70186a0f4fa02',\n",
      "              'metadata': {'abstract': '3D model decomposition is a '\n",
      "                                       'challenging and important problem in '\n",
      "                                       'computer graphics. Several '\n",
      "                                       'semantically based approaches have '\n",
      "                                       'been proposed in the literature, '\n",
      "                                       'however, due to the lack of proper '\n",
      "                                       'evaluation criteria, comparison of '\n",
      "                                       'these techniques is almost impossible. '\n",
      "                                       'In this paper we suggest to use animal '\n",
      "                                       'anatomy as the ground truth and '\n",
      "                                       'compare the result of different '\n",
      "                                       'segmentation techniques based on that. '\n",
      "                                       'Differing from previous approaches '\n",
      "                                       'which perform the evaluation based on '\n",
      "                                       'ground truth databases created '\n",
      "                                       'subjectively by human observers, we '\n",
      "                                       'consider expert knowledge on anatomy '\n",
      "                                       'of various animals. Based on this '\n",
      "                                       'knowledge we specify the ground truth '\n",
      "                                       'for different animals and compare '\n",
      "                                       'alternative algorithms.'},\n",
      "              'score': 0.60333091,\n",
      "              'values': []},\n",
      "             {'id': '5390882420f70186a0d88d49',\n",
      "              'metadata': {'abstract': 'From the Publisher:This book presents '\n",
      "                                       'a coherent approach to the fast-moving '\n",
      "                                       'field of computer vision, using a '\n",
      "                                       'consistent notation based on a '\n",
      "                                       'detailed understanding of the image '\n",
      "                                       'formation process. It covers even the '\n",
      "                                       'most recent research and will provide '\n",
      "                                       'a useful and current reference for '\n",
      "                                       'professionals working in the fields of '\n",
      "                                       'machine vision, image processing, and '\n",
      "                                       'pattern recognition. An outgrowth of '\n",
      "                                       \"the author's course at MIT, Robot \"\n",
      "                                       'Vision presents a solid framework for '\n",
      "                                       'understanding existing work and '\n",
      "                                       'planning future research. Its coverage '\n",
      "                                       'includes a great deal of material that '\n",
      "                                       'is important to engineers applying '\n",
      "                                       'machine vision methods in the real '\n",
      "                                       'world. The chapters on binary image '\n",
      "                                       'processing, for example, help explain '\n",
      "                                       'and suggest how to improve the many '\n",
      "                                       'commercial devices now available. And '\n",
      "                                       'the material on photometric stereo and '\n",
      "                                       'the extended Gaussian image points the '\n",
      "                                       'way to what may be the next thrust in '\n",
      "                                       'commercialization of the results in '\n",
      "                                       'this area. Chapters in the first part '\n",
      "                                       'of the book emphasize the development '\n",
      "                                       'of simple symbolic descriptions from '\n",
      "                                       'images, while the remaining chapters '\n",
      "                                       'deal with methods that exploit these '\n",
      "                                       'descriptions. The final chapter offers '\n",
      "                                       'a detailed description of how to '\n",
      "                                       'integrate a vision system into an '\n",
      "                                       'overall robotics system, in this case '\n",
      "                                       'one designed to pick parts out of a '\n",
      "                                       'bin. The many exercises complement and '\n",
      "                                       'extend the material in the text, and '\n",
      "                                       'an extensive bibliography will serve '\n",
      "                                       'as a useful guide to current research. '\n",
      "                                       'Errata (164k PDF)'},\n",
      "              'score': 0.599342,\n",
      "              'values': []},\n",
      "             {'id': '539090c420f70186a0ddddda',\n",
      "              'metadata': {'abstract': 'One of the main objectives of computer '\n",
      "                                       'vision systems is to produce '\n",
      "                                       'structural descriptions of the scenes '\n",
      "                                       'depicted in images. Knowledge of the '\n",
      "                                       'class of objects being imaged can '\n",
      "                                       'facilitate this objective by providing '\n",
      "                                       'models to guide interpretation, and by '\n",
      "                                       'furnishing a basis for the structural '\n",
      "                                       'descriptions. This document describes '\n",
      "                                       'research into techniques for the '\n",
      "                                       'representation and use of knowledge of '\n",
      "                                       'object classes, carried out within the '\n",
      "                                       'context of a computational vision '\n",
      "                                       'system which interprets line drawings '\n",
      "                                       'of human-like body forms. .br A '\n",
      "                                       'declarative schemata format has been '\n",
      "                                       'devised which represents structures of '\n",
      "                                       'image features which constitute dep- '\n",
      "                                       'ictions of body parts. The system '\n",
      "                                       'encodes relations between these image '\n",
      "                                       'constructions and an underlying three '\n",
      "                                       'dimensional model of the human body. '\n",
      "                                       'Using the component hierarchy as a '\n",
      "                                       'structural basis, two layers of '\n",
      "                                       'representation are developed. One '\n",
      "                                       'references the fine resolution '\n",
      "                                       'features, and the other references the '\n",
      "                                       'coarse resolution. These layers are '\n",
      "                                       'connected with links representative of '\n",
      "                                       'the specialization/generalization '\n",
      "                                       'hierarchy. The problem domain '\n",
      "                                       'description is declarative, and makes '\n",
      "                                       'no commitment to the nature of the '\n",
      "                                       'subsequent interpretation processes. '\n",
      "                                       'As a means of testing the adequacy of '\n",
      "                                       'the representation, portions have been '\n",
      "                                       'converted into a PROLOG formulation '\n",
      "                                       \"and used to ``prove'''' body parts in \"\n",
      "                                       'a data base of assertions about image '\n",
      "                                       'properties. .br The interpretation '\n",
      "                                       'phase relies on a cue/model approach, '\n",
      "                                       'using an extensive cue table which is '\n",
      "                                       'automatically generated from the '\n",
      "                                       'problem domain description. The '\n",
      "                                       'primary mechanisms for control of '\n",
      "                                       'interpretation possibilities are '\n",
      "                                       'fashioned after network consistency '\n",
      "                                       'methods. The operation of these '\n",
      "                                       'mechanisms is localized and separated '\n",
      "                                       'between operations at the feature '\n",
      "                                       'level and at the model level. .br The '\n",
      "                                       'body drawing interpretation system is '\n",
      "                                       'consistent with aspects of human '\n",
      "                                       'visual perception. The system is '\n",
      "                                       'capable of intelligent selection of '\n",
      "                                       'processing locations on the basis of '\n",
      "                                       'the progress of interpretation. A dual '\n",
      "                                       'resolution retina is moved about the '\n",
      "                                       'image collecting fine level features '\n",
      "                                       'in a small foveal area and coarse '\n",
      "                                       'level features in a wider peripheral '\n",
      "                                       'area. Separate interpretations are '\n",
      "                                       'developed locally on the basis of the '\n",
      "                                       'two different resolution levels, and '\n",
      "                                       'the relation between these two '\n",
      "                                       'interpretations is analyzed by the '\n",
      "                                       'system to determine locations of '\n",
      "                                       'potentially useful information.'},\n",
      "              'score': 0.598750412,\n",
      "              'values': []},\n",
      "             {'id': '53908bfb20f70186a0dc9833',\n",
      "              'metadata': {'abstract': 'Humans are quite adept at recognizing '\n",
      "                                       'and labeling regions and objects in '\n",
      "                                       'visual scenes. One of the cues for '\n",
      "                                       'such labeling is the spatial '\n",
      "                                       'relationships exhibited among the '\n",
      "                                       'regions. This is usually coupled with '\n",
      "                                       \"the interpreter's understanding and \"\n",
      "                                       'expectations of scene content. For '\n",
      "                                       'example, it is normally the case that, '\n",
      "                                       'in a natural outdoor scene, the sky '\n",
      "                                       'should be above the trees and that '\n",
      "                                       'vehicles should be on a road. Context '\n",
      "                                       'plays a very important role in the '\n",
      "                                       'interpretation of an image. This '\n",
      "                                       'determination of spatial relations has '\n",
      "                                       'been a difficult task to automate. '\n",
      "                                       'There have been several attempts at '\n",
      "                                       'defining spatial relationships between '\n",
      "                                       'regions in a digital image, most '\n",
      "                                       'recently, with the use of fuzzy set '\n",
      "                                       'theory. In this paper, we examine '\n",
      "                                       'three methods for defining spatial '\n",
      "                                       'relations to gain insight into this '\n",
      "                                       'complex situation.'},\n",
      "              'score': 0.59862572,\n",
      "              'values': []},\n",
      "             {'id': '539090c420f70186a0dddb8d',\n",
      "              'metadata': {'abstract': 'This paper is concerned with the '\n",
      "                                       'problem of attaching meaningful '\n",
      "                                       'symbols to aspects of the visible '\n",
      "                                       'environment in machine and biological '\n",
      "                                       'vision. It begins with a review of '\n",
      "                                       'some of the arguments commonly used to '\n",
      "                                       \"support either the ''symbolic'' or the \"\n",
      "                                       \"''behaviourist'' approach to vision. \"\n",
      "                                       'Having explored these avenues without '\n",
      "                                       'arriving at a satisfactory conclusion, '\n",
      "                                       'we then present a novel argument, '\n",
      "                                       'which starts from the question : given '\n",
      "                                       'a functional description of a vision '\n",
      "                                       'system, when could it be said to '\n",
      "                                       'support a symbolic interpretation? We '\n",
      "                                       'argue that to attach symbols to a '\n",
      "                                       'system, its behaviour must exhibit '\n",
      "                                       'certain well defined regularities in '\n",
      "                                       'its response to its visual input and '\n",
      "                                       'these are best described in terms of '\n",
      "                                       'invariance and equivariance to '\n",
      "                                       'transformations which act in the world '\n",
      "                                       'and induce corresponding changes of '\n",
      "                                       'the vision system state. This approach '\n",
      "                                       'is illustrated with a brief '\n",
      "                                       'exploration of the problem of '\n",
      "                                       'identifying and acquiring visual '\n",
      "                                       'representations having these symmetry '\n",
      "                                       'properties, which also highlights the '\n",
      "                                       \"advantages of using an ''active'' \"\n",
      "                                       'model of vision.'},\n",
      "              'score': 0.598531723,\n",
      "              'values': []},\n",
      "             {'id': '5390980720f70186a0e028ef',\n",
      "              'metadata': {'abstract': 'This paper summarizes the present '\n",
      "                                       'state of research in scene analysis. '\n",
      "                                       'It identifies fundamental information '\n",
      "                                       'processing principles relevant to '\n",
      "                                       'representation and Use of knowledge in '\n",
      "                                       'vision and traces limitations of '\n",
      "                                       'existing progams to compromises of '\n",
      "                                       'these principles necessitated by '\n",
      "                                       'extant processors Some specific and '\n",
      "                                       'general recommendations are offered '\n",
      "                                       'regarding a productive course of '\n",
      "                                       'research for the next decade.'},\n",
      "              'score': 0.597265303,\n",
      "              'values': []},\n",
      "             {'id': '53908e0020f70186a0dd6260',\n",
      "              'metadata': {'abstract': 'Methods are presented 1) to partition '\n",
      "                                       'or decompose a visual scene into the '\n",
      "                                       'bodies forming it; 2) to position '\n",
      "                                       'these bodies in three-dimensional '\n",
      "                                       'space, by combining two scenes that '\n",
      "                                       'make a stereoscopic pair; 3) to find '\n",
      "                                       'the regions or zones of a visual scene '\n",
      "                                       'that belong to its background; 4) to '\n",
      "                                       'carry out the isolation of the objects '\n",
      "                                       'in 1) when the input has inaccuracies. '\n",
      "                                       'Running computer programs implement '\n",
      "                                       'the methods and many examples '\n",
      "                                       'illustrate their behavior. The input '\n",
      "                                       'is a two-dimensional line-drawing of '\n",
      "                                       'the scene, assumed to contain '\n",
      "                                       'three-dimensional bodies possessing '\n",
      "                                       'flat faces (polyhedra); some of them '\n",
      "                                       'may be partially occluded. Suggestions '\n",
      "                                       'are made for extending the work to '\n",
      "                                       'curved objects. Some comparisons are '\n",
      "                                       'made with human visual perception. The '\n",
      "                                       'main conclusion is that it is possible '\n",
      "                                       'to separate a picture or scene into '\n",
      "                                       'the constituent objects exclusively on '\n",
      "                                       'the basis of monocular geometric '\n",
      "                                       'properties (on the basis of pure '\n",
      "                                       'form); in fact, successful methods are '\n",
      "                                       'shown.'},\n",
      "              'score': 0.597037315,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e84703',\n",
      "              'metadata': {'abstract': 'One of the most fundamental problems '\n",
      "                                       'in vision is segmentation; the way in '\n",
      "                                       'which parts of an image are perceived '\n",
      "                                       'as a meaningful whole. Recent work has '\n",
      "                                       'shown how to calculate images of '\n",
      "                                       'physical parameters from raw intensity '\n",
      "                                       'data. Such images are known as '\n",
      "                                       'intrinsic images, and examples are '\n",
      "                                       'images of velocity (optical flow), '\n",
      "                                       'surface orientation, occluding '\n",
      "                                       'contour, and disparity. While '\n",
      "                                       'intrinsic images are not segmented, '\n",
      "                                       'they are distinctly easier to segment '\n",
      "                                       'than the original intensity image. '\n",
      "                                       'Segments can be detected by a general '\n",
      "                                       'Hough transform technique. Networks of '\n",
      "                                       'feature parameters are appended to the '\n",
      "                                       'intrinsic image organization. Then the '\n",
      "                                       'intrinsic image points are mapped into '\n",
      "                                       'these networks. This mapping will be '\n",
      "                                       'many-to-one onto parameter values that '\n",
      "                                       'represent segments. This basic method '\n",
      "                                       'is extended into a general '\n",
      "                                       'representation and control technique '\n",
      "                                       'with the addition of three main ideas: '\n",
      "                                       'abstraction levels; sequential search; '\n",
      "                                       'and tight counting These ideas are a '\n",
      "                                       'nucleus of a connectionist theory of '\n",
      "                                       \"low 'eve and m'ermediate-level vision. \"\n",
      "                                       'This theory explains segmentation in '\n",
      "                                       'terms of massively parallel '\n",
      "                                       'cooperative computation among '\n",
      "                                       'intrinsic images and a set of '\n",
      "                                       'parameter spaces at different levels '\n",
      "                                       'of abstraction.'},\n",
      "              'score': 0.595318675,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda765',\n",
      "              'metadata': {'abstract': \"THE `SCHEMA SYSTEM'' IS A HIGH-LEVEL \"\n",
      "                                       'IMAGE UNDERSTANDING SYSTEM FOR THE '\n",
      "                                       'INTERPRETATION OF COMPLEX NATURAL '\n",
      "                                       'SCENES. IT IS OUR POSITION THAT THE '\n",
      "                                       'CONSTRAINTS AVAILABLE FROM GENERAL '\n",
      "                                       'KNOWLEDGE ABOUT THE WORLD MUST BE USED '\n",
      "                                       'TO ORGANIZE RAW IMAGE DESCRIPTIONS '\n",
      "                                       'INTO ABSTRACT INTERPRETATIONS. THE '\n",
      "                                       'OUTPUT OF LOW-LEVEL IMAGE OPERATIONS '\n",
      "                                       'DOES NOT MEET THE REQUIREMENTS OF '\n",
      "                                       'NATURAL-SCENE INTERPRETATION. '\n",
      "                                       'INTERMEDIATE-LEVEL GROUPING PROCEDURES '\n",
      "                                       'REPRESENT A PARTIAL SOLUTION TO THIS '\n",
      "                                       'MISMATCH, BUT ARE TOO EXPENSIVE TO '\n",
      "                                       'APPLY INDISCRIMINATELY. HIGH-LEVEL '\n",
      "                                       'PROCESSES USE OBJECT-SPECIFIC '\n",
      "                                       'KNOWLEDGE TO GUIDE INTERPRETATION BY '\n",
      "                                       'FOCUSING ATTENTION ON PROMISING AREAS '\n",
      "                                       'OF THE IMAGE. THIS PAPER DESCRIBES THE '\n",
      "                                       'SCHEMA SYSTEM - A FLEXIBLE, HIGH-LEVEL '\n",
      "                                       'SYSTEM SUPPORTING THE INTERACTION OF '\n",
      "                                       'OBJECT-SPECIFIC INTERPRETATION PRO- '\n",
      "                                       'CESSES - AND PRESENTS THE RESULTS IT '\n",
      "                                       'HAS PRODUCED ON ROAD SCENES AS A '\n",
      "                                       'JUSTIFICATION OF THE KNOWLEDGE '\n",
      "                                       'ENGINEERING APPROACH TO IMAGE '\n",
      "                                       'UNDERSTANDING.'},\n",
      "              'score': 0.595041215,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e85421',\n",
      "              'metadata': {'abstract': 'The goal of image understanding has '\n",
      "                                       'long been a shared goal in the field '\n",
      "                                       'of computer vision. Extracting the '\n",
      "                                       'required scene-level information from '\n",
      "                                       'image data is a formidable task, '\n",
      "                                       'however. While the raw data comes in '\n",
      "                                       'the form of matrices of numbers, the '\n",
      "                                       'inferences that we must perform occur '\n",
      "                                       'at a much higher level of abstraction. '\n",
      "                                       'Much progress has been made in recent '\n",
      "                                       'years in extracting the primitives of '\n",
      "                                       'an image in isolation, for example '\n",
      "                                       'detecting the objects, labeling the '\n",
      "                                       'regions, or extracting the surfaces. '\n",
      "                                       'Modeling the interactions and '\n",
      "                                       'fine-grained distinctions between '\n",
      "                                       'these primitives is an important next '\n",
      "                                       'step along the path to scene '\n",
      "                                       'understanding. Because this involves '\n",
      "                                       'reasoning about relationships between '\n",
      "                                       'heterogeneous entities at a high level '\n",
      "                                       'of abstraction, this problem lends '\n",
      "                                       'itself well to the tools of '\n",
      "                                       'probabilistic graphical models. In '\n",
      "                                       'this thesis we consider two important '\n",
      "                                       'challenges in this space. In the '\n",
      "                                       'first, we model interactions between '\n",
      "                                       'primitives of various types in order '\n",
      "                                       'to capture the contextual '\n",
      "                                       'relationships between them. For '\n",
      "                                       'example, we can expect to find a sheep '\n",
      "                                       '(a detected object) more often on a '\n",
      "                                       'field of grass (a labeled region) than '\n",
      "                                       'on a patch of water. By modeling the '\n",
      "                                       'interactions between these components, '\n",
      "                                       'we can expect to improve the quality '\n",
      "                                       'of classification by leveraging these '\n",
      "                                       'contextual cues. We first introduce '\n",
      "                                       'Cascaded Classification Models (CCM), '\n",
      "                                       'a flexible framework for combining '\n",
      "                                       'various state-of-the-art vision models '\n",
      "                                       'in a way that allows for improved '\n",
      "                                       'performance of each model. We next '\n",
      "                                       'consider the tasks of object detection '\n",
      "                                       'and region labeling and develop a more '\n",
      "                                       'sophisticated probabilistic model '\n",
      "                                       'aimed at capturing the contextual '\n",
      "                                       'relationships between these types of '\n",
      "                                       'primitives in a more targeted and '\n",
      "                                       'meaningful way. Our Things and Stuff '\n",
      "                                       '(TAS) context model learns to leverage '\n",
      "                                       'contextual cues directly from data. '\n",
      "                                       'Following this exploration of '\n",
      "                                       'interactions between objects, we '\n",
      "                                       'consider the interactions of \"parts\" '\n",
      "                                       'within an object, and in particular '\n",
      "                                       'tackle the problem of descriptive '\n",
      "                                       'querying of objects. This involves '\n",
      "                                       'making distinctions about the object '\n",
      "                                       'at a refined level, beyond mere '\n",
      "                                       'categorization. For example, we may '\n",
      "                                       'want to know whether a cheetah in an '\n",
      "                                       'image is running or standing still. We '\n",
      "                                       'introduce a probabilistic deformable '\n",
      "                                       'shape model (LOOPS) and a method for '\n",
      "                                       'matching this model to an image that '\n",
      "                                       'allows for precise localization of '\n",
      "                                       'several object classes. Using this '\n",
      "                                       'localization, we show how descriptive '\n",
      "                                       'distinctions can be drawn with a small '\n",
      "                                       'amount of training data.'},\n",
      "              'score': 0.594696224,\n",
      "              'values': []},\n",
      "             {'id': '539087a620f70186a0d49630',\n",
      "              'metadata': {'abstract': 'A data-driven system for segmenting '\n",
      "                                       'scenes into objects and their '\n",
      "                                       'components is presented. This '\n",
      "                                       'segmentation system generates '\n",
      "                                       'hierarchies of features that '\n",
      "                                       'correspond to structural elements such '\n",
      "                                       'as boundaries and surfaces of objects. '\n",
      "                                       'The technique is based on perceptual '\n",
      "                                       'organization, implemented as a '\n",
      "                                       'mechanism for exploiting geometrical '\n",
      "                                       'regularities in the shapes of objects '\n",
      "                                       'as projected on images. Edges are '\n",
      "                                       'recursively grouped on geometrical '\n",
      "                                       'relationships into a description '\n",
      "                                       'hierarchy ranging from edges to the '\n",
      "                                       'visible surfaces of objects. These '\n",
      "                                       'edge groupings, which are termed '\n",
      "                                       'collated features, are abstract '\n",
      "                                       'descriptors encoding structural '\n",
      "                                       'information. The geometrical '\n",
      "                                       'relationships employed are '\n",
      "                                       'quasi-invariant over 2-D projections '\n",
      "                                       'and are common to structures of most '\n",
      "                                       'objects. Thus, collations have a high '\n",
      "                                       'likelihood of corresponding to parts '\n",
      "                                       'of objects. Collations serve as '\n",
      "                                       'intermediate and high-level features '\n",
      "                                       'for various visual processes. '\n",
      "                                       'Applications of collations to stereo '\n",
      "                                       'correspondence, object-level '\n",
      "                                       'segmentation, and shape description '\n",
      "                                       'are illustrated.'},\n",
      "              'score': 0.594638526,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e850c5',\n",
      "              'metadata': {'abstract': 'We show that an intelligent approach '\n",
      "                                       'to color can be used to significantly '\n",
      "                                       'improve the capabilities of a vision '\n",
      "                                       'system. In previous work, we adopted '\n",
      "                                       'general physical models which describe '\n",
      "                                       'how objects interact with light. These '\n",
      "                                       'models are far more general than those '\n",
      "                                       'typically used in computer vision. In '\n",
      "                                       'this report, we use our physical '\n",
      "                                       'models to derive powerful algorithms '\n",
      "                                       'for extracting invariant properties of '\n",
      "                                       'objects from images. The first '\n",
      "                                       'algorithm is used for the generic '\n",
      "                                       'classification of objects according to '\n",
      "                                       'material. The second algorithm '\n",
      "                                       'provides a solution to the color '\n",
      "                                       'constancy problem. These algorithms '\n",
      "                                       'have been implemented and produce '\n",
      "                                       'correct results on real images. Some '\n",
      "                                       'examples of experimental results are '\n",
      "                                       'presented.'},\n",
      "              'score': 0.593849659,\n",
      "              'values': []},\n",
      "             {'id': '5390a9a520f70186a0ea6ba1',\n",
      "              'metadata': {'abstract': 'One of the big issues facing current '\n",
      "                                       'content-based image retrieval is how '\n",
      "                                       'to automatically extract the '\n",
      "                                       'high-level concepts from images. In '\n",
      "                                       'this paper, we present an efficient '\n",
      "                                       'system that automatically extracts the '\n",
      "                                       'high-level concepts from images by '\n",
      "                                       'using ontologies and semantic '\n",
      "                                       'inference rules. In our method, MPEG-7 '\n",
      "                                       'visual descriptors are used to extract '\n",
      "                                       'the visual features of image, and the '\n",
      "                                       'visual features are mapped to '\n",
      "                                       'semi-concepts via the mapping '\n",
      "                                       'algorithm. We also build the visual '\n",
      "                                       'and animal ontologies to bridge the '\n",
      "                                       'semantic gap. The visual ontology '\n",
      "                                       'allows the definition of relationships '\n",
      "                                       'among the classes describing the '\n",
      "                                       'visual features and has the values of '\n",
      "                                       'semi-concepts as the property values. '\n",
      "                                       'The animal ontology can be exploited '\n",
      "                                       'to identify the highlevel concept in '\n",
      "                                       'an image. Also, the semantic inference '\n",
      "                                       'rules are applied to the ontologies to '\n",
      "                                       'extract the high-level concept. '\n",
      "                                       'Finally, we evaluate the proposed '\n",
      "                                       'system using the image data set '\n",
      "                                       'including various animal objects and '\n",
      "                                       'discuss the limitations of our '\n",
      "                                       'system.'},\n",
      "              'score': 0.593736351,\n",
      "              'values': []},\n",
      "             {'id': '53908a9620f70186a0da4b3e',\n",
      "              'metadata': {'abstract': 'The increasing amount of remotely '\n",
      "                                       'sensed imagery from multiple platforms '\n",
      "                                       'requires efficient analysis '\n",
      "                                       'techniques. The leading idea of the '\n",
      "                                       'presented work is to automate the '\n",
      "                                       'interpretation of multisensor and '\n",
      "                                       'multitemporal remote sensing images by '\n",
      "                                       'the use of common prior knowledge '\n",
      "                                       'about landscape scenes. In addition '\n",
      "                                       'the system can use specific map '\n",
      "                                       'knowledge of a GIS, information about '\n",
      "                                       'sensor projections and temporal '\n",
      "                                       'changes of scene objects. Prior expert '\n",
      "                                       'knowledge about the scene content is '\n",
      "                                       'represented explicitly by a semantic '\n",
      "                                       'net. A common concept has been '\n",
      "                                       'developed to distinguish between the '\n",
      "                                       'semantics of objects and their visual '\n",
      "                                       'appearance in the different sensors '\n",
      "                                       'considering the physical principle of '\n",
      "                                       'the sensor and the material and '\n",
      "                                       'surface properties of the objects. A '\n",
      "                                       'flexible control system is used for '\n",
      "                                       'the automated analysis, which employs '\n",
      "                                       'mixtures of bottom up and top down '\n",
      "                                       'strategies for image analysis '\n",
      "                                       'dependent on the respective state of '\n",
      "                                       'interpretation. The control strategy '\n",
      "                                       'employs rule based systems and is '\n",
      "                                       'independent of the application. The '\n",
      "                                       'system permits the fusion of several '\n",
      "                                       'sensors like optical, infrared, and '\n",
      "                                       'SAR-images, laser-scans etc. and it '\n",
      "                                       'can be used for the fusion of images '\n",
      "                                       'taken at different instances of time. '\n",
      "                                       'Sensor fusion can be achieved on a '\n",
      "                                       'pixel level, which requires prior '\n",
      "                                       'rectification of the images, on '\n",
      "                                       'feature level, which means that the '\n",
      "                                       'same object may show up differently in '\n",
      "                                       'different sensors, and on object '\n",
      "                                       'level, which means that different '\n",
      "                                       'parts of an object can more accurately '\n",
      "                                       'be recognized in different sensors. '\n",
      "                                       'Results are shown for the extraction '\n",
      "                                       'of roads from multisensor images. The '\n",
      "                                       'approach for a multitemporal image '\n",
      "                                       'analysis is illustrated for the '\n",
      "                                       'recognition and extraction of an '\n",
      "                                       'industrial fairground from an '\n",
      "                                       'industrial area in an urban scene.'},\n",
      "              'score': 0.593575478,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e845c2',\n",
      "              'metadata': {'abstract': 'This paper overviews and discusses '\n",
      "                                       'model representations and control '\n",
      "                                       'structures in image understanding. '\n",
      "                                       'Hierarchies are observed in the levels '\n",
      "                                       'of description used in image '\n",
      "                                       'understanding along a few dimensions: '\n",
      "                                       'processing unit, detail, composition '\n",
      "                                       'and scene/view distinction . Emphasis '\n",
      "                                       'is placed on the importance of '\n",
      "                                       'explicitly handling the hierarchies '\n",
      "                                       'both in representing knowledge and in '\n",
      "                                       'using it . A scheme of \"knowledge '\n",
      "                                       'block\" representation which is '\n",
      "                                       'structured along the processing-unit '\n",
      "                                       'hierarchy is also presented.'},\n",
      "              'score': 0.593409419,\n",
      "              'values': []},\n",
      "             {'id': '5390958a20f70186a0defe3b',\n",
      "              'metadata': {'abstract': 'This paper presents a generic '\n",
      "                                       'cognitive vision platform for the '\n",
      "                                       'automatic recognition of natural '\n",
      "                                       'complex objects. The recognition '\n",
      "                                       'consists of three steps : image '\n",
      "                                       'processing for numerical object '\n",
      "                                       'description, mapping of numerical data '\n",
      "                                       'into symbolic data and semantic '\n",
      "                                       'interpretation for object recognition. '\n",
      "                                       'The focus of this paper is the '\n",
      "                                       'distributed platform architecture '\n",
      "                                       'composed of three highly specialized '\n",
      "                                       'Knowledge Based Systems (KBS). The '\n",
      "                                       'first KBS is dedicated to semantic '\n",
      "                                       'interpretation. The second one has to '\n",
      "                                       'deal with the anchoring of symbolic '\n",
      "                                       'data into image data. The last KBS is '\n",
      "                                       'dedicated to intelligent image '\n",
      "                                       'processing. Aftera brief overview of '\n",
      "                                       'the natural object recognition '\n",
      "                                       'problem, this paper describes the '\n",
      "                                       'three subcomponents of the platform.'},\n",
      "              'score': 0.592800379,\n",
      "              'values': []},\n",
      "             {'id': '53908b6c20f70186a0dbf5c9',\n",
      "              'metadata': {'abstract': 'Discusses the intention to provide '\n",
      "                                       'robots with a computer system which '\n",
      "                                       'gives them the capability to build an '\n",
      "                                       'abstract and condensed description of '\n",
      "                                       'their environment from low level data '\n",
      "                                       'provided by an artificial vision '\n",
      "                                       'system. This intention rapidly faced a '\n",
      "                                       'complexity barrier: on one hand, the '\n",
      "                                       'importance of the volume of '\n",
      "                                       'information conveyed by an image they '\n",
      "                                       'manipulate; on the other hand, the '\n",
      "                                       'access to pertinent information to '\n",
      "                                       'validate decision making. To reach '\n",
      "                                       'this goal, research is oriented '\n",
      "                                       'towards extracting a set of computing '\n",
      "                                       'tools learning in two ways: '\n",
      "                                       'descriptive process of the objects to '\n",
      "                                       'manipulate, and constructive process '\n",
      "                                       'of the pertinent information for each '\n",
      "                                       'operation to apply to an object.'},\n",
      "              'score': 0.592422783,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e84cf5',\n",
      "              'metadata': {'abstract': 'A model to extract a structure feature '\n",
      "                                       'of images belonging to one category is '\n",
      "                                       'proposed, implemented and studied. '\n",
      "                                       'This model consists of two parts i.e. '\n",
      "                                       'an extraction of partial geometrical '\n",
      "                                       'features and an extraction of '\n",
      "                                       'structure feature by using them under '\n",
      "                                       'the positional constraints. First, '\n",
      "                                       'necessary conditions which geometrical '\n",
      "                                       'features should satisfy is stated and '\n",
      "                                       'a statistical method to select a key '\n",
      "                                       'feature is described. Then the '\n",
      "                                       'algorithm to extract structure feature '\n",
      "                                       'as the maximal set of common subimages '\n",
      "                                       'which satisfy the positional '\n",
      "                                       'constraint is introduced. The '\n",
      "                                       'structure feature is regarded as a '\n",
      "                                       'intrinsic feature which is used when '\n",
      "                                       'an animal perceives an image at a '\n",
      "                                       'glance. The experiment was carried out '\n",
      "                                       'about image of human faces and had a '\n",
      "                                       'fairly good results.'},\n",
      "              'score': 0.591365337,\n",
      "              'values': []},\n",
      "             {'id': '5390893e20f70186a0d941ff',\n",
      "              'metadata': {'abstract': 'We present three unsupervised '\n",
      "                                       'artificial neural networksfor the '\n",
      "                                       'extraction of structural information '\n",
      "                                       'from visual data. Theability of each '\n",
      "                                       'network to represent structured '\n",
      "                                       'knowledge in a mannereasily accessible '\n",
      "                                       'to human interpretation is illustrated '\n",
      "                                       'usingartificial visual data. These '\n",
      "                                       'networks are used to '\n",
      "                                       'collectivelydemonstrate a variety of '\n",
      "                                       'unsupervised methods for '\n",
      "                                       'identifyingfeatures in visual data and '\n",
      "                                       'the structural representation of '\n",
      "                                       'thesefeatures in terms of orientation, '\n",
      "                                       'temporal and topographicalordering, '\n",
      "                                       'and stereo disparity.'},\n",
      "              'score': 0.591065109,\n",
      "              'values': []},\n",
      "             {'id': '5390ac1720f70186a0eb2689',\n",
      "              'metadata': {'abstract': 'The Holy Grail of computer vision and '\n",
      "                                       'image analysis is to develop an '\n",
      "                                       'artificial visual intelligence system '\n",
      "                                       'that can recognize, learn, and '\n",
      "                                       'identify, in the general case, '\n",
      "                                       'arbitrary objects in arbitrary '\n",
      "                                       'situations. To date, cognitive science '\n",
      "                                       'researchers have been studying for '\n",
      "                                       'centuries how biological systems '\n",
      "                                       'accomplish this. Concurrently, '\n",
      "                                       'research in the fields of computer '\n",
      "                                       'vision and image analysis has been '\n",
      "                                       'investigating how artificial systems '\n",
      "                                       'can accomplish this. This paper '\n",
      "                                       'proposes what could happen and '\n",
      "                                       'documents what has happened when these '\n",
      "                                       'fields, the biological, psychological, '\n",
      "                                       'and the artificial, are brought '\n",
      "                                       'together to offer a solution to the '\n",
      "                                       'intelligent image segmentation '\n",
      "                                       'problem—a precursor to obtaining the '\n",
      "                                       'grail.'},\n",
      "              'score': 0.591042638,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda80b',\n",
      "              'metadata': {'abstract': 'This paper addresses the issue of '\n",
      "                                       'combining intensity information from '\n",
      "                                       'multiple images for analysis of object '\n",
      "                                       'surface intensities to support model '\n",
      "                                       'refinement and scene rendering. Given '\n",
      "                                       'camera and light source parameters for '\n",
      "                                       'each image, and a 3D CAD model of '\n",
      "                                       'objects in the scene, the textures of '\n",
      "                                       'object surfaces are systematically '\n",
      "                                       'collected into an organized '\n",
      "                                       'orthographic library. Occlusions and '\n",
      "                                       'shadows caused by objects in the scene '\n",
      "                                       'are predicted from the model and '\n",
      "                                       'associated with each retrieved '\n",
      "                                       'surface. The problem of combining '\n",
      "                                       'intensities from multiple images is '\n",
      "                                       'explored, and the result is an '\n",
      "                                       'algorithm that produces a unique '\n",
      "                                       'surface intensity representation that '\n",
      "                                       'is as complete and consistent as '\n",
      "                                       'possible. A successful application of '\n",
      "                                       'this approach to model refinement and '\n",
      "                                       'scene visualization is shown, using '\n",
      "                                       'results from the RADIUS aerial image '\n",
      "                                       'understanding project.'},\n",
      "              'score': 0.590138793,\n",
      "              'values': []},\n",
      "             {'id': '53908d6520f70186a0dd2900',\n",
      "              'metadata': {'abstract': 'Object recognition systems needs of '\n",
      "                                       'image segmentation processes that '\n",
      "                                       'relate image regions to world objects. '\n",
      "                                       'These methods present often three '\n",
      "                                       'problems: the generation of a large '\n",
      "                                       'number of small regions, '\n",
      "                                       'under-segmentation (different objects '\n",
      "                                       'are associated to the same image '\n",
      "                                       'region) and over-segmentation (a scene '\n",
      "                                       'object is segmented in various '\n",
      "                                       'regions). In order to overcome these '\n",
      "                                       'problems, we propose an image '\n",
      "                                       'segmentation method that combines '\n",
      "                                       'depth information and object surface '\n",
      "                                       'properties obtained from a pair of '\n",
      "                                       'stereo images. The system work under '\n",
      "                                       'the standard assumption that 3D '\n",
      "                                       'objects have planar faces and regular '\n",
      "                                       'shapes. First, a region growing '\n",
      "                                       'segmentation process is applied to '\n",
      "                                       'both images generating two labeled '\n",
      "                                       'images. Then, depth information of the '\n",
      "                                       'region frontiers is obtained by '\n",
      "                                       'matching the labeled segments from '\n",
      "                                       'left and right image rows. Finding a '\n",
      "                                       'path through a 2D search plane whose '\n",
      "                                       'axes are the left and right-segmented '\n",
      "                                       'lines solves the stereo matching '\n",
      "                                       'problem. Original image regions are '\n",
      "                                       'then merged based on their size, '\n",
      "                                       'surface information and frontier depth '\n",
      "                                       'information. In this way, image '\n",
      "                                       'regions are associated to surfaces '\n",
      "                                       'that are contiguous in the 3D space, '\n",
      "                                       'and they present a common property (as '\n",
      "                                       'gray level, color or texture).'},\n",
      "              'score': 0.589661062,\n",
      "              'values': []},\n",
      "             {'id': '53909f8c20f70186a0e3f75c',\n",
      "              'metadata': {'abstract': 'In this work, we present effective '\n",
      "                                       'methods for detecting and representing '\n",
      "                                       'invariant features of three '\n",
      "                                       'dimensional (3-D) objects from single '\n",
      "                                       'images. Our methods are independent of '\n",
      "                                       'the viewpoint position, and can be '\n",
      "                                       'used in the recognition process of 3-D '\n",
      "                                       'objects. The invariant features are; '\n",
      "                                       'shapes of the object surfaces, '\n",
      "                                       'organization of the surfaces within '\n",
      "                                       'the object, and the new feature which '\n",
      "                                       'is shapes of privileged surfaces (PS) '\n",
      "                                       'of the object. Also we explain the way '\n",
      "                                       'these features are represented and '\n",
      "                                       'matched against stored model of the '\n",
      "                                       'object. Our work is built around the '\n",
      "                                       'knowledge based approach and depends '\n",
      "                                       'on the fact that while it is true that '\n",
      "                                       'the appearance of a 3-D object may '\n",
      "                                       'change completely as it is viewed from '\n",
      "                                       'different viewpoints, it is also true '\n",
      "                                       'that many aspects of the object '\n",
      "                                       'projection remain invariant over large '\n",
      "                                       'ranges of viewpoints. This approach is '\n",
      "                                       'different from the other approach that '\n",
      "                                       'derives or calculates depth '\n",
      "                                       'information and orientations of '\n",
      "                                       'surfaces using more than one image of '\n",
      "                                       'the object, or the approach that '\n",
      "                                       'statistically detects and matchs the '\n",
      "                                       'object features. The main contribution '\n",
      "                                       'of our work is the introduction of '\n",
      "                                       'privileged surface of 3-D object which '\n",
      "                                       'plays an important role in the '\n",
      "                                       'recognition process. Also we introduce '\n",
      "                                       'heuristic methods for detecting shapes '\n",
      "                                       'of surfaces, object organization, '\n",
      "                                       'object & model representations and '\n",
      "                                       'matching. The idea of matching '\n",
      "                                       'privileged surfaces reduces the '\n",
      "                                       'ambiguity that may arise due to '\n",
      "                                       'limited visibility of the object '\n",
      "                                       'surfaces (existence of hidden '\n",
      "                                       'surfaces) when matching features of '\n",
      "                                       'both the image and the model. Also '\n",
      "                                       'matching privileged surfaces reduces '\n",
      "                                       'the need of detecting and matching '\n",
      "                                       'orientations of surfaces. This '\n",
      "                                       'approach has been applied to 3-D '\n",
      "                                       'objects which are bounded by planar '\n",
      "                                       'surfaces. However, the work can be '\n",
      "                                       'extended in a straightforward manner '\n",
      "                                       'to support 3-D objects that are '\n",
      "                                       'bounded by curved surfaces.'},\n",
      "              'score': 0.589602,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e84978',\n",
      "              'metadata': {'abstract': 'We are developing a new paradigm for a '\n",
      "                                       'world model construction system which '\n",
      "                                       'interprets a scene and builds a world '\n",
      "                                       'model for a mobile robot using dynamic '\n",
      "                                       'semantic constraints. The system '\n",
      "                                       'represents a world model in '\n",
      "                                       'hierarchical form sensor-based maps to '\n",
      "                                       'a global map with both numerical and '\n",
      "                                       'symbolic descriptions. At the '\n",
      "                                       'beginning of interpretation, sensory '\n",
      "                                       'data (video and range images) are '\n",
      "                                       'analyzed in bottom-up fashion. A range '\n",
      "                                       'image is transformed into a height '\n",
      "                                       'map, and analyzed for the purpose of '\n",
      "                                       'generating a geometrical property list '\n",
      "                                       'for both obstacle and traversable '\n",
      "                                       'regions that is used as the initial '\n",
      "                                       'input to the interpretation process. '\n",
      "                                       'At each step of the scene '\n",
      "                                       'interpretation process, the most '\n",
      "                                       'reliable feature of an object is '\n",
      "                                       'selected in the region property list '\n",
      "                                       'to propagate semantic constraints on '\n",
      "                                       'other objects close to it. Geometrical '\n",
      "                                       'modeling for individual objects in the '\n",
      "                                       'scene is performed, and parameters of '\n",
      "                                       'each model are dynamically refined by '\n",
      "                                       'the scene interpretation process. '\n",
      "                                       'These model parameters and their '\n",
      "                                       'interrelationships make spatial '\n",
      "                                       'reasoning robust. Preliminary results '\n",
      "                                       'with video and range images are '\n",
      "                                       'shown.'},\n",
      "              'score': 0.588928342,\n",
      "              'values': []},\n",
      "             {'id': '5390b9d520f70186a0f307a5',\n",
      "              'metadata': {'abstract': 'With respect to the menagerie of '\n",
      "                                       'possible observed 3D scenes, no '\n",
      "                                       'algorithm today for reconstructing '\n",
      "                                       'such a scene from a stereo pair of '\n",
      "                                       'images is uniformly better than all '\n",
      "                                       'the others by their accuracy and '\n",
      "                                       'processing speed. Generally, '\n",
      "                                       'appropriate stereo reconstruction '\n",
      "                                       'algorithms should be selected in '\n",
      "                                       'accord with a type, or context of the '\n",
      "                                       'scene and in many cases different '\n",
      "                                       'parts of the same scene could be '\n",
      "                                       'reconstructed most accurately by using '\n",
      "                                       'different algorithms. To qualitatively '\n",
      "                                       'explore this problem, we collected a '\n",
      "                                       'database of more than 1,500 stereo '\n",
      "                                       'pairs of natural and artificial indoor '\n",
      "                                       'and outdoor 3D scenes, arranged into '\n",
      "                                       '25 types, such as animals, bars, city '\n",
      "                                       'roads, city trees, classrooms, coasts, '\n",
      "                                       'corridors, etc. The images were '\n",
      "                                       'processed with two algorithms: the 2D '\n",
      "                                       'graph-cut stereo (2DGCS) and the 1D '\n",
      "                                       'belief propagation stereo (1DBPS) -- '\n",
      "                                       'with automatically estimated '\n",
      "                                       'parameters. The obtained depth maps '\n",
      "                                       'were visually evaluated and compared '\n",
      "                                       'by a number of independent human '\n",
      "                                       'observers. Although in literature the '\n",
      "                                       '2DGCS is usually considered as more '\n",
      "                                       'accurate than the 1DBPS, its depth '\n",
      "                                       'maps were preferred by the observers '\n",
      "                                       'in these experiments only for about '\n",
      "                                       '58% of the images and 15 out of the 25 '\n",
      "                                       'types of scenes. The fast and easily '\n",
      "                                       'parallelised 1DBPS restores smooth '\n",
      "                                       'continuous curved surfaces but with '\n",
      "                                       'noisy object boundaries due to '\n",
      "                                       'horizontal streaks, whereas the much '\n",
      "                                       'slower and intrinsically sequential '\n",
      "                                       '2DGCS returns flattened depth maps '\n",
      "                                       'with distinctive object boundaries. '\n",
      "                                       'Based on these results, we implemented '\n",
      "                                       'context recognition to examine an '\n",
      "                                       'input scene and allocate the most '\n",
      "                                       'suitable algorithm and proposed to '\n",
      "                                       'combine the 2DGCS and 1DBPS into a '\n",
      "                                       'composite stereo reconstruction '\n",
      "                                       'technique, which is qualitatively '\n",
      "                                       'superior than each individual '\n",
      "                                       'algorithm and is able to return better '\n",
      "                                       'results and with good speed.'},\n",
      "              'score': 0.588881612,\n",
      "              'values': []},\n",
      "             {'id': '5390a6b120f70186a0e84575',\n",
      "              'metadata': {'abstract': 'Automatons and other robotic '\n",
      "                                       'applications which are designed to '\n",
      "                                       'move around and interact with their '\n",
      "                                       'physical environment need a computer '\n",
      "                                       'vision system for recognizing and '\n",
      "                                       'understanding the spatial '\n",
      "                                       'relationships of objects in real world '\n",
      "                                       'scenes. The perceptual system must be '\n",
      "                                       'able to identify salient objects in a '\n",
      "                                       'scene, develop an understanding of '\n",
      "                                       'their spatial relationships, and '\n",
      "                                       'maintain continuity from one view to '\n",
      "                                       'the next as either the objects or the '\n",
      "                                       \"system's camera moves through the \"\n",
      "                                       'scene. Outlined here and described in '\n",
      "                                       'more detail in Douglass, 1977, is a '\n",
      "                                       'system which has been implemented in '\n",
      "                                       'SIMULA and tested on hand coded '\n",
      "                                       'outdoor scenes of simple subjects such '\n",
      "                                       'as houses and automobiles. It uses a '\n",
      "                                       'recognition cone, a segmentation '\n",
      "                                       'algorithm for dividing a scene into '\n",
      "                                       'similar regions and a routine for '\n",
      "                                       'constructing a three dimensional world '\n",
      "                                       'model. Visual inference routines '\n",
      "                                       'interpret perspective, shadows, '\n",
      "                                       'highlights, occlusions, shading and '\n",
      "                                       'texture gradients, and monocular '\n",
      "                                       'motion parallax. Other visual '\n",
      "                                       'knowledge is added with long term '\n",
      "                                       'models and short term object '\n",
      "                                       'representations. The final program '\n",
      "                                       'will be tested on color photographs of '\n",
      "                                       'outdoor scenes using as input a series '\n",
      "                                       'of views of the same scene from '\n",
      "                                       'different angles which approximates '\n",
      "                                       'what an automaton would \"see\" as it '\n",
      "                                       'moves down a street.'},\n",
      "              'score': 0.588096142,\n",
      "              'values': []},\n",
      "             {'id': '5390878e20f70186a0d3b416',\n",
      "              'metadata': {'abstract': 'A model-based approach has been '\n",
      "                                       'proposed to make object recognition '\n",
      "                                       'computationally tractable. In this '\n",
      "                                       'approach, models associated with '\n",
      "                                       'objects expected to appear in the '\n",
      "                                       \"scene are recorded in the system's \"\n",
      "                                       'knowledge base. The system extracts '\n",
      "                                       'various features from the input images '\n",
      "                                       'using robust, low-level, '\n",
      "                                       'general-purpose operators. Finally, '\n",
      "                                       'matching is performed between the '\n",
      "                                       'image-derived features and the scene '\n",
      "                                       'domain models to recognize objects. '\n",
      "                                       'Factors affecting the successful '\n",
      "                                       'design and implementation of '\n",
      "                                       'model-based vision systems include the '\n",
      "                                       'ability to derive suitable object '\n",
      "                                       'models, the nature of image features '\n",
      "                                       'extracted by the operators, a '\n",
      "                                       'computationally effective matching '\n",
      "                                       'approach, knowledge representation '\n",
      "                                       'schemes, and effective control '\n",
      "                                       \"mechanisms for guiding the systems's \"\n",
      "                                       'overall operation. The vision system '\n",
      "                                       'they describe uses gray-scale images, '\n",
      "                                       'which can successfully handle complex '\n",
      "                                       'scenes with multiple object types.'},\n",
      "              'score': 0.587305486,\n",
      "              'values': []},\n",
      "             {'id': '539089d320f70186a0d9b0c3',\n",
      "              'metadata': {'abstract': 'Hardware, software tools, algorithms, '\n",
      "                                       'and performance metrics that have been '\n",
      "                                       'developed for image understanding are '\n",
      "                                       'presented. Three commercially built '\n",
      "                                       'examples reflecting three mature '\n",
      "                                       'approaches considered germane to '\n",
      "                                       'vision-single-instruction '\n",
      "                                       'multiple-data, multiple-instruction '\n",
      "                                       'multiple-data, and systolic '\n",
      "                                       'processing-were chosen. They are, '\n",
      "                                       'respectively, the Connection Machine, '\n",
      "                                       'the Butterfly, and the Warp. A fourth '\n",
      "                                       'approach, more specific to vision, was '\n",
      "                                       'also selected for noncommercial '\n",
      "                                       'implementation. This machine, the '\n",
      "                                       'Image-Understanding Architecture, '\n",
      "                                       'involves a heterogeneous combination '\n",
      "                                       'of parallel processors with '\n",
      "                                       'single-instruction multiple-data, '\n",
      "                                       'multiple-instruction multiple-data, '\n",
      "                                       'and other capabilities. Each site '\n",
      "                                       'employing one of the above '\n",
      "                                       'architectures developed a different '\n",
      "                                       'set of tools, leading to significant '\n",
      "                                       'cross-fertilization of ideas between '\n",
      "                                       'the sites. Algorithms for low-level '\n",
      "                                       'vision, shape from texture, fusing '\n",
      "                                       'stereo and texture, surface '\n",
      "                                       'interpolation, and robot navigation, '\n",
      "                                       'among others, are briefly discussed. '\n",
      "                                       'Benchmarks are described.'},\n",
      "              'score': 0.58675909,\n",
      "              'values': []},\n",
      "             {'id': '53908a4020f70186a0d9f0c5',\n",
      "              'metadata': {'abstract': 'Stereoscopic vision has a fundamental '\n",
      "                                       'role both for animals and humans. '\n",
      "                                       'Nonetheless, in the computer vision '\n",
      "                                       'literature there is limited reference '\n",
      "                                       'to biological models related to '\n",
      "                                       'stereoscopic vision and, in '\n",
      "                                       'particular, to the functional '\n",
      "                                       'properties and the organization of '\n",
      "                                       'binocular information within the '\n",
      "                                       'visual cortex. In this paper a simple '\n",
      "                                       'stereo technique, based on a space '\n",
      "                                       'variant mapping of the image data and '\n",
      "                                       'a multi-layered cortical stereoscopic '\n",
      "                                       'representation, mimicking the neural '\n",
      "                                       'organization of the early stages of '\n",
      "                                       'the human visual system, is proposed. '\n",
      "                                       'Radial disparity computed from a '\n",
      "                                       'stereo pair is used to map the '\n",
      "                                       'relative depth with respect to the '\n",
      "                                       'fixation point. A set of experiments '\n",
      "                                       'demonstrating the applicability of the '\n",
      "                                       'devised techniques is also presented.'},\n",
      "              'score': 0.585381806,\n",
      "              'values': []},\n",
      "             {'id': '5390882c20f70186a0d8c411',\n",
      "              'metadata': {'abstract': 'From the Publisher:with contributions '\n",
      "                                       'from Theo Papadopoulo Over the last '\n",
      "                                       'forty years, researchers have made '\n",
      "                                       'great strides in elucidating the laws '\n",
      "                                       'of image formation, processing, and '\n",
      "                                       'understanding by animals, humans, and '\n",
      "                                       'machines. This book describes the '\n",
      "                                       'state of knowledge in one subarea of '\n",
      "                                       'vision, the geometric laws that relate '\n",
      "                                       'different views of a scene. Geometry, '\n",
      "                                       'one of the oldest branches of '\n",
      "                                       'mathematics, is the natural language '\n",
      "                                       'for describing three-dimensional '\n",
      "                                       'shapes and spatial relations. '\n",
      "                                       'Projective geometry, the geometry that '\n",
      "                                       'best models image formation, provides '\n",
      "                                       'a unified framework for thinking about '\n",
      "                                       'many geometric problems relevant to '\n",
      "                                       'vision. The book formalizes and '\n",
      "                                       'analyzes the relations between '\n",
      "                                       'multiple views of a scene from the '\n",
      "                                       'perspective of various types of '\n",
      "                                       'geometries. A key feature is that it '\n",
      "                                       'considers Euclidean and affine '\n",
      "                                       'geometries as special cases of '\n",
      "                                       'projective geometry. Images play a '\n",
      "                                       'prominent role in computer '\n",
      "                                       'communications. Producers and users of '\n",
      "                                       'images, in particular '\n",
      "                                       'three-dimensional images, require a '\n",
      "                                       'framework for stating and solving '\n",
      "                                       'problems. The book offers a number of '\n",
      "                                       'conceptual tools and theoretical '\n",
      "                                       'results useful for the design of '\n",
      "                                       'machine vision algorithms. It also '\n",
      "                                       'illustrates these tools and results '\n",
      "                                       'with many examples of real '\n",
      "                                       'applications.'},\n",
      "              'score': 0.585118234,\n",
      "              'values': []},\n",
      "             {'id': '539090c420f70186a0ddde4c',\n",
      "              'metadata': {'abstract': 'The present work is based on the '\n",
      "                                       'Visual Routine theory of Shimon '\n",
      "                                       'Ullman. This theory holds that '\n",
      "                                       'efficient visual perception is managed '\n",
      "                                       'by first applying spatially parallel '\n",
      "                                       'methods to an initial input image in '\n",
      "                                       'order to construct the basic '\n",
      "                                       'representation-maps of features within '\n",
      "                                       'the image. Then, this phase is '\n",
      "                                       'followed by the application of serial '\n",
      "                                       'methods --- visual routines --- which '\n",
      "                                       'are applied to the most salient items '\n",
      "                                       'in these and other subsequently '\n",
      "                                       'created maps. Recent work in the '\n",
      "                                       'visual routine tradition is reviewed, '\n",
      "                                       'as well as relevant psychological work '\n",
      "                                       'on preattentive and attentive vision. '\n",
      "                                       'An analysis is made of the problem of '\n",
      "                                       'devising a visual routine language for '\n",
      "                                       'computing geometric properties and '\n",
      "                                       'relations. The most useful basic '\n",
      "                                       'representations to compute directly '\n",
      "                                       'from a world of 2-D geometric shapes '\n",
      "                                       'are determined. An argument is made '\n",
      "                                       'for the case that an experimental '\n",
      "                                       'program is required to establish which '\n",
      "                                       'basic operations and which methods for '\n",
      "                                       'controlling them will lead to the '\n",
      "                                       'efficient computation of geometric '\n",
      "                                       'properties and relations. A '\n",
      "                                       'description is given of an implemented '\n",
      "                                       'computer system which can correctly '\n",
      "                                       'compute, in images of simple 2-D '\n",
      "                                       'geometric shapes, the properties {\\\\em '\n",
      "                                       'vertical}, {\\\\em horizontal}, {\\\\em '\n",
      "                                       'closed}, and {\\\\em convex}, and the '\n",
      "                                       'relations {\\\\em inside}, {\\\\em '\n",
      "                                       'outside}, {\\\\em touching}, {\\\\em '\n",
      "                                       'centred-in}, {\\\\em connected}, {\\\\em '\n",
      "                                       'parallel}, and {\\\\em being-part-of}. '\n",
      "                                       'The visual routines which compute '\n",
      "                                       'these, the basic operations out of '\n",
      "                                       'which the visual routines are '\n",
      "                                       'composed, and the important logic '\n",
      "                                       'which controls the goal-directed '\n",
      "                                       'application of the routines to the '\n",
      "                                       'image are all described in detail. The '\n",
      "                                       'entire system is embedded in a '\n",
      "                                       'Question-and-Answer system which is '\n",
      "                                       'capable of answering questions of an '\n",
      "                                       'image, such as ``Find all the squares '\n",
      "                                       \"inside triangles'''' or ``Find all the \"\n",
      "                                       'vertical bars outside of closed convex '\n",
      "                                       \"shapes.'''' By asking many such \"\n",
      "                                       'questions about various test images, '\n",
      "                                       'the effectiveness of the visual '\n",
      "                                       'routines and their controlling logic '\n",
      "                                       'is demonstrated.'},\n",
      "              'score': 0.584074557,\n",
      "              'values': []},\n",
      "             {'id': '5390b0ca20f70186a0ed9506',\n",
      "              'metadata': {'abstract': 'The research presented in this paper '\n",
      "                                       'represents several novel conceptual '\n",
      "                                       'contributions to the computer vision '\n",
      "                                       'literature. In this position paper, '\n",
      "                                       'our goal is to define the scope of '\n",
      "                                       'computer vision analysis and discuss a '\n",
      "                                       'new categorisation of the computer '\n",
      "                                       'vision problem. We first provide a '\n",
      "                                       'novel decomposition of computer vision '\n",
      "                                       'into base components which we term the '\n",
      "                                       'axioms of vision. These are used to '\n",
      "                                       'define researcher-level and '\n",
      "                                       'developer-level access to vision '\n",
      "                                       'algorithms, in a way which does not '\n",
      "                                       'require expert knowledge of computer '\n",
      "                                       'vision. We discuss a new line of '\n",
      "                                       'thought for computer vision by basing '\n",
      "                                       'analyses on descriptions of the '\n",
      "                                       'problem instead of in terms of '\n",
      "                                       'algorithms. From this an abstraction '\n",
      "                                       'can be developed to provide a layer '\n",
      "                                       'above algorithmic details. This is '\n",
      "                                       'extended to the idea of a formal '\n",
      "                                       'description language which may be '\n",
      "                                       'automatically interpreted thus '\n",
      "                                       'allowing those not familiar with '\n",
      "                                       'computer vision techniques to utilise '\n",
      "                                       'sophisticated methods.'},\n",
      "              'score': 0.583681881,\n",
      "              'values': []},\n",
      "             {'id': '5390879920f70186a0d41e6f',\n",
      "              'metadata': {'abstract': 'An important research topic in AI is '\n",
      "                                       'the discovery of constraints. As '\n",
      "                                       'constraints. In this paper, we present '\n",
      "                                       'a model-directed image understanding '\n",
      "                                       'prototype to support computer vision, '\n",
      "                                       'which can recover constraints of an '\n",
      "                                       'image by means of the spatial relation '\n",
      "                                       'and spatial reasoning. according to a '\n",
      "                                       'model. In the prototype, an object is '\n",
      "                                       'viewed as an arrangement of two types '\n",
      "                                       'components, viz., crucial component '\n",
      "                                       'and ordinary component. Under the '\n",
      "                                       'direction of models, these components '\n",
      "                                       'are looked for in a top-down manner so '\n",
      "                                       'that the image can be recognized. For '\n",
      "                                       'this purpose, knowledge representation '\n",
      "                                       'and organization of knowledge base are '\n",
      "                                       'also presented. Some problems on this '\n",
      "                                       'research are discussed and some '\n",
      "                                       'reasons of this research as '\n",
      "                                       'conclusions are also given to serve '\n",
      "                                       'the future research.'},\n",
      "              'score': 0.582981169,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda92d',\n",
      "              'metadata': {'abstract': 'A CONSTRAINT-BASED APPROACH TO '\n",
      "                                       'UNIFORMLY COMBINING INFORMATION FROM '\n",
      "                                       'MULTIPLE REPRESENTATIONS AND SOURCES '\n",
      "                                       'OF SENSORY DATA IS DESCRIBED. THE '\n",
      "                                       'APPROACH IS IMPORTANT TO RESEARCH IN '\n",
      "                                       'INTERMEDIATE GROUPING, KNOWLEDGE-BASED '\n",
      "                                       'MODEL MATCHING, AND INFORMATION '\n",
      "                                       'FUSION. THE TECHNIQUES PRESENTED '\n",
      "                                       'EXTEND THE CAPABILITIES OF AN EARLIER '\n",
      "                                       'SYSTEM THAT APPLIED CONSTRAINTS TO '\n",
      "                                       'ATTRI- BUTES OF SINGLE TYPES OF '\n",
      "                                       'EXTRACTED IMAGE EVENTS CALLED TOKENS. '\n",
      "                                       'RELATIONAL MEASURES ARE DEFINED '\n",
      "                                       'BETWEEN SYMBOLIC TOKENS SO THAT SETS '\n",
      "                                       'OF TOKENS ACROSS REPRESENTATIONS CAN '\n",
      "                                       'BE SELECTED AND GROUPED ON THE BASIS '\n",
      "                                       'OF CONSTRAINT FUNC TIONS APPLIED TO '\n",
      "                                       'THESE RELATIONAL MEASURES. SINCE '\n",
      "                                       'TYPICAL LOW-LEVEL REPRESENTATIONS '\n",
      "                                       'INVOLVE HUNDREDS OR THOUSANDS OF '\n",
      "                                       'TOKENS IN EACH REPRESENTATION, EVEN '\n",
      "                                       'BINARY RELATIONAL MEASURES CAN INVOLVE '\n",
      "                                       'VERY LARGE NUMBERS OF TOKEN PAIRS. '\n",
      "                                       'CONTROL STRATEGIES FOR ORDERING AND '\n",
      "                                       'FILTERING TOKENS, BASED UPON '\n",
      "                                       'CONSTRAINTS ON TOKEN ATTRIBUTES AND '\n",
      "                                       'TOKEN RELATIONSHIPS, CAN BE FORMED TO '\n",
      "                                       'REDUCE THE COMPUTATION INVOLVED IN '\n",
      "                                       'PRODUC- ING TOKEN AGGREGATIONS. THEY '\n",
      "                                       'SYSTEM IS DEMONSTRATED USING REGION '\n",
      "                                       'AND LINE DATA AND AN ASSOCIATED SET OF '\n",
      "                                       'RELATIONAL MEASURES. THE APPROACH CAN '\n",
      "                                       'BE NATURALLY EXTENDED TO INCLUDE '\n",
      "                                       'TOKENS EXTRACTED FROM MOTION, STEREO, '\n",
      "                                       'AND RANGE DATA.'},\n",
      "              'score': 0.5829252,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda7e4',\n",
      "              'metadata': {'abstract': 'IMAGE UNDERSTANDING RESEARCH AT THE '\n",
      "                                       'UNIVERSITY OF MASSACHUSETTS ENCOM- '\n",
      "                                       'PASSES A RANGE OF RESEARCH, MOST OF '\n",
      "                                       'WHICH IS DIRECTED TOWARDS THE INTEGRA- '\n",
      "                                       'TION OF A DIVERSE SET OF PROCESSES TO '\n",
      "                                       'ACHIEVE A GENERAL REAL-TIME KNOWLEDGE '\n",
      "                                       '-BASED INTERPRETATION SYSTEM. IN '\n",
      "                                       'PARTICULAR WE ARE CONCENTRATING ON '\n",
      "                                       'INTE- GRATING PROJECTS INVOLVING '\n",
      "                                       'OBJECT IDENTIFICATION IN STATIC '\n",
      "                                       'IMAGES, DEPTH RECOVERY FROM MOTION '\n",
      "                                       'ANALYSIS, A REAL-TIME PARALLEL '\n",
      "                                       'ARCHITECTURE, AND MOBILE VEHICLE '\n",
      "                                       'NAVIGATION. THIS SYSTEM WILL BE '\n",
      "                                       'APPLIED TO A VARIETY OF TASK DOMAINS '\n",
      "                                       'OF NATURAL SCENES INCLUDING ROAD '\n",
      "                                       'SCENES AND AERIAL IMAGES, AND WILL '\n",
      "                                       'ALSO BE USED TO CONTROL A MOBILE ROBOT '\n",
      "                                       'MOVING THROUGH BOTH KNOWN AND UNKNOWN '\n",
      "                                       'OUTDOOR DOMAINS. THIS SUMMARY '\n",
      "                                       'DOCUMENTS SEVERAL AREAS OF RESEARCH AT '\n",
      "                                       'THE UNIVERSITY OF MASSACHUSETTS THAT '\n",
      "                                       'ARE ENTIRELY OR PARTIALLY SUPPORTED '\n",
      "                                       'UNDER THE DARPA IMAGE UNDERSTANDING '\n",
      "                                       'PROGRAM. THE WORK, MUCH OF WHICH IS '\n",
      "                                       'DOCUMENTED IN PAPERS IN THESE '\n",
      "                                       'PROCEEDINGS, IS DIVIDED INTO SEVERAL '\n",
      "                                       'AREAS: 1.) KNOWLEDGE-BASED VISION; 2.) '\n",
      "                                       'PERCEPTUAL ORGANIZATION (INTERMEDIATE '\n",
      "                                       'PROCESSING); 3.) 3D MODELS, MATCHING, '\n",
      "                                       'AND SURFACE RECOVERY; 4.) MOBILE ROBOT '\n",
      "                                       'NAVIGATION; 5.) IMAGE UNDERSTANDING '\n",
      "                                       'ARCHITECTURE; 6.) MOTION ANALYSIS; AND '\n",
      "                                       '7.) LOW-LEVEL VISION'},\n",
      "              'score': 0.581397355,\n",
      "              'values': []},\n",
      "             {'id': '5390a9a520f70186a0ea5c62',\n",
      "              'metadata': {'abstract': 'Image understanding and image '\n",
      "                                       'semantics processing have recently '\n",
      "                                       'become an issue of critical importance '\n",
      "                                       'in computer vision R&D. Biological '\n",
      "                                       'vision has always considered them as '\n",
      "                                       'an enigmatic mixture of perceptual and '\n",
      "                                       'cognitive processing faculties. In its '\n",
      "                                       'impetuous and rash development, '\n",
      "                                       'computer vision without any '\n",
      "                                       'hesitations has adopted this stance. I '\n",
      "                                       'will argue that such a segregation of '\n",
      "                                       'image processing faculties is wrong, '\n",
      "                                       'both for the biological and the '\n",
      "                                       'computer vision. My conjecture is that '\n",
      "                                       'images contain only one sort of '\n",
      "                                       'information - the perceptual '\n",
      "                                       '(physical) information, which can be '\n",
      "                                       'discovered in an image and elicited '\n",
      "                                       'for further processing. Cognitive '\n",
      "                                       '(semantic) information is not a part '\n",
      "                                       'of image-conveyed information. It '\n",
      "                                       'belongs to a human observer that '\n",
      "                                       'acquires and interprets the image. '\n",
      "                                       'Relying on a new definition of '\n",
      "                                       '\"information\", which can be derived '\n",
      "                                       \"from Kolmogorov's complexity theory \"\n",
      "                                       \"and Chaitin's notion of algorithmic \"\n",
      "                                       'information, I propose a unifying '\n",
      "                                       'framework for visual information '\n",
      "                                       'processing, which explicitly accounts '\n",
      "                                       'for perceptual and cognitive image '\n",
      "                                       'processing peculiarities. I believe, '\n",
      "                                       'it would provide better scaffolding '\n",
      "                                       'for modeling visual information '\n",
      "                                       'processing in human brain.'},\n",
      "              'score': 0.581328,\n",
      "              'values': []},\n",
      "             {'id': '5390a9a520f70186a0ea5c78',\n",
      "              'metadata': {'abstract': 'Binocular information about the '\n",
      "                                       'structure of a scene is contained in '\n",
      "                                       'the relative positions of '\n",
      "                                       'corresponding points in the two views. '\n",
      "                                       'If the eyes rotate, in order to fixate '\n",
      "                                       'a different target, then the disparity '\n",
      "                                       'at a given image location is likely to '\n",
      "                                       'change. Quite different disparities '\n",
      "                                       'can be produced at the same location, '\n",
      "                                       'as the eyes move from one '\n",
      "                                       'fixation-point to the next. The '\n",
      "                                       'pointwise variability of the disparity '\n",
      "                                       'map is problematic for biological '\n",
      "                                       'visual systems, in which stereopsis is '\n",
      "                                       'based on simple, short-range '\n",
      "                                       'mechanisms. It is argued here that the '\n",
      "                                       'problem can be addressed in two ways; '\n",
      "                                       'firstly by an appropriate '\n",
      "                                       'representation of disparity, and '\n",
      "                                       'secondly by learning the typical '\n",
      "                                       'pattern of image correspondences. It '\n",
      "                                       'is shown that the average spatial '\n",
      "                                       'structure of the disparity field can '\n",
      "                                       'be estimated, by integrating over a '\n",
      "                                       'series of binocular fixations. An '\n",
      "                                       'algorithm based on this idea is tested '\n",
      "                                       'on natural images. Finally, it is '\n",
      "                                       'shown how the average pattern of '\n",
      "                                       'disparities could help to put the '\n",
      "                                       'images into binocular correspondence.'},\n",
      "              'score': 0.581205845,\n",
      "              'values': []},\n",
      "             {'id': '5390b24420f70186a0ee850a',\n",
      "              'metadata': {'abstract': 'This paper proposes a model for image '\n",
      "                                       'representation and image analysis '\n",
      "                                       'using a multi-layer neural network, '\n",
      "                                       'which is rooted in the human vision '\n",
      "                                       'system. Having complex neural layers '\n",
      "                                       'to represent and process information, '\n",
      "                                       'the biological vision system is far '\n",
      "                                       'more efficient than machine vision '\n",
      "                                       'system. The neural model simulate '\n",
      "                                       'non-classical receptive field of '\n",
      "                                       'ganglion cell and its local feedback '\n",
      "                                       'control circuit, and can represent '\n",
      "                                       'images, beyond pixel level, '\n",
      "                                       'self-adaptively and regularly. The '\n",
      "                                       'results of experiments, rebuilding, '\n",
      "                                       'distribution and contour detection, '\n",
      "                                       'prove this method can represent image '\n",
      "                                       'faithfully with low cost, and can '\n",
      "                                       'produce a compact and abstract '\n",
      "                                       'approximation to facilitate successive '\n",
      "                                       'image segmentation and integration. '\n",
      "                                       'This representation schema is good at '\n",
      "                                       'extracting spatial relationships from '\n",
      "                                       'different components of images and '\n",
      "                                       'highlighting foreground objects from '\n",
      "                                       'background, especially for nature '\n",
      "                                       'images with complicated scenes. '\n",
      "                                       'Further it can be applied to object '\n",
      "                                       'recognition or image classification '\n",
      "                                       'tasks in future.'},\n",
      "              'score': 0.581125557,\n",
      "              'values': []},\n",
      "             {'id': '53908e0020f70186a0dd65f6',\n",
      "              'metadata': {'abstract': 'A computer may gather a lot of '\n",
      "                                       'information from its environment in an '\n",
      "                                       'optical or graphical manner. A scene, '\n",
      "                                       'as seen for instance from a TV camera '\n",
      "                                       'or a picture, can be transformed into '\n",
      "                                       'a symbolic description of points and '\n",
      "                                       'lines or surfaces. This thesis '\n",
      "                                       'describes several programs, written in '\n",
      "                                       'the language CONVERT , for the '\n",
      "                                       'analysis of such descriptions in order '\n",
      "                                       'to recognize, differentiate and '\n",
      "                                       'identify desired objects or classes or '\n",
      "                                       'objects in the scene. Examples are '\n",
      "                                       'given in each case. Although the '\n",
      "                                       'recognition might be in terms of '\n",
      "                                       'projections of 2-dim and 3-dim '\n",
      "                                       'objects, we do not deal with '\n",
      "                                       'stereoscopic information. One of our '\n",
      "                                       'programs (Polybrick) identifies '\n",
      "                                       'parallelepipeds in a scene which may '\n",
      "                                       'contain hidden bodies and '\n",
      "                                       'non-parallelepipedic objects. The '\n",
      "                                       'program TD works mainly with '\n",
      "                                       '2-dimensional figures, although under '\n",
      "                                       'certain conditions successfully '\n",
      "                                       'identifies 3-dim objects. Overlapping '\n",
      "                                       'objects are identified when they are '\n",
      "                                       'transparent. A third program, DT, '\n",
      "                                       'works with 3-dim and 2-dim objects, '\n",
      "                                       'and does not identify objects which '\n",
      "                                       'are not completely seen. Important '\n",
      "                                       'restrictions and suppositions are: (a) '\n",
      "                                       'the input is assumed perfect '\n",
      "                                       '(noiseless), and in a symbolic format; '\n",
      "                                       '(b) no perspective deformation is '\n",
      "                                       'considered. A portion of this thesis '\n",
      "                                       'is devoted to the study of models '\n",
      "                                       '(symbolic representation) of the '\n",
      "                                       'objects we want to identify; different '\n",
      "                                       'schemes, some of them already in use, '\n",
      "                                       'are discussed. Focusing our attention '\n",
      "                                       'on the more general problem of '\n",
      "                                       'identification of general objects when '\n",
      "                                       'they substantially overlap, we propose '\n",
      "                                       'some schemes for their recognition, '\n",
      "                                       'and also analyze some problems that '\n",
      "                                       'are met.'},\n",
      "              'score': 0.580991,\n",
      "              'values': []},\n",
      "             {'id': '5390a1bc20f70186a0e562f3',\n",
      "              'metadata': {'abstract': 'This paper presents a '\n",
      "                                       'biologically-inspired artificial '\n",
      "                                       'vision system. The goal of the '\n",
      "                                       'proposed vision system is to correctly '\n",
      "                                       'match regions among several images to '\n",
      "                                       'obtain scenes matching. Based on works '\n",
      "                                       'that consider that humans perceive '\n",
      "                                       'visual objects divided in its '\n",
      "                                       'cons-tituent parts, we assume that a '\n",
      "                                       'particular type of regions, called '\n",
      "                                       'curvilinear regions, can be easily '\n",
      "                                       'detected in digital images. These '\n",
      "                                       'features are more complex than the '\n",
      "                                       'basic features that human vision uses '\n",
      "                                       'in the very first steps in the visual '\n",
      "                                       'process. We assume that the '\n",
      "                                       'curvilinear regions can be compared in '\n",
      "                                       'their complexity to those features '\n",
      "                                       'analysed by the IT cortex for '\n",
      "                                       'achieving objects recognition. The '\n",
      "                                       'approach of our system is similar to '\n",
      "                                       'other existing methods that also use '\n",
      "                                       'intermediate complexity features for '\n",
      "                                       'achieving visual matching. The novelty '\n",
      "                                       'of our system is the curvilinear '\n",
      "                                       'features that we use.'},\n",
      "              'score': 0.580594361,\n",
      "              'values': []},\n",
      "             {'id': '53908bde20f70186a0dc8923',\n",
      "              'metadata': {'abstract': 'D.A. Forsyth and M.M. Fleck Abstract: '\n",
      "                                       'This paper describes a new '\n",
      "                                       'representation for people and animals, '\n",
      "                                       'called a body plan. The representation '\n",
      "                                       'is an organized collection of grouping '\n",
      "                                       'hints obtained from constraints on '\n",
      "                                       'color, texture, shape, and geometrical '\n",
      "                                       'relations. Body plans can be learned '\n",
      "                                       'from image data, using established '\n",
      "                                       'statistical learning techniques. Body '\n",
      "                                       'plans are well adapted to segmentation '\n",
      "                                       'and recognition in complex '\n",
      "                                       'environments, such as the huge '\n",
      "                                       'libraries of digitized images now '\n",
      "                                       'becoming widely available. Two '\n",
      "                                       'specific applications of body plans '\n",
      "                                       'are presented: an algorithm that '\n",
      "                                       'determines whether an image depicts a '\n",
      "                                       'scantily clad human and an algorithm '\n",
      "                                       'that learns and uses a body plan to '\n",
      "                                       'find pictures of horses. Both '\n",
      "                                       'algorithms demonstrate excellent '\n",
      "                                       'performance on large, poorly '\n",
      "                                       'controlled input data.'},\n",
      "              'score': 0.580430269,\n",
      "              'values': []},\n",
      "             {'id': '5390a1e620f70186a0e59e45',\n",
      "              'metadata': {'abstract': 'Our goal is to organize the image '\n",
      "                                       'contents semantically. In this paper, '\n",
      "                                       'we propose a method to classify the '\n",
      "                                       'images semantically, using the C-Fuzzy '\n",
      "                                       'algorithm to segment the natural '\n",
      "                                       'scenes into perceptually uniform '\n",
      "                                       'regions. The low-level characteristics '\n",
      "                                       'that are taken into account are: '\n",
      "                                       'color, texture, shape, absolute '\n",
      "                                       'spatial arrangement, spatial '\n",
      "                                       'coherency, and dimension. Since humans '\n",
      "                                       'are the ultimate users of most image '\n",
      "                                       'retrieval systems, it is important to '\n",
      "                                       'organize the contents semantically, '\n",
      "                                       'according to meaningful categories. '\n",
      "                                       'This requires an understanding of the '\n",
      "                                       'important semantic categories that '\n",
      "                                       'humans use for image classification, '\n",
      "                                       'and the extraction of meaningful image '\n",
      "                                       'features that can discriminate between '\n",
      "                                       'these categories. A lot of '\n",
      "                                       'experiments, in which the human '\n",
      "                                       'subjects had to group images into '\n",
      "                                       'semantic categories and to explain the '\n",
      "                                       'criteria for their choice, were '\n",
      "                                       'realized. From these experiments, we '\n",
      "                                       'identify the semantic categories '\n",
      "                                       '(landscapes, animals, flowers, etc), '\n",
      "                                       'the semantic indicators or '\n",
      "                                       'intermediate descriptors and their '\n",
      "                                       'visual characteristics.'},\n",
      "              'score': 0.579344749,\n",
      "              'values': []},\n",
      "             {'id': '5390879220f70186a0d3b563',\n",
      "              'metadata': {'abstract': 'The authors describe an approach to '\n",
      "                                       'perceptual grouping for detecting and '\n",
      "                                       'describing 3-D objects in complex '\n",
      "                                       'images and apply it to the task of '\n",
      "                                       'detecting and describing complex '\n",
      "                                       'buildings in aerial images. They argue '\n",
      "                                       'that representations of structural '\n",
      "                                       'relationships in the arrangements of '\n",
      "                                       'primitive image features, as detected '\n",
      "                                       'by the perceptual organization '\n",
      "                                       'process, are essential for analyzing '\n",
      "                                       'complex imagery. They term these '\n",
      "                                       'representations collated features. The '\n",
      "                                       'choice of collated features is '\n",
      "                                       'determined by the generic shape of the '\n",
      "                                       'desired objects in the scene. The '\n",
      "                                       'detection process for collated '\n",
      "                                       'features is more robust than the local '\n",
      "                                       'operations for region segmentation and '\n",
      "                                       'contour tracing. The important '\n",
      "                                       'structural information encoded in '\n",
      "                                       'collated features aids various visual '\n",
      "                                       'tasks such as object segmentation, '\n",
      "                                       'correspondence processes, and shape '\n",
      "                                       'description. The proposed method '\n",
      "                                       'initially detects all reasonable '\n",
      "                                       'feature groupings. A constraint '\n",
      "                                       'satisfaction network is then used to '\n",
      "                                       'model the complex interactions between '\n",
      "                                       'the collations and select the '\n",
      "                                       'promising ones. Stereo matching is '\n",
      "                                       'performed on the collations to obtain '\n",
      "                                       'height information. This aids in '\n",
      "                                       'further reasoning on the collated '\n",
      "                                       'features and results in the 3-D '\n",
      "                                       'description of the desired objects.'},\n",
      "              'score': 0.579065382,\n",
      "              'values': []},\n",
      "             {'id': '53908f5c20f70186a0ddb19e',\n",
      "              'metadata': {'abstract': 'A novel approach to object recognition '\n",
      "                                       'and scene analysis based on neural '\n",
      "                                       'network representation of visual '\n",
      "                                       'schemas is described. Given an input '\n",
      "                                       'scene, the VISOR system focuses '\n",
      "                                       'attention successfully at each '\n",
      "                                       'component, and the schema '\n",
      "                                       'representations cooperate and compete '\n",
      "                                       'to match the inputs. The schema '\n",
      "                                       'hierarchy is learned from examples '\n",
      "                                       'through unsupervised adaptation and '\n",
      "                                       'reinforcement learning. VISOR learns '\n",
      "                                       'that some objects are more important '\n",
      "                                       'than others in identifying a scene. It '\n",
      "                                       'learns three types of visual schemas: '\n",
      "                                       '(1) rigid spatial layouts of '\n",
      "                                       'components used primarily for '\n",
      "                                       'describing objects; (2) collections of '\n",
      "                                       'components located anywhere in the '\n",
      "                                       'scene for recognizing certain man-made '\n",
      "                                       'scenes (such as a dining table); and '\n",
      "                                       '(3) rough spatial layouts of regions '\n",
      "                                       'of uniform texture and no specific '\n",
      "                                       'shape that are often found in natural '\n",
      "                                       'scenes (such as a road scene). '\n",
      "                                       'Compared to traditional rule-based '\n",
      "                                       'systems, VISOR shows remarkable '\n",
      "                                       'robustness of recognition, and is able '\n",
      "                                       'to indicate the confidence of its '\n",
      "                                       'analysis as the inputs differ '\n",
      "                                       'increasingly from the schemas. With '\n",
      "                                       'such properties, VISOR is a promising '\n",
      "                                       'first step towards a general vision '\n",
      "                                       'system that can be used in different '\n",
      "                                       'applications after learning the '\n",
      "                                       'application-specific schemas.'},\n",
      "              'score': 0.578873456,\n",
      "              'values': []},\n",
      "             {'id': '5390bd1520f70186a0f44ff0',\n",
      "              'metadata': {'abstract': 'Machine learning algorithms have been '\n",
      "                                       'successfully utilized in various '\n",
      "                                       'systems/devices. They have the ability '\n",
      "                                       'to improve the usability/quality of '\n",
      "                                       'such systems in terms of intelligent '\n",
      "                                       'user interface, fast performance, and '\n",
      "                                       'more importantly, high accuracy. In '\n",
      "                                       'this research, machine learning '\n",
      "                                       'techniques are used in the field of '\n",
      "                                       'image understanding, which is a common '\n",
      "                                       'research area between image analysis '\n",
      "                                       'and computer vision, to involve higher '\n",
      "                                       'processing level of a target image to '\n",
      "                                       '“make sense” of the scene captured in '\n",
      "                                       'it. A general probabilistic framework '\n",
      "                                       'for image understanding where topics '\n",
      "                                       'associated with (i) collection of '\n",
      "                                       'images to generate a comprehensive and '\n",
      "                                       'valid database, (ii) generation of an '\n",
      "                                       'unbiased ground-truth for the '\n",
      "                                       'aforesaid database, (iii) selection of '\n",
      "                                       'classification features and '\n",
      "                                       'elimination of the redundant ones, and '\n",
      "                                       '(iv) usage of such information to test '\n",
      "                                       'a new sample set, are discussed. Two '\n",
      "                                       'research projects have been developed '\n",
      "                                       'as examples of the general image '\n",
      "                                       'understanding framework; '\n",
      "                                       'identification of region(s) of '\n",
      "                                       'interest, and image segmentation '\n",
      "                                       'evaluation. These techniques, in '\n",
      "                                       'addition to others, are combined in an '\n",
      "                                       'object-oriented rendering system for '\n",
      "                                       'printing applications. The discussion '\n",
      "                                       'included in this doctoral dissertation '\n",
      "                                       'explores the means for developing such '\n",
      "                                       'a system from an image understanding/ '\n",
      "                                       'processing aspect.It is worth noticing '\n",
      "                                       'that this work does not aim to develop '\n",
      "                                       'a printing system. It is only proposed '\n",
      "                                       'to add some essential features for '\n",
      "                                       'current printing pipelines to achieve '\n",
      "                                       'better visual quality while printing '\n",
      "                                       'images/photos. Hence, we assume that '\n",
      "                                       'image regions have been successfully '\n",
      "                                       'extracted from the printed document. '\n",
      "                                       'These images are used as input to the '\n",
      "                                       'proposed object-oriented rendering '\n",
      "                                       'algorithm where methodologies for '\n",
      "                                       'color image segmentation, '\n",
      "                                       'region-of-interest identification and '\n",
      "                                       'semantic features extraction are '\n",
      "                                       'employed. Probabilistic approaches '\n",
      "                                       'based on Bayesian statistics have been '\n",
      "                                       'utilized to develop the proposed image '\n",
      "                                       'understanding techniques.'},\n",
      "              'score': 0.578837633,\n",
      "              'values': []},\n",
      "             {'id': '5390a28020f70186a0e62930',\n",
      "              'metadata': {'abstract': 'We consider visual scenes composed by '\n",
      "                                       'the optical image of a group of '\n",
      "                                       'bodies. When such a scene is \"seen\" by '\n",
      "                                       'a computer through a film spot '\n",
      "                                       'scanner, image dissector, or similar '\n",
      "                                       'device, it can be treated as a '\n",
      "                                       'two-dimensional array of numbers, or '\n",
      "                                       'as a function of two variables.'},\n",
      "              'score': 0.578819275,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dd9390',\n",
      "              'metadata': {'abstract': 'In this paper we describe a fast, '\n",
      "                                       'feature-driven program for extracting '\n",
      "                                       'depth information from stereoscopic '\n",
      "                                       'sets of digitized TV images. This is '\n",
      "                                       'achieved by two means: in the simplest '\n",
      "                                       'case, by statistically correlating '\n",
      "                                       'variable-sized windows on the basis of '\n",
      "                                       'visual texture, and in the more '\n",
      "                                       'complex case by pre-processing the '\n",
      "                                       'images to extract significant visual '\n",
      "                                       'features such as corners, and then '\n",
      "                                       'using these features to control the '\n",
      "                                       'correlation process. The program runs '\n",
      "                                       'on the PDP-10 but uses a PDP-11/45 and '\n",
      "                                       'an SPS-41 Signal Processing Computer '\n",
      "                                       'as subsidiary processors. The use of '\n",
      "                                       'the two small, fast machines for the '\n",
      "                                       'performance of simple but '\n",
      "                                       'often-repeated computations effects an '\n",
      "                                       'increase in speed sufficient to allow '\n",
      "                                       'us to think of using this program as a '\n",
      "                                       'fast 3-dimensional segmentation '\n",
      "                                       'method, preparatory to more complex '\n",
      "                                       'image processing. It is also intended '\n",
      "                                       'for use in visual feedback tasks '\n",
      "                                       'involved in hand-eye coordination and '\n",
      "                                       'automated assembly. The current '\n",
      "                                       'program is able to calculate the '\n",
      "                                       'three-dimensional positions of 10 '\n",
      "                                       'points to within 5 millimeters, using '\n",
      "                                       '5 seconds of computation for '\n",
      "                                       'extracting features, 1 second per '\n",
      "                                       'image for correlation, and 0.1 second '\n",
      "                                       'for the depth calculation.'},\n",
      "              'score': 0.578758538,\n",
      "              'values': []},\n",
      "             {'id': '555a1d570cf2b21909ba35cf',\n",
      "              'metadata': {'abstract': 'An approach for scene understanding '\n",
      "                                       'based on qualitative descriptors, '\n",
      "                                       'domain knowledge and logics is '\n",
      "                                       'proposed in this paper. Qualitative '\n",
      "                                       'descriptors, qualitative models of '\n",
      "                                       'shape, colour, topology and location '\n",
      "                                       'are used for describing any object in '\n",
      "                                       'the scene. Two kinds of domain '\n",
      "                                       'knowledge are provided: (i) '\n",
      "                                       'categorizations of objects according '\n",
      "                                       'to their qualitative descriptors, and '\n",
      "                                       '(ii) semantics for describing the '\n",
      "                                       'affordances, mobility and other '\n",
      "                                       'functional properties of target '\n",
      "                                       'objects. First order logics are '\n",
      "                                       'obtained for reasoning and scene '\n",
      "                                       'understanding. Tests were carried out '\n",
      "                                       'at the Interact@Cartesium scenario and '\n",
      "                                       'promising results were obtained.'},\n",
      "              'score': 0.578746259,\n",
      "              'values': []},\n",
      "             {'id': '5390a80f20f70186a0e97333',\n",
      "              'metadata': {'abstract': 'The increasing availability and '\n",
      "                                       'deployment of imaging sensors '\n",
      "                                       'operating in multiple spectral bands '\n",
      "                                       'has led to a large research effort in '\n",
      "                                       'image fusion, resulting in a plethora '\n",
      "                                       'of pixel-level image fusion '\n",
      "                                       'algorithms. However, the cognitive '\n",
      "                                       'aspects of multisensor image fusion '\n",
      "                                       'have not received much attention in '\n",
      "                                       'the development of these methods. In '\n",
      "                                       'this study we investigate how humans '\n",
      "                                       'interpret visual and infrared images, '\n",
      "                                       'and we compare the interpretation of '\n",
      "                                       'these individual image modalities to '\n",
      "                                       'their fused counterparts, for '\n",
      "                                       'different image fusion schemes. This '\n",
      "                                       'was done in an attempt to test to what '\n",
      "                                       'degree image fusion schemes can '\n",
      "                                       'enhance human perception of the '\n",
      "                                       'structural layout and composition of '\n",
      "                                       'realistic outdoor scenes. We asked '\n",
      "                                       'human observers to manually segment '\n",
      "                                       'the details they perceived as most '\n",
      "                                       'prominent in a set of corresponding '\n",
      "                                       'visual, infrared and fused images. For '\n",
      "                                       'each scene, the segmentations of the '\n",
      "                                       'individual input image modalities were '\n",
      "                                       'used to derive a joint reference '\n",
      "                                       \"(''gold standard'') contour image that \"\n",
      "                                       'represents the visually most salient '\n",
      "                                       'details from both of these modalities '\n",
      "                                       'and for that particular scene. The '\n",
      "                                       'resulting reference images were then '\n",
      "                                       'used to evaluate the manual '\n",
      "                                       'segmentations of the fused images, '\n",
      "                                       'using a precision-recall measure as '\n",
      "                                       'the evaluation criterion. In this '\n",
      "                                       'sense, the best fusion method provides '\n",
      "                                       'the largest number of correctly '\n",
      "                                       'perceived details (originating from '\n",
      "                                       'each of the individual modalities that '\n",
      "                                       'were used as input for the fusion '\n",
      "                                       'scheme) and the smallest amount of '\n",
      "                                       'false alarms (fusion artifacts or '\n",
      "                                       'illusory details). A comparison with '\n",
      "                                       'an objective score of subject '\n",
      "                                       'performance indicates that the '\n",
      "                                       'reference contour method indeed '\n",
      "                                       'appears to characterize the '\n",
      "                                       'performance of observers using the '\n",
      "                                       'results of the fusion schemes. The '\n",
      "                                       'results show that this evaluation '\n",
      "                                       'method can provide valuable insight '\n",
      "                                       'into the way fusion schemes combine '\n",
      "                                       'perceptually important details from '\n",
      "                                       'the individual input image modalities. '\n",
      "                                       'Given a reference contour image, the '\n",
      "                                       'method can potentially be used to '\n",
      "                                       'design image fusion schemes that are '\n",
      "                                       'optimally tuned to human visual '\n",
      "                                       'perception for different applications '\n",
      "                                       'and scenarios (e.g. environmental or '\n",
      "                                       'weather conditions).'},\n",
      "              'score': 0.578657329,\n",
      "              'values': []},\n",
      "             {'id': '53908d6520f70186a0dd0ed3',\n",
      "              'metadata': {'abstract': 'Computational approaches to simulate '\n",
      "                                       'human beings interpreting pictures are '\n",
      "                                       'important for understanding perceptual '\n",
      "                                       'Gestalt and for building computer '\n",
      "                                       'systems that support visual '\n",
      "                                       'communication. Based on the minimum '\n",
      "                                       'principle, we present a new approach '\n",
      "                                       'to the interpretation of pictures. Our '\n",
      "                                       'contribution is that we developed a '\n",
      "                                       'novel way in which the geometrical '\n",
      "                                       'information is calculated. In our '\n",
      "                                       'approach, geometrical shapes are '\n",
      "                                       'divided into various sorts and the '\n",
      "                                       'sorts are organised into a '\n",
      "                                       'hierarchical structure. A sort '\n",
      "                                       'together with a number of points '\n",
      "                                       'determines an actual graphical object. '\n",
      "                                       'So, the objects themselves can be '\n",
      "                                       'represented by the combination of '\n",
      "                                       'their sort with certain points that '\n",
      "                                       'code their position in the field and '\n",
      "                                       'whatever other attributes they may '\n",
      "                                       'possess as members of the sort. The '\n",
      "                                       'geometrical information load is '\n",
      "                                       'calculated as the number of points '\n",
      "                                       'which are needed in the '\n",
      "                                       'representation. Pictures are '\n",
      "                                       'represented as a set of graphical '\n",
      "                                       'objects. There are no other '\n",
      "                                       'requirements on the input pictures. As '\n",
      "                                       'long as the objects in a list are '\n",
      "                                       'well-formed terms, interpretation can '\n",
      "                                       'start. An inference mechanism reduces '\n",
      "                                       'the terms in the list into terms which '\n",
      "                                       'have the lowest information load. The '\n",
      "                                       'deduced list of objects is the '\n",
      "                                       'interpretation of the picture.'},\n",
      "              'score': 0.578383565,\n",
      "              'values': []},\n",
      "             {'id': '5390bfa220f70186a0f54e53',\n",
      "              'metadata': {'abstract': 'In this groundbreaking new volume, '\n",
      "                                       'computer researchers discuss the '\n",
      "                                       'development of technologies and '\n",
      "                                       'specific systems that can interpret '\n",
      "                                       'data with respect to domain knowledge. '\n",
      "                                       'Although the chapters each illuminate '\n",
      "                                       'different aspects of image '\n",
      "                                       'interpretation, all utilize a common '\n",
      "                                       'approach - one that asserts such '\n",
      "                                       'interpretation must involve perceptual '\n",
      "                                       'learning in terms of automated '\n",
      "                                       'knowledge acquisition and application, '\n",
      "                                       'as well as feedback and consistency '\n",
      "                                       'checks between encoding, feature '\n",
      "                                       'extraction, and the known knowledge '\n",
      "                                       'structures in a given application '\n",
      "                                       'domain. The text is profusely '\n",
      "                                       'illustrated with numerous figures and '\n",
      "                                       'tables to reinforce the concepts '\n",
      "                                       'discussed.'},\n",
      "              'score': 0.578324258,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda540',\n",
      "              'metadata': {'abstract': 'THIS PAPER SUMMARIZES SEVERAL AREAS OF '\n",
      "                                       'RESEARCH AT THE UNIVERSITY OF '\n",
      "                                       'MASSACHUSETTS THAT ARE PARTIALLY OR '\n",
      "                                       'ENTIRELY SUPPORTED UNDER THE DARPA '\n",
      "                                       'IMAGE UNDERSTANDING PROGRAM. MANY OF '\n",
      "                                       'THE INDIVIDUAL EFFORTS DISCUSSED BELOW '\n",
      "                                       'ARE FURTHER DEVELOPED IN OTHER PAPERS '\n",
      "                                       'IN THESE PROCEEDINGS. THE SUMMARY IS '\n",
      "                                       'DIVIDED INTO SEVERAL AREAS: 1. '\n",
      "                                       'KNOWLEDGE-BASED VISION 2. DATABASE '\n",
      "                                       'SUPPORT FOR SYMBOLIC VISION PROCESSING '\n",
      "                                       '3. MOTION PROCESSING 4. PERCEPTUAL '\n",
      "                                       'ORGANIZATION (GROUPING) 5. IMAGE '\n",
      "                                       'UNDERSTANDING ARCHITECTURE 6. '\n",
      "                                       'INTEGRATED VISION BENCHMARK FOR '\n",
      "                                       'PARALLEL ARCHITECTURES 7. MOBILE '\n",
      "                                       'VEHICLE NAVIGATION. ALTHOUGH WE '\n",
      "                                       'DISCUSS EACH AREA SEPARATELY, A '\n",
      "                                       'FUNDAMENTAL GOAL OF THE COMPUTER '\n",
      "                                       'VISION RESEARCH ENVIRONMENT AT UMASS '\n",
      "                                       'IS THE INTEGRATION OF A DI- VERSE SET '\n",
      "                                       'OF RESEARCH EFFORTS INTO A SYSTEM THAT '\n",
      "                                       'IS ULTIMATELY INTENDED TO ACHIEVE '\n",
      "                                       'REAL-TIME IMAGE INTERPRETATION. TWO OF '\n",
      "                                       'OUR MAJOR SYSTEM INTEGRATION EFFORTS '\n",
      "                                       'ARE THE VISIONS STATIC INTERPRETATION '\n",
      "                                       'SYSTEM, WHICH IS A KNOWLEDGE- BASED '\n",
      "                                       'COMPUTER VISION SYSTEM UTILIZING '\n",
      "                                       'PARALLEL MODULAR PROCESSES THAT COM- '\n",
      "                                       'MUNICATE VIA A BLACKBOARD [DRA87B, '\n",
      "                                       'HAN87A]. THE SECOND SYSTEM INTEGRATION '\n",
      "                                       'EFFORT INVOLVES AN AUTONOMOUS MOBILE '\n",
      "                                       'VEHICLE FOR NAVIGATION THROUGH A '\n",
      "                                       'PARTI'},\n",
      "              'score': 0.578316033,\n",
      "              'values': []},\n",
      "             {'id': '53909f8c20f70186a0e3f0e3',\n",
      "              'metadata': {'abstract': 'A central problem in the area of scene '\n",
      "                                       'analysis is that of segmenting a scene '\n",
      "                                       'into its natural objects. Current work '\n",
      "                                       'emphasizes the semantic approach in '\n",
      "                                       'which a priori knowledge of the shape '\n",
      "                                       'of an object is used. Yet there is '\n",
      "                                       'much to learn about more primitive '\n",
      "                                       'cues for segmentation such as texture, '\n",
      "                                       'color, and brightness. In the case of '\n",
      "                                       'human perception, segmentation appears '\n",
      "                                       'to be due to a multiplicity of cues '\n",
      "                                       'which operate in a redundant fashion.'},\n",
      "              'score': 0.578215957,\n",
      "              'values': []},\n",
      "             {'id': '5390975920f70186a0dfcfc8',\n",
      "              'metadata': {'abstract': 'Typically any single sensor instrument '\n",
      "                                       'suffers from physical/observation '\n",
      "                                       'constraints. This paper discusses a '\n",
      "                                       'generalized framework, called '\n",
      "                                       'polymorphic visual information fusion '\n",
      "                                       'framework (PVIF) that can enable '\n",
      "                                       'information from multiple sensors to '\n",
      "                                       'be fused and compared to gain broader '\n",
      "                                       'understanding of a target of '\n",
      "                                       'observation in multidimensional space. '\n",
      "                                       'An automate software system supporting '\n",
      "                                       'comparative cognition has been '\n",
      "                                       'developed to form 3D models based on '\n",
      "                                       'the datasets from different sensors, '\n",
      "                                       'such as XPS and LSCM. This fusion '\n",
      "                                       'framework not only provides an '\n",
      "                                       'information engineering based tool to '\n",
      "                                       'overcome the limitations of individual '\n",
      "                                       'sensorýs scope of observation but also '\n",
      "                                       'provides a means where theoretical '\n",
      "                                       'understanding surrounding a complex '\n",
      "                                       'target can be mutually validated by '\n",
      "                                       'comparative cognition about the object '\n",
      "                                       'of interest and 3D model refinement. '\n",
      "                                       'Some polysensometric data '\n",
      "                                       'classification metrics are provided to '\n",
      "                                       'measure the quality of input datasets '\n",
      "                                       'for fusion visualization.'},\n",
      "              'score': 0.577961564,\n",
      "              'values': []},\n",
      "             {'id': '5390a93b20f70186a0ea046b',\n",
      "              'metadata': {'abstract': 'Stereopsis is the process of inferring '\n",
      "                                       'the distance to objects from two or '\n",
      "                                       'more images. It has applications in '\n",
      "                                       'areas such as: novel-view rendering, '\n",
      "                                       'motion capture, autonomous navigation, '\n",
      "                                       'and topographical mapping from remote '\n",
      "                                       'sensing data. Although it sounds '\n",
      "                                       'simple, in light of the effortlessness '\n",
      "                                       'with which we are able to perform the '\n",
      "                                       'task with our own eyes, a number of '\n",
      "                                       'factors that make it quite challenging '\n",
      "                                       'become apparent once one begins '\n",
      "                                       'delving into computational methods of '\n",
      "                                       'solving it. For example, occlusions '\n",
      "                                       'that block part of the scene from '\n",
      "                                       'being seen in one of the images, and '\n",
      "                                       'changes in the appearance of objects '\n",
      "                                       'between the two images due to: sensor '\n",
      "                                       'noise, view dependent effects, and/or '\n",
      "                                       'differences in the lighting/camera '\n",
      "                                       'conditions between the two '\n",
      "                                       'images.Global stereopsis algorithms '\n",
      "                                       'aim to solve this problem by making '\n",
      "                                       'assumptions about the smoothness of '\n",
      "                                       'the depth of surfaces in the scene, '\n",
      "                                       'and formulating stereopsis as an '\n",
      "                                       'optimization problem. As part of their '\n",
      "                                       'formulation, these algorithms include '\n",
      "                                       'a function that measures the '\n",
      "                                       'similarity between pixels in different '\n",
      "                                       'images to detect possible '\n",
      "                                       'correspondences. Which of these match '\n",
      "                                       'cost functions work better, when, and '\n",
      "                                       'why is not well understood. '\n",
      "                                       'Furthermore, in areas of computer '\n",
      "                                       'vision such as segmentation, face '\n",
      "                                       'detection, edge detection, texture '\n",
      "                                       'analysis and classification, and '\n",
      "                                       'optical flow, it is not uncommon to '\n",
      "                                       'use colour spaces other than the well '\n",
      "                                       'known RGB space to improve the '\n",
      "                                       'accuracy of algorithms. However, the '\n",
      "                                       'use of colour spaces other than RGB is '\n",
      "                                       'quite rare in stereopsis research.In '\n",
      "                                       'this dissertation we present results '\n",
      "                                       'from two, first of their kind, large '\n",
      "                                       'scale studies on global stereopsis '\n",
      "                                       'algorithms. In the first we compare '\n",
      "                                       'the relative performance of a '\n",
      "                                       'structured set of match cost cost '\n",
      "                                       'functions in five different global '\n",
      "                                       'stereopsis frameworks in such a way '\n",
      "                                       'that we are able to infer some general '\n",
      "                                       'rules to guide the choice of which '\n",
      "                                       'match cost functions to use in these '\n",
      "                                       'algorithms. In the second we '\n",
      "                                       'investigate how much accuracy can be '\n",
      "                                       'gained by simply changing the colour '\n",
      "                                       'representation used in the input to '\n",
      "                                       'global stereopsis algorithms.'},\n",
      "              'score': 0.577925324,\n",
      "              'values': []},\n",
      "             {'id': '539089d220f70186a0d9ad64',\n",
      "              'metadata': {'abstract': 'This paper presents a new approach to '\n",
      "                                       'the knowledge-based composition of '\n",
      "                                       'processes for image interpretation and '\n",
      "                                       'analysis. Its computer implementation '\n",
      "                                       'in the VISIPLAN (VISIon PLANner) '\n",
      "                                       'system provides a way of modeling the '\n",
      "                                       'composition of image analysis '\n",
      "                                       'processes within a unified, '\n",
      "                                       'object-centered hierarchical planning '\n",
      "                                       'framework. The approach has been '\n",
      "                                       'tested and shown to be flexible in '\n",
      "                                       'handling a reasonably broad class of '\n",
      "                                       'multi-modality biomedical image '\n",
      "                                       'analysis and interpretation problems. '\n",
      "                                       'It provides a relatively general '\n",
      "                                       'design or planning framework, within '\n",
      "                                       'which problem-specific image analysis '\n",
      "                                       'and recognition processes can be '\n",
      "                                       'generated more efficiently and '\n",
      "                                       'effectively. In this way, generality '\n",
      "                                       'is gained at the design and planning '\n",
      "                                       'stages, even though the final '\n",
      "                                       'implementation stage of interpretation '\n",
      "                                       'processes is almost invariably '\n",
      "                                       'problem- and domain-specific.'},\n",
      "              'score': 0.577483535,\n",
      "              'values': []},\n",
      "             {'id': '5390a30b20f70186a0e6a35b',\n",
      "              'metadata': {'abstract': 'A popular computation approach is to '\n",
      "                                       'process visual images by dividing them '\n",
      "                                       'into crisp (winner-takes-all) parts in '\n",
      "                                       'analog to properties of '\n",
      "                                       'neurophysiological receptive fields. '\n",
      "                                       'Problem with such symbolic '\n",
      "                                       'representation is that in a real '\n",
      "                                       'environment object attributes are '\n",
      "                                       'seldom invariant. We propose to divide '\n",
      "                                       'images into rough parts using '\n",
      "                                       'hierarchical, multi-valued processes. '\n",
      "                                       'The bottom-up computation (BUC) is '\n",
      "                                       'related to prediction where object '\n",
      "                                       'attributes are approximated by '\n",
      "                                       'different granules with properties '\n",
      "                                       'similar to different brain areas: by '\n",
      "                                       'dots as in the thalamus, by oriented '\n",
      "                                       'lines as in the primary visual cortex, '\n",
      "                                       'and by elementary shapes as in V4. '\n",
      "                                       'There are a large number of possible '\n",
      "                                       'combinations of elementary granules; '\n",
      "                                       'therefore objects in BUC are '\n",
      "                                       'overrepresented. The top-down '\n",
      "                                       'computation (TDC) fits prediction to '\n",
      "                                       'hypothesis posed by more complex '\n",
      "                                       'properties (higher brain areas). If '\n",
      "                                       'the hypothesis check is positive, TDC '\n",
      "                                       'verifies the object and eliminates '\n",
      "                                       'other possible patterns. Such '\n",
      "                                       'classifications take place in parallel '\n",
      "                                       'at many functional units. We show an '\n",
      "                                       'example of such hierarchical system '\n",
      "                                       'computation on experimentally recorded '\n",
      "                                       'data from monkey visual area (V4).'},\n",
      "              'score': 0.577400923,\n",
      "              'values': []},\n",
      "             {'id': '539099ec20f70186a0e1d57d',\n",
      "              'metadata': {'abstract': 'One of the major challenges in '\n",
      "                                       'computer vision is to create automated '\n",
      "                                       'systems that perform tasks with at '\n",
      "                                       'least the same competences as human '\n",
      "                                       'experts. In particular for automated '\n",
      "                                       'inspection of natural objects this is '\n",
      "                                       'not easy to achieve. The task is '\n",
      "                                       'hampered by large in-class variations '\n",
      "                                       'and complex 3D-morphology of the '\n",
      "                                       'objects and subtle argumentations of '\n",
      "                                       'experts. For example, in our '\n",
      "                                       'horticultural case we deal with '\n",
      "                                       'quality assessment of young tomato '\n",
      "                                       'plants, which requires experienced '\n",
      "                                       'specialists. We submit that automation '\n",
      "                                       'of such a task employing an explicit '\n",
      "                                       'model of the objects and their '\n",
      "                                       'assessment is preferred over a '\n",
      "                                       'black-box model obtained from '\n",
      "                                       'modelling input-output relations only. '\n",
      "                                       'We propose to employ ontologies for '\n",
      "                                       'representing the geometrical shapes, '\n",
      "                                       'object parts and quality classes '\n",
      "                                       'associated with the explicit models. '\n",
      "                                       'Our main contribution is the '\n",
      "                                       'description of a method to develop a '\n",
      "                                       'white-box computer vision application '\n",
      "                                       'in which the needed expert knowledge '\n",
      "                                       'is defined by: (i) decomposing the '\n",
      "                                       'task of the inspection system into '\n",
      "                                       'subtasks and (ii) identifying the '\n",
      "                                       'algorithms that execute the subtasks. '\n",
      "                                       'This method describes the interaction '\n",
      "                                       'between the task decomposition and the '\n",
      "                                       'needed task-specific knowledge, and '\n",
      "                                       'studies the delicate balance between '\n",
      "                                       'general domain knowledge and '\n",
      "                                       'task-specific details. As a proof of '\n",
      "                                       'principle of this methodology, we work '\n",
      "                                       'through a horticultural case study and '\n",
      "                                       'argue that the method leads to a '\n",
      "                                       'robust, well-performing, and '\n",
      "                                       'extendable computer vision system.'},\n",
      "              'score': 0.577001,\n",
      "              'values': []},\n",
      "             {'id': '5390b3da20f70186a0ef6719',\n",
      "              'metadata': {'abstract': 'In real world, a scene is composed by '\n",
      "                                       'many characteristics Intrinsic images '\n",
      "                                       'represent these characteristics by two '\n",
      "                                       'components, reflectance (the albedo of '\n",
      "                                       'each point) and shading (the '\n",
      "                                       'illumination of each point) Because '\n",
      "                                       'reflectance images are invariant under '\n",
      "                                       'different illumination conditions, '\n",
      "                                       'they are more appropriate for some '\n",
      "                                       'vision applications, such as '\n",
      "                                       'recognition, detection We develop the '\n",
      "                                       'system to separate them from a single '\n",
      "                                       'image Firstly, a presented method, '\n",
      "                                       'called Weighted-Map Method, is used to '\n",
      "                                       'separate reflectance and shading A '\n",
      "                                       'weighted map is created by first '\n",
      "                                       'transforming original color domain '\n",
      "                                       'into new color domain and then '\n",
      "                                       'extracting some useful property '\n",
      "                                       'Secondly, we build Markov Random '\n",
      "                                       'Fields and use Belief Propagation to '\n",
      "                                       'propagate local information in order '\n",
      "                                       'to help us correct misclassifications '\n",
      "                                       'from neighbors According to our '\n",
      "                                       'experimental results, our system can '\n",
      "                                       'apply to not only real images but also '\n",
      "                                       'synthesized images.'},\n",
      "              'score': 0.576753259,\n",
      "              'values': []},\n",
      "             {'id': '53908bcc20f70186a0dc542e',\n",
      "              'metadata': {'abstract': 'High computer performance depends only '\n",
      "                                       'partially on using faster and more '\n",
      "                                       'reliable hardware, but to a large '\n",
      "                                       'extent it depends on the architecture '\n",
      "                                       'and on the processing techniques. An '\n",
      "                                       'effective platform that matches '\n",
      "                                       'general planning strategies is given '\n",
      "                                       'by the hierarchical paradigm. This is '\n",
      "                                       'true particularly in the field of '\n",
      "                                       'image processing and computer vision, '\n",
      "                                       'which is characterized by very large '\n",
      "                                       'quantity of sensory data, but in which '\n",
      "                                       'most of the information collected is '\n",
      "                                       'meaningless for the task at end. Real '\n",
      "                                       'time performances can be achieved only '\n",
      "                                       'by applying some attentional '\n",
      "                                       'mechanisms that allow to restrict the '\n",
      "                                       'computation just on the relevant data, '\n",
      "                                       'at the right time. Several vision '\n",
      "                                       'systems have been proposed and '\n",
      "                                       'designed to support the implementation '\n",
      "                                       'of these strategies. In this work, '\n",
      "                                       'after introducing a taxonomy of the '\n",
      "                                       'hierarchical machine vision systems, a '\n",
      "                                       'short description of the most popular '\n",
      "                                       'implementations is given.'},\n",
      "              'score': 0.57635206,\n",
      "              'values': []},\n",
      "             {'id': '5390980720f70186a0e03067',\n",
      "              'metadata': {'abstract': 'In many operations the ability of a '\n",
      "                                       'machine to “see” is what will '\n",
      "                                       'determine its effectiveness in its '\n",
      "                                       'particular domain of operation. For '\n",
      "                                       'example, in a bin picking problem the '\n",
      "                                       'ability of the sensing system of a '\n",
      "                                       'robot to determine the position and '\n",
      "                                       'orientation of the individual parts '\n",
      "                                       \"will ultimately determine the system's \"\n",
      "                                       'success or failure. Most systems that '\n",
      "                                       'require this level of sensing, utilize '\n",
      "                                       'machine vision in which computers are '\n",
      "                                       'integrated with image acquisition '\n",
      "                                       'devices to provide the information '\n",
      "                                       'required for guidance; as would be '\n",
      "                                       'needed in a feedback loop for example. '\n",
      "                                       'The development of algorithms that '\n",
      "                                       'allow these computers to accomplish '\n",
      "                                       'the image interpretation has turned '\n",
      "                                       'out to be less than trivial. This is '\n",
      "                                       'especially true in the area of natural '\n",
      "                                       'products such as, meat products, fruit '\n",
      "                                       'or textile; where, because of their '\n",
      "                                       'natural variability the ability to '\n",
      "                                       'develop machine vision algorithms to '\n",
      "                                       'automatically inspect these products '\n",
      "                                       'reliably has been problematic. The '\n",
      "                                       'goal of this thesis is to attempt to '\n",
      "                                       'determine a methodology for the '\n",
      "                                       'integration and streamlining of the '\n",
      "                                       'process of algorithm development so as '\n",
      "                                       'to be able to more efficiently develop '\n",
      "                                       'effective and robust algorithms for '\n",
      "                                       'this class of problems. Humans, are '\n",
      "                                       'currently still the best available '\n",
      "                                       'solutions to these problems. This '\n",
      "                                       'thesis will examine an approach '\n",
      "                                       'towards the development of machine '\n",
      "                                       'vision algorithms using the primate '\n",
      "                                       'visual system as a model. The approach '\n",
      "                                       'taken in this work defines three '\n",
      "                                       'levels of processing for the visual '\n",
      "                                       'signal these are sensing, '\n",
      "                                       'ecoding/transfer, and classification. '\n",
      "                                       'In particular we examine the processes '\n",
      "                                       'of encoding/transfer derived from the '\n",
      "                                       'results of research in the area of '\n",
      "                                       'human/primate biological visual '\n",
      "                                       'processing and their representations. '\n",
      "                                       'We focus on the use of the receptive '\n",
      "                                       'field mechanisms that are commonly '\n",
      "                                       'observed in the human visual system '\n",
      "                                       'and their processing of contrast in '\n",
      "                                       'the scenes. We also show that features '\n",
      "                                       'derived from the responses of these '\n",
      "                                       'mechanisms are useful for image '\n",
      "                                       'classification. Algorithms for '\n",
      "                                       'implementing these operations are '\n",
      "                                       'developed using the technique and '\n",
      "                                       'demonstrated. The other aspect of the '\n",
      "                                       'approach provides for user guidance by '\n",
      "                                       'allowing an expert to teach the system '\n",
      "                                       'by identifying things that are of '\n",
      "                                       'interest in a particular scene. We '\n",
      "                                       'then demonstrate development of '\n",
      "                                       'solutions to three inspection problems '\n",
      "                                       'using the approach.'},\n",
      "              'score': 0.576048,\n",
      "              'values': []},\n",
      "             {'id': '5390b1d220f70186a0ee1abb',\n",
      "              'metadata': {'abstract': 'We describe the current state of the '\n",
      "                                       '3-D Mosaic project, whose goal is to '\n",
      "                                       'incrementally acquire a 3-D model of a '\n",
      "                                       'complex urban scene from images. The '\n",
      "                                       'notion of incremental acquisition '\n",
      "                                       'arises from the observations that 1) '\n",
      "                                       'single images contain only parfial '\n",
      "                                       'information about a scene, 2) complex '\n",
      "                                       'images are difficult to fully '\n",
      "                                       'interpret, and 3) different features '\n",
      "                                       'of a given scene tend to be easier to '\n",
      "                                       'extract in different images because of '\n",
      "                                       'differences in viewpoint and lighting '\n",
      "                                       'conditions. In our approach, multiple '\n",
      "                                       'images of the scene are sequentially '\n",
      "                                       'analyzed so as to incrementaly '\n",
      "                                       'construct the model. Each new image '\n",
      "                                       'provides information which refines the '\n",
      "                                       'model. We describe some experiments '\n",
      "                                       'toward this end. Our method of '\n",
      "                                       'extracting 3-D shape information from '\n",
      "                                       'the images is stereo analysis. Because '\n",
      "                                       'we are dealing with urban scenes, a '\n",
      "                                       'junction-based matching technique '\n",
      "                                       'proves very useful. This technique '\n",
      "                                       'produces rather sparse wire-frame '\n",
      "                                       'descriptions of the scene. A reasoning '\n",
      "                                       'system that relies on task-specific '\n",
      "                                       'knowledge generates an approximate '\n",
      "                                       'model of the scene from the stereo '\n",
      "                                       'output. Gray scale information is also '\n",
      "                                       'acquired for the faces in the model. '\n",
      "                                       'Finally, we describe an experiment in '\n",
      "                                       'combining two views of the scene to '\n",
      "                                       'obtain a rermed model.'},\n",
      "              'score': 0.575774729,\n",
      "              'values': []},\n",
      "             {'id': '5390aa0e20f70186a0ea7abb',\n",
      "              'metadata': {'abstract': 'Function-based object recognition '\n",
      "                                       'provides the framework to represent '\n",
      "                                       'and reason about object functionality '\n",
      "                                       'as a means to recognize novel objects '\n",
      "                                       'and produce plans for interaction with '\n",
      "                                       'the world. When function can be '\n",
      "                                       'perceived visually, function-based '\n",
      "                                       'computer vision is consistent with '\n",
      "                                       \"Gibson's theory of affordances. \"\n",
      "                                       'Objects are recognized by their '\n",
      "                                       'functional attributes. These '\n",
      "                                       'attributes can be segmented out of the '\n",
      "                                       'scene and given symbolic labels which '\n",
      "                                       'can then be used to guide the search '\n",
      "                                       'space for additional functional '\n",
      "                                       'attributes. An example of such '\n",
      "                                       'affordance-driven scene segmentation '\n",
      "                                       'would be the process of attaching '\n",
      "                                       'symbolic labels to the areas that '\n",
      "                                       'afford sitting (functional seats) and '\n",
      "                                       'using these areas to guide parameter '\n",
      "                                       'selection for deriving nearby surfaces '\n",
      "                                       'that potentially afford back support. '\n",
      "                                       'The Generic Recognition Using Form and '\n",
      "                                       'Function (GRUFF) object recognition '\n",
      "                                       'system reasons about and generates '\n",
      "                                       'plans for understanding 3-D scenes of '\n",
      "                                       'objects by performing such a '\n",
      "                                       'functional attribute-based labelling '\n",
      "                                       'process. An avenue explored here is '\n",
      "                                       'based on a novel approach of '\n",
      "                                       'autonomously directing image '\n",
      "                                       'acquisition and range segmentation by '\n",
      "                                       'determining the extent to which '\n",
      "                                       'surfaces in the scene meet specified '\n",
      "                                       'functional requirements, or provide '\n",
      "                                       'affordances associated with a generic '\n",
      "                                       'category of objects.'},\n",
      "              'score': 0.575766921,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda691',\n",
      "              'metadata': {'abstract': 'COMPUTER VISION IMPOSES UNIQUE '\n",
      "                                       'REQUIREMENTS ON THE REPRESENTATION AND '\n",
      "                                       'MANIPULATION OF IMAGE DATA AND '\n",
      "                                       'KNOWLEDGE. THE INTERPRETATION OF AN '\n",
      "                                       'IMAGE CAN GENERATE THOUSANDS OF '\n",
      "                                       'INTERMEDIATE LEVEL DESCRIPTIONS, MANY '\n",
      "                                       'OF WHICH MUST BE REPEATEDLY ACCESSED '\n",
      "                                       'AND PROCESSED. TRADITIONAL KNOWLEDGE '\n",
      "                                       'REPRE- SENTATION METHODS DO NOT '\n",
      "                                       'PROVIDE MECHANISMS TO ACCOMPLISH THIS '\n",
      "                                       'EFFECTIVELY. WE DESCRIBE A DATABASE '\n",
      "                                       'MANAGEMENT SYSTEM CALLED THE '\n",
      "                                       'INTERMEDIATE SYMBOLIC REPRESENTATION '\n",
      "                                       '(ISR) WHICH IS SUITABLE FOR USE AT THE '\n",
      "                                       'INTERMEDIATE (SYMBOLIC) LEVEL OF '\n",
      "                                       'VISION. THE ISR, WHICH IS BASED ON '\n",
      "                                       'DATABASE MANAGEMENT METHODOLOGY, '\n",
      "                                       'MEDIATES ACCESS TO MASSIVE QUANTITIES '\n",
      "                                       'OF INTERMEDIATE LEVEL VISION DATA, AND '\n",
      "                                       'FORMS AN ACTIVE INTERFACE TO THE '\n",
      "                                       'HIGHER LEVEL INFERENCE PROCESSES '\n",
      "                                       'RESPONSIBLE FOR CONSTRUCTING THE '\n",
      "                                       'INTERPRETATION OF AN IMAGE. WE ALSO '\n",
      "                                       'PRESENT SEVERAL EXAMPLES ILLUSTRATING '\n",
      "                                       'THE USE OF THE ISR.'},\n",
      "              'score': 0.575689852,\n",
      "              'values': []},\n",
      "             {'id': '5390bfa220f70186a0f52ec8',\n",
      "              'metadata': {'abstract': 'To quickly synthesize complex scenes, '\n",
      "                                       'digital artists often collage together '\n",
      "                                       'visual elements from multiple sources: '\n",
      "                                       'for example, mountains from New '\n",
      "                                       'Zealand behind a Scottish castle with '\n",
      "                                       'wisps of Saharan sand in front. In '\n",
      "                                       'this paper, we propose to use a '\n",
      "                                       'similar process in order to parse a '\n",
      "                                       'scene. We model a scene as a collage '\n",
      "                                       'of warped, layered objects sampled '\n",
      "                                       'from labeled, reference images. Each '\n",
      "                                       'object is related to the rest by a set '\n",
      "                                       'of support constraints. Scene parsing '\n",
      "                                       'is achieved through '\n",
      "                                       'analysis-by-synthesis. Starting with a '\n",
      "                                       'dataset of labeled exemplar scenes, we '\n",
      "                                       'retrieve a dictionary of candidate '\n",
      "                                       'object segments that match a query '\n",
      "                                       'image. We then combine elements of '\n",
      "                                       'this set into a \"scene collage\" that '\n",
      "                                       'explains the query image. Beyond just '\n",
      "                                       'assigning object labels to pixels, '\n",
      "                                       'scene collaging produces a lot more '\n",
      "                                       'information such as the number of each '\n",
      "                                       'type of object in the scene, how they '\n",
      "                                       'support one another, the ordinal depth '\n",
      "                                       'of each object, and, to some degree, '\n",
      "                                       'occluded content. We exploit this '\n",
      "                                       'representation for several '\n",
      "                                       'applications: image editing, random '\n",
      "                                       'scene synthesis, and '\n",
      "                                       'image-to-anaglyph.'},\n",
      "              'score': 0.575497687,\n",
      "              'values': []},\n",
      "             {'id': '539090c420f70186a0ddde47',\n",
      "              'metadata': {'abstract': 'We propose a theory of depiction and '\n",
      "                                       'interpretation that formalizes image '\n",
      "                                       'domain knowledge, scene domain '\n",
      "                                       'knowledge and the depiction mapping '\n",
      "                                       'between the image and scene domains. '\n",
      "                                       'This theory is illustrated by '\n",
      "                                       'specifying some general knowledge '\n",
      "                                       'about maps, geographic objects and '\n",
      "                                       'their depiction relationships in first '\n",
      "                                       'order logic with equality. An '\n",
      "                                       'interpretation of an image is defined '\n",
      "                                       'to be a logical model of the general '\n",
      "                                       'knowledge and a description of that '\n",
      "                                       'image. For the simple map world we '\n",
      "                                       'show how the task level specification '\n",
      "                                       'may be refined to a provably correct '\n",
      "                                       'implementation by invoking model '\n",
      "                                       'preserving transformations on the '\n",
      "                                       'logical representation. In addition, '\n",
      "                                       'we sketch logical treatments for '\n",
      "                                       'querying an image, incorporating '\n",
      "                                       'contingent scene knowledge into the '\n",
      "                                       'interpretation process, occlusion, '\n",
      "                                       'ambiguous image descriptions, and '\n",
      "                                       'composition. This approach provides a '\n",
      "                                       'formal framework for analyzing '\n",
      "                                       'existing systems such as Mapsee, and '\n",
      "                                       'for understanding the use of '\n",
      "                                       'constraint satisfaction techniques. It '\n",
      "                                       'also can be used to design and '\n",
      "                                       'implement vision and graphics systems '\n",
      "                                       'that are correct with respect to the '\n",
      "                                       'task and algorithm levels.'},\n",
      "              'score': 0.575394571,\n",
      "              'values': []},\n",
      "             {'id': '5390a5b020f70186a0e7cb36',\n",
      "              'metadata': {'abstract': 'Object recognition in stereo sequences '\n",
      "                                       'is a simulation of human visual '\n",
      "                                       'systems on how to analyze and '\n",
      "                                       'understand various scenes. A pair of '\n",
      "                                       'stereo sequences is a type of '\n",
      "                                       'complicated information with huge '\n",
      "                                       'amount of raw data and features '\n",
      "                                       'associated with different parameter '\n",
      "                                       'spaces. Therefore the automatic object '\n",
      "                                       'recognition in stereo sequences is a '\n",
      "                                       'difficult and unsolved task '\n",
      "                                       'challenging many researchers. This '\n",
      "                                       'paper puts its emphasis on solving '\n",
      "                                       'this problem based on the data fusion '\n",
      "                                       'theory. A new algorithm is proposed '\n",
      "                                       'and experimented on real data. The '\n",
      "                                       'results show that the accuracy of the '\n",
      "                                       'object recognition is improved by '\n",
      "                                       'applying the fusion of both 3D and '\n",
      "                                       'motion parameters.'},\n",
      "              'score': 0.575063467,\n",
      "              'values': []},\n",
      "             {'id': '53908bcc20f70186a0dc5165',\n",
      "              'metadata': {'abstract': 'A scene analysis system for the 3-D '\n",
      "                                       'modeling of objects is presented. It '\n",
      "                                       'combines surface reconstruction '\n",
      "                                       'techniques with object recognition for '\n",
      "                                       'the generation of 3-D models for '\n",
      "                                       'computer graphic applications. The '\n",
      "                                       'system permits the insertion of '\n",
      "                                       'highlevel constraints, like a specific '\n",
      "                                       'angle between two house walls, in an '\n",
      "                                       'explicit knowledge base implemented as '\n",
      "                                       'a semantic net. The applicability of '\n",
      "                                       'those constraints is proved by '\n",
      "                                       'asserting and testing hypotheses in an '\n",
      "                                       'interpretation phase. In the case of '\n",
      "                                       'rejection a more general constraint or '\n",
      "                                       'model is selected. The capabilities of '\n",
      "                                       'the system were shown for the modeling '\n",
      "                                       'of buildings using depth from stereo '\n",
      "                                       'and contour information. The system '\n",
      "                                       'reconstructs the surface of the scene '\n",
      "                                       'objects using the constraints selected '\n",
      "                                       'in the prior interpretation.'},\n",
      "              'score': 0.574867368,\n",
      "              'values': []},\n",
      "             {'id': '53908f5b20f70186a0dda609',\n",
      "              'metadata': {'abstract': 'THE SCHEMA SYSTEM EMBODIES A '\n",
      "                                       'KNOWLEDGE-BASED APPROACH TO SCENE '\n",
      "                                       'INTERPRE- TATION. LOW-LEVEL ROUTINES '\n",
      "                                       'ARE APPLIED TO EXTRACT IMAGE '\n",
      "                                       'DESCRIPTORS CALLED TOKENS, AND THESE '\n",
      "                                       'TOKENS ARE FURTHER ORGANIZED BY '\n",
      "                                       'INTERMEDIATE-LEVEL ROUT- INES INTO '\n",
      "                                       'MORE ABSTRACT STRUCTURES THAT CAN BE '\n",
      "                                       'ASSOCIATED WITH OBJECT INST- ANCES. '\n",
      "                                       'THE THOUSANDS OF TOKENS THAT ARE '\n",
      "                                       'EXTRACTED FROM AN IMAGE CAN BE GROUPED '\n",
      "                                       'IN A COMBINATORIALLY EXPLOSIVE MANNER. '\n",
      "                                       'THEREFORE, KNOWLEDGE IN THE SCHEMA '\n",
      "                                       'SYSTEM IS NOT LIMITED TO THE '\n",
      "                                       'DESCRIPTIONS OF OBJECTS; IT INCLUDES '\n",
      "                                       'INFORMATION ABOUT HOW EACH OBJECT CAN '\n",
      "                                       'BE RECOGNIZED. OBJECT SCHEMAS CONTROL '\n",
      "                                       'THE INVOCATION AND EXECUTION OF THE '\n",
      "                                       'LOW-LEVEL AND INTERMEDIATE-LEVEL ROUT- '\n",
      "                                       'INES WITH THE GOAL OF FORMING '\n",
      "                                       'HYPOTHESES ABOUT OBJECTS IN THE SCENE. '\n",
      "                                       'THE SYSTEM DESCRIBED PRODUCES IMAGE '\n",
      "                                       'INTERPRETATIONS BASED ON '\n",
      "                                       'TWO-DIMENSIONAL REASONING, ALTHOUGH '\n",
      "                                       'NOTHING IN THE SYSTEM ORGANIZATION AND '\n",
      "                                       'CONTROL STRATEG- IES PRECLUDE THE '\n",
      "                                       'INCLUSION OF THREE-DIMENSIONAL '\n",
      "                                       'INFORMATION. THE SCHEMA FRAMEWORK '\n",
      "                                       'EXPLOITS COARSE-GRAINED PARALLELISM IN '\n",
      "                                       'A COOPERA- TIVE INTERPRETATION '\n",
      "                                       'PROCESS. SCHEMA INSTANCES RUN '\n",
      "                                       'CONCURRENTLY, AND AN OB- JECT SCHEMA '\n",
      "                                       'OFTEN HAS AVAILABLE A VARIETY OF '\n",
      "                                       'STRATEGIES FOR IDENTIFICATION, EACH '\n",
      "                                       'ONE INVOKING KNOWLEDGE SOURCES TO '\n",
      "                                       'GATHER SUPPORT FOR THE PRESENCE OF A '\n",
      "                                       'HYPOTHESIZED OBJECT. INTER-SCHEMA '\n",
      "                                       'COMMUNICATION IS CARRIED OUT '\n",
      "                                       'ASYNCHRON- OUSLY THROUGH A GLOBAL '\n",
      "                                       'BLACKBOARD. IN THIS WAY SCHEMA '\n",
      "                                       'INSTANCES COOPERATE TO IDENTIFY AND '\n",
      "                                       'LOCATE THE SIGNIFICANT OBJECTS PRESENT '\n",
      "                                       'IN THE SCENE.'},\n",
      "              'score': 0.57446146,\n",
      "              'values': []},\n",
      "             {'id': '539087a620f70186a0d49619',\n",
      "              'metadata': {'abstract': 'A representation paradigm for '\n",
      "                                       'instantiating and refining multiple, '\n",
      "                                       'concurrent descriptions of an object '\n",
      "                                       'from a sequence of imagery is '\n",
      "                                       'presented. It is designed for the '\n",
      "                                       'perception system of an autonomous '\n",
      "                                       'robot that needs to describe many '\n",
      "                                       'types of objects, initially detects '\n",
      "                                       'objects at a distance and gradually '\n",
      "                                       'acquires higher resolution data, and '\n",
      "                                       'continuously collects sensory input. '\n",
      "                                       'Since the data change significantly '\n",
      "                                       'over time, the paradigm supports the '\n",
      "                                       'evolution of descriptions, progressing '\n",
      "                                       \"from crude 2-D 'blob' descriptions to \"\n",
      "                                       'complete semantic models. To control '\n",
      "                                       'this accumulation of new descriptions, '\n",
      "                                       'the authors introduce the idea of '\n",
      "                                       'representation space, a lattice of '\n",
      "                                       'representations that specifies the '\n",
      "                                       'order in which they should be '\n",
      "                                       'considered for describing an object. A '\n",
      "                                       'system, TraX, that constructs and '\n",
      "                                       'refines models of outdoor objects '\n",
      "                                       'detected in sequences of range data is '\n",
      "                                       'described.'},\n",
      "              'score': 0.573985934,\n",
      "              'values': []},\n",
      "             {'id': '53909fbd20f70186a0e43439',\n",
      "              'metadata': {'abstract': 'A scheme, named tower of knowledge '\n",
      "                                       '(ToK), is proposed for interpreting 3D '\n",
      "                                       'scenes. The ToK encapsulates causal '\n",
      "                                       'dependencies between object appearance '\n",
      "                                       'and functionality. We demonstrate it '\n",
      "                                       'by labelling the components of the 3D '\n",
      "                                       'model of a building, reconstructed '\n",
      "                                       'from images of multiple views, by '\n",
      "                                       'using utility theory.'},\n",
      "              'score': 0.573711812,\n",
      "              'values': []},\n",
      "             {'id': '5390b1d220f70186a0ee19ed',\n",
      "              'metadata': {'abstract': 'A computer vision system is proposed, '\n",
      "                                       'in which the recogni-tion of an object '\n",
      "                                       'involves two interacting processes: '\n",
      "                                       'model retrieval and model '\n",
      "                                       'verification. The goal of the model '\n",
      "                                       'retrieval process is to generate a '\n",
      "                                       'proper structural description of the '\n",
      "                                       'object in the input image, and use the '\n",
      "                                       'description to retrieve candidate '\n",
      "                                       'object models from the associative '\n",
      "                                       'memory of the vision system. The '\n",
      "                                       'present study explores one way of '\n",
      "                                       'deriving such an object shape '\n",
      "                                       'description from a single image. '\n",
      "                                       'Regularity constraints and a '\n",
      "                                       'preference rule are used to restrict '\n",
      "                                       'the solutions to a preferred '\n",
      "                                       'interpretation of geometric contours. '\n",
      "                                       'Local interpretation is then '\n",
      "                                       'propagated to neighboring regions. '\n",
      "                                       'Through a proper control on the '\n",
      "                                       'interaction between constraints and '\n",
      "                                       'consistency checking, a rough object '\n",
      "                                       'description in terms of visible '\n",
      "                                       'surface orientations can be '\n",
      "                                       'gener-ated. A computer vision system '\n",
      "                                       'using this approach has been '\n",
      "                                       'imple-mented and it is described in '\n",
      "                                       'some details.'},\n",
      "              'score': 0.57366389,\n",
      "              'values': []},\n",
      "             {'id': '5390b8d720f70186a0f2c17b',\n",
      "              'metadata': {'abstract': 'The DISCOV (DImensionless Shunting '\n",
      "                                       'COlor Vision) system models a cascade '\n",
      "                                       'of primate color vision neurons: '\n",
      "                                       'retinal ganglion, thalamic single '\n",
      "                                       'opponent, and cortical double '\n",
      "                                       'opponent. A unified model derived from '\n",
      "                                       'psychophysical axioms produces '\n",
      "                                       'transparent network dynamics and '\n",
      "                                       'principled parameter settings. DISCOV '\n",
      "                                       'fits an array of physiological data '\n",
      "                                       'for each cell type, and makes testable '\n",
      "                                       'experimental predictions. Binary '\n",
      "                                       'DISCOV augments an earlier version of '\n",
      "                                       'the model to achieve stable '\n",
      "                                       'computations for spatial data '\n",
      "                                       'analysis. The model is described in '\n",
      "                                       'terms of RGB images, but inputs may '\n",
      "                                       'consist of any number of spatially '\n",
      "                                       'defined components. System dynamics '\n",
      "                                       'are derived using algebraic '\n",
      "                                       'computations, and robust parameter '\n",
      "                                       'ranges that meet experimental data are '\n",
      "                                       'fully specified. Assuming default '\n",
      "                                       'values, the only free parameter for '\n",
      "                                       'the user to specify is the spatial '\n",
      "                                       'scale. Multi-scale analysis '\n",
      "                                       'accommodates items of various sizes '\n",
      "                                       'and perspective. Image inputs are '\n",
      "                                       'first processed by complement coding, '\n",
      "                                       'which produces an ON channel stream '\n",
      "                                       'and an OFF channel stream for each '\n",
      "                                       'component. Subsequent computations are '\n",
      "                                       'on-center/off-surround, with the OFF '\n",
      "                                       'channel replacing the '\n",
      "                                       'off-center/on-surround fields of other '\n",
      "                                       'models. Together with an orientation '\n",
      "                                       'filter, DISCOV provides feature input '\n",
      "                                       'vectors for an integrated recognition '\n",
      "                                       'system. The development of DISCOV '\n",
      "                                       'models is being carried out in the '\n",
      "                                       'context of a large-scale research '\n",
      "                                       'program that is integrating cognitive '\n",
      "                                       'and neural systems derived from '\n",
      "                                       'analyses of vision and recognition to '\n",
      "                                       'produce both biological models and '\n",
      "                                       'technological applications.'},\n",
      "              'score': 0.573586404,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 23}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'id': '5390a6b120f70186a0e8462d', 'score': 0.664827943, 'text': 'For human vision to be explained by a computational theory, the first question is plain: What are the problems the brain solves when we see? It is argued that vision is the construction of efficient symbolic descriptions from images of the world. An important aspect of vision is therefore the choice of representations for the different kinds of information in a visual scene. An overall framework is suggested for extracting shape information from images, in which the analysis proceeds through three representations; (1) the primal sketch, which makes explicit the intensity changes and local two-dimensional geometry of an image, (2) the 2 1/2-D sketch, which is a viewer-centred representation of the depth, orientation and discontinuities of the visible surfaces, and (3) the 3-D model representation, which allows an object-centred description of the threedimensional structure and organization of a viewed shape. The critical act in formulating computational theories for processes capable of constructing these representations is the discovery of valid constraints on the way the world behaves, that provide sufficient additional information to allow recovery of the desired characteristic. Finally, once a computational theory for a process has been formulated, algorithms for implementing it may be designed, and their performance compared with that of the human visual processor.'}\n",
      "\n",
      "2. {'id': '5390b2d720f70186a0eeced2', 'score': 0.658404469, 'text': 'Using stereo vision in the field of mapping and localization is an intuitive idea, as demonstrated by the number of animals that have developed the ability. Though it seems logical to use vision, the problem is a very difficult one to solve. It requires the ability to identify objects in the field of view, and classify their relationship to the observer. A procedure for extracting and matching object data using a stereo vision system is introduced, and initial results are provided to demonstrate the potential of this system.'}\n",
      "\n",
      "3. {'id': '53908f5b20f70186a0dda6f9', 'score': 0.655618966, 'text': 'IN THIS PAPER WE CONSIDER SOME OF THE PROBLEMS CONFRONTING THE DEVELOP- MENT OF GENERAL INTEGRATED COMPUTER VISION SYSTEMS, AND THE STATUS OF THE VISIONS PROJECT WHICH HAS BECOME AN EXPERIMENTAL TESTBED FOR THE CONSTRUC- TION OF KNOWLEDGE-BASED IMAGE INTERPRETATION SYSTEMS. THE GOAL IS THE CONSTRUCTION OF A SYMBOLIC REPRESENTATION OF THE THREE-DIMENSIONAL WORLD DEPICTED IN A TWO-DIMENSIONAL IMAGE, INCLUDING THE LABELING OF OBJECTS, THE DETERMINATION OF THEIR LOCATION IN SPACE, AND TO THE DEGREE POSSIBLE THE CONSTRUCTION OF A SURFACE REPRESENTATION OF THE ENVIRONMENT. OUR SYSTEM INVOLVES THREE LEVELS OF PROCESSING FOR STATIC IMAGE INTER- PRETATION. LOW-LEVEL PROCESSES MANIPULATE PIXEL DATA AND PRODUCE INTERME- DIATE SYMBOLIC EVENTS SUCH AS REGIONS AND LINES WITH THEIR ATTRIBUTES. HIGH-LEVEL PROCESSES FOCUS ATTENTION ON AGGREGATES OF THESE EVENTS VIA RULE-BASED OBJECT HYPOTHESES IN ORDER TO SELECTIVELY INVOKE SCHEMAS, WHICH CONTAIN MORE COMPLEX KNOWLEDGE-BASED INTERPRETATION STRATEGIES. INTERMED- IATE-LEVEL PROCESSES CARRY OUT GROUPING AND REORGANIZATION OF THE ERROR- PRONE SYMBOLIC REPRESENTATION EXTRACTED FROM THE SENSORY DATA, UTILIZING BOTH \"TOP-DOWN\" CONTROL OF THE PROCESSING BY THE SCHEMA INTERPRETATION STRATEGIES AS WELL AS \"BOTTOM-UP\" DATA-DIRECTED ORGANIZATION OF INTERESTING PERCEPTUAL EVENTS. OUR DESIGN IS BEING EXTENDED TO INTEGRATE THE RESULTS OF MOTION AND STEREO PROCESSING THROUGHOUT THE THREE LEVELS OF PROCESSING,'}\n",
      "\n",
      "4. {'id': '53908bad20f70186a0dc3545', 'score': 0.647426665, 'text': 'Perception is best understood as the interpretation of sensory data in terms of models of how the world is structured and how it behaves; these models are exactly those that are most useful for generation of computer images. By recognizing and exploiting this commonality we have been able to make surprising progress in both fields.'}\n",
      "\n",
      "5. {'id': '5390a6b120f70186a0e8482e', 'score': 0.642689884, 'text': 'Depth reconstruction from the two-dimensional image plays an important role in certain visual tasks and has been a major focus of computer vision research. However, in this paper we argue that most instances of recognition in human and machine vision can best be performed without the preliminary reconstruction of depth. Three other mechanisms are described that can be used to bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization can be used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Secondly, evidential reasoning can be used to combine evidence from these groupings and other sources of information to reduce the size of the search-space during model-based matching. Finally, a process of spatial correspondence can be used to bring the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. These methods have been combined in an experimental computer vision system named SCERPO. This system has demonstrated the use of these methods for the recognition of objects from unknown viewpoints in single gray-scale images.'}\n",
      "\n",
      "6. {'id': '53909f2c20f70186a0e3719d', 'score': 0.636233807, 'text': 'This thesis describes an effort to construct a scene understanding system that is able to analyze the content of real images. While constructing the system we had to provide solutions to many of the fundamental questions that every student of object recognition deals with daily. These include the choice of data set, the choice of success measurement, the representation of the image content, the selection of inference engine, and the representation of the relations between objects. The main test-bed for our system is the CBCL StreetScenes data base. It is a carefully labeled set of images, much larger than any similar data set available at the time it was collected. Each image in this data set was labeled for 9 common classes such as cars, pedestrians, roads and trees. Our system represents each image using a set of features that are based on a model of the human visual system constructed in our lab. We demonstrate that this biologically motivated image representation, along with its extensions, constitutes an effective representation for object detection, facilitating unprecedented levels of detection accuracy. Similarly to biological vision systems, our system uses hierarchical representations. We therefore explore the possible ways of combining information across the hierarchy into the final perception. Our system is trained using standard machine learning machinery, which was first applied to computer vision in earlier work of Prof. Poggio and others. We demonstrate how the same standard methods can be used to model relations between objects in images as well, capturing context information. The resulting system detects and localizes, using a unified set of tools and image representations, compact objects such as cars, amorphous objects such as trees and roads, and the relations between objects within the scene. The same representation also excels in identifying objects in clutter without scanning the image. Much of the work presented in the thesis was devoted to a rigorous comparison of our system to alternative object recognition systems. The results of these experiments support the effectiveness of simple feed-forward systems for the basic tasks involved in scene understanding. We make our results fully available to the public by publishing our code and data sets in hope that others may improve and extend our results. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)'}\n",
      "\n",
      "7. {'id': '53908b6c20f70186a0dbf5c7', 'score': 0.63575834, 'text': 'The image represents an information structure of an extreme complexity. We present a method for symbolic and structured description with several levels of abstraction. The application area concerns the construction and interpretation knowledge on 3D objects in a machine perception. The knowledge representation and scenes interpretation tasks based on 2D image used \"Perceived Aspects Tables\" and \"Quality Tables\" proposed by our vision system. The necessary knowledge to this task are formalised. Then, we resolve the main interpretation problem, the control and the identification of objects, thanks to the approach called: \"Prediction-Checking of Hypotheses\". The representation, which is suggested, is based on the \"Frames\" Model. With this end in view, an organisation system of perception and scenes interpretation tasks in order to maintain a coherent representation of a structured and evolutive environment is designed. The released concepts and the proposed system are realised in a LE-LISP Object Oriented environment based on the SHIRKA knowledge representation system.'}\n",
      "\n",
      "8. {'id': '5390a6b120f70186a0e8537d', 'score': 0.633365571, 'text': 'Understanding machine vision can certainly improve our understanding of artificial intelligence as vision happens to be one of the basic intellectual activities of living beings. Since the notion of computation unifies the concept of a machine, computer vision can be understood as an application of modern approaches for achieving artificial intelligence, like machine learning and cognitive psychology. Computer vision mainly involves processing of different types of sensor data resulting in ”perception of machines”. Perception of machines plays a very important role in several artificial intelligence applications with sensors. There are numerous practical situations where we acquire sensor data for e.g. from mobile robots, security cameras, service and recreational robots. Making sense of this sensor data is very important so that we have increased automation in using the data. Tools from image processing, shape analysis and probabilistic inferences i.e. learning theory form the artillery for current generation of computer vision researchers. In my thesis I will address some of the most annoying components of two important open problems viz. object recognition and autonomous navigation that remain central in robotic, or in other words computational, intelligence. These problems are concerned with inducing computers, the abilities to recognize and navigate similar to those of humans. Object boundaries are very useful descriptors for recognizing objects. Extracting boundaries from real images has been a notoriously open problem for several decades in the vision community. In the first part I will present novel techniques for extracting object boundaries. The techniques are based on practically successful state-of-the-art Bayesian filtering framework, well founded geometric properties relating boundaries and skeletons and robust high-level shape analyses. Acquiring global maps of the environments is crucial for robots to localize and be able to navigate autonomously. Though there has been a lot of progress in achieving autonomous mobility, for e.g. as in DARPA grand-challenges of 2005 and 2007, the mapping problem itself remains to be unsolved which is essential for robust autonomy in hard cases like rescue arenas and collaborative exploration. In the second part I will present techniques for merging maps acquired by multiple and single robots. We developed physics-based energy minimization techniques and also shape based techniques for scalable merging of maps. Our shape based techniques are a product of combining of high-level vision techniques that exploit similarities among maps and strong statistical methods that can handle uncertainties in Bayesian sense.'}\n",
      "\n",
      "9. {'id': '5390a6b120f70186a0e846a1', 'score': 0.631801665, 'text': 'Among the sources of information that can be used to guide the processing of visual sensory data are the constraints implied by the geometry of the objects being viewed. Experiments have been performed using representations of this type of knowledge to control image processing. They show how such geometric knowledge can be used to aid in the identification of object projections in images of natural outdoor scenes.'}\n",
      "\n",
      "10. {'id': '5390b20120f70186a0ee5e5f', 'score': 0.625960231, 'text': \"One of the grand challenges of artificial intelligence is to enable computers to interpret 3D scenes and objects from imagery. This book organizes and introduces major concepts in 3D scene and object representation and inference from still images, with a focus on recent efforts to fuse models of geometry and perspective with statistical machine learning. The book is organized into three sections: (1) Interpretation of Physical Space; (2) Recognition of 3D Objects; and (3) Integrated 3D Scene Interpretation. The first discusses representations of spatial layout and techniques to interpret physical scenes from images. The second section introduces representations for 3D object categories that account for the intrinsically 3D nature of objects and provide robustness to change in viewpoints. The third section discusses strategies to unite inference of scene geometry and object pose and identity into a coherent scene interpretation. Each section broadly surveys important ideas from cognitive science and artificial intelligence research, organizes and discusses key concepts and techniques from recent work in computer vision, and describes a few sample approaches in detail. Newcomers to computer vision will benefit from introductions to basic concepts, such as single-view geometry and image classification, while experts and novices alike may find inspiration from the book's organization and discussion of the most recent ideas in 3D scene understanding and 3D object recognition. Specific topics include: mathematics of perspective geometry; visual elements of the physical scene, structural 3D scene representations; techniques and features for image and region categorization; historical perspective, computational models, and datasets and machine learning techniques for 3D object recognition; inferences of geometrical attributes of objects, such as size and pose; and probabilistic and feature-passing approaches for contextual reasoning about 3D objects and scenes. Table of Contents: Background on 3D Scene Models / Single-view Geometry / Modeling the Physical Scene / Categorizing Images and Regions / Examples of 3D Scene Interpretation / Background on 3D Recognition / Modeling 3D Objects / Recognizing and Understanding 3D Objects / Examples of 2D 1/2 Layout Models / Reasoning about Objects and Scenes / Cascades of Classifiers / Conclusion and Future Directions\"}\n",
      "\n",
      "11. {'id': '53908bfb20f70186a0dcb3bf', 'score': 0.624751329, 'text': 'The symbolic level of a dynamic scene interpretation system is presented. This symbolic level is based on plan prototypes represented by Petri nets whose interpretation is expressed thanks to 1st order constrained cubes, and on a reasoning aiming at instantiating the plan prototypes with objects delivered by the numerical processing of sensor data. An example on real world data is given.'}\n",
      "\n",
      "12. {'id': '539089d220f70186a0d9ad54', 'score': 0.620958686, 'text': 'It is generally agreed that individual visual cues are fallible and often ambiguous. This has generated a lot of interest in design of integrated vision systems which are expected to give a reliable performance in practical situations. The design of such systems is challenging since each vision module works under a different and possibly conflicting set of assumptions. We have proposed and implemented a multiresolution system which integrates perceptual organization (grouping), segmentation, stereo, shape from shading, and line labeling modules. We demonstrate the efficacy of our approach using images of several different realistic scenes. The output of the integrated system is shown to be insensitive to the constraints imposed by the individual modules. The numerical accuracy of the recovered depth is assessed in case of synthetically generated data. Finally, we have qualitatively evaluated our approach by reconstructing geons from the depth data obtained from the integrated system. These results indicate that integrated vision systems are likely to produce better reconstruction of the input scene than the individual modules.'}\n",
      "\n",
      "13. {'id': '5390881820f70186a0d8204b', 'score': 0.620689273, 'text': 'A framework for high-level representations in computer visionarchitectures is described.The framework is based on the notion of conceptual space.This approach allows us to define a conceptual semantics for the symbolic representations of the vision system. In this way, the semantics of the symbols can be grounded to the data coming fromthe sensors. In addition, the proposed approach generalizesthe most popular frameworks adopted in computer vision.'}\n",
      "\n",
      "14. {'id': '53909f2c20f70186a0e3707c', 'score': 0.619793952, 'text': \"The goal of computer vision is to use an image to recover the characteristics of scene, such as its shape or illumination. This is difficult because an image is the mixture of multiple characteristics. For example, an edge in an image could be caused by either an edge on a surface or a change in the surface's color. Distinguishing the effects of different scene characteristics is an important step towards high-level analysis of an image. This thesis describes how to use machine learning to build a system that recovers different characteristics of the scene from a single, gray-scale image of the scene. The goal of the system is to use, the observed image to recover images, referred to as Intrinsic Component Images, that represent the scene's characteristics. The development of the system is focused on estimating two important characteristics of a scene, its shading and reflectance, from a single image. From the observed image, the system estimates a shading image, which captures the interaction of the illumination and shape of the scene pictured, and an albedo image, which represents ow the surfaces in the image reflect light. Measured both qualitatively and quantitatively, this system produces state-of-the-art estimates of shading and albedo images. This system is also flexible enough to be used for the separate problem of removing noise from an image. Building this system requires algorithms for continuous regression and learning the parameters of a Conditionally Gaussian Markov Random Field. Unlike previous work, this system is trained using real-world surfaces with ground-truth shading and albedo images. The learning algorithms are designed to accommodate the large amount of data in this training set. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)\"}\n",
      "\n",
      "15. {'id': '5390981d20f70186a0e04411', 'score': 0.615413725, 'text': 'Ongoing research at Boston University has produced computational models of biological vision and learning that embody a growing corpus of scientific data and predictions. Vision models perform long-range grouping and figure/ground segmentation, and memory models create attentionally controlled recognition codes that intrinsically combine bottom-up activation and top-down learned expectations. These two streams of research form the foundation of novel dynamically integrated systems for image understanding. Simulations using multispectral images illustrate road completion across occlusions in a cluttered scene and information fusion from input labels that are simultaneously inconsistent and correct. The CNS Vision and Technology Labs (cns.bu.edu/visionlab and cns.bu.edu/techlab) are further integrating science and technology through analysis, testing, and development of cognitive and neural models for large-scale applications, complemented by software specification and code distribution.'}\n",
      "\n",
      "16. {'id': '53908e0020f70186a0dd63ba', 'score': 0.611579359, 'text': 'The problems under consideration center around the interpretation of binocular stereo disparity. In particular, the goal is to establish a set of mappings from stereo disparity to corresponding three-dimensional scene geometry. An analysis has been developed that shows how disparity information can be interpreted in terms of three-dimensional scene properties, such as surface depth, discontinuities, and orientation. These theoretical developments have been embodied in a set of computer algorithms for the recovery of scene geometry from input stereo disparity. The results of applying these algorithms to several disparity maps are presented. Comparisons are made to the interpretation of stereo disparity by biological systems.'}\n",
      "\n",
      "17. {'id': '53908b9320f70186a0dc0050', 'score': 0.611554921, 'text': 'This paper describes a representation for people and animals, called a body plan, which is adapted to segmentation and to recognition in complex environments. The representation is an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties such as the structure of individual parts and the relationships between parts. Body plans can be learned from image data, using established statistical learning techniques. The approach is illustrated with two examples of programs that successfully use body plans for recognition: one example involves determining whether a picture contains a scantily clad human, using a body plan built by hand; the other involves determining whether a picture contains a horse, using a body plan learned from image data. In both cases, the system demonstrates excellent performance on large, uncontrolled test sets and very large and diverse control sets.'}\n",
      "\n",
      "18. {'id': '53909a0320f70186a0e20b09', 'score': 0.610388935, 'text': 'Understanding objects in an image is a current problem in computer vision. This paper propose an image understanding system that recognizes complex objects based on geometric shapes and color. This system is based on a blackboard architecture that uses image processing algorithms as Knowledge Sources (KSs) to extract features in the image, and then infer the presence of certain objects based on the these features. The management of these KSs will be driven by a reasoning system like a belief network or a rule-based system.'}\n",
      "\n",
      "19. {'id': '53908f5b20f70186a0dda5f2', 'score': 0.608045876, 'text': \"IMAGE INTERPRETATION IS A COMPLEX PROCESS THROUGH WHICH A NUMERIC ARRAY, REPRESENTING A DIGITIZED VISUAL SCENE, CAN BE ANALYZED TO PROVIDE A SEMAN- TIC DESCRIPTION OF THE SCENE CONTENT. ONE SUBGOAL OF THE INTERPRETATION PROCESS IS IMAGE SEGMENTATION, THE LOW-LEVEL PROCESS BY WHICH THE DIGITIZED IMAGE IS ABSTRACTED INTO A SET OF PRIMITIVE ELEMENTS THAT MAY BE USED AS THE BASIS FOR THE CONSTRUCTION OF AN ABSTRACT SYMBOLIC MODEL OF THE ORIGIN- AL SCENE. A COMMONLY ACCEPTED VIEW IS THAT IMAGE SEGMENTATION IS SIMPLY THE FIRST STAGE OF THE INTERPRETATION PROCESS. WE TAKE THE VIEW, HOWEVER, THAT SEGMENTATION IS A PROCESS WHICH DOES NOT EXIST IN ISOLATION, BUT RATHER AS AN INTEGRAL PART OF THE OVERALL IMAGE INTERPRETATION PROCESS. THIS DISSERTATION PRESENTS `GOLDIE'',AN INTERMEDIATE-LEVEL, GOAL-DIRECTED SYSTEM THAT HAS BEEN DEVELOPED WITHIN THE VISIONS SYSTEM TO ACCOMPLISH THE INTEGRATION OF THE HIGH AND LOW LEVELS OF THE IMAGE UNDERSTANDING PROCESS. GOLDIE IS A SYSTEM DESIGNED TO OPERATE IN EITHER A DATA-DRIVEN OR INTERPRE- TATION-DRIVEN MANNER. IN THE DATA-DRIVEN MODE, GOLDIE FUNCTIONS AS A SEG- MENTATION SYSTEM THAT IS ABLE TO CHOOSE BETWEEN A VARIETY OF ALGORITHMS AND IMAGE FEATURES BASED ON THE CHARACTERISTICS OF THE IMAGE DATA. IN THE IN- TERPRETATION-DRIVEN MODE, THE SYSTEM PROVIDES THE MECHANISM BY WHICH HIGH- LEVEL REQUESTS FOR DATA (GOALS) CAN ACTIVATE A SET OF APPROPRIATE LOW OR INTERMEDIATE-LEVEL PROCESSES THAT MAY BE CAPABLE OF PRODUCING THE DESIRED\"}\n",
      "\n",
      "20. {'id': '539087a120f70186a0d475d7', 'score': 0.606724441, 'text': 'Results from an ongoing project concerned with recognizing objects in complex scene domains, especially in the domain that includes the natural outdoor world, are described. Traditional machine recognition paradigms assume either that all objects of interest are definable by a relatively small number of explicit shape models or that all objects of interest have characteristic, locally measurable features. The failure of both assumptions has a dramatic impact on the form of an acceptable architecture for an object recognition system. In this work, the use of the contextual information is a central issue, and a system is explicitly designed to identify and use context as an integral part of recognition that eliminates the traditional dependence on stored geometric models and universal image partitioning algorithms. This paradigm combines the results of many simple procedures that analyze monochrome, color, stereo, or 3D range images. Interpreting the results along with relevant contextual knowledge makes it possible to achieve a reliable recognition result, even when using imperfect visual procedures. Initial experimentation with the system on ground-level outdoor imagery has demonstrated competence beyond what is attainable with other vision systems.'}\n",
      "\n",
      "21. {'id': '5390a37f20f70186a0e6c6f8', 'score': 0.606442451, 'text': \"Based on the neurobiological and cognitive principles of human information processing, we develop a system for the automatic visual identification and exploration of scenes. The system architecture consists of three layers: a bottom-up feature extraction stage, a top-down object identification stage and knowledge from a domain ontology for scene analysis. The uncertainty in the latter two stages is managed by Dempster-Shafer belief measures. The system sequentially selects ''informative'' image regions, identifies the local structure in these regions, and uses this information for drawing efficient conclusions about an object in the scene. The selection process involves low-level, bottom-up processes for sensory feature extraction, and cognitive top-down processes for the generation of active motor commands that control the positioning of the sensors towards the most informative regions. Both processing levels have to deal with uncertain data, and have to take into account learned statistical knowledge. For bottom-up feature extraction this is achieved by integrating a nonlinear filtering stage modeled after the neural computations performed in the early stages of the visual system. The top-down cognitive reasoning strategy operates in an adaptive fashion on a belief distribution. The resulting object hypotheses in combination with knowledge from the domain ontology in the third layer are used for generating a scene hypothesis.\"}\n",
      "\n",
      "22. {'id': '5390985d20f70186a0e088fb', 'score': 0.605695367, 'text': 'Computer vision is the process of using computers to extract from images useful information about the physical world, including meaningful descriptions of physical objects. For example, if an image sensor, such as a digitizing video camera, captured an image of a physical scene, and the digital image was input to a computer vision system, the desired output would be a description of the physical scene in terms that would be useful for the particular task at hand. Computer vision has many applications, including robotics, industrial automation, document processing, remote sensing, navigation, microscopy, medical imaging, and the development of visual prostheses for the blind.'}\n",
      "\n",
      "23. {'id': '5390877920f70186a0d2d883', 'score': 0.605682552, 'text': 'Superimposition of two image data sets allows the spatial distribution of one to be directly related to that of the other. If the two data sets have different spatial structures, the composite image is generally confusing and difficult to interpret. A method of representing image data sets in the form of naturally occurring variables in a realistic apparently three-dimensional scene is presented. One data set is represented by the topography of a surface, depicted by shaded-relief methods, while another is represented by the color of the surface, or by the color of an overlaid transparency. Presentation in this form exploits the normal scene decomposition abilities of the human visual system, allowing intuitive appreciation and separation of the scene, and hence data set, variables. The method relies on techniques for the modeling of surfaces and surface reflectance to render the synthesised scenes realistically.'}\n",
      "\n",
      "24. {'id': '5390958920f70186a0dee255', 'score': 0.605640292, 'text': \"Scientists and experts have explored the mechanism of visual systems for decades for smart image processing and pattern recognition in order to satisfy sophisticated engineering applications. In this paper we apply independent component analyses' (ICA) unsupervised learning to natural images, topography images and other special environment images for demonstrating the simple-cell's process in animal vision. Our results confirm an early biological experiment (Nature 228 (1970) 419) about the growth of simple cells in cat's V1 area. Furthermore, by applying ICA methodology and the simplex algorithm, the unsupervised neural synapse's learning can obtain the receptive fields in visual cortex and can simulate the growth of the visual cortex of young animal in the special environment. These findings imply that an input image can be efficiently represented by ICA bases. An application of image matching in the navigation by ICA is shown that the animal visual system method is indeed better than those classical methods at least more than 5%.\"}\n",
      "\n",
      "25. {'id': '53908a7420f70186a0da2e98', 'score': 0.605498493, 'text': 'The task of visual classification is thce recognition of an object in the image as belonging to a general class of similar objects, such as a face, a car, a dog, and the like. This is a fundamental and natural task for biological visual systems, but it has proven difficult to perform visual classification by artificial computer vision systems. The main reason for this difficulty is the variability of shape within a class: different objects vary widely in appearance, and it is difficult to capture the essential shape features that characterize the members of one category and distinguish them from another, such as dogs from cats. In this paper we describe an approach to classification using a fragment-based representation. In this approach, objects within a class are represented in terms of common image fragments that are used as building blocks for representing a large variety of different objects that belong to a common class. The fragments are selected from a training set of images based on a criterion of maximizing the mutual information of the fragments and the class they represent. For the purpose of classification the fragments are also organized into types, where each type is a collection of alternative fragments, such as different hairline or eye regions for face classification. During classification, the algorithm detects fragments of the different types, and then combines the evidence for the detected fragments to reach a final decision. Experiments indicate that it is possible to trade off the complexity of fragments with the complexity of the combination and decision stage, and this tradeoff is discussed. The method is different from previous part-based methods in using class-specific object fragments of varying complexity, the method of selecting fragments, and the organization into fragment types. Experimental results of detecting face and car views show that the fragment-based approach can generalize well to a variety of novel image views within a class while maintaining low mis-classification error rates. We briefly discuss relationships between the proposed method and properties of parts of the primate visual system involved in object perception'}\n",
      "\n",
      "26. {'id': '53908f5b20f70186a0dd9493', 'score': 0.605486572, 'text': 'It is too hard to tell vision systems what things look like. It is easier to talk about purpose and what things are for. Consequently, we want vision systems to use functional descriptions to identify things, when necessary, and we want them to learn physical descriptions for themselves, when possible. This paper describes a theory that explains how to make such systems work. The theory is a synthesis of two sets of ideas: ideas about learning from precedents and exercises developed at MIT and ideas about physical description developed at Stanford. The strength of the synthesis is illustrated by way of representative experiments. All of these experiments have been performed with an implementation system.'}\n",
      "\n",
      "27. {'id': '5390a93b20f70186a0ea02a3', 'score': 0.604416, 'text': 'Image parsing continues to be a challenging research task in the field of computer vision. In this dissertation, we have developed a hybrid image parser which accounts for different vision-related phenomena (i) the perception of objects, their parts and the relationships between them; (ii) the use of semantic, spoken language to describe attributes of objects in images to be parsed and a heterogeneous computational model for object/part recognition; (iii) an image segmentation process which uses multiple visual cues and (iv) an optimization technique which reduces the solution space for scene identification. The parser is built on an image grammar-based framework. Because the patterns to be analyzed are often multifarious, with one element having numerous diverse parts to it, we have developed a general symbol-based ontology paradigm that describes complex image patterns in terms of a hierarchical composition of simpler subpatterns. The relationships between objects, and with their parts, are presented using first order formal logic. In order to perform rapid scene parsing and identification, the input image is over-segmented to yield \"superpixels\", which are a locally, coherent grouping of pixels that preserve the structure necessary for image parsing. Their usage greatly reduces the computational complexity of the parser.In the thesis, we also present an algorithm for performing a (near) global Markov Random Field (MRF) optimization for labeling segmented images. Our previous results of labeling images using the pairwise and local interactions only, are also presented. The labeling results are produced primarily using the semantic attributes of objects, such as, blue skies, green vegetation etc. Because semantic attributes alone are not sufficient to fully describe objects and their parts, we develop/use different computational models such as a human skin color model, the output of a probabilistic classifier for detecting the presence of buildings, a face detector etc. These different models result in the hybrid nature of the image parser. Going forward, we intend to include the use of more visual perception cues such as depth and motion to further constrain the labeling process. Also, although the MRF optimization algorithm greatly reduces the search space for a (near) global solution, it still runs in an order exponential in the number of nodes present in the graph. We can further investigate methods to reduce its computational complexity. Lastly, this method will be useful for 3-dimensional data labeling, where many different spatial constraints can be better enforced than in 2-dimensional images. We therefore intend to apply this labeling technique to 3-D medical data in the future.The symbol-based ontology was developed for the natural images domain, specifically for outdoor images. Several segmentation algorithms including our in-house technique were compared using image benchmark data found in the Berkeley Database System (BDS). The image parser was tested on natural images from the BDS, from the Lotus Hill dataset and on natural photographs from flickr.com.'}\n",
      "\n",
      "28. {'id': '5390bed320f70186a0f4fa02', 'score': 0.60333091, 'text': '3D model decomposition is a challenging and important problem in computer graphics. Several semantically based approaches have been proposed in the literature, however, due to the lack of proper evaluation criteria, comparison of these techniques is almost impossible. In this paper we suggest to use animal anatomy as the ground truth and compare the result of different segmentation techniques based on that. Differing from previous approaches which perform the evaluation based on ground truth databases created subjectively by human observers, we consider expert knowledge on anatomy of various animals. Based on this knowledge we specify the ground truth for different animals and compare alternative algorithms.'}\n",
      "\n",
      "29. {'id': '5390882420f70186a0d88d49', 'score': 0.599342, 'text': \"From the Publisher:This book presents a coherent approach to the fast-moving field of computer vision, using a consistent notation based on a detailed understanding of the image formation process. It covers even the most recent research and will provide a useful and current reference for professionals working in the fields of machine vision, image processing, and pattern recognition. An outgrowth of the author's course at MIT, Robot Vision presents a solid framework for understanding existing work and planning future research. Its coverage includes a great deal of material that is important to engineers applying machine vision methods in the real world. The chapters on binary image processing, for example, help explain and suggest how to improve the many commercial devices now available. And the material on photometric stereo and the extended Gaussian image points the way to what may be the next thrust in commercialization of the results in this area. Chapters in the first part of the book emphasize the development of simple symbolic descriptions from images, while the remaining chapters deal with methods that exploit these descriptions. The final chapter offers a detailed description of how to integrate a vision system into an overall robotics system, in this case one designed to pick parts out of a bin. The many exercises complement and extend the material in the text, and an extensive bibliography will serve as a useful guide to current research. Errata (164k PDF)\"}\n",
      "\n",
      "30. {'id': '539090c420f70186a0ddddda', 'score': 0.598750412, 'text': \"One of the main objectives of computer vision systems is to produce structural descriptions of the scenes depicted in images. Knowledge of the class of objects being imaged can facilitate this objective by providing models to guide interpretation, and by furnishing a basis for the structural descriptions. This document describes research into techniques for the representation and use of knowledge of object classes, carried out within the context of a computational vision system which interprets line drawings of human-like body forms. .br A declarative schemata format has been devised which represents structures of image features which constitute dep- ictions of body parts. The system encodes relations between these image constructions and an underlying three dimensional model of the human body. Using the component hierarchy as a structural basis, two layers of representation are developed. One references the fine resolution features, and the other references the coarse resolution. These layers are connected with links representative of the specialization/generalization hierarchy. The problem domain description is declarative, and makes no commitment to the nature of the subsequent interpretation processes. As a means of testing the adequacy of the representation, portions have been converted into a PROLOG formulation and used to ``prove'''' body parts in a data base of assertions about image properties. .br The interpretation phase relies on a cue/model approach, using an extensive cue table which is automatically generated from the problem domain description. The primary mechanisms for control of interpretation possibilities are fashioned after network consistency methods. The operation of these mechanisms is localized and separated between operations at the feature level and at the model level. .br The body drawing interpretation system is consistent with aspects of human visual perception. The system is capable of intelligent selection of processing locations on the basis of the progress of interpretation. A dual resolution retina is moved about the image collecting fine level features in a small foveal area and coarse level features in a wider peripheral area. Separate interpretations are developed locally on the basis of the two different resolution levels, and the relation between these two interpretations is analyzed by the system to determine locations of potentially useful information.\"}\n",
      "\n",
      "31. {'id': '53908bfb20f70186a0dc9833', 'score': 0.59862572, 'text': \"Humans are quite adept at recognizing and labeling regions and objects in visual scenes. One of the cues for such labeling is the spatial relationships exhibited among the regions. This is usually coupled with the interpreter's understanding and expectations of scene content. For example, it is normally the case that, in a natural outdoor scene, the sky should be above the trees and that vehicles should be on a road. Context plays a very important role in the interpretation of an image. This determination of spatial relations has been a difficult task to automate. There have been several attempts at defining spatial relationships between regions in a digital image, most recently, with the use of fuzzy set theory. In this paper, we examine three methods for defining spatial relations to gain insight into this complex situation.\"}\n",
      "\n",
      "32. {'id': '539090c420f70186a0dddb8d', 'score': 0.598531723, 'text': \"This paper is concerned with the problem of attaching meaningful symbols to aspects of the visible environment in machine and biological vision. It begins with a review of some of the arguments commonly used to support either the ''symbolic'' or the ''behaviourist'' approach to vision. Having explored these avenues without arriving at a satisfactory conclusion, we then present a novel argument, which starts from the question : given a functional description of a vision system, when could it be said to support a symbolic interpretation? We argue that to attach symbols to a system, its behaviour must exhibit certain well defined regularities in its response to its visual input and these are best described in terms of invariance and equivariance to transformations which act in the world and induce corresponding changes of the vision system state. This approach is illustrated with a brief exploration of the problem of identifying and acquiring visual representations having these symmetry properties, which also highlights the advantages of using an ''active'' model of vision.\"}\n",
      "\n",
      "33. {'id': '5390980720f70186a0e028ef', 'score': 0.597265303, 'text': 'This paper summarizes the present state of research in scene analysis. It identifies fundamental information processing principles relevant to representation and Use of knowledge in vision and traces limitations of existing progams to compromises of these principles necessitated by extant processors Some specific and general recommendations are offered regarding a productive course of research for the next decade.'}\n",
      "\n",
      "34. {'id': '53908e0020f70186a0dd6260', 'score': 0.597037315, 'text': 'Methods are presented 1) to partition or decompose a visual scene into the bodies forming it; 2) to position these bodies in three-dimensional space, by combining two scenes that make a stereoscopic pair; 3) to find the regions or zones of a visual scene that belong to its background; 4) to carry out the isolation of the objects in 1) when the input has inaccuracies. Running computer programs implement the methods and many examples illustrate their behavior. The input is a two-dimensional line-drawing of the scene, assumed to contain three-dimensional bodies possessing flat faces (polyhedra); some of them may be partially occluded. Suggestions are made for extending the work to curved objects. Some comparisons are made with human visual perception. The main conclusion is that it is possible to separate a picture or scene into the constituent objects exclusively on the basis of monocular geometric properties (on the basis of pure form); in fact, successful methods are shown.'}\n",
      "\n",
      "35. {'id': '5390a6b120f70186a0e84703', 'score': 0.595318675, 'text': \"One of the most fundamental problems in vision is segmentation; the way in which parts of an image are perceived as a meaningful whole. Recent work has shown how to calculate images of physical parameters from raw intensity data. Such images are known as intrinsic images, and examples are images of velocity (optical flow), surface orientation, occluding contour, and disparity. While intrinsic images are not segmented, they are distinctly easier to segment than the original intensity image. Segments can be detected by a general Hough transform technique. Networks of feature parameters are appended to the intrinsic image organization. Then the intrinsic image points are mapped into these networks. This mapping will be many-to-one onto parameter values that represent segments. This basic method is extended into a general representation and control technique with the addition of three main ideas: abstraction levels; sequential search; and tight counting These ideas are a nucleus of a connectionist theory of low 'eve and m'ermediate-level vision. This theory explains segmentation in terms of massively parallel cooperative computation among intrinsic images and a set of parameter spaces at different levels of abstraction.\"}\n",
      "\n",
      "36. {'id': '53908f5b20f70186a0dda765', 'score': 0.595041215, 'text': \"THE `SCHEMA SYSTEM'' IS A HIGH-LEVEL IMAGE UNDERSTANDING SYSTEM FOR THE INTERPRETATION OF COMPLEX NATURAL SCENES. IT IS OUR POSITION THAT THE CONSTRAINTS AVAILABLE FROM GENERAL KNOWLEDGE ABOUT THE WORLD MUST BE USED TO ORGANIZE RAW IMAGE DESCRIPTIONS INTO ABSTRACT INTERPRETATIONS. THE OUTPUT OF LOW-LEVEL IMAGE OPERATIONS DOES NOT MEET THE REQUIREMENTS OF NATURAL-SCENE INTERPRETATION. INTERMEDIATE-LEVEL GROUPING PROCEDURES REPRESENT A PARTIAL SOLUTION TO THIS MISMATCH, BUT ARE TOO EXPENSIVE TO APPLY INDISCRIMINATELY. HIGH-LEVEL PROCESSES USE OBJECT-SPECIFIC KNOWLEDGE TO GUIDE INTERPRETATION BY FOCUSING ATTENTION ON PROMISING AREAS OF THE IMAGE. THIS PAPER DESCRIBES THE SCHEMA SYSTEM - A FLEXIBLE, HIGH-LEVEL SYSTEM SUPPORTING THE INTERACTION OF OBJECT-SPECIFIC INTERPRETATION PRO- CESSES - AND PRESENTS THE RESULTS IT HAS PRODUCED ON ROAD SCENES AS A JUSTIFICATION OF THE KNOWLEDGE ENGINEERING APPROACH TO IMAGE UNDERSTANDING.\"}\n",
      "\n",
      "37. {'id': '5390a6b120f70186a0e85421', 'score': 0.594696224, 'text': 'The goal of image understanding has long been a shared goal in the field of computer vision. Extracting the required scene-level information from image data is a formidable task, however. While the raw data comes in the form of matrices of numbers, the inferences that we must perform occur at a much higher level of abstraction. Much progress has been made in recent years in extracting the primitives of an image in isolation, for example detecting the objects, labeling the regions, or extracting the surfaces. Modeling the interactions and fine-grained distinctions between these primitives is an important next step along the path to scene understanding. Because this involves reasoning about relationships between heterogeneous entities at a high level of abstraction, this problem lends itself well to the tools of probabilistic graphical models. In this thesis we consider two important challenges in this space. In the first, we model interactions between primitives of various types in order to capture the contextual relationships between them. For example, we can expect to find a sheep (a detected object) more often on a field of grass (a labeled region) than on a patch of water. By modeling the interactions between these components, we can expect to improve the quality of classification by leveraging these contextual cues. We first introduce Cascaded Classification Models (CCM), a flexible framework for combining various state-of-the-art vision models in a way that allows for improved performance of each model. We next consider the tasks of object detection and region labeling and develop a more sophisticated probabilistic model aimed at capturing the contextual relationships between these types of primitives in a more targeted and meaningful way. Our Things and Stuff (TAS) context model learns to leverage contextual cues directly from data. Following this exploration of interactions between objects, we consider the interactions of \"parts\" within an object, and in particular tackle the problem of descriptive querying of objects. This involves making distinctions about the object at a refined level, beyond mere categorization. For example, we may want to know whether a cheetah in an image is running or standing still. We introduce a probabilistic deformable shape model (LOOPS) and a method for matching this model to an image that allows for precise localization of several object classes. Using this localization, we show how descriptive distinctions can be drawn with a small amount of training data.'}\n",
      "\n",
      "38. {'id': '539087a620f70186a0d49630', 'score': 0.594638526, 'text': 'A data-driven system for segmenting scenes into objects and their components is presented. This segmentation system generates hierarchies of features that correspond to structural elements such as boundaries and surfaces of objects. The technique is based on perceptual organization, implemented as a mechanism for exploiting geometrical regularities in the shapes of objects as projected on images. Edges are recursively grouped on geometrical relationships into a description hierarchy ranging from edges to the visible surfaces of objects. These edge groupings, which are termed collated features, are abstract descriptors encoding structural information. The geometrical relationships employed are quasi-invariant over 2-D projections and are common to structures of most objects. Thus, collations have a high likelihood of corresponding to parts of objects. Collations serve as intermediate and high-level features for various visual processes. Applications of collations to stereo correspondence, object-level segmentation, and shape description are illustrated.'}\n",
      "\n",
      "39. {'id': '5390a6b120f70186a0e850c5', 'score': 0.593849659, 'text': 'We show that an intelligent approach to color can be used to significantly improve the capabilities of a vision system. In previous work, we adopted general physical models which describe how objects interact with light. These models are far more general than those typically used in computer vision. In this report, we use our physical models to derive powerful algorithms for extracting invariant properties of objects from images. The first algorithm is used for the generic classification of objects according to material. The second algorithm provides a solution to the color constancy problem. These algorithms have been implemented and produce correct results on real images. Some examples of experimental results are presented.'}\n",
      "\n",
      "40. {'id': '5390a9a520f70186a0ea6ba1', 'score': 0.593736351, 'text': 'One of the big issues facing current content-based image retrieval is how to automatically extract the high-level concepts from images. In this paper, we present an efficient system that automatically extracts the high-level concepts from images by using ontologies and semantic inference rules. In our method, MPEG-7 visual descriptors are used to extract the visual features of image, and the visual features are mapped to semi-concepts via the mapping algorithm. We also build the visual and animal ontologies to bridge the semantic gap. The visual ontology allows the definition of relationships among the classes describing the visual features and has the values of semi-concepts as the property values. The animal ontology can be exploited to identify the highlevel concept in an image. Also, the semantic inference rules are applied to the ontologies to extract the high-level concept. Finally, we evaluate the proposed system using the image data set including various animal objects and discuss the limitations of our system.'}\n",
      "\n",
      "41. {'id': '53908a9620f70186a0da4b3e', 'score': 0.593575478, 'text': 'The increasing amount of remotely sensed imagery from multiple platforms requires efficient analysis techniques. The leading idea of the presented work is to automate the interpretation of multisensor and multitemporal remote sensing images by the use of common prior knowledge about landscape scenes. In addition the system can use specific map knowledge of a GIS, information about sensor projections and temporal changes of scene objects. Prior expert knowledge about the scene content is represented explicitly by a semantic net. A common concept has been developed to distinguish between the semantics of objects and their visual appearance in the different sensors considering the physical principle of the sensor and the material and surface properties of the objects. A flexible control system is used for the automated analysis, which employs mixtures of bottom up and top down strategies for image analysis dependent on the respective state of interpretation. The control strategy employs rule based systems and is independent of the application. The system permits the fusion of several sensors like optical, infrared, and SAR-images, laser-scans etc. and it can be used for the fusion of images taken at different instances of time. Sensor fusion can be achieved on a pixel level, which requires prior rectification of the images, on feature level, which means that the same object may show up differently in different sensors, and on object level, which means that different parts of an object can more accurately be recognized in different sensors. Results are shown for the extraction of roads from multisensor images. The approach for a multitemporal image analysis is illustrated for the recognition and extraction of an industrial fairground from an industrial area in an urban scene.'}\n",
      "\n",
      "42. {'id': '5390a6b120f70186a0e845c2', 'score': 0.593409419, 'text': 'This paper overviews and discusses model representations and control structures in image understanding. Hierarchies are observed in the levels of description used in image understanding along a few dimensions: processing unit, detail, composition and scene/view distinction . Emphasis is placed on the importance of explicitly handling the hierarchies both in representing knowledge and in using it . A scheme of \"knowledge block\" representation which is structured along the processing-unit hierarchy is also presented.'}\n",
      "\n",
      "43. {'id': '5390958a20f70186a0defe3b', 'score': 0.592800379, 'text': 'This paper presents a generic cognitive vision platform for the automatic recognition of natural complex objects. The recognition consists of three steps : image processing for numerical object description, mapping of numerical data into symbolic data and semantic interpretation for object recognition. The focus of this paper is the distributed platform architecture composed of three highly specialized Knowledge Based Systems (KBS). The first KBS is dedicated to semantic interpretation. The second one has to deal with the anchoring of symbolic data into image data. The last KBS is dedicated to intelligent image processing. Aftera brief overview of the natural object recognition problem, this paper describes the three subcomponents of the platform.'}\n",
      "\n",
      "44. {'id': '53908b6c20f70186a0dbf5c9', 'score': 0.592422783, 'text': 'Discusses the intention to provide robots with a computer system which gives them the capability to build an abstract and condensed description of their environment from low level data provided by an artificial vision system. This intention rapidly faced a complexity barrier: on one hand, the importance of the volume of information conveyed by an image they manipulate; on the other hand, the access to pertinent information to validate decision making. To reach this goal, research is oriented towards extracting a set of computing tools learning in two ways: descriptive process of the objects to manipulate, and constructive process of the pertinent information for each operation to apply to an object.'}\n",
      "\n",
      "45. {'id': '5390a6b120f70186a0e84cf5', 'score': 0.591365337, 'text': 'A model to extract a structure feature of images belonging to one category is proposed, implemented and studied. This model consists of two parts i.e. an extraction of partial geometrical features and an extraction of structure feature by using them under the positional constraints. First, necessary conditions which geometrical features should satisfy is stated and a statistical method to select a key feature is described. Then the algorithm to extract structure feature as the maximal set of common subimages which satisfy the positional constraint is introduced. The structure feature is regarded as a intrinsic feature which is used when an animal perceives an image at a glance. The experiment was carried out about image of human faces and had a fairly good results.'}\n",
      "\n",
      "46. {'id': '5390893e20f70186a0d941ff', 'score': 0.591065109, 'text': 'We present three unsupervised artificial neural networksfor the extraction of structural information from visual data. Theability of each network to represent structured knowledge in a mannereasily accessible to human interpretation is illustrated usingartificial visual data. These networks are used to collectivelydemonstrate a variety of unsupervised methods for identifyingfeatures in visual data and the structural representation of thesefeatures in terms of orientation, temporal and topographicalordering, and stereo disparity.'}\n",
      "\n",
      "47. {'id': '5390ac1720f70186a0eb2689', 'score': 0.591042638, 'text': 'The Holy Grail of computer vision and image analysis is to develop an artificial visual intelligence system that can recognize, learn, and identify, in the general case, arbitrary objects in arbitrary situations. To date, cognitive science researchers have been studying for centuries how biological systems accomplish this. Concurrently, research in the fields of computer vision and image analysis has been investigating how artificial systems can accomplish this. This paper proposes what could happen and documents what has happened when these fields, the biological, psychological, and the artificial, are brought together to offer a solution to the intelligent image segmentation problem—a precursor to obtaining the grail.'}\n",
      "\n",
      "48. {'id': '53908f5b20f70186a0dda80b', 'score': 0.590138793, 'text': 'This paper addresses the issue of combining intensity information from multiple images for analysis of object surface intensities to support model refinement and scene rendering. Given camera and light source parameters for each image, and a 3D CAD model of objects in the scene, the textures of object surfaces are systematically collected into an organized orthographic library. Occlusions and shadows caused by objects in the scene are predicted from the model and associated with each retrieved surface. The problem of combining intensities from multiple images is explored, and the result is an algorithm that produces a unique surface intensity representation that is as complete and consistent as possible. A successful application of this approach to model refinement and scene visualization is shown, using results from the RADIUS aerial image understanding project.'}\n",
      "\n",
      "49. {'id': '53908d6520f70186a0dd2900', 'score': 0.589661062, 'text': 'Object recognition systems needs of image segmentation processes that relate image regions to world objects. These methods present often three problems: the generation of a large number of small regions, under-segmentation (different objects are associated to the same image region) and over-segmentation (a scene object is segmented in various regions). In order to overcome these problems, we propose an image segmentation method that combines depth information and object surface properties obtained from a pair of stereo images. The system work under the standard assumption that 3D objects have planar faces and regular shapes. First, a region growing segmentation process is applied to both images generating two labeled images. Then, depth information of the region frontiers is obtained by matching the labeled segments from left and right image rows. Finding a path through a 2D search plane whose axes are the left and right-segmented lines solves the stereo matching problem. Original image regions are then merged based on their size, surface information and frontier depth information. In this way, image regions are associated to surfaces that are contiguous in the 3D space, and they present a common property (as gray level, color or texture).'}\n",
      "\n",
      "50. {'id': '53909f8c20f70186a0e3f75c', 'score': 0.589602, 'text': 'In this work, we present effective methods for detecting and representing invariant features of three dimensional (3-D) objects from single images. Our methods are independent of the viewpoint position, and can be used in the recognition process of 3-D objects. The invariant features are; shapes of the object surfaces, organization of the surfaces within the object, and the new feature which is shapes of privileged surfaces (PS) of the object. Also we explain the way these features are represented and matched against stored model of the object. Our work is built around the knowledge based approach and depends on the fact that while it is true that the appearance of a 3-D object may change completely as it is viewed from different viewpoints, it is also true that many aspects of the object projection remain invariant over large ranges of viewpoints. This approach is different from the other approach that derives or calculates depth information and orientations of surfaces using more than one image of the object, or the approach that statistically detects and matchs the object features. The main contribution of our work is the introduction of privileged surface of 3-D object which plays an important role in the recognition process. Also we introduce heuristic methods for detecting shapes of surfaces, object organization, object & model representations and matching. The idea of matching privileged surfaces reduces the ambiguity that may arise due to limited visibility of the object surfaces (existence of hidden surfaces) when matching features of both the image and the model. Also matching privileged surfaces reduces the need of detecting and matching orientations of surfaces. This approach has been applied to 3-D objects which are bounded by planar surfaces. However, the work can be extended in a straightforward manner to support 3-D objects that are bounded by curved surfaces.'}\n",
      "\n",
      "51. {'id': '5390a6b120f70186a0e84978', 'score': 0.588928342, 'text': 'We are developing a new paradigm for a world model construction system which interprets a scene and builds a world model for a mobile robot using dynamic semantic constraints. The system represents a world model in hierarchical form sensor-based maps to a global map with both numerical and symbolic descriptions. At the beginning of interpretation, sensory data (video and range images) are analyzed in bottom-up fashion. A range image is transformed into a height map, and analyzed for the purpose of generating a geometrical property list for both obstacle and traversable regions that is used as the initial input to the interpretation process. At each step of the scene interpretation process, the most reliable feature of an object is selected in the region property list to propagate semantic constraints on other objects close to it. Geometrical modeling for individual objects in the scene is performed, and parameters of each model are dynamically refined by the scene interpretation process. These model parameters and their interrelationships make spatial reasoning robust. Preliminary results with video and range images are shown.'}\n",
      "\n",
      "52. {'id': '5390b9d520f70186a0f307a5', 'score': 0.588881612, 'text': 'With respect to the menagerie of possible observed 3D scenes, no algorithm today for reconstructing such a scene from a stereo pair of images is uniformly better than all the others by their accuracy and processing speed. Generally, appropriate stereo reconstruction algorithms should be selected in accord with a type, or context of the scene and in many cases different parts of the same scene could be reconstructed most accurately by using different algorithms. To qualitatively explore this problem, we collected a database of more than 1,500 stereo pairs of natural and artificial indoor and outdoor 3D scenes, arranged into 25 types, such as animals, bars, city roads, city trees, classrooms, coasts, corridors, etc. The images were processed with two algorithms: the 2D graph-cut stereo (2DGCS) and the 1D belief propagation stereo (1DBPS) -- with automatically estimated parameters. The obtained depth maps were visually evaluated and compared by a number of independent human observers. Although in literature the 2DGCS is usually considered as more accurate than the 1DBPS, its depth maps were preferred by the observers in these experiments only for about 58% of the images and 15 out of the 25 types of scenes. The fast and easily parallelised 1DBPS restores smooth continuous curved surfaces but with noisy object boundaries due to horizontal streaks, whereas the much slower and intrinsically sequential 2DGCS returns flattened depth maps with distinctive object boundaries. Based on these results, we implemented context recognition to examine an input scene and allocate the most suitable algorithm and proposed to combine the 2DGCS and 1DBPS into a composite stereo reconstruction technique, which is qualitatively superior than each individual algorithm and is able to return better results and with good speed.'}\n",
      "\n",
      "53. {'id': '5390a6b120f70186a0e84575', 'score': 0.588096142, 'text': 'Automatons and other robotic applications which are designed to move around and interact with their physical environment need a computer vision system for recognizing and understanding the spatial relationships of objects in real world scenes. The perceptual system must be able to identify salient objects in a scene, develop an understanding of their spatial relationships, and maintain continuity from one view to the next as either the objects or the system\\'s camera moves through the scene. Outlined here and described in more detail in Douglass, 1977, is a system which has been implemented in SIMULA and tested on hand coded outdoor scenes of simple subjects such as houses and automobiles. It uses a recognition cone, a segmentation algorithm for dividing a scene into similar regions and a routine for constructing a three dimensional world model. Visual inference routines interpret perspective, shadows, highlights, occlusions, shading and texture gradients, and monocular motion parallax. Other visual knowledge is added with long term models and short term object representations. The final program will be tested on color photographs of outdoor scenes using as input a series of views of the same scene from different angles which approximates what an automaton would \"see\" as it moves down a street.'}\n",
      "\n",
      "54. {'id': '5390878e20f70186a0d3b416', 'score': 0.587305486, 'text': \"A model-based approach has been proposed to make object recognition computationally tractable. In this approach, models associated with objects expected to appear in the scene are recorded in the system's knowledge base. The system extracts various features from the input images using robust, low-level, general-purpose operators. Finally, matching is performed between the image-derived features and the scene domain models to recognize objects. Factors affecting the successful design and implementation of model-based vision systems include the ability to derive suitable object models, the nature of image features extracted by the operators, a computationally effective matching approach, knowledge representation schemes, and effective control mechanisms for guiding the systems's overall operation. The vision system they describe uses gray-scale images, which can successfully handle complex scenes with multiple object types.\"}\n",
      "\n",
      "55. {'id': '539089d320f70186a0d9b0c3', 'score': 0.58675909, 'text': 'Hardware, software tools, algorithms, and performance metrics that have been developed for image understanding are presented. Three commercially built examples reflecting three mature approaches considered germane to vision-single-instruction multiple-data, multiple-instruction multiple-data, and systolic processing-were chosen. They are, respectively, the Connection Machine, the Butterfly, and the Warp. A fourth approach, more specific to vision, was also selected for noncommercial implementation. This machine, the Image-Understanding Architecture, involves a heterogeneous combination of parallel processors with single-instruction multiple-data, multiple-instruction multiple-data, and other capabilities. Each site employing one of the above architectures developed a different set of tools, leading to significant cross-fertilization of ideas between the sites. Algorithms for low-level vision, shape from texture, fusing stereo and texture, surface interpolation, and robot navigation, among others, are briefly discussed. Benchmarks are described.'}\n",
      "\n",
      "56. {'id': '53908a4020f70186a0d9f0c5', 'score': 0.585381806, 'text': 'Stereoscopic vision has a fundamental role both for animals and humans. Nonetheless, in the computer vision literature there is limited reference to biological models related to stereoscopic vision and, in particular, to the functional properties and the organization of binocular information within the visual cortex. In this paper a simple stereo technique, based on a space variant mapping of the image data and a multi-layered cortical stereoscopic representation, mimicking the neural organization of the early stages of the human visual system, is proposed. Radial disparity computed from a stereo pair is used to map the relative depth with respect to the fixation point. A set of experiments demonstrating the applicability of the devised techniques is also presented.'}\n",
      "\n",
      "57. {'id': '5390882c20f70186a0d8c411', 'score': 0.585118234, 'text': 'From the Publisher:with contributions from Theo Papadopoulo Over the last forty years, researchers have made great strides in elucidating the laws of image formation, processing, and understanding by animals, humans, and machines. This book describes the state of knowledge in one subarea of vision, the geometric laws that relate different views of a scene. Geometry, one of the oldest branches of mathematics, is the natural language for describing three-dimensional shapes and spatial relations. Projective geometry, the geometry that best models image formation, provides a unified framework for thinking about many geometric problems relevant to vision. The book formalizes and analyzes the relations between multiple views of a scene from the perspective of various types of geometries. A key feature is that it considers Euclidean and affine geometries as special cases of projective geometry. Images play a prominent role in computer communications. Producers and users of images, in particular three-dimensional images, require a framework for stating and solving problems. The book offers a number of conceptual tools and theoretical results useful for the design of machine vision algorithms. It also illustrates these tools and results with many examples of real applications.'}\n",
      "\n",
      "58. {'id': '539090c420f70186a0ddde4c', 'score': 0.584074557, 'text': \"The present work is based on the Visual Routine theory of Shimon Ullman. This theory holds that efficient visual perception is managed by first applying spatially parallel methods to an initial input image in order to construct the basic representation-maps of features within the image. Then, this phase is followed by the application of serial methods --- visual routines --- which are applied to the most salient items in these and other subsequently created maps. Recent work in the visual routine tradition is reviewed, as well as relevant psychological work on preattentive and attentive vision. An analysis is made of the problem of devising a visual routine language for computing geometric properties and relations. The most useful basic representations to compute directly from a world of 2-D geometric shapes are determined. An argument is made for the case that an experimental program is required to establish which basic operations and which methods for controlling them will lead to the efficient computation of geometric properties and relations. A description is given of an implemented computer system which can correctly compute, in images of simple 2-D geometric shapes, the properties {\\\\em vertical}, {\\\\em horizontal}, {\\\\em closed}, and {\\\\em convex}, and the relations {\\\\em inside}, {\\\\em outside}, {\\\\em touching}, {\\\\em centred-in}, {\\\\em connected}, {\\\\em parallel}, and {\\\\em being-part-of}. The visual routines which compute these, the basic operations out of which the visual routines are composed, and the important logic which controls the goal-directed application of the routines to the image are all described in detail. The entire system is embedded in a Question-and-Answer system which is capable of answering questions of an image, such as ``Find all the squares inside triangles'''' or ``Find all the vertical bars outside of closed convex shapes.'''' By asking many such questions about various test images, the effectiveness of the visual routines and their controlling logic is demonstrated.\"}\n",
      "\n",
      "59. {'id': '5390b0ca20f70186a0ed9506', 'score': 0.583681881, 'text': 'The research presented in this paper represents several novel conceptual contributions to the computer vision literature. In this position paper, our goal is to define the scope of computer vision analysis and discuss a new categorisation of the computer vision problem. We first provide a novel decomposition of computer vision into base components which we term the axioms of vision. These are used to define researcher-level and developer-level access to vision algorithms, in a way which does not require expert knowledge of computer vision. We discuss a new line of thought for computer vision by basing analyses on descriptions of the problem instead of in terms of algorithms. From this an abstraction can be developed to provide a layer above algorithmic details. This is extended to the idea of a formal description language which may be automatically interpreted thus allowing those not familiar with computer vision techniques to utilise sophisticated methods.'}\n",
      "\n",
      "60. {'id': '5390879920f70186a0d41e6f', 'score': 0.582981169, 'text': 'An important research topic in AI is the discovery of constraints. As constraints. In this paper, we present a model-directed image understanding prototype to support computer vision, which can recover constraints of an image by means of the spatial relation and spatial reasoning. according to a model. In the prototype, an object is viewed as an arrangement of two types components, viz., crucial component and ordinary component. Under the direction of models, these components are looked for in a top-down manner so that the image can be recognized. For this purpose, knowledge representation and organization of knowledge base are also presented. Some problems on this research are discussed and some reasons of this research as conclusions are also given to serve the future research.'}\n",
      "\n",
      "61. {'id': '53908f5b20f70186a0dda92d', 'score': 0.5829252, 'text': 'A CONSTRAINT-BASED APPROACH TO UNIFORMLY COMBINING INFORMATION FROM MULTIPLE REPRESENTATIONS AND SOURCES OF SENSORY DATA IS DESCRIBED. THE APPROACH IS IMPORTANT TO RESEARCH IN INTERMEDIATE GROUPING, KNOWLEDGE-BASED MODEL MATCHING, AND INFORMATION FUSION. THE TECHNIQUES PRESENTED EXTEND THE CAPABILITIES OF AN EARLIER SYSTEM THAT APPLIED CONSTRAINTS TO ATTRI- BUTES OF SINGLE TYPES OF EXTRACTED IMAGE EVENTS CALLED TOKENS. RELATIONAL MEASURES ARE DEFINED BETWEEN SYMBOLIC TOKENS SO THAT SETS OF TOKENS ACROSS REPRESENTATIONS CAN BE SELECTED AND GROUPED ON THE BASIS OF CONSTRAINT FUNC TIONS APPLIED TO THESE RELATIONAL MEASURES. SINCE TYPICAL LOW-LEVEL REPRESENTATIONS INVOLVE HUNDREDS OR THOUSANDS OF TOKENS IN EACH REPRESENTATION, EVEN BINARY RELATIONAL MEASURES CAN INVOLVE VERY LARGE NUMBERS OF TOKEN PAIRS. CONTROL STRATEGIES FOR ORDERING AND FILTERING TOKENS, BASED UPON CONSTRAINTS ON TOKEN ATTRIBUTES AND TOKEN RELATIONSHIPS, CAN BE FORMED TO REDUCE THE COMPUTATION INVOLVED IN PRODUC- ING TOKEN AGGREGATIONS. THEY SYSTEM IS DEMONSTRATED USING REGION AND LINE DATA AND AN ASSOCIATED SET OF RELATIONAL MEASURES. THE APPROACH CAN BE NATURALLY EXTENDED TO INCLUDE TOKENS EXTRACTED FROM MOTION, STEREO, AND RANGE DATA.'}\n",
      "\n",
      "62. {'id': '53908f5b20f70186a0dda7e4', 'score': 0.581397355, 'text': 'IMAGE UNDERSTANDING RESEARCH AT THE UNIVERSITY OF MASSACHUSETTS ENCOM- PASSES A RANGE OF RESEARCH, MOST OF WHICH IS DIRECTED TOWARDS THE INTEGRA- TION OF A DIVERSE SET OF PROCESSES TO ACHIEVE A GENERAL REAL-TIME KNOWLEDGE -BASED INTERPRETATION SYSTEM. IN PARTICULAR WE ARE CONCENTRATING ON INTE- GRATING PROJECTS INVOLVING OBJECT IDENTIFICATION IN STATIC IMAGES, DEPTH RECOVERY FROM MOTION ANALYSIS, A REAL-TIME PARALLEL ARCHITECTURE, AND MOBILE VEHICLE NAVIGATION. THIS SYSTEM WILL BE APPLIED TO A VARIETY OF TASK DOMAINS OF NATURAL SCENES INCLUDING ROAD SCENES AND AERIAL IMAGES, AND WILL ALSO BE USED TO CONTROL A MOBILE ROBOT MOVING THROUGH BOTH KNOWN AND UNKNOWN OUTDOOR DOMAINS. THIS SUMMARY DOCUMENTS SEVERAL AREAS OF RESEARCH AT THE UNIVERSITY OF MASSACHUSETTS THAT ARE ENTIRELY OR PARTIALLY SUPPORTED UNDER THE DARPA IMAGE UNDERSTANDING PROGRAM. THE WORK, MUCH OF WHICH IS DOCUMENTED IN PAPERS IN THESE PROCEEDINGS, IS DIVIDED INTO SEVERAL AREAS: 1.) KNOWLEDGE-BASED VISION; 2.) PERCEPTUAL ORGANIZATION (INTERMEDIATE PROCESSING); 3.) 3D MODELS, MATCHING, AND SURFACE RECOVERY; 4.) MOBILE ROBOT NAVIGATION; 5.) IMAGE UNDERSTANDING ARCHITECTURE; 6.) MOTION ANALYSIS; AND 7.) LOW-LEVEL VISION'}\n",
      "\n",
      "63. {'id': '5390a9a520f70186a0ea5c62', 'score': 0.581328, 'text': 'Image understanding and image semantics processing have recently become an issue of critical importance in computer vision R&D. Biological vision has always considered them as an enigmatic mixture of perceptual and cognitive processing faculties. In its impetuous and rash development, computer vision without any hesitations has adopted this stance. I will argue that such a segregation of image processing faculties is wrong, both for the biological and the computer vision. My conjecture is that images contain only one sort of information - the perceptual (physical) information, which can be discovered in an image and elicited for further processing. Cognitive (semantic) information is not a part of image-conveyed information. It belongs to a human observer that acquires and interprets the image. Relying on a new definition of \"information\", which can be derived from Kolmogorov\\'s complexity theory and Chaitin\\'s notion of algorithmic information, I propose a unifying framework for visual information processing, which explicitly accounts for perceptual and cognitive image processing peculiarities. I believe, it would provide better scaffolding for modeling visual information processing in human brain.'}\n",
      "\n",
      "64. {'id': '5390a9a520f70186a0ea5c78', 'score': 0.581205845, 'text': 'Binocular information about the structure of a scene is contained in the relative positions of corresponding points in the two views. If the eyes rotate, in order to fixate a different target, then the disparity at a given image location is likely to change. Quite different disparities can be produced at the same location, as the eyes move from one fixation-point to the next. The pointwise variability of the disparity map is problematic for biological visual systems, in which stereopsis is based on simple, short-range mechanisms. It is argued here that the problem can be addressed in two ways; firstly by an appropriate representation of disparity, and secondly by learning the typical pattern of image correspondences. It is shown that the average spatial structure of the disparity field can be estimated, by integrating over a series of binocular fixations. An algorithm based on this idea is tested on natural images. Finally, it is shown how the average pattern of disparities could help to put the images into binocular correspondence.'}\n",
      "\n",
      "65. {'id': '5390b24420f70186a0ee850a', 'score': 0.581125557, 'text': 'This paper proposes a model for image representation and image analysis using a multi-layer neural network, which is rooted in the human vision system. Having complex neural layers to represent and process information, the biological vision system is far more efficient than machine vision system. The neural model simulate non-classical receptive field of ganglion cell and its local feedback control circuit, and can represent images, beyond pixel level, self-adaptively and regularly. The results of experiments, rebuilding, distribution and contour detection, prove this method can represent image faithfully with low cost, and can produce a compact and abstract approximation to facilitate successive image segmentation and integration. This representation schema is good at extracting spatial relationships from different components of images and highlighting foreground objects from background, especially for nature images with complicated scenes. Further it can be applied to object recognition or image classification tasks in future.'}\n",
      "\n",
      "66. {'id': '53908e0020f70186a0dd65f6', 'score': 0.580991, 'text': 'A computer may gather a lot of information from its environment in an optical or graphical manner. A scene, as seen for instance from a TV camera or a picture, can be transformed into a symbolic description of points and lines or surfaces. This thesis describes several programs, written in the language CONVERT , for the analysis of such descriptions in order to recognize, differentiate and identify desired objects or classes or objects in the scene. Examples are given in each case. Although the recognition might be in terms of projections of 2-dim and 3-dim objects, we do not deal with stereoscopic information. One of our programs (Polybrick) identifies parallelepipeds in a scene which may contain hidden bodies and non-parallelepipedic objects. The program TD works mainly with 2-dimensional figures, although under certain conditions successfully identifies 3-dim objects. Overlapping objects are identified when they are transparent. A third program, DT, works with 3-dim and 2-dim objects, and does not identify objects which are not completely seen. Important restrictions and suppositions are: (a) the input is assumed perfect (noiseless), and in a symbolic format; (b) no perspective deformation is considered. A portion of this thesis is devoted to the study of models (symbolic representation) of the objects we want to identify; different schemes, some of them already in use, are discussed. Focusing our attention on the more general problem of identification of general objects when they substantially overlap, we propose some schemes for their recognition, and also analyze some problems that are met.'}\n",
      "\n",
      "67. {'id': '5390a1bc20f70186a0e562f3', 'score': 0.580594361, 'text': 'This paper presents a biologically-inspired artificial vision system. The goal of the proposed vision system is to correctly match regions among several images to obtain scenes matching. Based on works that consider that humans perceive visual objects divided in its cons-tituent parts, we assume that a particular type of regions, called curvilinear regions, can be easily detected in digital images. These features are more complex than the basic features that human vision uses in the very first steps in the visual process. We assume that the curvilinear regions can be compared in their complexity to those features analysed by the IT cortex for achieving objects recognition. The approach of our system is similar to other existing methods that also use intermediate complexity features for achieving visual matching. The novelty of our system is the curvilinear features that we use.'}\n",
      "\n",
      "68. {'id': '53908bde20f70186a0dc8923', 'score': 0.580430269, 'text': 'D.A. Forsyth and M.M. Fleck Abstract: This paper describes a new representation for people and animals, called a body plan. The representation is an organized collection of grouping hints obtained from constraints on color, texture, shape, and geometrical relations. Body plans can be learned from image data, using established statistical learning techniques. Body plans are well adapted to segmentation and recognition in complex environments, such as the huge libraries of digitized images now becoming widely available. Two specific applications of body plans are presented: an algorithm that determines whether an image depicts a scantily clad human and an algorithm that learns and uses a body plan to find pictures of horses. Both algorithms demonstrate excellent performance on large, poorly controlled input data.'}\n",
      "\n",
      "69. {'id': '5390a1e620f70186a0e59e45', 'score': 0.579344749, 'text': 'Our goal is to organize the image contents semantically. In this paper, we propose a method to classify the images semantically, using the C-Fuzzy algorithm to segment the natural scenes into perceptually uniform regions. The low-level characteristics that are taken into account are: color, texture, shape, absolute spatial arrangement, spatial coherency, and dimension. Since humans are the ultimate users of most image retrieval systems, it is important to organize the contents semantically, according to meaningful categories. This requires an understanding of the important semantic categories that humans use for image classification, and the extraction of meaningful image features that can discriminate between these categories. A lot of experiments, in which the human subjects had to group images into semantic categories and to explain the criteria for their choice, were realized. From these experiments, we identify the semantic categories (landscapes, animals, flowers, etc), the semantic indicators or intermediate descriptors and their visual characteristics.'}\n",
      "\n",
      "70. {'id': '5390879220f70186a0d3b563', 'score': 0.579065382, 'text': 'The authors describe an approach to perceptual grouping for detecting and describing 3-D objects in complex images and apply it to the task of detecting and describing complex buildings in aerial images. They argue that representations of structural relationships in the arrangements of primitive image features, as detected by the perceptual organization process, are essential for analyzing complex imagery. They term these representations collated features. The choice of collated features is determined by the generic shape of the desired objects in the scene. The detection process for collated features is more robust than the local operations for region segmentation and contour tracing. The important structural information encoded in collated features aids various visual tasks such as object segmentation, correspondence processes, and shape description. The proposed method initially detects all reasonable feature groupings. A constraint satisfaction network is then used to model the complex interactions between the collations and select the promising ones. Stereo matching is performed on the collations to obtain height information. This aids in further reasoning on the collated features and results in the 3-D description of the desired objects.'}\n",
      "\n",
      "71. {'id': '53908f5c20f70186a0ddb19e', 'score': 0.578873456, 'text': 'A novel approach to object recognition and scene analysis based on neural network representation of visual schemas is described. Given an input scene, the VISOR system focuses attention successfully at each component, and the schema representations cooperate and compete to match the inputs. The schema hierarchy is learned from examples through unsupervised adaptation and reinforcement learning. VISOR learns that some objects are more important than others in identifying a scene. It learns three types of visual schemas: (1) rigid spatial layouts of components used primarily for describing objects; (2) collections of components located anywhere in the scene for recognizing certain man-made scenes (such as a dining table); and (3) rough spatial layouts of regions of uniform texture and no specific shape that are often found in natural scenes (such as a road scene). Compared to traditional rule-based systems, VISOR shows remarkable robustness of recognition, and is able to indicate the confidence of its analysis as the inputs differ increasingly from the schemas. With such properties, VISOR is a promising first step towards a general vision system that can be used in different applications after learning the application-specific schemas.'}\n",
      "\n",
      "72. {'id': '5390bd1520f70186a0f44ff0', 'score': 0.578837633, 'text': 'Machine learning algorithms have been successfully utilized in various systems/devices. They have the ability to improve the usability/quality of such systems in terms of intelligent user interface, fast performance, and more importantly, high accuracy. In this research, machine learning techniques are used in the field of image understanding, which is a common research area between image analysis and computer vision, to involve higher processing level of a target image to “make sense” of the scene captured in it. A general probabilistic framework for image understanding where topics associated with (i) collection of images to generate a comprehensive and valid database, (ii) generation of an unbiased ground-truth for the aforesaid database, (iii) selection of classification features and elimination of the redundant ones, and (iv) usage of such information to test a new sample set, are discussed. Two research projects have been developed as examples of the general image understanding framework; identification of region(s) of interest, and image segmentation evaluation. These techniques, in addition to others, are combined in an object-oriented rendering system for printing applications. The discussion included in this doctoral dissertation explores the means for developing such a system from an image understanding/ processing aspect.It is worth noticing that this work does not aim to develop a printing system. It is only proposed to add some essential features for current printing pipelines to achieve better visual quality while printing images/photos. Hence, we assume that image regions have been successfully extracted from the printed document. These images are used as input to the proposed object-oriented rendering algorithm where methodologies for color image segmentation, region-of-interest identification and semantic features extraction are employed. Probabilistic approaches based on Bayesian statistics have been utilized to develop the proposed image understanding techniques.'}\n",
      "\n",
      "73. {'id': '5390a28020f70186a0e62930', 'score': 0.578819275, 'text': 'We consider visual scenes composed by the optical image of a group of bodies. When such a scene is \"seen\" by a computer through a film spot scanner, image dissector, or similar device, it can be treated as a two-dimensional array of numbers, or as a function of two variables.'}\n",
      "\n",
      "74. {'id': '53908f5b20f70186a0dd9390', 'score': 0.578758538, 'text': 'In this paper we describe a fast, feature-driven program for extracting depth information from stereoscopic sets of digitized TV images. This is achieved by two means: in the simplest case, by statistically correlating variable-sized windows on the basis of visual texture, and in the more complex case by pre-processing the images to extract significant visual features such as corners, and then using these features to control the correlation process. The program runs on the PDP-10 but uses a PDP-11/45 and an SPS-41 Signal Processing Computer as subsidiary processors. The use of the two small, fast machines for the performance of simple but often-repeated computations effects an increase in speed sufficient to allow us to think of using this program as a fast 3-dimensional segmentation method, preparatory to more complex image processing. It is also intended for use in visual feedback tasks involved in hand-eye coordination and automated assembly. The current program is able to calculate the three-dimensional positions of 10 points to within 5 millimeters, using 5 seconds of computation for extracting features, 1 second per image for correlation, and 0.1 second for the depth calculation.'}\n",
      "\n",
      "75. {'id': '555a1d570cf2b21909ba35cf', 'score': 0.578746259, 'text': 'An approach for scene understanding based on qualitative descriptors, domain knowledge and logics is proposed in this paper. Qualitative descriptors, qualitative models of shape, colour, topology and location are used for describing any object in the scene. Two kinds of domain knowledge are provided: (i) categorizations of objects according to their qualitative descriptors, and (ii) semantics for describing the affordances, mobility and other functional properties of target objects. First order logics are obtained for reasoning and scene understanding. Tests were carried out at the Interact@Cartesium scenario and promising results were obtained.'}\n",
      "\n",
      "76. {'id': '5390a80f20f70186a0e97333', 'score': 0.578657329, 'text': \"The increasing availability and deployment of imaging sensors operating in multiple spectral bands has led to a large research effort in image fusion, resulting in a plethora of pixel-level image fusion algorithms. However, the cognitive aspects of multisensor image fusion have not received much attention in the development of these methods. In this study we investigate how humans interpret visual and infrared images, and we compare the interpretation of these individual image modalities to their fused counterparts, for different image fusion schemes. This was done in an attempt to test to what degree image fusion schemes can enhance human perception of the structural layout and composition of realistic outdoor scenes. We asked human observers to manually segment the details they perceived as most prominent in a set of corresponding visual, infrared and fused images. For each scene, the segmentations of the individual input image modalities were used to derive a joint reference (''gold standard'') contour image that represents the visually most salient details from both of these modalities and for that particular scene. The resulting reference images were then used to evaluate the manual segmentations of the fused images, using a precision-recall measure as the evaluation criterion. In this sense, the best fusion method provides the largest number of correctly perceived details (originating from each of the individual modalities that were used as input for the fusion scheme) and the smallest amount of false alarms (fusion artifacts or illusory details). A comparison with an objective score of subject performance indicates that the reference contour method indeed appears to characterize the performance of observers using the results of the fusion schemes. The results show that this evaluation method can provide valuable insight into the way fusion schemes combine perceptually important details from the individual input image modalities. Given a reference contour image, the method can potentially be used to design image fusion schemes that are optimally tuned to human visual perception for different applications and scenarios (e.g. environmental or weather conditions).\"}\n",
      "\n",
      "77. {'id': '53908d6520f70186a0dd0ed3', 'score': 0.578383565, 'text': 'Computational approaches to simulate human beings interpreting pictures are important for understanding perceptual Gestalt and for building computer systems that support visual communication. Based on the minimum principle, we present a new approach to the interpretation of pictures. Our contribution is that we developed a novel way in which the geometrical information is calculated. In our approach, geometrical shapes are divided into various sorts and the sorts are organised into a hierarchical structure. A sort together with a number of points determines an actual graphical object. So, the objects themselves can be represented by the combination of their sort with certain points that code their position in the field and whatever other attributes they may possess as members of the sort. The geometrical information load is calculated as the number of points which are needed in the representation. Pictures are represented as a set of graphical objects. There are no other requirements on the input pictures. As long as the objects in a list are well-formed terms, interpretation can start. An inference mechanism reduces the terms in the list into terms which have the lowest information load. The deduced list of objects is the interpretation of the picture.'}\n",
      "\n",
      "78. {'id': '5390bfa220f70186a0f54e53', 'score': 0.578324258, 'text': 'In this groundbreaking new volume, computer researchers discuss the development of technologies and specific systems that can interpret data with respect to domain knowledge. Although the chapters each illuminate different aspects of image interpretation, all utilize a common approach - one that asserts such interpretation must involve perceptual learning in terms of automated knowledge acquisition and application, as well as feedback and consistency checks between encoding, feature extraction, and the known knowledge structures in a given application domain. The text is profusely illustrated with numerous figures and tables to reinforce the concepts discussed.'}\n",
      "\n",
      "79. {'id': '53908f5b20f70186a0dda540', 'score': 0.578316033, 'text': 'THIS PAPER SUMMARIZES SEVERAL AREAS OF RESEARCH AT THE UNIVERSITY OF MASSACHUSETTS THAT ARE PARTIALLY OR ENTIRELY SUPPORTED UNDER THE DARPA IMAGE UNDERSTANDING PROGRAM. MANY OF THE INDIVIDUAL EFFORTS DISCUSSED BELOW ARE FURTHER DEVELOPED IN OTHER PAPERS IN THESE PROCEEDINGS. THE SUMMARY IS DIVIDED INTO SEVERAL AREAS: 1. KNOWLEDGE-BASED VISION 2. DATABASE SUPPORT FOR SYMBOLIC VISION PROCESSING 3. MOTION PROCESSING 4. PERCEPTUAL ORGANIZATION (GROUPING) 5. IMAGE UNDERSTANDING ARCHITECTURE 6. INTEGRATED VISION BENCHMARK FOR PARALLEL ARCHITECTURES 7. MOBILE VEHICLE NAVIGATION. ALTHOUGH WE DISCUSS EACH AREA SEPARATELY, A FUNDAMENTAL GOAL OF THE COMPUTER VISION RESEARCH ENVIRONMENT AT UMASS IS THE INTEGRATION OF A DI- VERSE SET OF RESEARCH EFFORTS INTO A SYSTEM THAT IS ULTIMATELY INTENDED TO ACHIEVE REAL-TIME IMAGE INTERPRETATION. TWO OF OUR MAJOR SYSTEM INTEGRATION EFFORTS ARE THE VISIONS STATIC INTERPRETATION SYSTEM, WHICH IS A KNOWLEDGE- BASED COMPUTER VISION SYSTEM UTILIZING PARALLEL MODULAR PROCESSES THAT COM- MUNICATE VIA A BLACKBOARD [DRA87B, HAN87A]. THE SECOND SYSTEM INTEGRATION EFFORT INVOLVES AN AUTONOMOUS MOBILE VEHICLE FOR NAVIGATION THROUGH A PARTI'}\n",
      "\n",
      "80. {'id': '53909f8c20f70186a0e3f0e3', 'score': 0.578215957, 'text': 'A central problem in the area of scene analysis is that of segmenting a scene into its natural objects. Current work emphasizes the semantic approach in which a priori knowledge of the shape of an object is used. Yet there is much to learn about more primitive cues for segmentation such as texture, color, and brightness. In the case of human perception, segmentation appears to be due to a multiplicity of cues which operate in a redundant fashion.'}\n",
      "\n",
      "81. {'id': '5390975920f70186a0dfcfc8', 'score': 0.577961564, 'text': 'Typically any single sensor instrument suffers from physical/observation constraints. This paper discusses a generalized framework, called polymorphic visual information fusion framework (PVIF) that can enable information from multiple sensors to be fused and compared to gain broader understanding of a target of observation in multidimensional space. An automate software system supporting comparative cognition has been developed to form 3D models based on the datasets from different sensors, such as XPS and LSCM. This fusion framework not only provides an information engineering based tool to overcome the limitations of individual sensorýs scope of observation but also provides a means where theoretical understanding surrounding a complex target can be mutually validated by comparative cognition about the object of interest and 3D model refinement. Some polysensometric data classification metrics are provided to measure the quality of input datasets for fusion visualization.'}\n",
      "\n",
      "82. {'id': '5390a93b20f70186a0ea046b', 'score': 0.577925324, 'text': 'Stereopsis is the process of inferring the distance to objects from two or more images. It has applications in areas such as: novel-view rendering, motion capture, autonomous navigation, and topographical mapping from remote sensing data. Although it sounds simple, in light of the effortlessness with which we are able to perform the task with our own eyes, a number of factors that make it quite challenging become apparent once one begins delving into computational methods of solving it. For example, occlusions that block part of the scene from being seen in one of the images, and changes in the appearance of objects between the two images due to: sensor noise, view dependent effects, and/or differences in the lighting/camera conditions between the two images.Global stereopsis algorithms aim to solve this problem by making assumptions about the smoothness of the depth of surfaces in the scene, and formulating stereopsis as an optimization problem. As part of their formulation, these algorithms include a function that measures the similarity between pixels in different images to detect possible correspondences. Which of these match cost functions work better, when, and why is not well understood. Furthermore, in areas of computer vision such as segmentation, face detection, edge detection, texture analysis and classification, and optical flow, it is not uncommon to use colour spaces other than the well known RGB space to improve the accuracy of algorithms. However, the use of colour spaces other than RGB is quite rare in stereopsis research.In this dissertation we present results from two, first of their kind, large scale studies on global stereopsis algorithms. In the first we compare the relative performance of a structured set of match cost cost functions in five different global stereopsis frameworks in such a way that we are able to infer some general rules to guide the choice of which match cost functions to use in these algorithms. In the second we investigate how much accuracy can be gained by simply changing the colour representation used in the input to global stereopsis algorithms.'}\n",
      "\n",
      "83. {'id': '539089d220f70186a0d9ad64', 'score': 0.577483535, 'text': 'This paper presents a new approach to the knowledge-based composition of processes for image interpretation and analysis. Its computer implementation in the VISIPLAN (VISIon PLANner) system provides a way of modeling the composition of image analysis processes within a unified, object-centered hierarchical planning framework. The approach has been tested and shown to be flexible in handling a reasonably broad class of multi-modality biomedical image analysis and interpretation problems. It provides a relatively general design or planning framework, within which problem-specific image analysis and recognition processes can be generated more efficiently and effectively. In this way, generality is gained at the design and planning stages, even though the final implementation stage of interpretation processes is almost invariably problem- and domain-specific.'}\n",
      "\n",
      "84. {'id': '5390a30b20f70186a0e6a35b', 'score': 0.577400923, 'text': 'A popular computation approach is to process visual images by dividing them into crisp (winner-takes-all) parts in analog to properties of neurophysiological receptive fields. Problem with such symbolic representation is that in a real environment object attributes are seldom invariant. We propose to divide images into rough parts using hierarchical, multi-valued processes. The bottom-up computation (BUC) is related to prediction where object attributes are approximated by different granules with properties similar to different brain areas: by dots as in the thalamus, by oriented lines as in the primary visual cortex, and by elementary shapes as in V4. There are a large number of possible combinations of elementary granules; therefore objects in BUC are overrepresented. The top-down computation (TDC) fits prediction to hypothesis posed by more complex properties (higher brain areas). If the hypothesis check is positive, TDC verifies the object and eliminates other possible patterns. Such classifications take place in parallel at many functional units. We show an example of such hierarchical system computation on experimentally recorded data from monkey visual area (V4).'}\n",
      "\n",
      "85. {'id': '539099ec20f70186a0e1d57d', 'score': 0.577001, 'text': 'One of the major challenges in computer vision is to create automated systems that perform tasks with at least the same competences as human experts. In particular for automated inspection of natural objects this is not easy to achieve. The task is hampered by large in-class variations and complex 3D-morphology of the objects and subtle argumentations of experts. For example, in our horticultural case we deal with quality assessment of young tomato plants, which requires experienced specialists. We submit that automation of such a task employing an explicit model of the objects and their assessment is preferred over a black-box model obtained from modelling input-output relations only. We propose to employ ontologies for representing the geometrical shapes, object parts and quality classes associated with the explicit models. Our main contribution is the description of a method to develop a white-box computer vision application in which the needed expert knowledge is defined by: (i) decomposing the task of the inspection system into subtasks and (ii) identifying the algorithms that execute the subtasks. This method describes the interaction between the task decomposition and the needed task-specific knowledge, and studies the delicate balance between general domain knowledge and task-specific details. As a proof of principle of this methodology, we work through a horticultural case study and argue that the method leads to a robust, well-performing, and extendable computer vision system.'}\n",
      "\n",
      "86. {'id': '5390b3da20f70186a0ef6719', 'score': 0.576753259, 'text': 'In real world, a scene is composed by many characteristics Intrinsic images represent these characteristics by two components, reflectance (the albedo of each point) and shading (the illumination of each point) Because reflectance images are invariant under different illumination conditions, they are more appropriate for some vision applications, such as recognition, detection We develop the system to separate them from a single image Firstly, a presented method, called Weighted-Map Method, is used to separate reflectance and shading A weighted map is created by first transforming original color domain into new color domain and then extracting some useful property Secondly, we build Markov Random Fields and use Belief Propagation to propagate local information in order to help us correct misclassifications from neighbors According to our experimental results, our system can apply to not only real images but also synthesized images.'}\n",
      "\n",
      "87. {'id': '53908bcc20f70186a0dc542e', 'score': 0.57635206, 'text': 'High computer performance depends only partially on using faster and more reliable hardware, but to a large extent it depends on the architecture and on the processing techniques. An effective platform that matches general planning strategies is given by the hierarchical paradigm. This is true particularly in the field of image processing and computer vision, which is characterized by very large quantity of sensory data, but in which most of the information collected is meaningless for the task at end. Real time performances can be achieved only by applying some attentional mechanisms that allow to restrict the computation just on the relevant data, at the right time. Several vision systems have been proposed and designed to support the implementation of these strategies. In this work, after introducing a taxonomy of the hierarchical machine vision systems, a short description of the most popular implementations is given.'}\n",
      "\n",
      "88. {'id': '5390980720f70186a0e03067', 'score': 0.576048, 'text': \"In many operations the ability of a machine to “see” is what will determine its effectiveness in its particular domain of operation. For example, in a bin picking problem the ability of the sensing system of a robot to determine the position and orientation of the individual parts will ultimately determine the system's success or failure. Most systems that require this level of sensing, utilize machine vision in which computers are integrated with image acquisition devices to provide the information required for guidance; as would be needed in a feedback loop for example. The development of algorithms that allow these computers to accomplish the image interpretation has turned out to be less than trivial. This is especially true in the area of natural products such as, meat products, fruit or textile; where, because of their natural variability the ability to develop machine vision algorithms to automatically inspect these products reliably has been problematic. The goal of this thesis is to attempt to determine a methodology for the integration and streamlining of the process of algorithm development so as to be able to more efficiently develop effective and robust algorithms for this class of problems. Humans, are currently still the best available solutions to these problems. This thesis will examine an approach towards the development of machine vision algorithms using the primate visual system as a model. The approach taken in this work defines three levels of processing for the visual signal these are sensing, ecoding/transfer, and classification. In particular we examine the processes of encoding/transfer derived from the results of research in the area of human/primate biological visual processing and their representations. We focus on the use of the receptive field mechanisms that are commonly observed in the human visual system and their processing of contrast in the scenes. We also show that features derived from the responses of these mechanisms are useful for image classification. Algorithms for implementing these operations are developed using the technique and demonstrated. The other aspect of the approach provides for user guidance by allowing an expert to teach the system by identifying things that are of interest in a particular scene. We then demonstrate development of solutions to three inspection problems using the approach.\"}\n",
      "\n",
      "89. {'id': '5390b1d220f70186a0ee1abb', 'score': 0.575774729, 'text': 'We describe the current state of the 3-D Mosaic project, whose goal is to incrementally acquire a 3-D model of a complex urban scene from images. The notion of incremental acquisition arises from the observations that 1) single images contain only parfial information about a scene, 2) complex images are difficult to fully interpret, and 3) different features of a given scene tend to be easier to extract in different images because of differences in viewpoint and lighting conditions. In our approach, multiple images of the scene are sequentially analyzed so as to incrementaly construct the model. Each new image provides information which refines the model. We describe some experiments toward this end. Our method of extracting 3-D shape information from the images is stereo analysis. Because we are dealing with urban scenes, a junction-based matching technique proves very useful. This technique produces rather sparse wire-frame descriptions of the scene. A reasoning system that relies on task-specific knowledge generates an approximate model of the scene from the stereo output. Gray scale information is also acquired for the faces in the model. Finally, we describe an experiment in combining two views of the scene to obtain a rermed model.'}\n",
      "\n",
      "90. {'id': '5390aa0e20f70186a0ea7abb', 'score': 0.575766921, 'text': \"Function-based object recognition provides the framework to represent and reason about object functionality as a means to recognize novel objects and produce plans for interaction with the world. When function can be perceived visually, function-based computer vision is consistent with Gibson's theory of affordances. Objects are recognized by their functional attributes. These attributes can be segmented out of the scene and given symbolic labels which can then be used to guide the search space for additional functional attributes. An example of such affordance-driven scene segmentation would be the process of attaching symbolic labels to the areas that afford sitting (functional seats) and using these areas to guide parameter selection for deriving nearby surfaces that potentially afford back support. The Generic Recognition Using Form and Function (GRUFF) object recognition system reasons about and generates plans for understanding 3-D scenes of objects by performing such a functional attribute-based labelling process. An avenue explored here is based on a novel approach of autonomously directing image acquisition and range segmentation by determining the extent to which surfaces in the scene meet specified functional requirements, or provide affordances associated with a generic category of objects.\"}\n",
      "\n",
      "91. {'id': '53908f5b20f70186a0dda691', 'score': 0.575689852, 'text': 'COMPUTER VISION IMPOSES UNIQUE REQUIREMENTS ON THE REPRESENTATION AND MANIPULATION OF IMAGE DATA AND KNOWLEDGE. THE INTERPRETATION OF AN IMAGE CAN GENERATE THOUSANDS OF INTERMEDIATE LEVEL DESCRIPTIONS, MANY OF WHICH MUST BE REPEATEDLY ACCESSED AND PROCESSED. TRADITIONAL KNOWLEDGE REPRE- SENTATION METHODS DO NOT PROVIDE MECHANISMS TO ACCOMPLISH THIS EFFECTIVELY. WE DESCRIBE A DATABASE MANAGEMENT SYSTEM CALLED THE INTERMEDIATE SYMBOLIC REPRESENTATION (ISR) WHICH IS SUITABLE FOR USE AT THE INTERMEDIATE (SYMBOLIC) LEVEL OF VISION. THE ISR, WHICH IS BASED ON DATABASE MANAGEMENT METHODOLOGY, MEDIATES ACCESS TO MASSIVE QUANTITIES OF INTERMEDIATE LEVEL VISION DATA, AND FORMS AN ACTIVE INTERFACE TO THE HIGHER LEVEL INFERENCE PROCESSES RESPONSIBLE FOR CONSTRUCTING THE INTERPRETATION OF AN IMAGE. WE ALSO PRESENT SEVERAL EXAMPLES ILLUSTRATING THE USE OF THE ISR.'}\n",
      "\n",
      "92. {'id': '5390bfa220f70186a0f52ec8', 'score': 0.575497687, 'text': 'To quickly synthesize complex scenes, digital artists often collage together visual elements from multiple sources: for example, mountains from New Zealand behind a Scottish castle with wisps of Saharan sand in front. In this paper, we propose to use a similar process in order to parse a scene. We model a scene as a collage of warped, layered objects sampled from labeled, reference images. Each object is related to the rest by a set of support constraints. Scene parsing is achieved through analysis-by-synthesis. Starting with a dataset of labeled exemplar scenes, we retrieve a dictionary of candidate object segments that match a query image. We then combine elements of this set into a \"scene collage\" that explains the query image. Beyond just assigning object labels to pixels, scene collaging produces a lot more information such as the number of each type of object in the scene, how they support one another, the ordinal depth of each object, and, to some degree, occluded content. We exploit this representation for several applications: image editing, random scene synthesis, and image-to-anaglyph.'}\n",
      "\n",
      "93. {'id': '539090c420f70186a0ddde47', 'score': 0.575394571, 'text': 'We propose a theory of depiction and interpretation that formalizes image domain knowledge, scene domain knowledge and the depiction mapping between the image and scene domains. This theory is illustrated by specifying some general knowledge about maps, geographic objects and their depiction relationships in first order logic with equality. An interpretation of an image is defined to be a logical model of the general knowledge and a description of that image. For the simple map world we show how the task level specification may be refined to a provably correct implementation by invoking model preserving transformations on the logical representation. In addition, we sketch logical treatments for querying an image, incorporating contingent scene knowledge into the interpretation process, occlusion, ambiguous image descriptions, and composition. This approach provides a formal framework for analyzing existing systems such as Mapsee, and for understanding the use of constraint satisfaction techniques. It also can be used to design and implement vision and graphics systems that are correct with respect to the task and algorithm levels.'}\n",
      "\n",
      "94. {'id': '5390a5b020f70186a0e7cb36', 'score': 0.575063467, 'text': 'Object recognition in stereo sequences is a simulation of human visual systems on how to analyze and understand various scenes. A pair of stereo sequences is a type of complicated information with huge amount of raw data and features associated with different parameter spaces. Therefore the automatic object recognition in stereo sequences is a difficult and unsolved task challenging many researchers. This paper puts its emphasis on solving this problem based on the data fusion theory. A new algorithm is proposed and experimented on real data. The results show that the accuracy of the object recognition is improved by applying the fusion of both 3D and motion parameters.'}\n",
      "\n",
      "95. {'id': '53908bcc20f70186a0dc5165', 'score': 0.574867368, 'text': 'A scene analysis system for the 3-D modeling of objects is presented. It combines surface reconstruction techniques with object recognition for the generation of 3-D models for computer graphic applications. The system permits the insertion of highlevel constraints, like a specific angle between two house walls, in an explicit knowledge base implemented as a semantic net. The applicability of those constraints is proved by asserting and testing hypotheses in an interpretation phase. In the case of rejection a more general constraint or model is selected. The capabilities of the system were shown for the modeling of buildings using depth from stereo and contour information. The system reconstructs the surface of the scene objects using the constraints selected in the prior interpretation.'}\n",
      "\n",
      "96. {'id': '53908f5b20f70186a0dda609', 'score': 0.57446146, 'text': 'THE SCHEMA SYSTEM EMBODIES A KNOWLEDGE-BASED APPROACH TO SCENE INTERPRE- TATION. LOW-LEVEL ROUTINES ARE APPLIED TO EXTRACT IMAGE DESCRIPTORS CALLED TOKENS, AND THESE TOKENS ARE FURTHER ORGANIZED BY INTERMEDIATE-LEVEL ROUT- INES INTO MORE ABSTRACT STRUCTURES THAT CAN BE ASSOCIATED WITH OBJECT INST- ANCES. THE THOUSANDS OF TOKENS THAT ARE EXTRACTED FROM AN IMAGE CAN BE GROUPED IN A COMBINATORIALLY EXPLOSIVE MANNER. THEREFORE, KNOWLEDGE IN THE SCHEMA SYSTEM IS NOT LIMITED TO THE DESCRIPTIONS OF OBJECTS; IT INCLUDES INFORMATION ABOUT HOW EACH OBJECT CAN BE RECOGNIZED. OBJECT SCHEMAS CONTROL THE INVOCATION AND EXECUTION OF THE LOW-LEVEL AND INTERMEDIATE-LEVEL ROUT- INES WITH THE GOAL OF FORMING HYPOTHESES ABOUT OBJECTS IN THE SCENE. THE SYSTEM DESCRIBED PRODUCES IMAGE INTERPRETATIONS BASED ON TWO-DIMENSIONAL REASONING, ALTHOUGH NOTHING IN THE SYSTEM ORGANIZATION AND CONTROL STRATEG- IES PRECLUDE THE INCLUSION OF THREE-DIMENSIONAL INFORMATION. THE SCHEMA FRAMEWORK EXPLOITS COARSE-GRAINED PARALLELISM IN A COOPERA- TIVE INTERPRETATION PROCESS. SCHEMA INSTANCES RUN CONCURRENTLY, AND AN OB- JECT SCHEMA OFTEN HAS AVAILABLE A VARIETY OF STRATEGIES FOR IDENTIFICATION, EACH ONE INVOKING KNOWLEDGE SOURCES TO GATHER SUPPORT FOR THE PRESENCE OF A HYPOTHESIZED OBJECT. INTER-SCHEMA COMMUNICATION IS CARRIED OUT ASYNCHRON- OUSLY THROUGH A GLOBAL BLACKBOARD. IN THIS WAY SCHEMA INSTANCES COOPERATE TO IDENTIFY AND LOCATE THE SIGNIFICANT OBJECTS PRESENT IN THE SCENE.'}\n",
      "\n",
      "97. {'id': '539087a620f70186a0d49619', 'score': 0.573985934, 'text': \"A representation paradigm for instantiating and refining multiple, concurrent descriptions of an object from a sequence of imagery is presented. It is designed for the perception system of an autonomous robot that needs to describe many types of objects, initially detects objects at a distance and gradually acquires higher resolution data, and continuously collects sensory input. Since the data change significantly over time, the paradigm supports the evolution of descriptions, progressing from crude 2-D 'blob' descriptions to complete semantic models. To control this accumulation of new descriptions, the authors introduce the idea of representation space, a lattice of representations that specifies the order in which they should be considered for describing an object. A system, TraX, that constructs and refines models of outdoor objects detected in sequences of range data is described.\"}\n",
      "\n",
      "98. {'id': '53909fbd20f70186a0e43439', 'score': 0.573711812, 'text': 'A scheme, named tower of knowledge (ToK), is proposed for interpreting 3D scenes. The ToK encapsulates causal dependencies between object appearance and functionality. We demonstrate it by labelling the components of the 3D model of a building, reconstructed from images of multiple views, by using utility theory.'}\n",
      "\n",
      "99. {'id': '5390b1d220f70186a0ee19ed', 'score': 0.57366389, 'text': 'A computer vision system is proposed, in which the recogni-tion of an object involves two interacting processes: model retrieval and model verification. The goal of the model retrieval process is to generate a proper structural description of the object in the input image, and use the description to retrieve candidate object models from the associative memory of the vision system. The present study explores one way of deriving such an object shape description from a single image. Regularity constraints and a preference rule are used to restrict the solutions to a preferred interpretation of geometric contours. Local interpretation is then propagated to neighboring regions. Through a proper control on the interaction between constraints and consistency checking, a rough object description in terms of visible surface orientations can be gener-ated. A computer vision system using this approach has been imple-mented and it is described in some details.'}\n",
      "\n",
      "100. {'id': '5390b8d720f70186a0f2c17b', 'score': 0.573586404, 'text': 'The DISCOV (DImensionless Shunting COlor Vision) system models a cascade of primate color vision neurons: retinal ganglion, thalamic single opponent, and cortical double opponent. A unified model derived from psychophysical axioms produces transparent network dynamics and principled parameter settings. DISCOV fits an array of physiological data for each cell type, and makes testable experimental predictions. Binary DISCOV augments an earlier version of the model to achieve stable computations for spatial data analysis. The model is described in terms of RGB images, but inputs may consist of any number of spatially defined components. System dynamics are derived using algebraic computations, and robust parameter ranges that meet experimental data are fully specified. Assuming default values, the only free parameter for the user to specify is the spatial scale. Multi-scale analysis accommodates items of various sizes and perspective. Image inputs are first processed by complement coding, which produces an ON channel stream and an OFF channel stream for each component. Subsequent computations are on-center/off-surround, with the OFF channel replacing the off-center/on-surround fields of other models. Together with an orientation filter, DISCOV provides feature input vectors for an integrated recognition system. The development of DISCOV models is being carried out in the context of a large-scale research program that is integrating cognitive and neural systems derived from analyses of vision and recognition to produce both biological models and technological applications.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(response['matches'])\n",
    "matches = response['matches']\n",
    "\n",
    "matches_with_details=[]\n",
    "for i,match in enumerate(matches):\n",
    "    doc_id = match['id']\n",
    "    score = match['score']\n",
    "    # title = data[data['id'] == int(doc_id)]['title'].fillna(\"\").values[0]\n",
    "    #abstract = data[data['index'] == doc_id]['abstract'].fillna(\"\").values[0]\n",
    "    # text_doc=\"title- \"+title+\" abstract- \"+abstract\n",
    "    abstract=match['metadata']['abstract']\n",
    "    print_check= {\n",
    "        'id': doc_id,\n",
    "        'score': score,\n",
    "        'text': abstract\n",
    "    }\n",
    "    print(str(i+1)+'. '+str(print_check)+'\\n')\n",
    "\n",
    "    match_info = {\n",
    "        'id': doc_id,\n",
    "        'text': abstract\n",
    "    }\n",
    "    matches_with_details.append(match_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches_with_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_documents = pc.inference.rerank(\n",
    "    model=\"bge-reranker-v2-m3\",\n",
    "    query=query,\n",
    "    documents=matches_with_details,\n",
    "    top_n=TOP_N,\n",
    "    return_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RerankResult(\n",
       "  model='bge-reranker-v2-m3',\n",
       "  data=[\n",
       "    { index=21, score=0.5627172,\n",
       "      document={id=\"5390985d20f70186a...\", text=\"Computer vision i...\"} },\n",
       "    { index=58, score=0.55438,\n",
       "      document={id=\"5390b0ca20f70186a...\", text=\"The research pres...\"} },\n",
       "    ... (6 more documents) ...,\n",
       "    { index=74, score=0.22541662,\n",
       "      document={id=\"555a1d570cf2b2190...\", text=\"An approach for s...\"} },\n",
       "    { index=54, score=0.21733753,\n",
       "      document={id=\"539089d320f70186a...\", text=\"Hardware, softwar...\"} }\n",
       "  ],\n",
       "  usage={'rerank_units': 1}\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. id: 5390985d20f70186a0e088fb   score: 0.5627172   abstract: Computer vision is the process of using computers to extract from images useful information about the physical world, including meaningful descriptions of physical objects. For example, if an image sensor, such as a digitizing video camera, captured an image of a physical scene, and the digital image was input to a computer vision system, the desired output would be a description of the physical scene in terms that would be useful for the particular task at hand. Computer vision has many applications, including robotics, industrial automation, document processing, remote sensing, navigation, microscopy, medical imaging, and the development of visual prostheses for the blind.\n",
      "\n",
      "2. id: 5390b0ca20f70186a0ed9506   score: 0.55438   abstract: The research presented in this paper represents several novel conceptual contributions to the computer vision literature. In this position paper, our goal is to define the scope of computer vision analysis and discuss a new categorisation of the computer vision problem. We first provide a novel decomposition of computer vision into base components which we term the axioms of vision. These are used to define researcher-level and developer-level access to vision algorithms, in a way which does not require expert knowledge of computer vision. We discuss a new line of thought for computer vision by basing analyses on descriptions of the problem instead of in terms of algorithms. From this an abstraction can be developed to provide a layer above algorithmic details. This is extended to the idea of a formal description language which may be automatically interpreted thus allowing those not familiar with computer vision techniques to utilise sophisticated methods.\n",
      "\n",
      "3. id: 5390980720f70186a0e028ef   score: 0.50027657   abstract: This paper summarizes the present state of research in scene analysis. It identifies fundamental information processing principles relevant to representation and Use of knowledge in vision and traces limitations of existing progams to compromises of these principles necessitated by extant processors Some specific and general recommendations are offered regarding a productive course of research for the next decade.\n",
      "\n",
      "4. id: 5390882420f70186a0d88d49   score: 0.49090677   abstract: From the Publisher:This book presents a coherent approach to the fast-moving field of computer vision, using a consistent notation based on a detailed understanding of the image formation process. It covers even the most recent research and will provide a useful and current reference for professionals working in the fields of machine vision, image processing, and pattern recognition. An outgrowth of the author's course at MIT, Robot Vision presents a solid framework for understanding existing work and planning future research. Its coverage includes a great deal of material that is important to engineers applying machine vision methods in the real world. The chapters on binary image processing, for example, help explain and suggest how to improve the many commercial devices now available. And the material on photometric stereo and the extended Gaussian image points the way to what may be the next thrust in commercialization of the results in this area. Chapters in the first part of the book emphasize the development of simple symbolic descriptions from images, while the remaining chapters deal with methods that exploit these descriptions. The final chapter offers a detailed description of how to integrate a vision system into an overall robotics system, in this case one designed to pick parts out of a bin. The many exercises complement and extend the material in the text, and an extensive bibliography will serve as a useful guide to current research. Errata (164k PDF)\n",
      "\n",
      "5. id: 5390881820f70186a0d8204b   score: 0.34411275   abstract: A framework for high-level representations in computer visionarchitectures is described.The framework is based on the notion of conceptual space.This approach allows us to define a conceptual semantics for the symbolic representations of the vision system. In this way, the semantics of the symbols can be grounded to the data coming fromthe sensors. In addition, the proposed approach generalizesthe most popular frameworks adopted in computer vision.\n",
      "\n",
      "6. id: 5390bfa220f70186a0f54e53   score: 0.33676714   abstract: In this groundbreaking new volume, computer researchers discuss the development of technologies and specific systems that can interpret data with respect to domain knowledge. Although the chapters each illuminate different aspects of image interpretation, all utilize a common approach - one that asserts such interpretation must involve perceptual learning in terms of automated knowledge acquisition and application, as well as feedback and consistency checks between encoding, feature extraction, and the known knowledge structures in a given application domain. The text is profusely illustrated with numerous figures and tables to reinforce the concepts discussed.\n",
      "\n",
      "7. id: 53908f5b20f70186a0dda540   score: 0.27115518   abstract: THIS PAPER SUMMARIZES SEVERAL AREAS OF RESEARCH AT THE UNIVERSITY OF MASSACHUSETTS THAT ARE PARTIALLY OR ENTIRELY SUPPORTED UNDER THE DARPA IMAGE UNDERSTANDING PROGRAM. MANY OF THE INDIVIDUAL EFFORTS DISCUSSED BELOW ARE FURTHER DEVELOPED IN OTHER PAPERS IN THESE PROCEEDINGS. THE SUMMARY IS DIVIDED INTO SEVERAL AREAS: 1. KNOWLEDGE-BASED VISION 2. DATABASE SUPPORT FOR SYMBOLIC VISION PROCESSING 3. MOTION PROCESSING 4. PERCEPTUAL ORGANIZATION (GROUPING) 5. IMAGE UNDERSTANDING ARCHITECTURE 6. INTEGRATED VISION BENCHMARK FOR PARALLEL ARCHITECTURES 7. MOBILE VEHICLE NAVIGATION. ALTHOUGH WE DISCUSS EACH AREA SEPARATELY, A FUNDAMENTAL GOAL OF THE COMPUTER VISION RESEARCH ENVIRONMENT AT UMASS IS THE INTEGRATION OF A DI- VERSE SET OF RESEARCH EFFORTS INTO A SYSTEM THAT IS ULTIMATELY INTENDED TO ACHIEVE REAL-TIME IMAGE INTERPRETATION. TWO OF OUR MAJOR SYSTEM INTEGRATION EFFORTS ARE THE VISIONS STATIC INTERPRETATION SYSTEM, WHICH IS A KNOWLEDGE- BASED COMPUTER VISION SYSTEM UTILIZING PARALLEL MODULAR PROCESSES THAT COM- MUNICATE VIA A BLACKBOARD [DRA87B, HAN87A]. THE SECOND SYSTEM INTEGRATION EFFORT INVOLVES AN AUTONOMOUS MOBILE VEHICLE FOR NAVIGATION THROUGH A PARTI\n",
      "\n",
      "8. id: 53908e0020f70186a0dd6260   score: 0.2683658   abstract: Methods are presented 1) to partition or decompose a visual scene into the bodies forming it; 2) to position these bodies in three-dimensional space, by combining two scenes that make a stereoscopic pair; 3) to find the regions or zones of a visual scene that belong to its background; 4) to carry out the isolation of the objects in 1) when the input has inaccuracies. Running computer programs implement the methods and many examples illustrate their behavior. The input is a two-dimensional line-drawing of the scene, assumed to contain three-dimensional bodies possessing flat faces (polyhedra); some of them may be partially occluded. Suggestions are made for extending the work to curved objects. Some comparisons are made with human visual perception. The main conclusion is that it is possible to separate a picture or scene into the constituent objects exclusively on the basis of monocular geometric properties (on the basis of pure form); in fact, successful methods are shown.\n",
      "\n",
      "9. id: 555a1d570cf2b21909ba35cf   score: 0.22541662   abstract: An approach for scene understanding based on qualitative descriptors, domain knowledge and logics is proposed in this paper. Qualitative descriptors, qualitative models of shape, colour, topology and location are used for describing any object in the scene. Two kinds of domain knowledge are provided: (i) categorizations of objects according to their qualitative descriptors, and (ii) semantics for describing the affordances, mobility and other functional properties of target objects. First order logics are obtained for reasoning and scene understanding. Tests were carried out at the Interact@Cartesium scenario and promising results were obtained.\n",
      "\n",
      "10. id: 539089d320f70186a0d9b0c3   score: 0.21733753   abstract: Hardware, software tools, algorithms, and performance metrics that have been developed for image understanding are presented. Three commercially built examples reflecting three mature approaches considered germane to vision-single-instruction multiple-data, multiple-instruction multiple-data, and systolic processing-were chosen. They are, respectively, the Connection Machine, the Butterfly, and the Warp. A fourth approach, more specific to vision, was also selected for noncommercial implementation. This machine, the Image-Understanding Architecture, involves a heterogeneous combination of parallel processors with single-instruction multiple-data, multiple-instruction multiple-data, and other capabilities. Each site employing one of the above architectures developed a different set of tools, leading to significant cross-fertilization of ideas between the sites. Algorithms for low-level vision, shape from texture, fusing stereo and texture, surface interpolation, and robot navigation, among others, are briefly discussed. Benchmarks are described.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(reranked_documents.data):\n",
    "    print(str(i+1)+'. id: '+doc['document']['id']+'   score: '+str(doc['score'])+'   abstract: '+doc['document']['text']+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
