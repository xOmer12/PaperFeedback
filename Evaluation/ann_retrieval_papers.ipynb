{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = pd.read_csv('/home/student/FinalProject/PaperFeedback/Datasets/acm_citation_network_v8_labeled.csv')\n",
    "network_data = initial_data[initial_data['references'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data=network_data[network_data['abstract'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015.0\n"
     ]
    }
   ],
   "source": [
    "max_year=network_data['year'].max()\n",
    "print(max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year_data=network_data[network_data['year']==max_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          6578\n",
       "index               6578\n",
       "title               6578\n",
       "authors             6506\n",
       "year                6578\n",
       "venue               6578\n",
       "references          6578\n",
       "abstract            6578\n",
       "id                  6578\n",
       "clustered_labels    6578\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_year_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '703c7c8a-2b8f-46bc-b2f7-ede6b037b3fa'\n",
    "index_name = 'ann-embeddings'\n",
    "TOP_N=10\n",
    "TOP_K=100\n",
    "DOC_NUMBER=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=API_KEY)\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('using GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('using CPUG')\n",
    "    device = 'cpu'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample random papers that have references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_documents = last_year_data.sample(n=DOC_NUMBER, random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "# Display the sampled documents\n",
    "print(sampled_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find top k closest documents for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Pinecone and SentenceTransformer\n",
    "pc = Pinecone(api_key=API_KEY)\n",
    "index = pc.Index(index_name)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each sampled document to locate them in the Pinecone index\n",
    "for _, row in sampled_documents.iterrows():\n",
    "    # Assuming each document has an 'id' or some unique identifier that corresponds to the index\n",
    "    document_index = row['index']  # Replace 'index' with the appropriate column name for your document identifiers\n",
    "\n",
    "    # Create an embedding for the document\n",
    "    embedded_query = model.encode([row['abstract']])  # Replace 'abstract' with the appropriate column name for the text data\n",
    "\n",
    "    # Run the query on Pinecone\n",
    "    response = index.query(\n",
    "        vector=embedded_query.tolist(),  # Convert to list if needed\n",
    "        top_k=TOP_K+1,\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Filter out the original document from the query response\n",
    "    filtered_matches = [match for match in response['matches'] if match['id'] != document_index]\n",
    "    print(filtered_matches)\n",
    "    \n",
    "    # Append the results\n",
    "    results.append({\n",
    "        'document': row,\n",
    "        'closest_documents': filtered_matches\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate retrieved top k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of y_true: 32737, Size of y_pred: 32737\n",
      "y_true samples: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_pred samples: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Precision: 0.0295\n",
      "Recall: 0.2441\n",
      "F1 Score: 0.0526\n",
      "MAP: 0.1925\n",
      "MRR: 0.3029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_results(results):\n",
    "    # Create lists to hold true labels and predicted labels\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    average_precisions = []\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for result in results:\n",
    "        # Get the actual references (ensure it's a set) and closest document IDs\n",
    "        actual_references = set(result['document']['references'].split(';'))  # Actual references\n",
    "        predicted_ids = [match['id'] for match in result['closest_documents']]  # Predicted IDs (list for ranking)\n",
    "\n",
    "        # Create a combined set of all document IDs (true and predicted)\n",
    "        all_ids = actual_references.union(predicted_ids)\n",
    "\n",
    "        # Append true positives and false negatives for precision, recall, F1 calculations\n",
    "        for doc_id in all_ids:\n",
    "            y_true.append(1 if doc_id in actual_references else 0)\n",
    "            y_pred.append(1 if doc_id in predicted_ids else 0)\n",
    "\n",
    "        # Calculate Average Precision (AP) for MAP\n",
    "        relevant_docs_retrieved = 0\n",
    "        score_sum = 0\n",
    "        for i, doc_id in enumerate(predicted_ids):\n",
    "            if doc_id in actual_references:\n",
    "                relevant_docs_retrieved += 1\n",
    "                precision_at_k = relevant_docs_retrieved / (i + 1)\n",
    "                score_sum += precision_at_k\n",
    "        if relevant_docs_retrieved > 0:\n",
    "            average_precisions.append(score_sum / relevant_docs_retrieved)\n",
    "        else:\n",
    "            average_precisions.append(0)\n",
    "\n",
    "        # Calculate Reciprocal Rank (RR) for MRR\n",
    "        for i, doc_id in enumerate(predicted_ids):\n",
    "            if doc_id in actual_references:\n",
    "                reciprocal_ranks.append(1 / (i + 1))\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)  # No relevant doc found\n",
    "\n",
    "    # Debugging: Print sizes and samples of y_true and y_pred\n",
    "    print(f'Size of y_true: {len(y_true)}, Size of y_pred: {len(y_pred)}')\n",
    "    print(f'y_true samples: {y_true[:10]}')  # Print first 10 elements for verification\n",
    "    print(f'y_pred samples: {y_pred[:10]}')  # Print first 10 elements for verification\n",
    "\n",
    "    # Ensure y_true and y_pred are the same size before calculating metrics\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(\"Error: y_true and y_pred are not the same length.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    map_score = sum(average_precisions) / len(average_precisions)\n",
    "    mrr_score = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "    return precision, recall, f1, map_score, mrr_score\n",
    "\n",
    "# Assuming results is your list of results from previous queries\n",
    "precision, recall, f1, map_score, mrr_score = evaluate_results(results)\n",
    "\n",
    "# Print evaluation metrics if valid\n",
    "if precision is not None:\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'MAP: {map_score:.4f}')\n",
    "    print(f'MRR: {mrr_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank docuemnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: Unnamed: 0                                                    1655939\n",
      "index                                        559147bc0cf232eb904fb961\n",
      "title               Using a Tangible Versus a Multi-touch Graphica...\n",
      "authors             Joyce Ma, Lisa Sindorf, Isaac Liao, Jennifer F...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Ninth International Confere...\n",
      "references          5390ba3820f70186a0f373ef;5390881820f70186a0d81...\n",
      "abstract            We describe a study comparing the behavior of ...\n",
      "id                                                            1655939\n",
      "clustered_labels                                                    0\n",
      "Name: 1655939, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908b4920f70186a0dbacc5   score: 0.7821637   abstract: This describes the usability and interaction challenges in creating a unique museum exhibit which utilizes real-time compositing, and hides complex computational tasks behind a simple user interface.\n",
      "\n",
      "2. id: 5390a37f20f70186a0e6c3a3   score: 0.73829186   abstract: Much of the work done in the field of tangible interaction has focused on creating tools for learning; however, in many cases, little evidence has been provided that tangible interfaces offer educational benefits compared to more conventional interaction techniques. In this paper, we present a study comparing the use of a tangible and a graphical interface as part of an interactive computer programming and robotics exhibit that we designed for the Boston Museum of Science. In this study, we have collected observations of 260 museum visitors and conducted interviews with 13 family groups. Our results show that visitors found the tangible and the graphical systems equally easy to understand. However, with the tangible interface, visitors were significantly more likely to try the exhibit and significantly more likely to actively participate in groups. In turn, we show that regardless of the\n",
      "\n",
      "3. id: 5591327d0cf232eb904fb336   score: 0.73659015   abstract: Museum objects have fascinating stories but are often presented in a detached, objective way that tends to keep visitors at a distance. In a collaborative research we have explored a different way to present museum objects: fifteen exhibits from the museum deposit compete for one of the four display cases on the exhibit floor. Objects are given a personal voice and a character and they talk directly to the visitor: those that capture visitors' interest as physical presence or Twitter conversation stay on display; the lower scorer is replaced. We report the co-design and preliminary evaluation carried out in the museum with both museum professionals and casual visitors.\n",
      "\n",
      "4. id: 559121d80cf232eb904fae89   score: 0.7337381   abstract: Interactive surfaces are increasingly common in museums and other informal learning environments where they are seen as a medium for promoting social engagement. However, despite their increasing prevalence, we know very little about factors that contribute to collaboration and learning around interactive surfaces. In this paper we present analyses of visitor engagement around several multi-touch tabletop science exhibits. Observations of 629 visitors were collected through two widely used techniques: video study and shadowing. We make four contributions: 1) we present an algorithm for identifying groups within a dynamic flow of visitors through an exhibit hall; 2) we present measures of group-level engagement along with methods for statistically analyzing these measures; 3) we assess the effect of observational techniques on visitors' engagement, demonstrating that consented video studi\n",
      "\n",
      "5. id: 5390a80f20f70186a0e971fc   score: 0.65390086   abstract: This paper discusses observations of visitor interactions around a museum installation, focusing on how physical setup and shape of two variants of the installation, a telescope-like viewer and a barrier-free screen, shaped visitor experiences and interactions around and with the system. The analysis investigates contextual embedding, and how the two system variants affected people's ability of sharing the experience and negotiating use.\n",
      "\n",
      "6. id: 5390adfd20f70186a0ec6429   score: 0.54483163   abstract: Improving the visiting experience of exhibitions and public spaces in general has been the subject of several studies over the past years. In the study presented here, we were particularly interested in understanding the potential of combining explicit and implicit interaction modes between virtual characters and visitors. We present in this paper scenarios that exemplify the combination of these interaction modes, and an initial study with user involvement, based on a software platform that we are currently developing. We report on first feedback from users about the system-level interaction, usability, and visiting experience. The tested case study should create a sense of ubiquity of the virtual characters throughout the visit, and take advantage of their communication skills, at the same time giving freedom to visiting groups to interact with each other and to make visits to the exhi\n",
      "\n",
      "7. id: 53909ed120f70186a0e30957   score: 0.5364647   abstract: This work presents the results of a comparative study in which we investigate the ways manipulation of physical versus digital media are fundamentally different from one another. Participants carried out both a puzzle task and a photo sorting task in two different modes: in a physical 3-dimensional space and on a multi-touch, interactive tabletop in which the digital items resembled their physical counterparts in terms of appearance and behavior. By observing the interaction behaviors of 12 participants, we explore the main differences and discuss what this means for designing interactive surfaces which use aspects of the physical world as a design resource.\n",
      "\n",
      "8. id: 53909ee020f70186a0e3391f   score: 0.45701554   abstract: In this research I explore what elements there may be in common between tangible interactive-technology works that successfully engage their participants. An exploration of existing methods for obtaining useful evaluations for non-use and ambiguous environments forms a part of the discussion.\n",
      "\n",
      "9. id: 5390ae2e20f70186a0ec841a   score: 0.45653093   abstract: A frequent need of museums is to provide visitors with context-sensitive information about exhibits in the form of maps, or scale models. This paper suggests an augmented-reality approach for supplementing physical surfaces with digital information, through the use of pieces of plain paper that act as personal, location-aware, interactive screens. The technologies employed are presented, along with the interactive behavior of the system, which was instantiated and tested in the form of two prototype setups: a wooden table covered with a printed map and a glass case containing a scale model. The paper also discusses key issues stemming from experience and observations in the course of qualitative evaluation sessions.\n",
      "\n",
      "10. id: 5390980720f70186a0e01fd1   score: 0.44040912   abstract: Diverse museum artifacts, such as ceramics, porcelain, and ritual bronzes, can convey a sense of a people's history and culture, time, or place. Following specific criteria about the protection, maintenance, and preservation of these artifacts ensures their proper care and restoration. Often, this means visitors must view the artifacts statically in a glass showcase, precluding any kind of physical interaction. Moreover, because of limited exhibition space, many equally precious, beautiful, and important objects in the museum's possession are unfortunately out of sight in storerooms. To makethese objects more accessible, we developed a tangible photorealistic virtual museum system that lets people interact naturally and have an immersive experience with museum exhibits.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707101\n",
      "index                                        55323b8945cec66b6f9da198\n",
      "title               Mathematical Modeling of the Dynamics of a Non...\n",
      "authors                                V. M. Bulavatsky, A. V. Gladky\n",
      "year                                                           2015.0\n",
      "venue                                Cybernetics and Systems Analysis\n",
      "references          539099a220f70186a0e17160;5390b29820f70186a0ee9...\n",
      "abstract            This article considers the problem of mathemat...\n",
      "id                                                            1707101\n",
      "clustered_labels                                                    0\n",
      "Name: 1707101, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bf1320f70186a0f50d8a   score: 0.98327893   abstract: The analytical solutions of boundary-value problems with nonlocal boundary conditions are presented for two fractional differential mathematical models of the dynamics of a geomigration process non-equilibrium in time. The models based on the equations with the Caputo and Hilfer derivatives of fractional order are considered.\n",
      "\n",
      "2. id: 5390b7ff20f70186a0f27b77   score: 0.9737447   abstract: A mathematical model is set up to analyze the dynamics of locally time- and space-nonequilibrium migration---consolidation processes in a porous earth saturated with salt solutions under mass transfer. The corresponding nonlinear boundary-value problem is stated, an algorithm of its approximate solution is presented, and the results of the numerical implementation of the algorithm are given.\n",
      "\n",
      "3. id: 5390980720f70186a0e02d04   score: 0.9230392   abstract: A numerical method for solving the fractional diffusion equation, which could also be easily extended to other fractional partial differential equations, is considered. In this paper we combine the forward time centered space (FTCS) method, well known for the numerical integration of ordinary diffusion equations, with the Grünwald--Letnikov discretization of the Riemann--Liouville derivative to obtain an explicit FTCS scheme for solving the fractional diffusion equation. The stability analysis of this scheme is carried out by means of a powerful and simple new procedure close to the well-known von Neumann method for nonfractional partial differential equations. The analytical stability bounds are in excellent agreement with numerical test. A comparison between exact analytical solutions and numerical predictions is made.\n",
      "\n",
      "4. id: 5390a2be20f70186a0e64cbf   score: 0.8978745   abstract: In the present paper the Analytical approximate solution of a fractional diffusion equation is deduced with the help of powerful Variational Iteration method. By using an initial value, the explicit solutions of the equation for different cases have been derived, which accelerate the rapid convergence of the series solution. The present method performs extremely well in terms of efficiency and simplicity. Numerical results for different particular cases of the problem are presented graphically.\n",
      "\n",
      "5. id: 5390b9d520f70186a0f324c0   score: 0.8850366   abstract: The evolution process of fractional order describes some phenomenon of anomalous diffusion and transport dynamics in complex system. The equation containing fractional derivatives provides a suitable mathematical model for describing such a process. The initial boundary value problem is hard to solve due to the nonlocal property of the fractional order derivative. We consider a final value problem in a bounded domain for fractional evolution process with respect to time, which means to recover the initial state for some slow diffusion process from its present status. For this ill-posed problem, we construct a regularizing solution using quasi-reversible method. The well-posedness of the regularizing solution as well as the convergence property is rigorously analyzed. The advantage of the proposed scheme is that the regularizing solution is of the explicit analytic solution and therefore \n",
      "\n",
      "6. id: 5390c04520f70186a0f56fb6   score: 0.882022   abstract: An initial-boundary value problem for fractional in time diffusion equation with interface is considered. Its well-posedness in the corresponding Sobolev spaces is proved. Some finite difference schemes approximating the problem are proposed and their stability and convergence are investigated.\n",
      "\n",
      "7. id: 539099ec20f70186a0e1c2c4   score: 0.86703575   abstract: This paper deals with numerical solutions to a partial differential equation of fractional order. Generally this type of equation describes a transition from anomalous diffusion to transport processes. From a phenomenological point of view, the equation includes at least two fractional derivatives: spatial and temporal. In this paper we proposed a new numerical scheme for the spatial derivative, the so-called Riesz-Feller operator. Moreover, using the finite difference method, we show how to employ this scheme in the numerical solution of fractional partial differential equations. In other words, we considered an initial-boundary value problem in one-dimensional space. In the final part of this paper some numerical results and plots of simulations are shown as examples.\n",
      "\n",
      "8. id: 5390b0ca20f70186a0ed9bfa   score: 0.81257004   abstract: Fractional differentials provide more accurate models of systems under consideration. In this paper, approximation techniques based on the shifted Legendre-tau idea are presented to solve a class of initial-boundary value problems for the fractional diffusion equations with variable coefficients on a finite domain. The fractional derivatives are described in the Caputo sense. The technique is derived by expanding the required approximate solution as the elements of shifted Legendre polynomials. Using the operational matrix of the fractional derivative the problem can be reduced to a set of linear algebraic equations. From the computational point of view, the solution obtained by this method is in excellent agreement with those obtained by previous work in the literature and also it is efficient to use.\n",
      "\n",
      "9. id: 5390a1f820f70186a0e5d41b   score: 0.7902067   abstract: We analyze self-similar solutions to a nonlinear fractional diffusion equation and fractional Burgers/Korteweg-deVries equation in one spatial variable. By using Lie-group scaling transformation, we determined the similarity solutions. After the introduction of the similarity variables, both problems are reduced to ordinary nonlinear fractional differential equations. In two special cases exact solutions to the ordinary fractional differential equation, which is derived from the diffusion equation, are presented. In several other cases the ordinary fractional differential equations are solved numerically, for several values of governing parameters. In formulating the numerical procedure, we use special representation of a fractional derivative that is recently obtained.\n",
      "\n",
      "10. id: 558b9520612c6b62e5e8c13c   score: 0.7643643   abstract: We formulate a numerical method to solve the porous medium type equation with fractional diffusion $$\\\\begin{aligned} \\\\frac{\\\\partial u}{\\\\partial t}+(-\\\\Delta )^{1/2} (u^m)=0. \\\\end{aligned}$$ ¿ u ¿ t + ( - Δ ) 1 / 2 ( u m ) = 0 . The problem is posed in $$x\\\\in {\\\\mathbb {R}}^N$$ x ¿ R N , $$m\\\\ge 1$$ m ¿ 1 and with nonnegative initial data. The fractional Laplacian is implemented via the so-called Caffarelli---Silvestre extension. We prove existence and uniqueness of the solution of this method and also the convergence to the theoretical solution of the equation. We run numerical experiments on typical initial data as well as a section that summarizes and concludes the proposed method.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672677\n",
      "index                                        559129920cf232eb904fb0cb\n",
      "title               Designing a Micro-Volunteering Platform for Si...\n",
      "authors                                                Yi-Ching Huang\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference Compani...\n",
      "references          558b35ca612c41e6b9d46bbc;5390bb1d20f70186a0f3e...\n",
      "abstract            Situated crowdsourcing has emerged to overcome...\n",
      "id                                                            1672677\n",
      "clustered_labels                                                    3\n",
      "Name: 1672677, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba0a20f70186a0f33867   score: 0.95447797   abstract: Finding and retaining volunteers is a challenge for most of the NGOs (non-government-organizations) or non-profit organizations worldwide. Quite often, volunteers have a desire to help but are hesitant in making time commitments due to busy lives or demanding schedules. Micro-volunteering or crowdsourced volunteering has taken off in the last few years where a task is divided into fragments and accomplished collectively by the crowd. Individuals are only required to work on small chunks of tasks during their bits of short free times during the day. This panel brings in an interesting mix of researchers from the crowdsourcing/development space and social entrepreneurs to discuss the pros and cons of micro-volunteering for non-profits and identify the missing blocks in enabling us to replicate this concept in developing regions worldwide.\n",
      "\n",
      "2. id: 5390a5b020f70186a0e7d3a7   score: 0.9347534   abstract: Crowdsourcing is emerging as the new on-line distributed problem solving and production model in which networked people collaborate to complete a task.Enterprises are increasingly employing crowdsourcing to access scalable workforce on-line. In parallel, cloud computing has emerged as a new paradigm for delivering computational services, which seamlessly interweave physical and digital worlds through a common infrastructure.This paper presents a sample crowdsourcing scenario in software development domain to derive the requirements for delivering a general-purpose crowdsourcing service in the Cloud. It proposes taxonomy for categorization of crowdsourcing platforms, and evaluates a number of existing systems against the set of identified features. Finally, the paper outlines a research agenda for enhancing crowdsourcing capabilities, with focus on virtual team building and task-based ser\n",
      "\n",
      "3. id: 558b35c2612c41e6b9d46ba6   score: 0.93171144   abstract: Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.\n",
      "\n",
      "4. id: 558c09ed0cf20e727d0f595a   score: 0.9049869   abstract: Crowdsourcing has emerged in recent years as a potential strategy to enlist the general public to solve a wide variety of tasks. With the advent of ubiquitous Internet access, it is now feasible to ask an Internet crowd to conduct QoE (Quality of Experience) experiments on their personal computers in their own residences rather than in a laboratory. The considerable size of the Internet crowd allows researchers to crowdsource their experiments to a more diverse set of participant pool at a relatively low economic cost. However, as participants carry out experiments without supervision, the uncertainty of the quality of their experiment results is a challenging problem.\n",
      "\n",
      "5. id: 5390b95420f70186a0f2d8fb   score: 0.8311431   abstract: Crowdsourcing involves outsourcing some job to a distributed group of people online, typically by breaking the job down into microtasks. Online markets offer human users payment for completing small tasks, or users can participate in nonpaid platforms such as games and volunteer sites. These platforms' general availability has enabled researchers to recruit large numbers of participants for user studies, generate third-party content and assessments, or even build novel user experiences. This special issue provides a snapshot of the most recent crowdsourcing research.\n",
      "\n",
      "6. id: 559154b60cf232eb904fbc82   score: 0.7744129   abstract: A major research challenge for spatial crowdsourcing is to improve the expected quality of the results. However, existing research in this field mostly focuses on achieving this objective in volunteer-based spatial crowdsourcing. In this paper, we introduce the budget limitations into the above problem and consider realistic cases where workers are paid unequally based on their trustworthiness. We propose a novel quality and budget aware spatial task allocation approach which jointly considers the workers' reputation and proximity to the task locations to maximize the expected quality of the results while staying within a limited budget.\n",
      "\n",
      "7. id: 5390ae2e20f70186a0ec8481   score: 0.7465088   abstract: Crowdsourcing has emerged in recent years as an exciting new avenue for leveraging the tremendous potential and resources of today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this glut of a still largely under-utilized workforce. Crowdsourcing offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite unders\n",
      "\n",
      "8. id: 5390b04120f70186a0ed8759   score: 0.7163179   abstract: Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best prac\n",
      "\n",
      "9. id: 5390afc920f70186a0ed2053   score: 0.6666053   abstract: Crowdsourcing is an increasingly viable approach for a range of problem-solving contexts. The author presents and illustrates the concept of crowdservicing, which is enabled by the rapid evolution of Web 3.0. Ongoing developments in and emerging scenarios of crowdservicing attempt to facilitate a balanced integration of diverse services provided by human agents and machines over the World Wide Web.\n",
      "\n",
      "10. id: 5390b4c420f70186a0efe58a   score: 0.66432256   abstract: Crowdsourcing is an effective tool to solve hard tasks. By bringing 100,000s of people to work on simple tasks that only humans can do, we can go far beyond traditional models of data analysis and machine learning. As technologies and processes mature, crowdsourcing is becoming mainstream. It powers many leading Internet companies and a wide variety of novel projects: from content moderation and business listing verification to real-time SMS translation for disaster response. However, quality assurance can be a major challenge. In this paper CrowdFlower presents various crowdsourcing applications, from business to ethics, to money and survival, all of which showcase the power of labor-on-demand, otherwise known as the human cloud.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1716911\n",
      "index                                        55923cea612c4fa28ff7a30a\n",
      "title               Link weight based truth discovery in social se...\n",
      "authors                                         Chao Huang, Dong Wang\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 14th International Conferen...\n",
      "references          539087fe20f70186a0d73a8b;5390ad0720f70186a0ebb...\n",
      "abstract            This paper presents a link weight based maximu...\n",
      "id                                                            1716911\n",
      "clustered_labels                                                    3\n",
      "Name: 1716911, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a54720f70186a0e78948   score: 0.99687636   abstract: Using a ground truth extracted from the Wikipedia, and a ground truth created through manual assessment, we show that the apparent performance advantage seen in machine learning approaches to link discovery are an artifact of trivial links that are actively rejected by manual assessors.\n",
      "\n",
      "2. id: 5390bda020f70186a0f46ff2   score: 0.9888419   abstract: This article addresses the challenge of truth discovery from noisy social sensing data. The work is motivated by the emergence of social sensing as a data collection paradigm of growing interest, where humans perform sensory data collection tasks. Unlike the case with well-calibrated and well-tested infrastructure sensors, humans are less reliable, and the likelihood that participants' measurements are correct is often unknown a priori. Given a set of human participants of unknown trustworthiness together with their sensory measurements, we pose the question of whether one can use this information alone to determine, in an analytically founded manner, the probability that a given measurement is true. In our previous conference paper, we offered the first maximum likelihood solution to the aforesaid truth discovery problem for corroborating observations only. In contrast, this article ext\n",
      "\n",
      "3. id: 5390bfa220f70186a0f54f09   score: 0.9872773   abstract: The explosive growth in social network content suggests that the largest \"sensor network\" yet might be human. Extending the participatory sensing model, this paper explores the prospect of utilizing social networks as sensor networks, which gives rise to an interesting reliable sensing problem. In this problem, individuals are represented by sensors (data sources) who occasionally make observations about the physical world. These observations may be true or false, and hence are viewed as binary claims. The reliable sensing problem is to determine the correctness of reported observations. From a networked sensing standpoint, what makes this sensing problem formulation different is that, in the case of human participants, not only is the reliability of sources usually unknown but also the original data provenance may be uncertain. Individuals may report observations made by others as their\n",
      "\n",
      "4. id: 558b4efc612c41e6b9d48cda   score: 0.9849274   abstract: Truth discovery is a long-standing problem for assessing the validity of information from various data sources that may provide different and conflicting information. With the increasing prominence of data streams arising in a wide range of applications such as weather forecast and stock price prediction, effective techniques for truth discovery in data streams are demanded. However, existing work mainly focuses on truth discovery in the context of static databases, which is not applicable in applications involving streaming data. This motivates us to develop new techniques to tackle the problem of truth discovery in data streams. In this paper, we propose a probabilistic model that transforms the problem of truth discovery over data streams into a probabilistic inference problem. We first design a streaming algorithm that infers the truth as well as source quality in real time. Then, we\n",
      "\n",
      "5. id: 559166b80cf2e89307ca9937   score: 0.9799382   abstract: The proliferation of mobile sensing and communication devices in the possession of the average individual generated much recent interest in social sensing applications. Significant advances were made on the problem of uncovering ground truth from observations made by participants of unknown reliability. The problem, also called fact-finding commonly arises in applications where unvetted individuals may opt in to report phenomena of interest. For example, reliability of individuals might be unknown when they can join a participatory sensing campaign simply by downloading a smartphone app. This paper extends past social sensing literature by offering a scalable approach for exploiting dependencies between observed variables to increase fact-finding accuracy. Prior work assumed that reported facts are independent, or incurred exponential complexity when dependencies were present. In contras\n",
      "\n",
      "6. id: 55909b480cf28af999b589db   score: 0.971564   abstract: We present a decision-theoretic approach for sampling information sources in resource-constrained environments, where there is uncertainty regarding source trustworthiness. We exploit diversity among sources to stratify the population into homogeneous subgroups to both minimise redundant sampling and mitigate the effect of source collusion. We show through empirical evaluation that our model is as effective as existing truth discovery approaches with respect to accuracy, while significantly reducing sampling cost.\n",
      "\n",
      "7. id: 5390bfa120f70186a0f52c33   score: 0.97118384   abstract: This paper develops and evaluates algorithms for exploiting physical constraints to improve the reliability of social sensing. Social sensing refers to applications where a group of sources (e.g., individuals and their mobile devices) volunteer to collect observations about the physical world. A key challenge in social sensing is that the reliability of sources and their devices is generally unknown, which makes it non-trivial to assess the correctness of collected observations. To solve this problem, the paper adopts a cyber-physical approach, where assessment of correctness of individual observations is aided by knowledge of physical constraints on both sources and observed variables to compensate for the lack of information on source reliability. We cast the problem as one of maximum likelihood estimation. The goal is to jointly estimate both (i) the latent physical state of the obser\n",
      "\n",
      "8. id: 5390b0ca20f70186a0edb5e6   score: 0.96909106   abstract: This keynote reflects on the value and potential of social networks, interlinked data, semantic web and data-mining. At the same time I would like to elicit new research directions, which are only enabled by the sheer mass of data, sensors, facts, reports, opinions and inter-linkage of people.\n",
      "\n",
      "9. id: 5390b20120f70186a0ee5780   score: 0.96405166   abstract: At CPSWeek 2011, the authors presented a demonstration of Apollo, a fact-finder for participatory sensing that ranks archived human-centric and sensor data by credibility. The current demonstration significantly extends our previous work by allowing Apollo to operate on live streaming data; in this case, live Twitter feeds. As the role of humans as sensors increases in emerging sensing applications, a principled approach becomes necessary to address the problem of ascertaining the veracity of sources and observations made by them. Participatory and social sensing applications may use potentially unreliable or unverified sources, such as a phone-based sensing application that grows virally in a large un-vetted population, a disaster-response application, where conflicting damage assessment reports may come from large numbers of different volunteers, or a military application, where friend\n",
      "\n",
      "10. id: 5390bae620f70186a0f3c76f   score: 0.95775455   abstract: In this paper, we address the problem of fusing untrustworthy reports provided from a crowd of observers, while simultaneously learning the trustworthiness of individuals. To achieve this, we construct a likelihood model of the users's trustworthiness by scaling the uncertainty of its multiple estimates with trustworthiness parameters. We incorporate our trust model into a fusion method that merges estimates based on the trust parameters and we provide an inference algorithm that jointly computes the fused output and the individual trustworthiness of the users based on the maximum likelihood framework. We apply our algorithm to cell tower local- isation using real-world data from the OpenSignal project and we show that it outperforms the state-of-the-art methods in both accuracy, by up to 21%, and consistency, by up to 50% of its predictions.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1694889\n",
      "index                                        5592483a0cf28b1a968ff636\n",
      "title               Optimal base stations location and configurati...\n",
      "authors             Shokri Z. Selim, Yasser A. Almoghathawi, Manso...\n",
      "year                                                           2015.0\n",
      "venue                                               Wireless Networks\n",
      "references          5390b56a20f70186a0f0515d;5390a17720f70186a0e51...\n",
      "abstract            In this paper, we study the problem of base st...\n",
      "id                                                            1694889\n",
      "clustered_labels                                                    3\n",
      "Name: 1694889, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4cb5f   score: 0.9800149   abstract: It is gradually more significant to optimally select base stations in the design of cellular networks, as the customers stipulate cheaper and better wireless services. From a set of prospective site locations, a subset needs to be preferred which optimizes two critical objectives: service coverage and financial cost. Discovering the optimum base station locations for a cellular radio network is considered as a mathematical optimization problem. In the context of mobile communication, an efficient algorithm for the base-station placement problem is developed in this paper. The intention is to place a given number of base-stations in a given convex region and to assign range to each of them such that every point in the region is covered by at least one base-station and the maximum range assigned is curtailed. It is basically covering a region by a given number of equal radius circles where\n",
      "\n",
      "2. id: 5390af8920f70186a0ed00df   score: 0.9596866   abstract: In this paper, various parameters of cellular base station (BS) placement problem such as site coordinates, transmitting power, height and tilt angle are determined using evolutionary multiobjective algorithm to obtain better compromised solutions. The maximization of service coverage and minimization of cost are considered as conflicting objectives by satisfying inequality constraints such as handover, traffic demand and overlap. For the purpose of simulation, a 15 脳 15 Km2 synthetic test system is discretized as hexagonal cell structure and necessary simulations are carried out to calculate receiving field strength at various points. The path loss is calculated using Hata model. To improve the diversity and uniformity of the obtained nondominated solutions, controlled elitism and dynamic crowding distance operators are introduced in non-dominated sorting genetic algorithm-II (NSGA-II) \n",
      "\n",
      "3. id: 5390980720f70186a0e02c74   score: 0.9428234   abstract: The antenna placement problem, or cell planning problem, involves locating and configuring infrastructure for cellular wireless networks. From candidate site locations, a set needs to be selected against objectives relating to issues such as financial cost and service provision. This is an NP-hard optimization problem and consequently heuristic approaches are necessary for large problem instances. In this study, we use a greedy algorithm to select and configure base station locations. The performance of this greedy approach is dependent on the order in which the candidate sites are considered. We compare the ability of four state-of-the-art multiple objective genetic algorithms to find an optimal ordering of potential base stations. Results and discussion on the performance of the algorithms are provided.\n",
      "\n",
      "4. id: 5390b36120f70186a0ef1606   score: 0.92468774   abstract: We consider the following model of cellular networks. Each base station has a given finite capacity, and each client has some demand and profit. A client can be covered by a specific subset of the base stations, and its profit is obtained only if its demand is provided in full. The goal is to assign clients to base stations, so that the overall profit is maximized subject to base station capacity constraints. In this work, we present a distributed algorithm for the problem, that runs in polylogarithmic time, and guarantees an approximation ratio close to the best known ratio achievable by a centralized algorithm.\n",
      "\n",
      "5. id: 5390a2e920f70186a0e672b5   score: 0.92262185   abstract: We consider the following model of cellular networks. Each base station has a given finite capacity, and each client has some demand and profit. A client can be covered by a specific subset of the base stations, and its profit is obtained only if its demand is provided in full. The goal is to assign clients to base stations, so that the overall profit is maximized subject to base station capacity constraints. In this work we present a distributed algorithm for the problem, that runs in polylogarithmic time, and guarantees an approximation ratio close to the best known ratio achievable by a centralized algorithm.\n",
      "\n",
      "6. id: 5390880d20f70186a0d7bd48   score: 0.8789391   abstract: Finding optimum base station locations for a cellular radio network is considered as a mathematical optimization problem. Dependent on the channel assignment policy, the minimization of interferences or the number of blocked channels, respectively, may be more favourable. In this paper, a variety of according analytical optimization problems are introduced. Each is formalized as an integer linear program, and in most cases optimum solutions can be given. Whenever by the complexity of the problem an exact solution is out of reach, simulated annealing is used as an approximate optimization technique. The performance of the different approaches is compared by extensive numerical tests.\n",
      "\n",
      "7. id: 5390a80f20f70186a0e9623b   score: 0.87578696   abstract: We consider two optimization problems for cellular telephone networks, that arise in a recently discussed ITU proposal for a traffic load model. These problems address the positioning of base stations (on given possible locations) with the aim to maximize the number of supplied demand nodes and minimize the number of stations that have to be built. We show that these problems are hard to approximate, but their Euclidean versions allow a polynomial-time approximation scheme (PTAS). Furthermore, we consider other related optimization problems.\n",
      "\n",
      "8. id: 5390985d20f70186a0e08850   score: 0.87578696   abstract: We consider two optimization problems for cellular telephone networks, that arise in a recently discussed ITU proposal for a traffic load model. These problems address the positioning of base stations (on given possible locations) with the aim to maximize the number of supplied demand nodes and minimize the number of stations that have to be built. We show that these problems are hard to approximate, but their Euclidean versions allow a polynomial-time approximation scheme (PTAS). Furthermore, we consider other related optimization problems.\n",
      "\n",
      "9. id: 5390b2d620f70186a0eeb59e   score: 0.8456877   abstract: An important problem of mobile communication is placing a given number of base-stations in a given convex region, and to assign range to each of them such that every point in the region is covered by at least one base-station, and the maximum range assigned is minimized. The algorithm proposed in this paper uses Voronoi diagram, and it works for covering a convex region of arbitrary shape. Experimental results justify the efficiency of our algorithm and the quality of the solution produced.\n",
      "\n",
      "10. id: 5390bb7b20f70186a0f40849   score: 0.84504944   abstract: Recently a new approach to modeling cellular networks has been proposed based on the Poisson point process (PPP). Unlike the traditional, popular hexagonal grid model for the locations of base stations, the PPP model is tractable. It has been shown by Andrews et al. (in IEEE Trans Commun 59(11):3122---3134, 2011) that the hexagonal grid model provides upper bounds of the coverage probability while the PPP model gives lower bounds. In this paper, we perform a comprehensive comparison of the PPP and the hexagonal grid models with real base station deployments in urban areas worldwide provided by the open source project OpenCellID. Our simulations show that the PPP model gives upper bounds of the coverage probabilities for urban areas and is more accurate than the hexagonal grid model. In addition, we show that the Poisson cluster process is able to accurately model the base station locatio\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1742629\n",
      "index                                        5543b2f40cf276d80f9048fb\n",
      "title               Hierarchical task mapping for parallel applica...\n",
      "authors                       Jingjin Wu, Xuanxing Xiong, Zhiling Lan\n",
      "year                                                           2015.0\n",
      "venue                                   The Journal of Supercomputing\n",
      "references                                   558b21ba612c41e6b9d445a3\n",
      "abstract            As the scale of supercomputers grows, so does ...\n",
      "id                                                            1742629\n",
      "clustered_labels                                                    1\n",
      "Name: 1742629, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908e0020f70186a0dd4386   score: 0.8891444   abstract: Abstract: Clusters have become a very cost-effective platform for high-performance computing. In these systems, the trend is towards the interconnection network becoming the system bottleneck. Therefore, in the future, scheduling strategies will have to take into account the communication requirements of the applications and the communication bandwidth that the network can offer. One of the key issues in these strategies is the task mapping technique used when the network becomes the system bottleneck. In this paper, we propose an enhanced version of a previously proposed mapping technique that takes into account not only the existing network resources, but also the traffic generated by the applications. Also, we evaluate the mapping technique using real MPI application traces with timestamps. Evaluation results show that the use of the new mapping technique fully exploits the available \n",
      "\n",
      "2. id: 5390b1d220f70186a0ee2e4b   score: 0.85752964   abstract: As the high performance computing systems scale up, mapping the tasks of a parallel application onto physical processors to allow efficient communication becomes one of the critical performance issues. Existing algorithms were usually designed to map applications with regular communication patterns. Their mapping criterion usually overlooks the size of communicated messages, which is the primary factor of communication time. In addition, most of their time complexities are too high to process large scale problems. In this paper, we present a hierarchical mapping algorithm (HMA), which is capable of mapping applications with irregular communication patterns. It first partitions tasks according to their run-time communication information. The tasks that communicate with each others more frequently are regarded as strongly connected. Based on their connectivity strength, the tasks are parti\n",
      "\n",
      "3. id: 539089d220f70186a0d9aebe   score: 0.8474635   abstract: A generalized mapping strategy that uses a combination of graph theory, mathematicalprogramming, and heuristics is proposed. The authors use the knowledge from the givenalgorithm and the architecture to guide the mapping. The approach begins with agraphical representation of the parallel algorithm (problem graph) and the parallelcomputer (host graph). Using these representations, the authors generate a newgraphical representation (extended host graph) on which the problem graph is mapped.An accurate characterization of the communication overhead is used in the objectivefunctions to evaluate the optimality of the mapping. An efficient mapping scheme isdeveloped which uses two levels of optimization procedures. The objective functionsinclude minimizing the communication overhead and minimizing the total execution timewhich includes both computation and communication times. The mapping sche\n",
      "\n",
      "4. id: 5390b95520f70186a0f2e90e   score: 0.72242916   abstract: Increasing the number of processors in a single chip toward network-based many-core systems requires a run-time task allocation algorithm. We propose an efficient mapping algorithm that assigns communicating tasks of incoming applications onto resources of a many-core system utilizing Network-on-Chip paradigm. In our contiguous neighborhood allocation (CoNA) algorithm, we target at the reduction of both internal and external congestion due to detrimental impact of congestion on the network performance. We approach the goal by keeping the mapped region contiguous and placing the communicating tasks in a close neighborhood. A completely synthesizable simulation environment where none of the system objects are assumed to be ideal is provided. Experiments show at least 40% gain in different mapping cost functions, as well as 16% reduction in average network latency compared to existing algor\n",
      "\n",
      "5. id: 53909ed120f70186a0e30f02   score: 0.70911986   abstract: As we enter the era of peta-scale computing, system architects must plan for machines composed of tens or even hundreds of thousands of processors. Although fully connected networks such as fat-tree configurations currently dominate HPC interconnect designs, such approaches are inadequate for ultra-scale concurrencies due to the superlinear growth of component costs. Traditional low-degree interconnect topologies, such as 3D tori, have reemerged as a competitive solution due to the linear scaling of system components relative to the node count; however, such networks are poorly suited for the requirements of many scientific applications at extreme concurrencies. To address these limitations, we propose HFAST, a hybrid switch architecture that uses circuit switches to dynamically reconfigure lower-degree interconnects to suit the topological requirements of a given scientific application.\n",
      "\n",
      "6. id: 5390aeba20f70186a0ecbd8b   score: 0.68961924   abstract: Optimal network performance is critical for efficient parallel scaling of communication-bound applications on large machines. No-load latencies do not increase significantly with the number of hops traveled when wormhole routing is deployed. Yet, we and others have recently shown that in the presence of contention, message latencies can grow substantially large. Hence, task mapping strategies should take the topology of the machine into account on large machines. In this paper, we present topology aware mapping as a technique to optimize communication on three-dimensional mesh interconnects and hence improve the performance. Our methodology is facilitated by the idea of object-based decomposition used in Charm++ which separates the processes of decomposition from mapping of computation to processors and allows a more flexible mapping based on communication patterns between objects. Explo\n",
      "\n",
      "7. id: 5390b1d220f70186a0ee2c50   score: 0.66530186   abstract: Communication-aware task mapping algorithms, which map parallel tasks onto processing nodes according to the communication patterns of applications, are essential to reduce the communication time in modern high performance computing. In this paper, we design algorithms specifically for interconnected multicore systems, whose architectural property, namely small number of cores per node, large number of nodes, and large performance gap between the communication within a multicore and among multicores, had brought new challenges and opportunities to the mapping problem. Let k be the number of cores per multicore and n be the number of tasks. We consider the practical case that k is much smaller than n, for k = 2, 4, and 6. The designed algorithms are optimal for the mapping measurement, called Maximum Interconnective Message Size (MIMS), and of time complexity merely O(mlogm) for m communi\n",
      "\n",
      "8. id: 5390972920f70186a0dfa298   score: 0.6629056   abstract: Many large and complex computational applications canbe modeled as irregular graphs and are typically characterizedby a large number of vertices and edges. This paper proposesa new and fast mapping heuristic, called FastMap, tomap this class of applications onto heterogeneous metacomputingplatforms such as computational grids. While previousapproaches have delved into graph partitioning of the application,we attempt to solve this problem from the clusteringperspective. We exploit a hierarchical resource managementinfrastructure on the grid to distribute the overhead of mappingamong a tree of schedulers and develop a scheme thatproves to be almost linear in its scalability. Furthermore, weoptimize on the result of the mapping with the help of a geneticalgorithm at each scheduler node. Our experiments include a50,000-node application graph from NASA and several othersynthetically-generated\n",
      "\n",
      "9. id: 5390b72e20f70186a0f21ce9   score: 0.65290564   abstract: Communication-aware task mapping algorithms, which map parallel tasks onto processing nodes according to the communication patterns of applications, are essential to reduce the communication time in modern high performance computing. In this paper, we design algorithms specifically for interconnected multicore systems, whose architectural property, namely small number of cores per node, large number of nodes, and large performance gap between the communication within a multicore and among multicores, had brought new challenges and opportunities to the mapping problem. Let k be the number of cores per multicore and n be the number of tasks. We consider the practical case that k 芦 n for k = 2, 4, and 6. The designed algorithms are optimal for the mapping measurement, called Maximum Interconnective Message Size (MIMS), and of time complexity merely O(mlogm) for m communication pairs. Thus, \n",
      "\n",
      "10. id: 5390bd1520f70186a0f452f8   score: 0.590425   abstract: Governments, universities, and companies expend vast resources building the top supercomputers. The processors and interconnect networks become faster, while the number of nodes grows exponentially. Problems of scale emerge, not least of which is collective performance. This thesis identifies and proposes solutions for two major scalability problems. Our first contribution is a novel algorithm for process-partitioning and remapping for exascale systems that has far better time and space scaling than known algorithms. Our evaluations predict an improvement of up to 60x for large exascale systems and arbitrary reduction in the large temporary buffer space required for generating new communicators.Our second contribution consists of several novel collective algorithms for Clos and torus networks. Known allgather, reduce-scatter, and composite algorithms for Clos networks suffer the worst co\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1729919\n",
      "index                                        55323d8045cec66b6f9de8a7\n",
      "title               Temperature-aware software-based self-testing ...\n",
      "authors             Ying Zhang, Zebo Peng, Jianhui Jiang, Huawei L...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references          558ce9470cf23fdd601e1142;558bebb90cf2e30013db322d\n",
      "abstract            Delay defects under high temperature have been...\n",
      "id                                                            1729919\n",
      "clustered_labels                                                    0\n",
      "Name: 1729919, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ab8820f70186a0eb10d5   score: 0.97706646   abstract: This article discusses the potential role of software-based self-testing in the microprocessor test and validation process, as well as its supplementary role in other classic functional- and structural-test methods. In addition, the article proposes a taxonomy for different SBST methodologies according to their test program development philosophy, and summarizes research approaches based on SBST techniques for optimizing other key aspects.\n",
      "\n",
      "2. id: 53908bad20f70186a0dc33c7   score: 0.93980294   abstract: A new approach to test pattern generation which is particularly suitable for self-test is described. Required computation time is much less than for present-day automatic test pattern generation (ATPG) programs. Fault simulation is not required. More patterns may be obtained than from standard ATPG programs. However, fault coverage is much higher - all irredundant multiple as well as single stuck faults are detected. Test length is easily controlled. The test patterns are easily generated algorithmically either by program or hardware.\n",
      "\n",
      "3. id: 53909ee020f70186a0e33347   score: 0.93403506   abstract: We present a technique for generating instruction sequences to test a processor functionally. We target delay defects with this technique using an ATPG engine to generate delay tests locally, a verification engine to map the tests globally, and a feedback mechanism that makes the entire procedure faster. We demonstrate nearly 96% coverage of delay faults with the instruction sequences generated. These instruction sequences can be loaded into the cache to test the processor functionally.\n",
      "\n",
      "4. id: 53909f8c20f70186a0e3f02f   score: 0.9298237   abstract: A new approach to test pattern generation which is particularly suitable for self-test is described. Required computation time is much less than for present day automatic test pattern generation (ATPG) programs. Fault simulation or fault modeling is not required. More patterns may be obtained than from standard ATPG programs. However, fault coverage is much higher all irredundant multiple as well as single stuck faults are detected. The test patterns are easily generated algorithmically either by program or hardware.\n",
      "\n",
      "5. id: 5390995d20f70186a0e15862   score: 0.9253649   abstract: Software-based self-test (SBST) of processors offers many benefits, such as dispense with expensive test equipments, test execution during maintenance and in the field or initialization tests for the whole system. In this paper, for the first time a structural SBST methodology is proposed which optimizes energy, average power consumption, test length and fault coverage at the same time.\n",
      "\n",
      "6. id: 5390b00c20f70186a0ed4f9d   score: 0.9020933   abstract: Software-based self-test (SBST) has recently emerged as an effective methodology for the manufacturing test of processors and other components in systems-on-chip (SoCs). By moving test related functions from external resources to the SoC's interior, in the form of test programs that the on-chip processor executes, SBST significantly reduces the need for high-cost, big-iron testers, and enables high-quality at-speed testing and performance binning. Thus far, SBST approaches have focused almost exclusively on the functional (programmer visible) components of the processor. In this paper, we analyze the challenges involved in testing an important component of modern processors, namely, the pipelining logic, and propose a systematic SBST methodology to address them. We first demonstrate that SBST programs that only target the functional components of the processor are not sufficient to test \n",
      "\n",
      "7. id: 539099a220f70186a0e18ed1   score: 0.90000373   abstract: Software-based self-test (SBST) has recently emerged as an effective methodology for the manufacturing test of processors and other components in Systems-on-Chip (SoCs). By moving test related functions from external resources to the SoC's interior, in the form of test programs that the on-chip processor executes, SBST eliminates the need for high-cost testers, and enables high-quality at-speed testing. Thus far, SBST approaches have focused almost exclusively on the functional (directly programmer visible) components of the processor. In this paper, we analyze the challenges involved in testing an important component of modern processors, namely, the pipelining logic, and propose a systematic SBST methodology to address them. We first demonstrate that SBST programs that only target the functional components of the processor are insufficient to test the pipeline logic, resulting in a sig\n",
      "\n",
      "8. id: 5390aa7620f70186a0eaad4d   score: 0.8967949   abstract: With delay defects becoming more common due to the properties of the newer process technologies, at- speed functional tests have become indispensable. Traditionally, functional tests needed expensive automatic testing equipment due to the memory and speed requirements associated. This cost issue was solved by native-mode (or cache resident) testing which uses the intelligence of the processor to test itself. In the native-mode self-test (also known as software-based self-test) paradigm, instruction sequences are loaded into the cache (and also made cache resident) to test the processor for defects. Generally, only random instructions are used in native mode tests. As with any random sequence based testing, there are faults that are left undetected by random instructions. Manual effort is necessary to generate the tests that can detect those faults, requiring a detailed knowledge of the i\n",
      "\n",
      "9. id: 5390a05a20f70186a0e4a2f6   score: 0.89533985   abstract: We present a technique that deals with the problem of efficiently generating instruction sequences to test for delay defects in a processor. These instruction sequences are loaded into the cache of a processor and the processor is run in its normal functional (native) mode to test itself. The methodology that we present avoids the significant increase in search space of a previous method while generating tests. We also present a technique which increases the probability of detecting multiple delay faults with a single instruction sequence. This technique can help immensely in reducing the cost of test. We demonstrate the effectiveness of our technique on an off-the shelf processor.\n",
      "\n",
      "10. id: 558aee15612c41e6b9d3da31   score: 0.88700897   abstract: Very long instruction word (VLIW) processors are increasingly employed in a large range of embedded signal processing applications, mainly due to their ability to provide high performances with reduced clock rate and power consumption. At the same time, there is an increasing request for efficient and optimal test techniques able to detect permanent faults in VLIW processors. Software-based self-test (SBST) methods are a consolidated and effective solution to detect faults in a processor both at the end of the production phase or during the operational life; however, when traditional SBST techniques are applied to VLIW processors, they may prove to be ineffective (especially in terms of size and duration), due to their inability to exploit the parallelism intrinsic in these architectures. In this paper, we present a new method for the automatic generation of efficient test programs speci\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675453\n",
      "index                                        55913bae0cf232eb904fb60e\n",
      "title               Standards and/as Innovation: Protocols, Creati...\n",
      "authors                              Steven J. Jackson, Sarah Barbrow\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390881220f70186a0d7f3e9;5390b2fc20f70186a0eef...\n",
      "abstract            Standards and protocols play important but und...\n",
      "id                                                            1675453\n",
      "clustered_labels                                                    0\n",
      "Name: 1675453, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a30b20f70186a0e6a847   score: 0.96021223   abstract: This paper presents some of the lessons that we have learned during our involvement with the development of learning technology standards. We believe that the decision to develop a new standards is sometimes taken too quickly and that, when possible, existing generic standards should be profiled for the domain of learning. At this point in time, the development of tools, technologies and methodologies that fully exploit the affordances of existing standards is more relevant than the development of new standards. Moreover, we believe that the relationship between technical standardization and research is often misunderstood: the main role of standards for research is to enable a large-scale technological infrastructure that promotes innovation through its open nature.\n",
      "\n",
      "2. id: 558c9dde0cf296c8c28cbc20   score: 0.8489722   abstract: In this forum we highlight innovative thought, design, and research in the area of interaction design and sustainability, illustrating the diversity of approaches across HCI communities. ---Lisa Nathan and Samuel Mann, Editors\n",
      "\n",
      "3. id: 559247d50cf26384af04a04f   score: 0.83748704   abstract: In this forum we highlight innovative thought, design, and research in the area of interaction design and sustainability, illustrating the diversity of approaches across HCI communities. --- Lisa Nathan and Samuel Mann, Editors\n",
      "\n",
      "4. id: 558da0b70cf2af9ee80e9ce9   score: 0.83748704   abstract: In this forum we highlight innovative thought, design, and research in the area of interaction design and sustainability, illustrating the diversity of approaches across HCI communities. --- Lisa Nathan and Samuel Mann, Editors\n",
      "\n",
      "5. id: 5390aeba20f70186a0ec99c6   score: 0.8003801   abstract: We report preliminary results from a socio-technical analysis of scientific collaboration, specifically a loosely connected group of physical anthropology researchers. Working from a combination of interview data and artifact analysis, we identify current barriers to the scientists' collaboration as it relates to a valuable but scarce resource, a high-resolution computer tomography scanner. We analyze a two-layer structure of the collaboration, one that is loosely coupled through shared scanner access and use; and one that is tightly coupled through shared creative development of research questions, data analysis and interpretation. We conclude with implications for enhancements to the sociotechnical context and supporting infrastructure.\n",
      "\n",
      "6. id: 5390ad0720f70186a0ebae31   score: 0.7509182   abstract: This paper addresses the collaborative development of information infrastructure for supporting data-rich scientific collaboration. Studying infrastructure development empirically not only in terms of spatial issues but also, and equally importantly, temporal ones, we illustrate how the long-term matters. Our case is about the collaborative development of a metadata standard for an ecological research domain. It is a complex example where standards are recognized as one element of infrastructure and standard-making efforts include integration of semantic work and software tools development. With a focus on the temporal scales of short-term and long-term, we analyze the practices and views of the main parties involved in the development of the standard. Our contributions are three-fold: 1) extension of the notion of infrastructure to more explicitly include the temporal dimension; 2) iden\n",
      "\n",
      "7. id: 5390b64020f70186a0f18fb0   score: 0.7350715   abstract: As software engineering (and other) standards are developed over a period of years or decades, the suite of standards thus developed often begins to lose any cohesion that it originally possessed. This has led to discussions in the standards communities of possible collaborative development, interoperability and harmonization of their existing standards. Here, I assess how such harmonization efforts may be aided by recent research results to create better quality standards to replace the status quo.\n",
      "\n",
      "8. id: 5390b52620f70186a0f04516   score: 0.70568365   abstract: This forum looks at how the fields of interaction design and HCI can extend to cover \"developing\" communities around the world, ones that are gaining access to digital technology for the first time.Gary Marsden, Editor\n",
      "\n",
      "9. id: 5390bded20f70186a0f496db   score: 0.62658125   abstract: This forum presents innovative thought, design, and research in the area of interaction design and environmental sustainability. We explore how HCI can contribute to the complex, interdisciplinary efforts to address sustainability challenges. ---Elaine M. Huang, Editor\n",
      "\n",
      "10. id: 5390a93b20f70186a0ea0b08   score: 0.6206216   abstract: We examine the expansion of topic areas for qualitative research in HCI publications, focusing on representations of users and field sites. We examine further developments in anthropological methodologies during a critical period of the late 1980s and 90s. We identify concerns shared by both research communities, in particular, the relationships between researcher and informant, and the construction of bounded settings for field work. We then argue that ethnographic approaches and theoretical commitments which came to the fore after Anthropology's critical turn can be usefully applied, in ways that can inspire design, to investigations of social practice and technology appropriation.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1695049\n",
      "index                                        559249410cf28b1a968ff6cb\n",
      "title               Network construction with subgraph connectivit...\n",
      "authors                        Dana Angluin, James Aspnes, Lev Reyzin\n",
      "year                                                           2015.0\n",
      "venue                           Journal of Combinatorial Optimization\n",
      "references          5390981d20f70186a0e05b11;53908b6c20f70186a0dbd...\n",
      "abstract            We consider the problem introduced by Korach a...\n",
      "id                                                            1695049\n",
      "clustered_labels                                                    1\n",
      "Name: 1695049, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ad5620f70186a0ebefa5   score: 0.92482364   abstract: We consider the problem of inferring the most likely social network given connectivity constraints imposed by observations of outbreaks within the network. Given a set of vertices (or agents) V and constraints (or observations) Si ⊆ V we seek to find a minimum log-likelihood cost (or maximum likelihood) set of edges (or connections) E such that each Si induces a connected subgraph of (V, E). For the offline version of the problem, we prove an Ω(log(n)) hardness of approximation result for uniform cost networks and give an algorithm that almost matches this bound, even for arbitrary costs. Then we consider the online problem, where the constraints are satisfied as they arrive. We give an O(n log(n))-competitive algorithm for the arbitrary cost online problem, which has an Ω(n)-competitive lower bound.We look at the uniform cost case as well and give an O(n2/3 log2/3(n))-competitive algori\n",
      "\n",
      "2. id: 53908cde20f70186a0dcee6b   score: 0.8895288   abstract: We present a general approximation technique for a class of network design problems where we seek a network of minimum cost that satisfies certain communication requirements and is resilient to worst-case single-link failures. Our algorithm runs in $O(n^2 \\log n)$ time on a graph ith $n$ nodes and outputs a solution of cost at most thrice the optimum. We extend our technique to obtain approximation algorithms for augmenting a given network so as to satisfy certain communication requirements and achieve resilience to single-link failures. Our technique allows one to find nearly minimum-cost two-connected networks for a variety of connectivity requirements. For example, our result generalizes earlier results on finding a minimum-cost two-connected subgraph of a given edge-weighted graph and an earlier result on finding a minimum-cost subgraph two-connecting a specified subset of the nodes.\n",
      "\n",
      "3. id: 5390a28020f70186a0e613ee   score: 0.8879841   abstract: In the Survivable Network Design Problem (SNDP) the goal is tofind a minimum cost subset of edges that satisfies a given set ofpairwise connectivity requirements among the vertices. Thisgeneral network design framework has been studied extensively andis tied to the development of major algorithmic techniques. Forthe edge-connectivity version of the problem, a $2$-approximationalgorithm is known for arbitrary pairwise connectivityrequirements. However, no non-trivial algorithms are known for itsvertex connectivity counterpart. In fact, even highly restrictedspecial cases of the vertex connectivity version remain poorlyunderstood.We study the single-source $k$-vertex connectivity version ofSNDP. We are given a graph $G(V,E)$ with a subset $T$ of terminalsand a source vertex $s$, and the goal is to find a minimum costsubset of edges ensuring that every terminal is $k$-vertexconnected to $s$\n",
      "\n",
      "4. id: 539096cb20f70186a0df6ed9   score: 0.84376645   abstract: In the survivable network design problem (SNDP), the goal is to find a minimum-cost spanning subgraph satisfying certain connectivity requirements. We study the vertex-connectivity variant of SNDP in which the input specifies, for each pair of vertices, a required number of vertex-disjoint paths connecting them.We give the first strong lower bound on the approximability of SNDP, showing that the problem admits no efficient $2^{\\log^{1-\\epsilon} n}$ ratio approximation for any fixed $\\epsilon\\! \\! 0$, unless $\\NP\\subseteq \\DTIME(n^{\\polylog(n)})$. We show hardness of approximation results for some important special cases of SNDP, and we exhibit the first lower bound on the approximability of the related classical NP-hard problem of augmenting the connectivity of a graph using edges from a given set.\n",
      "\n",
      "5. id: 53908adf20f70186a0dac103   score: 0.81420064   abstract: In the survivable network design problem SNDP, the goal is to find a minimum-cost subgraph satisfying certain connectivity requirements. We study the vertex-connectivity variant of SNDP in which the input specifies, for each pair of vertices, a required number of vertexdisjoint paths connecting them.We give the first lower bound on the approximability of SNDP, showing that the problem admits no efficient 2log1-驴n ratio approximation for any fixed 驴 0 unless NP 驴 DTIME(npolylog(n)). We also show hardness of approximation results for several important special cases of SNDP, including constant factor hardness for the k-vertex connected spanning subgraph problem (k-VCSS) and for the vertex-connectivity augmentation problem, even when the edge costs are severely restricted.\n",
      "\n",
      "6. id: 539098b820f70186a0e0a99a   score: 0.7490872   abstract: Let G = (V, E) be a simple undirected graph with a set V of vertices and a set E of edges. Each vertex v ε V has a demand d(ν) ε Z+ and a cost c(ν) ε R+, where Z+ and R+ denote the set of nonnegative integers and the set of nonnegative reals, respectively. The source location problem with vertex-connectivity requirements in a given graph G asks to find a set S of vertices minimizing ΣνεS c(ν) such that there are at least d(ν) pairwise vertex-disjoint paths from S to ν for each vertex ν ε V - S. It is known that if there exists a vertex ν ε V with d(ν) ≥ 4, then the problem is NP-hard even in the case where every vertex has a uniform cost. In this paper, we show that the problem can be solved in O(|V|4(log|V|)2) time if d(ν) ≤ 3 holds for each vertex v ε V.\n",
      "\n",
      "7. id: 53908cde20f70186a0dcee9a   score: 0.7262315   abstract: We give approximation algorithms for two network design problems. These problems arise in designing communication networks where a prespecified set of sites needs to be connected through a network. Different applications judge the merit of the networks using different cost criteria. We consider two such cost criteria. Under the first criterion, a network of minimum total edge-cost is to be designed. This problem is known as the {\\em Steiner-tree problem.} Under the second criterion, the network''s maximum degree is to be minimized; this is referred to as the {\\em min-degree problem.} Both these problems are known to be NP-complete. Our algorithms output a network in polynomial time whose cost is within a small multiplicative factor of the minimum network cost. Our algorithm for the Steiner-tree problem extends to a more general setting of the connectivity requirements of the network know\n",
      "\n",
      "8. id: 53908cde20f70186a0dcef21   score: 0.7192851   abstract: We give the first approximation algorithm for the {\\em generalized network Steiner tree problem}, a problem in network design. An instance consists of a network with link-costs and, for each pair ${i,j}$ of nodes, an edge-connectivity requirement. The goal is to find a minimum-cost network using the available links and satisfying the requirements. Our algorithm outputs a solution whose cost is within $ 2 \\log R $ of optimal, where $R$ is the highest requirement value. In the course of proving the performance guarantee, we prove a combinatorial min-max approximate equality relating minimum-cost networks to maximum packings of certain kinds of cuts. As a consequence of the proof of this theorem, we obtain an approximation algorithm for optimally packing these cuts; we show that this algorithm has application to estimating the reliability of a probabilistic network.\n",
      "\n",
      "9. id: 5390a40520f70186a0e6f321   score: 0.71849567   abstract: In the Survivable Network Design Problem (SNDP) one seeks to find a minimum cost subgraph that satisfies prescribed node-connectivity requirements. We give a novel approximation ratio preserving reduction from Directed SNDP to Undirected SNDP. Our reduction extends and widely generalizes as well as significantly simplifies the main results of [G. Kortsarz, R. Krauthgamer, J.R. Lee, Hardness of approximation for vertex-connectivity network design problems, SIAM Journal on Computing 33 (3) (2004) 704-720]. Using it, we derive some new hardness of approximation results, as follows. We show that directed and undirected variants of SNDP and of k-Connected Subgraph are equivalent w.r.t. approximation, and that a @r-approximation for Undirected Rooted SNDP implies a @r-approximation for Directed Steiner Tree.\n",
      "\n",
      "10. id: 5390a40520f70186a0e70576   score: 0.70426184   abstract: Consider the edge-connectivity survivable network design problem: given a graph G = (V,E) with edge-costs, and edge-connectivity requirements rij for every pair of vertices i,j, find an (approximately) minimum-cost network that provides the required connectivity. While this problem is known to admit good approximation algorithms in the offline case, no algorithms were known for this problem in the online setting. In this paper, we give a randomized O(rmax log3 n) competitive online algorithm for this edge-connectivity network design problem, where rmax = maxij rij. Our algorithms use the standard embeddings of graphs into random subtrees (i.e., into singly connected subgraphs) as an intermediate step to get algorithms for higher connectivity. Our results for the online problem give us approximation algorithms that admit strict cost-shares with the same strictness value. This, in turn, im\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1683750\n",
      "index                                        55923d3f612c4fa28ff7a364\n",
      "title               Optimal phasor measurement units placement to ...\n",
      "authors                                 Ebrahim Abiri, Farzan Rashidi\n",
      "year                                                           2015.0\n",
      "venue               Journal of Intelligent & Fuzzy Systems: Applic...\n",
      "references          5390b44620f70186a0ef902a;5390aeba20f70186a0eca...\n",
      "abstract            This paper presents a hybrid modified binary p...\n",
      "id                                                            1683750\n",
      "clustered_labels                                                    1\n",
      "Name: 1683750, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390aeba20f70186a0eca45d   score: 0.97161794   abstract: This study presents a binary particle swarm optimization (BPSO) based methodology for the optimal placement of phasor measurement units (PMUs) when using a mixed measurement set. The optimal PMU placement problem is formulated to minimize the number of PMUs installation subject to full network observability and to maximize the measurement redundancy at the power system buses. In order to ensure full network observability in an electric power network the topology-based algorithm is used and Several factors considered; such as the available data from existing conventional measurements, the number and location of zero injection buses, the number and location of installed PMUs and of course, the system topology. The efficiency of the proposed method is verified by the simulation results of IEEE 14-bus, 30-bus, 57-bus-118 bus systems, respectively. The results show that the whole system can b\n",
      "\n",
      "2. id: 5390a06e20f70186a0e4bcec   score: 0.9635062   abstract: The analysis of power system observability and the rules of PMU placement are concisely presented. Several algorithms of PMU placement as well as their differences and relations are discussed in details: a graph-theoretic procedure based on Depth First Search can find the optimal placement the most quickly; Simulated Annealing Method can help the algorithm converge to global optimum. Base on these two algorithms, an improved algorithm: Minimum Spanning Tree Method is proposed. It keeps good balance of quality and efficiency of the optimal placement, and improves the multiformity of the results by improving the optimization rule of Depth First Search Method.\n",
      "\n",
      "3. id: 5390b95520f70186a0f2ea82   score: 0.96102494   abstract: Particle Swarm Optimization has been applied in many optimization problems. In this paper, we proposed a new algorithm based on PSO and applied to optimal the weighted fuzzy neural network. It takes some measures to avoid the algorithm into a local convergence and determine the structure of the fuzzy neural network based on the effective rules. Satisfactory results through experiments are obtained.\n",
      "\n",
      "4. id: 5390b44620f70186a0ef902a   score: 0.9272316   abstract: The paper proposes a multi-objective biogeography based optimization (MO-BBO) algorithm to design optimal placement of phasor measurement units (PMU) which makes the power system network completely observable. The simultaneous optimization of the two conflicting objectives such as minimization of the number of PMUs and maximization of measurement redundancy are performed. The Pareto optimal solution is obtained using the non-dominated sorting and crowding distance. The compromised solution is chosen using a fuzzy based mechanism from the Pareto optimal solution. Simulation results are compared with Non-dominated Sorting Genetic Algorithm-II (NSGA-II) and Non-dominated Sorting Differential Evolution (NSDE). Developed PMU placement method is illustrated using IEEE standard systems to demonstrate the effectiveness of the proposed algorithm.\n",
      "\n",
      "5. id: 5390b78a20f70186a0f24e42   score: 0.9219218   abstract: Particle swarm optimization algorithm was applied to reduce power loss and to prevent the decline of the power supply quality caused by the imbalance of reactive power, but reactive power optimization is a mixed non-linear programming problem with lots of variables and uncertain parameters, PSO algorithm also has some limitations such as premature convergence, which causes the bad accuracy of convergence. And then the coevolution of Particle Swarm Optimization (PSO) with nonlinear inertia weight factor (w) and Simulated Annealing algorithm (SA) is established to improve the original algorithm which is named as SA-NLWPSO. Compared with the algorithms such as PSO, SA-PSO and SA-WPSO, SA-NLWPSO is better for global convergence and higher accuracy of reactive power optimization by using IEEE-10 bus system as a model for the simulation.\n",
      "\n",
      "6. id: 5390bed320f70186a0f4d973   score: 0.87472075   abstract: In this paper, we solve the optimal power flow problem using by the new hybrid fuzzy particle swarm optimisation and Nelder-Mead (NM) algorithm (HFPSO-NM). The goal of combining the NM simplex method and the particle swarm optimisation (PSO) method is to integrate their advantages and avoid their disadvantages. The NM simplex method is a very efficient local search procedure, but its convergence is extremely sensitive to the selected starting point. In addition, PSO belongs to the class of global search procedures, but it requires significant computational effort. In the other side, in the PSO algorithm, two variables (@F\"1,@F\"2) are traditionally constant; in this case, due to the importance of these two factors, we decided to obtain these two as fuzzy parameters. The proposed method is firstly examined on some benchmark mathematical functions. Then, it is tested an IEEE 30-bus standard\n",
      "\n",
      "7. id: 5390a4d020f70186a0e75f10   score: 0.8609547   abstract: An improved particle swarms optimization algorithm based on Pareto Optimal set is proposed to optimize the reactive power in power system, which is a multiple objectives optimization problem. The proposed algorithm develops the new fitness assignment and random inertia weight strategy, problem-specific linkages can be learned by examining a randomly chosen collection of points in the search space, the improved algorithm also has the ability to avoid getting trapped in local optima due to prematurity, applying it to the calculation of the power systems of IEEE6-bus and IEEE14-bus, the calculation results prove its effectiveness.\n",
      "\n",
      "8. id: 5390a96e20f70186a0ea281d   score: 0.84325075   abstract: A genetic algorithm-based procedure for solving the optimal phasor measurement units (PMUs) placement problem is presented. A PMU measures voltage and current phasors at the bus where is placed. These measurements must realize the electrical power system observable, in order to perform the state estimation. Our proposal have two essential advantages: (1) it determines the minimal number of PMUs and their geographic distribution making the network observable; (2) it shows the relationship between the number of current phasors that must be measured on each PMUs and the necessary number of PMLrs for a given network. The placement algorithm has been tested on 4 standards IEEE-bus, ranging in size from 14 to 118 buses.\n",
      "\n",
      "9. id: 5390ad5620f70186a0ebd55a   score: 0.8080672   abstract: Conventional power system optimization problems deal with the power demand and spinning reserve through real values. In this research, we employ fuzzy variables to better characterize these values in uncertain environment. In building the fuzzy power system reliable model, fuzzy Value-at-Risk (VaR) can evaluate the greatest value under given confidence level and is a new technique to measure the constraints and system reliability. The proposed model is a complex nonlinear optimization problem which cannot be solved by simplex algorithm. In this paper, particle swarm optimization (PSO) is used to find optimal solution. The original PSO algorithm is improved to straighten out local convergence problem. Finally, the proposed model and algorithm are exemplified by one numerical example.\n",
      "\n",
      "10. id: 5390c04520f70186a0f55e6f   score: 0.8080672   abstract: One type of Evolutionary Optimization Technique namely Particle Swarm Optimization (PSO) have been proposed in this paper to obtain the optimal placement a Shunt Flexible AC Transmission System (FACTS) Controller i.e. Static VAr Compensator (SVC) in the network. An Optimal Power Flow (OPF) problem with mixed integer programming has been formulated for simultaneously optimizing multi-objectives optimization problem viz., enhancing the power system load ability, minimizing the active power loss of transmission line, and by considering installation cost of the controller whereas maintaining the system security and stability margins, e.g., small signal stability, fast voltage stability index, and line stability factor in their acceptable margins. The effectiveness of the proposed methodology has been investigated on a practical Java-Bali 24-bus Indonesia system. Results demonstrate that the \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652423\n",
      "index                                        55913a3b0cf2127aa930c41b\n",
      "title               Illustrating the Interaction of Algorithms and...\n",
      "authors                                                 Joan M. Lucas\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 46th ACM Technical Symposiu...\n",
      "references          5390aefb20f70186a0ecc4c9;5390ba0a20f70186a0f34...\n",
      "abstract            A thorough understanding of algorithms and dat...\n",
      "id                                                            1652423\n",
      "clustered_labels                                                    0\n",
      "Name: 1652423, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390995d20f70186a0e149e2   score: 0.9552357   abstract: Data structures is a well known course taught in most computer science departments. This course typically includes a study of the most conventional data structures and their associated algorithms. Complexity of the algorithms is often a major focus of the course. To facilitate the student's understanding of the algorithms and their complexity, three projects are proposed. This paper concerns itself with an empirical approach to these projects. In addition to the usual knowledge gained from performing these projects, students learn an analytical technique that can be applied to many algorithms and to research.\n",
      "\n",
      "2. id: 5390958a20f70186a0def555   score: 0.9474783   abstract: Every student of Computer Science will encounter certain very important and fundamental data structures and algorithms. Many of those structures are relatively simple to understand conceptually, but exceedingly difficult to analyze, especially when average behavior is taken into account. Therefore, if students are to choose the best algorithm for a problem or optimize the parameters of an algorithm, they are faced with mathematical challenges beyond their means. We suggest that Computer Algebra tools can make a dramatic difference in the ability of students to analyze very common algorithms and data structures and make meaningful design decisions when implementing such structures. Initial studies in the classroom support our suggestion.\n",
      "\n",
      "3. id: 5390881d20f70186a0d82d9d   score: 0.89423656   abstract: From the Publisher:Clifford A. Shaffer provides a treatment of fundamental data structures and the principles of algorithm analysis understandable by sophomore and junior level students in Computer Science and related fields. The author focuses on teaching students and practitioners to understand the principles required to select or design the data structure that will best solve his/her problem.\n",
      "\n",
      "4. id: 539087f320f70186a0d6fd6b   score: 0.88856554   abstract: Data Structures and Algorithms is clearly a very important topic and course in the Computer Science curriculum. It has been taught at several levels by a number of approaches. Should the approach be mathematical, theoretical and abstract or very concrete and \"hands on\"? Whichever method is used, the ultimate goal is the same: enhancing student comprehension. The panelists discuss three distinct and well-defined approaches.\n",
      "\n",
      "5. id: 539088b920f70186a0d9150f   score: 0.84859616   abstract: From the Publisher:With this text, you gain an understanding of the fundamental concepts of algorithms, the very heart of computer science. It introduces the basic data structures and programming techniques often used in efficient algorithms. Covers use of lists, push-down stacks, queues, trees, and graphs. Later chapters go into sorting, searching and graphing algorithms, the string-matching algorithms, and the Schonhage-Strassen integer-multiplication algorithm. Provides numerous graded exercises at the end of each chapter. 0201000296B04062001\n",
      "\n",
      "6. id: 5390a17720f70186a0e53cc3   score: 0.8305941   abstract: This paper advocates a particular approach to the teaching of data structures and algorithm design by supplementing the usual topics with an intensive study of implementations of a single algorithm. This approach intends to provide a consistent overall focus for the students as they move through the course and create an appreciation of the practical impact of the techniques learned in the course.\n",
      "\n",
      "7. id: 5390ae2e20f70186a0ec71e5   score: 0.8269914   abstract: According to a study by Zendler and Spannagel [5], the concept of algorithm is one of the most important basic subjects in computer science. It is extremely important, then, for students to get a good grasp of this concept from the very start of their training. Being a highly abstract concept, first-year students do not find algorithms easy to understand and use. In this respect, having a tool that helps and shepherds students through the process of learning this concept can make a huge difference to their training. This demo sets out to present a new environment designed to facilitate graph algorithms comprehension and learning. The contextualization of this contribution and the relations between this environment and other implemented systems are discussed in [2].\n",
      "\n",
      "8. id: 53908f5b20f70186a0dd935b   score: 0.8268516   abstract: A matching in a graph is a collection of edges, no two of which share a vertex. A maximum matching contains the greatest number of edges possible. This paper presents an efficient implementation of Edmonds'' algorithm for finding maximum matchings. The computation time is proportional to $V^3$, where V is the number of vertices; previous algorithms have computation time proportional to $V^4$. The implementation avoids Edmonds'' blossom reduction by using pointers to encode the structure of alternating paths.\n",
      "\n",
      "9. id: 539087f920f70186a0d7336b   score: 0.761893   abstract: A matching on a graph is a set of edges, no two of which share a vertex. A maximum matching contains the greatest number of edges possible. This paper presents an efficient implementation of Edmonds' algorithm for finding a maximum matching. The computation time is proportional to V3, where V is the number of vertices; previous implementations of Edmonds' algorithm have computation time proportional to V4. The implementation is based on a system of labels that encodes the structure of alternating paths.\n",
      "\n",
      "10. id: 5390881720f70186a0d805e5   score: 0.75635755   abstract: Data structures and algorithms are presented at the college level in a highly accessible format that presents material with one-page displays in a way that will appeal to both teachers and students. The thirteen chapters cover: Models of Computation, Lists, Induction and Recursion, Trees, Algorithm Design, Hashing, Heaps, Balanced Trees, Sets Over a Small Universe, Graphs, Strings, Discrete Fourier Transform, and Parallel Computation.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1673068\n",
      "index                                        559125b00cf232eb904fafd4\n",
      "title               A Tool for Analog/RF BIST Evaluation Using Sta...\n",
      "authors             Kamel Beznia, Ahcene Bounceur, Reinhardt Euler...\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Design Automation of Elect...\n",
      "references          5390b2fc20f70186a0eefc9f;53909fbc20f70186a0e41...\n",
      "abstract            Testing analog integrated circuits is expensiv...\n",
      "id                                                            1673068\n",
      "clustered_labels                                                    3\n",
      "Name: 1673068, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b3ae20f70186a0ef3289   score: 0.91117966   abstract: The high cost of analog circuit testing has sparked off intensified efforts to identify robust and low-cost alternative tests that could effectively replace the standard specification-based tests. Nevertheless, the current practice is still specification-based testing. One of the primary reasons is the lack of tools to evaluate in advance the indirect costs (e.g. parametric test escape and yield loss) associated with alternative tests. To this end, in this paper, we present a method to estimate test escape and yield loss that occur as a result of replacing one costly specification test by one low-cost alternative test. This evaluation is performed at the design or test development stage with parts per million (PPM) accuracy. The method is based on extreme value theory and on a fast simulation technique of extreme events called statistical blockade.\n",
      "\n",
      "2. id: 5390a8dc20f70186a0e9f409   score: 0.8938666   abstract: This paper presents an overview of test techniques that offer promising features when Built-In-Self-Test (BIST) must be applied to complex intgrated systems including analog, mixed-signal and RF parts. Emphasis is on techniques exhibiting a good trade-off between test requirements (basically in terms of signal accuracy and frequency) and test quality.\n",
      "\n",
      "3. id: 5390a74f20f70186a0e8ba34   score: 0.8653379   abstract: We present a method that is capable of handling process variations to evaluate analog/RF test measurements at the design stage. The method can readily be used to estimate test metrics, such as parametric test escape and yield loss, with parts per million accuracy, and to fix test limits that satisfy specific tradeoffs between test metrics of interest. Furthermore, it provides a general framework to compare alternative test solutions that are continuously being proposed toward reducing the high cost of specification-based tests. The key idea of the method is to build a statistical model of the circuit under test and the test measurements using nonparametric density estimation. Thereafter, the statistical model can be simulated very fast to generate an arbitrarily large volume of new data. The method is demonstrated for a previously proposed built-in self-test measurement for low-noise amp\n",
      "\n",
      "4. id: 5390956e20f70186a0ded863   score: 0.8255897   abstract: Analog integrated circuit testing and diagnosis is a verychallenging problem.The inaccuracy of measurements,the infinite domain of possible values and the parameterdeviations are among the major difficulties.During theprocess of optimizing production tests, MonteCarlo simulation is often needed due to parameter variations,but because of its expensive computational cost,it becomes the bottleneck of such a process.This paperdescribes a new technique to reduce the number of simulationsrequired during analog fault simulation.Thisleads to the optimization of production tests subjectedto parameter variations.In section I a review of thestate of the art is presented, section II introduces the algorithmand describes the methodology of our approach.The results on CMOS 2-stage opamp and conclusionsare given in sections III and IV.\n",
      "\n",
      "5. id: 5390aa0f20f70186a0ea87a4   score: 0.7525585   abstract: In this paper we propose a method for evaluating test measurements for complex circuits that are difficult to simulate. The evaluation aims at estimating test metrics, such as parametric test escape and yield loss, with parts per million (ppm) accuracy. To achieve this, the method combines behavioral modeling, density estimation, and regression. The method is demonstrated for a previously proposed Built-In Self-Test (BIST) technique for ΣΔ Analog-to-Digital Converters (ADC) explaining in detail the derivation of a behavioral model that captures the main nonidealities in the circuit. The estimated test metrics are further analyzed in order to uncover trends in a large device sample that explain the source of erroneous test decisions.\n",
      "\n",
      "6. id: 53909eef20f70186a0e3634c   score: 0.6550051   abstract: For Design-For-Test (DFT) purposes, analogue and mixed-signal testing has to cope with the difficulty of test evaluation before production. This paper aims at evaluating test measures for RF components in order to optimize production test sets and thus reduce test cost. For this, we have first developed a statistical model of the performances and possible test measures of the Circuit Under Test (a Low Noise Amplifier). The statistical multi-normal model is derived from data obtained using Monte-Carlo circuit simulation (five hundred iterations). This statistical model is then used to generate a larger circuit population (one million instances) from which test metrics can be estimated with ppm precision at the design stage, considering just process deviations. With the use of this model, a trade-off between defect level and yield loss resulting from process deviations is used to set test \n",
      "\n",
      "7. id: 53909fbc20f70186a0e41b50   score: 0.6253238   abstract: The estimation of test metrics such as defect level, test yield or yield loss is important in order to quantify the quality and cost of a test approach. For design-for-test purposes, this is important in order to select the best test measurements but this must be done at the design stage, before production test data is made available. In the analogue domain, previous works have considered the estimation of these metrics for the case of single faults, either catastrophic or parametric. The consideration of single parametric faults is sensitive for a production test technique if the design is robust. However, in the case that production test limits are tight, test escapes resulting from multiple parametric deviations may become important. In addition, aging mechanisms result in field failures that are often caused by multiple parametric deviations. In this paper, we will consider the estim\n",
      "\n",
      "8. id: 53908b1820f70186a0db492e   score: 0.57439476   abstract: This paper analyzes an environment which utilizes Built-In Self-Test (BIST) and AutomaticTest Equipment (ATE), to reduce the overall time for manufacturing test of complex digital chips. This requires properly establishing the time to switch from BIST to ATE (referred to as switchover time), thus utilizing ATE generated vectors to finally achieve the desired level of fault coverage. For this environment we model fault coverage as a function of the testability of the circuit under test and the numbers of vectors which are supplied by the BIST circuitry and the ATE. A novel approach is proposed; this approach is initially based on fault simulation using a small set of random patterns; an estimate of the so-called detection profile of the circuit under test is established as basis of the test model. This analytical model effectively relates the testable features of the circuit under test to\n",
      "\n",
      "9. id: 5390b68720f70186a0f1c56f   score: 0.56403846   abstract: A new technique for the estimation of analog parametric test metrics at the design stage is presented in this paper. This technique employs the copulas theory to estimate the distribution between random variables that represent the performances and the test measurements of the circuit under test (CUT). A copulas-based model separates the dependencies between these random variables from their marginal distributions, providing a complete and scale-free description of dependence that is more suitable to be modeled using well-known multivariate parametric laws. The model can be readily used for the generation of an arbitrarily large sample of CUT instances. This sample is thereafter used for estimating parametric test metrics such as defect level (or test escapes) and yield loss. We demonstrate the usefulness of the proposed technique to evaluate a built-in-test technique for a radio frequen\n",
      "\n",
      "10. id: 53909eef20f70186a0e35cbb   score: 0.50410455   abstract: We present a new method of testing analog circuits based on spectral estimation using auto regressive moving average (ARMA) models. We use a tiered testing approach where each tier becomes progressively more computationally expensive. Badly damaged circuits are quickly eliminated without wasting tester time. The models are generated using input and output analog circuit samples. Results are presented for two passive filters and an active multiplier/modulator. The first two levels of the tiered testing approach use model parameters and the third tier estimates the spectral content of the circuits' output. Incorporating this method into an analog/mixed-signal built-in-self-test (BIST) environment is also discussed, including hardware overhead and general implementation. Our 3-tiered ARMA testing method was highly effective, had 0% yield loss, and achieved low defect level of 0.0821514 on a\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1724497\n",
      "index                                        55323d0e45cec66b6f9dd6b6\n",
      "title               Having your cake and eating it too: jointly op...\n",
      "authors             K. V. Rashmi, Preetum Nakkiran, Jingyan Wang, ...\n",
      "year                                                           2015.0\n",
      "venue               FAST'15 Proceedings of the 13th USENIX Confere...\n",
      "references          558d49110cf2ffbd8570ba9f;558af35f612c41e6b9d3e...\n",
      "abstract            Erasure codes, such as Reed-Solomon (RS) codes...\n",
      "id                                                            1724497\n",
      "clustered_labels                                                    3\n",
      "Name: 1724497, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558af35f612c41e6b9d3e577   score: 0.9662198   abstract: Erasure codes such as Reed-Solomon (RS) codes are being extensively deployed in data centers since they offer significantly higher reliability than data replication methods at much lower storage overheads. These codes however mandate much higher resources with respect to network bandwidth and disk IO during reconstruction of data that is missing or otherwise unavailable. Existing solutions to this problem either demand additional storage space or severely limit the choice of the system parameters. In this paper, we present \\\"Hitchhiker\\\", a new erasure-coded storage system that reduces both network traffic and disk IO by around 25% to 45% during reconstruction of missing or otherwise unavailable data, with no additional storage, the same fault tolerance, and arbitrary flexibility in the choice of parameters, as compared to RS-based systems. Hitchhiker 'rides' on top of RS codes, and is b\n",
      "\n",
      "2. id: 558b0fd6612c41e6b9d41dac   score: 0.94176143   abstract: Erasure codes are applied in distributed storage systems for fault-tolerance with lower storage overhead than replications. Later, decentralized erasure codes are proposed for decentralized or loosely-organized storage systems. Repair mechanisms aim at maintaining redundancy over time such that stored data are still retrievable. Two recent repair mechanisms, Noop and Coop, are designed for decentralized erasure code based distributed storage system to minimize connection cost in theoretical manner. We propose a generalized repair framework, which includes Noop and Coop as two extreme cases. We then investigate trade-off between connection cost and data retrievability from an experimental aspect in our repair framework. Our results show that a reasonable data retrievability is achievable with constant connection cost, which is less than previously analytical values. These results are valu\n",
      "\n",
      "3. id: 5390b04120f70186a0ed6cbf   score: 0.94154674   abstract: We address the problem of minimizing the I/O needed to recover from disk failures in erasure-coded storage systems. The principal result is an algorithm that finds the optimal I/O recovery from an arbitrary number of disk failures for any XOR-based erasure code. We also describe a family of codes with high-fault tolerance and low recovery I/O, e.g. one instance tolerates up to 11 failures and recovers a lost block in 4 I/Os. While we have determined I/O optimal recovery for any given code, it remains an open problem to identify codes with the best recovery properties. We describe our ongoing efforts toward characterizing space overhead versus recovery I/O tradeoffs and generating codes that realize these bounds.\n",
      "\n",
      "4. id: 55323d0e45cec66b6f9dd6bd   score: 0.92563426   abstract: Distributed storage systems are increasingly transitioning to the use of erasure codes since they offer higher reliability at significantly lower storage costs than data replication. However, these codes tradeoff recovery performance as they require multiple disk reads and network transfers for reconstructing an unavailable data block. As a result, most existing systems use an erasure code either optimized for storage overhead or recovery performance. In this paper, we present HACFS, a new erasure-coded storage system that instead uses two different erasure codes and dynamically adapts to workload changes. It uses a fast code to optimize for recovery performance and a compact code to reduce the storage overhead. A novel conversion mechanism is used to efficiently upcode and downcode data blocks between fast and compact codes. We show that HACFS design techniques are generic and successfu\n",
      "\n",
      "5. id: 5390ad8920f70186a0ebf256   score: 0.87652874   abstract: In a storage system where individual storage nodes are prone to failure, the redundant storage of data in a distributed manner across multiple nodes is a must to ensure reliability. Reed-Solomon codes possess the reconstruction property under which the stored data can be recovered by connecting to any k of the n nodes in the network across which data is dispersed. This property can be shown to lead to vastly improved network reliability over simple replication schemes. Also of interest in such storage systems is the minimization of the repair bandwidth, i.e., the amount of data needed to be downloaded from the network in order to repair a single failed node. Reed-Solomon codes perform poorly here as they require the entire data to be downloaded. Regenerating codes are a new class of codes which minimize the repair bandwidth while retaining the reconstruction property. This paper provides\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3d32d   score: 0.8736466   abstract: Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2× on the repair disk I/O an\n",
      "\n",
      "7. id: 5390b52620f70186a0f03694   score: 0.8692713   abstract: To reduce storage overhead, cloud file systems are transitioning from replication to erasure codes. This process has revealed new dimensions on which to evaluate the performance of different coding schemes: the amount of data used in recovery and when performing degraded reads. We present an algorithm that finds the optimal number of codeword symbols needed for recovery for any XOR-based erasure code and produces recovery schedules that use a minimum amount of data. We differentiate popular erasure codes based on this criterion and demonstrate that the differences improve I/O performance in practice for the large block sizes used in cloud file systems. Several cloud systems [15, 10] have adopted Reed-Solomon (RS) codes, because of their generality and their ability to tolerate larger numbers of failures. We define a new class of rotated Reed-Solomon codes that perform degraded reads more\n",
      "\n",
      "8. id: 5390bae520f70186a0f3b7ea   score: 0.85752964   abstract: Erasure coding is a storage-efficient alternative to replication for achieving reliable data backup in distributed storage systems. During the storage process, traditional erasure codes require a unique source node to create and upload all the redundant data to the different storage nodes. However, such a source node may have limited communication and computation capabilities, which constrain the storage process throughput. Moreover, the source node and the different storage nodes might not be able to send and receive data simultaneously-e.g., nodes might be busy in a data center setting, or simply be offline in a peer-to-peer setting-which can further threaten the efficacy of the overall storage process. In this paper, we propose an ''in-network'' redundancy generation process which distributes the data insertion load among the source and storage nodes by allowing the storage nodes to g\n",
      "\n",
      "9. id: 5390bae520f70186a0f3af18   score: 0.8333246   abstract: The explosion of the amount of data stored in cloud systems calls for more efficient paradigms for redundancy. While replication is widely used to ensure data availability, erasure correcting codes provide a much better trade-off between storage and availability. Regenerating codes are good candidates for they also offer low repair costs in term of network bandwidth. While they have been proven optimal, they are difficult to understand and parameterize. In this paper we provide an analysis of regenerating codes for practitioners to grasp the various trade-offs. More specifically we make two contributions: (i) we study the impact of the parameters by conducting an analysis at the level of the system, rather than at the level of a single device, (ii) we compare the computational costs of various implementations of codes and highlight the most efficient ones. Our goal is to provide system d\n",
      "\n",
      "10. id: 5390b7fe20f70186a0f2602a   score: 0.8272706   abstract: Nowadays, erasure codes have been widely used in data storage to achieve high fault-tolerance. However, compared with replica-based storage, erasure-coded system may suffer significant performance overhead in encoding, decoding and updating. Traditional updating schemes(e.g. DUM and PUM) use an individual manager node to accomplish the updating. In this paper, we propose two partial-updating schemes (i.e. PUM-P and PDN-P) to improve the small update in erasure coded storage clusters, where both schemes only read a portion of data, including the data blocks to be updated and the parity blocks, and utilize the calculation capacity of the storage nodes. We implement four updating algorithms (DUM, PUM, PUM-P and PDN-P) upon an erasure-coded storage cluster platform, and conduct a set of comparative tests under two real-world workloads with different fault-tolerance parameters. The experiment\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1688511\n",
      "index                                        55916fe70cf2e89307ca9cca\n",
      "title               TLS Record Protocol: Security Analysis and Def...\n",
      "authors              Olivier Levillain, Baptiste Gourdin, Hervé Debar\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 10th ACM Symposium on Infor...\n",
      "references          5390bda020f70186a0f47d3a;53908adf20f70186a0dac...\n",
      "abstract            TLS and its main application HTTPS are an esse...\n",
      "id                                                            1688511\n",
      "clustered_labels                                                    3\n",
      "Name: 1688511, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bda020f70186a0f47d3a   score: 0.79101497   abstract: The Transport Layer Security (TLS) protocol aims to provide confidentiality and integrity of data in transit across untrusted networks. TLS has become the de facto protocol standard for secured Internet and mobile applications. TLS supports several symmetric encryption options, including a scheme based on the RC4 stream cipher. In this paper, we present ciphertext-only plaintext recovery attacks against TLS when RC4 is selected for encryption. Our attacks build on recent advances in the statistical analysis of RC4, and on new findings announced in this paper. Our results are supported by an experimental evaluation of the feasibility of the attacks. We also discuss countermeasures.\n",
      "\n",
      "2. id: 5390b8d720f70186a0f2c374   score: 0.6361271   abstract: In 2009 Moxie Marlinspike proposed a new Man-in-the- Middle (MitM) attack on secure socket layer (SSL) called SSLStrip attack at Black Hat DC, which is a serious threat to Web users. Some solutions have been proposed in literature. However, until now there is no practical countermeasure to resist on such attack. In this paper, we propose a new scheme to defend against SSLStrip attack by improving the previous secure cookie protocols and using proxy pattern and reverse proxy pattern. It implements a secure LAN guaranteed proxy in client-side, a secure server guaranteed proxy in server-side and a cookie authentication mechanism to provide the following security services: source authentication, integrity control and defending SSLStrip attack.\n",
      "\n",
      "3. id: 5390a1f820f70186a0e5dd81   score: 0.60929173   abstract: Based on recently proposed attack scenarios, we show that SAML assertions and SAML artifacts are still vulnerable to real-world attacks on browser-based implementations. We propose two different bindings of SAML assertions and SAML artifacts to the TLS security layer and show that these bindings protect against all known attacks. The two bindings are based on TLS client certificates, and on a variant of the well-known Same Origin Policy of browsers.\n",
      "\n",
      "4. id: 558b94d4612c6b62e5e8c0c3   score: 0.5050047   abstract: TLS was designed as a transparent channel abstraction to allow developers with no cryptographic expertise to protect their application against attackers that may control some clients, some servers, and may have the capability to tamper with network connections. However, the security guarantees of TLS fall short of those of a secure channel, leading to a variety of attacks. We show how some widespread false beliefs about these guarantees can be exploited to attack popular applications and defeat several standard authentication methods that rely too naively on TLS. We present new client impersonation attacks against TLS renegotiations, wireless networks, challenge-response protocols, and channel-bound cookies. Our attacks exploit combinations of RSA and Diffie-Hellman key exchange, session resumption, and renegotiation to bypass many recent countermeasures. We also demonstrate new ways to \n",
      "\n",
      "5. id: 5390bb7b20f70186a0f3f42f   score: 0.45471433   abstract: The Transport Layer Security (TLS) protocol aims to provide confidentiality and integrity of data in transit across untrusted networks. TLS has become the de facto secure protocol of choice for Internet and mobile applications. DTLS is a variant of TLS that is growing in importance. In this paper, we present distinguishing and plaintext recovery attacks against TLS and DTLS. The attacks are based on a delicate timing analysis of decryption processing in the two protocols. We include experimental results demonstrating the feasibility of the attacks in realistic network environments for several different implementations of TLS and DTLS, including the leading OpenSSL implementations. We provide countermeasures for the attacks. Finally, we discuss the wider implications of our attacks for the cryptographic design used by TLS and DTLS.\n",
      "\n",
      "6. id: 558b8074612c6b62e5e8a195   score: 0.43974733   abstract: In this paper we consider TLS Man-In-The-Middle (MITM) attacks in the context of web applications, where the attacker is able to successfully impersonate the legitimate server to the user, with the goal of impersonating the user to the server and thus compromising the user's online account and data. We describe in detail why the recently proposed client authentication protocols based on TLS Channel IDs, as well as client web authentication in general, cannot fully prevent such attacks. Nevertheless, we show that strong client authentication, such as Channel ID-based authentication, can be combined with the concept of server invariance, a weaker and easier to achieve property than server authentication, in order to protect against the considered attacks. We specifically leverage Channel ID-based authentication in combination with server invariance to create a novel mechanism that we call \n",
      "\n",
      "7. id: 5390b63320f70186a0f17910   score: 0.40321434   abstract: We consider the setting of HTTP traffic over encrypted tunnels, as used to conceal the identity of websites visited by a user. It is well known that traffic analysis (TA) attacks can accurately identify the website a user visits despite the use of encryption, and previous work has looked at specific attack/countermeasure pairings. We provide the first comprehensive analysis of general-purpose TA countermeasures. We show that nine known countermeasures are vulnerable to simple attacks that exploit coarse features of traffic (e.g., total time and bandwidth). The considered countermeasures include ones like those standardized by TLS, SSH, and IPsec, and even more complex ones like the traffic morphing scheme of Wright et al. As just one of our results, we show that despite the use of traffic morphing, one can use only total upstream and downstream bandwidth to identify--with 98% accuracy --\n",
      "\n",
      "8. id: 5390b4c320f70186a0efdd87   score: 0.3678378   abstract: We analyze the security of the TLS Record Protocol, a MAC-then-Encode-then-Encrypt (MEE) scheme whose design targets confidentiality and integrity for application layer communications on the Internet. Our main results are twofold. First, we give a new distinguishing attack against TLS when variable length padding and short (truncated) MACs are used. This combination will arise when standardized TLS 1.2 extensions (RFC 6066) are implemented. Second, we show that when tags are longer, the TLS Record Protocol meets a new length-hiding authenticated encryption security notion that is stronger than IND-CCA.\n",
      "\n",
      "9. id: 53908b9320f70186a0dc0292   score: 0.35835305   abstract: Many security protocols have appeared in the literature, with aims such as agreeing upon a cryptographic key, or achieving authentication. However, many of these have been shown to be flawed. In this paper we present a number of new attacks upon security protocols, and discuss ways in which we may avoid designing incorrect protocols in the future.\n",
      "\n",
      "10. id: 53909eef20f70186a0e366d8   score: 0.3002324   abstract: The Internet Engineering Task Force (IETF) is in the proces of adopting standards for IP-layer encryption and authentication (IPSEC). We describe a number of attacks against various versions of these protocols, including confidentiality failures and authentication failures. The implications of these attacks are troubling for the utility of this entire effort.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691710\n",
      "index                                        559256510cf2aff368683c65\n",
      "title               High Frame Rate 3-D Ultrasound Imaging Using S...\n",
      "authors             Ming Yang, Richard Sampson, Siyuan Wei, Thomas...\n",
      "year                                                           2015.0\n",
      "venue                            Journal of Signal Processing Systems\n",
      "references                                   5390bb1d20f70186a0f3e83e\n",
      "abstract            Recently, there has been great interest in 3-D...\n",
      "id                                                            1691710\n",
      "clustered_labels                                                    1\n",
      "Name: 1691710, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a40520f70186a0e6f817   score: 0.8494724   abstract: In conventional ultrasound imaging systems with phased arrays, the improvement of lateral resolution of images requires enlarging of the number of array elements that in turn increases both, the complexity and the cost, of imaging systems. Multi-element synthetic aperture focusing (MSAF) systems are a very good alternative to conventional systems with phased arrays. The benefit of the synthetic aperture is in reduction of the system complexity, cost, and acquisition time.A general technique for parameter optimization of an MSAF system is described and evaluated in this paper. The locations of all \"transmit-receive\" subaperture centers over a virtual linear array are optimized using the simulated annealing algorithm. The optimization criterion is expressed in terms of the beam characteristics -- beam width and side lobe level.The comparison analysis between an optimized MSAF system and an\n",
      "\n",
      "2. id: 5390a37f20f70186a0e6cb54   score: 0.8337311   abstract: The quantization error of phase delay in an ultrasonic annular arrays imaging system is analyzed which impairs image resolution, and proper sampling rate is considered to reduce system complexity.\n",
      "\n",
      "3. id: 5390b64020f70186a0f197b2   score: 0.7916601   abstract: High resolution ultrasonic imaging may be achieved via the synthetic aperture focusing technique, provided that the synthetic antenna is correctly spatially sampled. We show that spatial undersampling is possible, using a digital spotlighting technique that permits to recover the correct reconstructed images from aliased data. The proposed method is applied to experimental ultrasonic data\n",
      "\n",
      "4. id: 5390bb1d20f70186a0f3e83e   score: 0.77424216   abstract: Three-dimensional (3D) ultrasound is becoming common for non-invasive medical imaging because of its high accuracy, safety, and ease of use. Unlike other modalities, ultrasound transducers require little power, which makes hand-held imaging platforms possible, and several low-resolution 2D devices are commercially available today. However, the extreme computational requirements (and associated power requirements) of 3D ultrasound image formation has, to date, precluded hand-held 3D capable devices. We describe the Sonic Millip3De, a new system architecture and accelerator for 3D ultrasound beamformation—the most computationally intensive aspect of image formation. Our three-layer die-stacked design features a custom beamsum accelerator that employs massive data parallelism and a streaming transform-select-reduce pipeline architecture enabled by our new iterative beamsum delay calculation\n",
      "\n",
      "5. id: 5390a01420f70186a0e46638   score: 0.6423203   abstract: The high frame rate (HFR) method, derived from the limited diffraction beams [Jian-yu Lu. 2D and 3D high frame rate imaging with limited diffraction beams. IEEE Trans Ultrason Ferroelectrics Freq Control 1997;44(4):839-56; Jian-yu Lu. Experimental study of high frame rate imaging with limited diffraction beams. IEEE Trans Ultrason Ferroelectrics Freq Control 1998;45(1):84-97; Hu Peng, Jianyu Lu, XueMei Han. High frame rate ultrasonic imaging system based on the angular spectrum principle. Ultrason 2006;44(Suppl):e97-9] theory, has a drawback that its imaging area, as wide as its array beam transmission field, can generally not be wider than the transducer is long; and that the only way to widen the area is to steer the transmission beams several times from different angles, which lowers the frame rate. Besides, the array beam field is quite demanding to produce. To widen the imaged area,\n",
      "\n",
      "6. id: 5390b24420f70186a0ee7cef   score: 0.62016165   abstract: The use of adaptive beamforming is a viable solution to provide high-resolution real-time medical ultrasound imaging. However, the increase in image resolution comes at an expense of a significant increase in compute requirement over conventional algorithms. In a bedside diagnosis setting where plug-in power is available, GPUs are promising accelerators to address the processing demand. However, in the case of point-of-care diagnostics where portable ultrasound imaging devices must be used, alternative power-efficient computer systems must be employed, possibly at the expense of lower image resolution in order to maintain real-time performance. This paper presents an initial design space exploration on viable compute architectures that might address the drastically different requirements between bedside and portable medical ultrasound imaging systems using adaptive beamforming. The desig\n",
      "\n",
      "7. id: 5390ad5620f70186a0ebed22   score: 0.61236763   abstract: Application-specific ICs have been traditionally used to support the high computational and data rate requirements in medical ultrasound systems, particularly in receive beamforming. Utilizing the previously developed efficient front-end algorithms, in this paper, we present a simple programmable computing architecture, consisting of a field-programmable gate array (FPGA) and a digital signal processor (DSP), to support core ultrasound signal processing. It was found that 97.3% and 51.8% of the FPGA and DSP resources are, respectively, needed to support all the front-end and back-end processing for B-mode imaging with 64 channels and 120 scanlines per frame at 30 frames/s. These results indicate that this programmable architecture can meet the requirements of low-and medium-level ultrasound machines while providing a flexible platform for supporting the development and deployment of new \n",
      "\n",
      "8. id: 5390aaf920f70186a0eaeb28   score: 0.5875882   abstract: Synthetic aperture (SA) techniques have been frequently used to reduce the volume and complexity of the imaging systems. A useful tool for designing synthetic aperture configurations is the coarray. This is the virtual aperture that produces in one way the same beam pattern as the SA system in emission and reception. In this correspondence, we propose a new algorithm, based on the polynomial decomposition, that allows to obtain any wanted coarray on a linear array using whatever synthetic aperture configuration. With this fast and simple algorithm, the desired coarray is decomposed into a set of sub-apertures, whose length is determined by the requirements and resources of the system. The result is the set of weights that have to be applied on the sub-apertures to get the desired coarray, and consequently, the wanted beam pattern.\n",
      "\n",
      "9. id: 5390a8b120f70186a0e9b1e3   score: 0.5723643   abstract: High frame rate(HFR) medical imaging system was developed by Lu Jian-yu in 1997. One of the important steps of this system is weighting with exact limited-diffraction array beams. Recently, Lu Jian-yu developed a new imaging method with square-wave aperture weightings instead of sine and cosine aperture weightings. Experimental results show that almost the same quality of images can be obtained with the square-wave aperture weighting method and with the exact limited-diffraction array beam weightings. But the method with square-wave aperture weighting is more simple, and this method costs less time than the method with exact limited-diffraction array beam weightings. In this paper, a fast square-wave transform is introduced to the HFR imaging method. This fast square-wave transform can be used to deal with the data when the number of data samples is prime. This method can reduce about ha\n",
      "\n",
      "10. id: 53908bcc20f70186a0dc65dc   score: 0.5613351   abstract: In many implementations of digital delay and sum beamforming, a sample rate much higher than the Nyquist rate is used. This allows for many synchronous beamsteering directions. Severe demands are made upon the analogue to digital converters however. Several methods have been proposed for reducing the sample rate required. These methods incorporate the delays that are needed for beamforming in the time domain, or in the frequency domain. A more efficient method for implementing a time-domain delay and sum (interpolation) beamformer using polyphase decomposition is presented. This method results in significant computational savings when the desired angular resolution is high compared to the number of sensors used and the number of simultaneously formed beams.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1731150\n",
      "index                                        5534d8bb45cedae85c3795fc\n",
      "title               The federated scheduling of constrained-deadli...\n",
      "authors                                                 Sanjoy Baruah\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references          558b5577612c41e6b9d497ec;558b94e1612c6b62e5e8c0d7\n",
      "abstract            In the federated approach to multiprocessor sc...\n",
      "id                                                            1731150\n",
      "clustered_labels                                                    0\n",
      "Name: 1731150, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ab8820f70186a0eb0e81   score: 0.9079672   abstract: The problem of scheduling a set of tasks on a multiprocessor architecture is addressed. Tasks are assumed to be sporadic with arbitrary deadlines and may migrate between processors. The execution of migrating tasks is controlled by a bandwidth reservation scheme so that schedulability is guaranteed by EDF. Task migration costs are taken into consideration. Results from experiments indicate that the proposed approach performs well in terms of schedulability.\n",
      "\n",
      "2. id: 53909e8b20f70186a0e2f158   score: 0.9069832   abstract: The priority-driven scheduling of periodic and sporadic task systems upon identical multiprocessor platforms is considered, under the restrictions that (i) each job may be assigned exactly one priority throughout its lifetime, and (ii) each job may execute upon only a single processor. It is shown that feasibility-analysis under these restrictions is intractable (NP-hard in the strong sense). A scheduling algorithm is presented that satisfies these restrictions, and that has a worst-case utilization bound comparable to the worst-case utilization bounds of partitioned scheduling algorithms, and of scheduling algorithms that retain the priority-assignment restriction but allow arbitrary interprocessor migration.\n",
      "\n",
      "3. id: 53908f5b20f70186a0dd9352   score: 0.8264319   abstract: In this paper we examine the problem of scheduling a set of tasks on a system with a number of identical processors. Several timing anomalies are known to exist for the general case, in which the execution time can increase when inter-task constraints are removed or processors are added. It is shown that these anomalies also exist when tasks are restricted to be of equal (unit) length. Several, increasingly restrictive, heuristic scheduling algorithms are reviewed. The \"added processor\" anomaly is shown to persist through all of them, though in successively weaker form.\n",
      "\n",
      "4. id: 5390b4c320f70186a0efdbbe   score: 0.7970834   abstract: Prior work has provided bounds on the deadline tardiness that a set of sporadic real-time tasks may incur when scheduled using the global earliest-deadline-first (G-EDF) scheduling algorithm. Under the sporadic task model, it is necessary that no individual task overutilize a single processor and that the set of all tasks does not overutilize the set of all processors. In this work we generalize the task model by allowing jobs within a single task to run concurrently. In doing so we remove the requirement that no task overutilize a single processor. We also provide tardiness bounds that are better than those available with the standard sporadic task model.\n",
      "\n",
      "5. id: 539098b820f70186a0e0a09a   score: 0.746139   abstract: In the sporadic task model, a task is characterized by three parameters 驴 an execution requirement, a relative deadline, and a period parameter 驴 and has the interpretation that it generates an infinite sequence of jobs, such that (i) the arrival-times of any two successive jobs are separated by a time-interval at least as long as the period parameter; (ii) each job has a deadline that is separated from its arrival-time by a time-interval exactly equal to the relative deadline parameter of the task; and (iii) each job must execute for an amount equal to its execution requirement by its deadline. Most previous research concerning the scheduling of collections of sporadic tasks upon multiprocessor platforms has added the additional constraint that all tasks have their relative deadline parameters equal to their period parameters. In this research, we consider the scheduling of systems of s\n",
      "\n",
      "6. id: 539087f820f70186a0d7317b   score: 0.7200731   abstract: A model for multiprocessor control is considered in which jobs are broken into various pieces, called tasks. Tasks are executed by single processing units. In this paper the structure controlling the assignment of tasks to processors is the task list, which orders all tasks according to servicing priority. When a processors becomes free, it simply picks up the highest priority task that is executable at that moment.The job and its component tasks are imagined to be interacting with an environment consisting of a set of rigid timing constraints. Such constraints are of two types, called start-times and deadlines. The interaction is specified by requiring that certain distinguished tasks conform directly to one or more of these constraints. Tasks conforming to a start-time cannot begin until the start-time has passed, and tasks conforming to a deadline cannot proceed beyond the deadline. I\n",
      "\n",
      "7. id: 5390b44620f70186a0ef9a3d   score: 0.70232594   abstract: We consider the partitioned scheduling of sporadic, hard-real-time tasks on a multiprocessor platform with static-priority scheduling policies. Most previous work on the static-priority scheduling of sporadic tasks upon multiprocessors has assumed implicit deadlines (i.e. a task's relative deadline is equal to its period). We relax the equality constraint on a task's deadline and consider task systems with constrained deadlines (i.e. relative deadlines are at most periods). In particular, we consider the first-fit decreasing partitioning algorithm. Since the partitioning problem is easily seen to be NP-hard in the strong sense, this algorithm is unlikely to be optimal. We quantitatively characterize the partitioning algorithm's worst-case performance in terms of resource augmentation.\n",
      "\n",
      "8. id: 5390a1f720f70186a0e5ba1b   score: 0.6814089   abstract: Recent results on the global multiprocessor EDFscheduling of sporadic task systems are, for the mostpart, applicable only to task systems in which eachtask’s relative deadline parameter is constrained to beno larger than its period. This paper introduces newanalysis techniques that allow for similar results to bederived for task systems in which individual tasks arenot constrained in this manner.\n",
      "\n",
      "9. id: 5390a63c20f70186a0e81f33   score: 0.68056023   abstract: Recent results on the global multiprocessor edf scheduling of sporadic task systems are, for the most part, applicable only to task systems in which each task's relative deadline parameter is constrained to be no larger than its minimum inter-arrival separation. This paper introduces new analysis techniques that allow for similar results to be derived for task systems in which individual tasks are not constrained in this manner. For tasks with deadlines greater than their minimum inter-arrival separation, two models are considered, with and without an implicit intra-task job precedence constraint. The new analyses yield schedulability conditions that strictly dominate some previously proposed tests that are generally accepted to represent the current state of the art in multiprocessor edf schedulability analysis, and permits the derivation of an improved speed-up bound.\n",
      "\n",
      "10. id: 5390bfa220f70186a0f536ae   score: 0.6738359   abstract: The scheduling of mixed-criticality implicit-deadline sporadic task systems on identical multiprocessor platforms is considered. Two approaches, one for global and another for partitioned scheduling, are described. Theoretical analyses and simulation experiments are used to compare the global and partitioned scheduling approaches.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1669588\n",
      "index                                        559256d30cf2aff368683cc8\n",
      "title               Factors impacting the adoption of social netwo...\n",
      "authors             Wencui Han, Serkan Ada, Raj Sharman, Robin Hat...\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Business Information ...\n",
      "references          53908b4920f70186a0dbb36f;539098b820f70186a0e0b...\n",
      "abstract            The increasing number of campuses incidents ha...\n",
      "id                                                            1669588\n",
      "clustered_labels                                                    1\n",
      "Name: 1669588, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5537da0e0cf23ee1cc767bf7   score: 0.9152046   abstract: This study explores the main determinants of social network adoption at the country level. We use the technology-organization-environment (TOE) framework to investigate factors influencing social network adoption. We utilize cross-sectional data from 130 countries. Results indicate that social network adoption, at the country level, is positively influenced by the technological maturity, public readiness, and ICT-laws sophistication. Technological, organizational, and environmental factors account for 67% of variance in social network adoption. These findings provide initial insight into the usage of social network sites at the country level, as well as the main factors that influence public adoption. Implications for research and practice are discussed.\n",
      "\n",
      "2. id: 5390a40520f70186a0e6f5d2   score: 0.7749242   abstract: The purpose of this study is to understand the laggard adoption of an SMS-based emergency alert system on a university campus. Based on findings from in-depth interviews and a focus group, we discuss some critical issues in designing and implementing such alert systems, with a focus on the socio-cultural factors that de-motivate people to use them. Our findings show that, even for a system with simple technology, the adoption process involves complex interactions between individual perceptions and the social context in which the system is situated.\n",
      "\n",
      "3. id: 5390a30b20f70186a0e6a76f   score: 0.5991334   abstract: This paper presents an activity based model for the adoption of technology in emergency management. Furthermore, multiple case study has been employed as a research method to validate the proposed conceptual model. The empirical findings from multiple case studies are also reported in this paper. It is hoped that our research findings will better inform researchers in the field and facilitate organizations in adopting technology for emergency management.\n",
      "\n",
      "4. id: 55323d8745cec66b6f9dea26   score: 0.5197651   abstract: Social networks are virtual communication sites that allow its participants to connect, building relationships, and collaborate on social issues. It became part of our lives and spread rapidly among youth. Young people join these sites to keep strong relationships with friends and to make new ones. Therefore, it is important to investigate the factors that influence the intention to use social networking sites SNSs to gain better position in the social reform among young people. This study developed an integrated theoretical model which has five major factors that predict the intention to use SNSs. An empirical test was conducted, where a sample of 302 university students and an instrument containing 27 items was used. The results provide consistent evidence that all hypothesized positive associations exist except for the isolation variable. After taking into account different demographi\n",
      "\n",
      "5. id: 558b3d38612c41e6b9d47590   score: 0.48009777   abstract: The social networking sites are related to a kind of virtual communication that allows its participants to connect with each other, and building relations among people who have the same interests and activities. It has been become part of our life and spread rapidly among the young; biggest motivation to the Young people to join these sites is to save strong relationships with friends and to make a strong relation with new acquaintances. Therefore, it is important to investigate the factors that influence the intention to use social networking sites in social reform among the young. This study developed a theoretical model with five major predictors of the intention to use SNSs. An empirical test was conducted; a sample of 302 university students and an instrument of 27 items was used in the study. Results provide consistent evidence that all positive associations exist except the isolat\n",
      "\n",
      "6. id: 5390b78a20f70186a0f24ed7   score: 0.39899173   abstract: The use of social media for communication and interaction is becoming more and more frequent, which is also the case during crises. To monitor social media may therefore be a useful capability from a crisis management perspective, both for detecting new or emergent crises, as well as for getting a better situation awareness of how people react to a particular crisis. The work presented in this paper is part of the EU research project Alert4All, having the overall goal of improving the effectiveness of alert and communication toward the population in crises.\n",
      "\n",
      "7. id: 5390a40520f70186a0e6f157   score: 0.37972334   abstract: University campus communities face a variety of hazards, from natural and technological disasters to terrorism and violence. In response to recent events, many campuses within the United States have begun to implement emergency notification systems utilizing email, text, and telephone-based messaging. These alerts are designed to reach members of the campus community, including faculty, staff, and students and most rely upon an opt-in model for participation. The present design of both the registration process and the notification messages raise several concerns as to the effectiveness of notification systems. This research presents findings from an examination of emergency notification systems on one campus and discusses approaches to improve such systems for all members of the campus population.\n",
      "\n",
      "8. id: 5390b2fc20f70186a0eef6f4   score: 0.37949336   abstract: Our understanding of the impacts of social media on individuals who receive warnings of extreme events is limited. There is to date no uniform approach to integrating social media as part of emergency management strategies. This research addresses the question of the role of social media in the effectiveness of the warning response process in the context of a naturally occurring experiment. The results of the experiment contribute to our understanding of how social media complements as well as facilitates the warning response process.\n",
      "\n",
      "9. id: 5390adfd20f70186a0ec614a   score: 0.21157968   abstract: The scale and frequency of large-scale disasters and the wide range of severities and the myriad ways a nation has been affected by, reveals the importance of a reliable communication system in emergency response scenarios. Communication may fail for a broken network component, infrastructure failure, or unreachability. A variety of communication technologies have been deployed at crisis sites but the problems of interoperability, unreachability, unclear communication plan and resource allocation still exist.Communication in emergency response applications has unique demands for the minimum or no a priori knowledge, unpredictability, and short or no advance warning. Through participation in several real-life scenario exercises, analysis of network data, examination of after-incident reports, and interviews with first responders, this dissertation investigates the complex communication pr\n",
      "\n",
      "10. id: 5390b19020f70186a0ede70d   score: 0.17412923   abstract: This poster presents one of our efforts in the context of the Crisis, Tragedy, and Recovery Network (CTRnet) project. One topic studied in this project is the use of social media by government to respond to emergency events in towns and counties. Monitoring social media information for unusual behavior can help identify these events once we can characterize their patterns. As an example, we analyzed the campus shooting in the University of Texas, Austin, on September 28, 2010. In order to study the pattern of communication and the information communicated using social media on that day, we collected publicly available data from Twitter. Collected tweets were analyzed and visualized using the Natural Language Toolkit, word clouds, and graphs. They showed how news and posts related to this event swamped the discussions of other issues.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691691\n",
      "index                                        55914b980cf232eb904fba35\n",
      "title                         On Resilient System Performance Binning\n",
      "authors               Qiang Han, Jianghao Guo, Qiang Xu, Wen-Ben Jone\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Symposium on Internati...\n",
      "references          53908bde20f70186a0dc91fa;5390990f20f70186a0e10...\n",
      "abstract            By allowing timing errors to occur and recover...\n",
      "id                                                            1691691\n",
      "clustered_labels                                                    3\n",
      "Name: 1691691, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539095ba20f70186a0df1bc4   score: 0.60504085   abstract: A novel technique for improving the energy efficiency of microprocessors is disclosed. This new method relies on a fault-tolerance mechanism for timing violations, based on a speculative execution technique. Since power reduces quadratically with supply voltage, supply voltage reductions can result in substantial power savings. However, these reductions also cause a longer gate delay, and so the clock frequency must be reduced so that timing constraints of critical paths are not violated. If any fault-tolerance mechanism is provided for timing faults, it is not necessary to maintain the constraints. From these observations, we propose a fault-tolerance technique for timing violations, that efficiently utilizes the speculative execution mechanism and reduces power consumption. We call the technique constructive timing violation. The present study evaluated our proposal regarding this tech\n",
      "\n",
      "2. id: 5390bed320f70186a0f4d76c   score: 0.563378   abstract: With technology scaling, integrated circuits suffer from increasingly severe static and dynamic variations, which often manifest themselves as infrequent timing errors on circuit speed paths, if a large timing guard-band is not reserved. This paper presents a new forward timing error correction scheme, namely ForTER, which predicts whether the occurrence of timing errors would propagate to the next level of sequential elements and corrects them without necessarily borrowing timing slack. The proposed technique can be combined with other timing error resilient circuit design techniques to further improve circuit performance, as demonstrated in our experimental results with various benchmark circuits.\n",
      "\n",
      "3. id: 5390b3ae20f70186a0ef2f7a   score: 0.4411914   abstract: The timing performance and yield of integrated circuits can be improved by carefully assigning intentional clock skews to flipflops. Due to the ever-increasing process, voltage, and temperature variations with technology scaling, however, traditional clock skew optimization solutions that work in a conservative manner to guarantee \"always correct\" computation cannot perform as well as expected. By allowing infrequent timing errors and recovering from them with minor performance impact, the concept of timing speculation has attracted lots of research attention since it enables \"better than worst-case design\". In this work, we propose a novel online clock skew tuning technique for circuits equipped with timing speculation capability. By observing the occurrence of timing errors at runtime and tuning clock skews accordingly, the proposed technique is able to achieve much better timing perfo\n",
      "\n",
      "4. id: 5390ab8820f70186a0eb0204   score: 0.3143662   abstract: Yield loss due to timing failures results in diminished returns for field-programmable gate arrays (FPGAs), and is aggravated under increased process variations in scaled technologies. The uncertainty in the critical delay of a circuit under process variations exists because the delay of each logic element in the circuit is no longer deterministic. Traditionally, FPGAs have been designed to manage process variations through speed binning, which works well for inter-die variations, but not for intra-die variations resulting in reduced timing yield for FPGAs. FPGAs present a unique challenge because of their programmability and unknown end user application. In this paper, a novel architecture and computer-aided design co-design technique is proposed to improve the timing yield. Experimental results indicate that the use of proposed design technique can achieve timing yield improvement of u\n",
      "\n",
      "5. id: 53909eef20f70186a0e353c6   score: 0.30393836   abstract: This paper concerns the variation tolerance in signal processing integrated circuits. Motivated by the fact that variation-induced timing faults at different locations in signal processing circuits have different effects on the sig- nal processing performance, we developed an importance- aware clock skew scheduling technique, called soft clock skew scheduling, that can realize system-level tolerance to variation-induced timing faults. With state-parallel Viterbi decoders as test vehicles, we demonstrated its effectiveness on increasing the achievable clock frequency in presence of significant variation-induced timing faults, while maintain- ing good decoding performance.\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3d4d1   score: 0.22850065   abstract: Recovery based design (RBD) is a promising approach for the design of energy-efficient circuits under variations. RBD instruments circuits with mechanisms to identify and correct timing violations, thereby allowing reduced guard bands or design margins. In addition, RBD enables aggressive voltage overscaling to a point where timing errors occur even under nominal conditions. A major barrier to the widespread adoption of RBD is that traditional design practices and synthesis tools result in circuits with so-called\"path walls\", leading to an explosion in the number of timing errors beyond a certain critical operating voltage. To alleviate this effect, previous techniques focused on combinational circuit optimizations such as sizing, use of dual Vth cells, re-structuring, etc. to favorably reshape the path delay distribution. However, these techniques are limited by the inherent sequential \n",
      "\n",
      "7. id: 558b7b52612c6b62e5e8999b   score: 0.21766995   abstract: Modern IC designs are exposed to a wide range of dynamic variations. Traditionally, a conservative timing guardband is required to guarantee correct operations under the worst-case variation, thus leading to performance degradation. To remove the guardband, resilient circuits are proposed. However, the short-path padding (hold time fixing) problem in resilient circuits is far severer than conventional IC design. Therefore, in this paper, we focus on the short-path padding problem to enable the timing error detection and correction mechanism of resilient circuits. Unlike recent prior work adopts greedy heuristics with a local view, we determine the padding values and locations with a global view. Moreover, we utilize spare cells and a dummy metal to further achieve the derived padding values at physical implementation. Experimental results show that our method is promising to validate tim\n",
      "\n",
      "8. id: 5390ad0620f70186a0eba838   score: 0.20979328   abstract: Increasing dynamic variability with technology scaling has made it essential to incorporate large design-time timing margins to ensure yield and reliable operation. Online techniques for timing error resilience help recover timing margins, improving performance and/or power consumption. This paper presents TIMBER, a technique for online timing error resilience that masks timing errors by borrowing time from successive pipeline stages. TIMBER-based error masking can recover timing margins without instruction replay or roll-back support. Two sequential circuit elements --- TIMBER flip-flop and TIMBER latch --- that implement error masking based on time-borrowing are described. Both circuit elements are validated using corner-case circuit simulations, and the overhead and trade-offs of TIMBER-based error masking are evaluated on an industrial processor.\n",
      "\n",
      "9. id: 5390ac1820f70186a0eb38d8   score: 0.20866227   abstract: Conventional microprocessors require a clock frequency (FCLK) guardband to ensure correct functionality during infrequent dynamic operating variations in supply voltage (VCC), temperature, and transistor aging. Consequently, these inflexible designs cannot exploit opportunities for higher performance by increasing FCLK or lower energy by reducing VCC during favorable operating conditions. This presentation describes a 45nm resilient microprocessor with error-detection and recovery circuits to detect and correct timing errors from dynamic variations to mitigate the FCLK guardband, thus enabling higher performance or lower energy as compared to a conventional design. The microprocessor core supports two distinct error-detection designs and two separate error-recovery techniques, allowing a direct comparison of the relative trade-offs. Silicon measurements demonstrate that resilient circuit\n",
      "\n",
      "10. id: 5390ba3820f70186a0f35651   score: 0.20753574   abstract: Modern IC designs are exposed to a wide range of dynamic variations. Traditionally, a conservative timing guardband is required to guarantee correct operations under the worst-case variation, thus leading to performance degradation. To remove the guardband, resilient circuits are proposed. However, the short path padding (hold time fixing) problem in resilient circuits is severer than conventional IC design. Therefore, in this paper, we focus on the short path padding problem to enable the timing error detection and correction mechanism of resilient circuits. Unlike recent prior work adopts greedy heuristics with a local view, we determine the padding values and locations with a global view. Moreover, we propose coarse-grained and fine-grained padding allocation methods to further achieve the derived padding values at physical implementation. Experimental results show that our method is \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717214\n",
      "index                                        55323c7145cec66b6f9dc19c\n",
      "title               Dynamic partition lock method to reduce transa...\n",
      "authors                                      Taehwan Kim, Seokil Song\n",
      "year                                                           2015.0\n",
      "venue                                               Cluster Computing\n",
      "references          558b09da612c41e6b9d410f7;558b76ed612c6b62e5e89...\n",
      "abstract            Generally, existing cloud data management syst...\n",
      "id                                                            1717214\n",
      "clustered_labels                                                    2\n",
      "Name: 1717214, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390878720f70186a0d35ecf   score: 0.8987666   abstract: One of the issues in distributed databases is to maintain the data consistency when a database is replicated for higher availability. In a real-time database system, availability may be more important than consistency since a result must be produced before a deadline. We propose techniques to increase the availability in a partitioned real-time database. We also suggest that a transaction may execute even when the most up-to-date information is not available or when a serializable execution cannot be guaranteed. As long as data integrity is maintained, serializable execution may not be necessary.\n",
      "\n",
      "2. id: 5390b72e20f70186a0f20d98   score: 0.83999664   abstract: Data replication is a well-known strategy to achieve the availability, scalability and performance improvement goals in the data management world. However, the cost of maintaining several database replicas always strongly consistent is very high. The CAP theorem shows that a shared-data system can choose at most two out of three properties: consistency, availability, and tolerance to partitions. In practice, most of the cloud-based data management systems tend to overcome the difficulties of distributed replication by relaxing the consistency guarantees of the system. In particular, they implement various forms of weaker consistency models such as eventual consistency. This solution is accepted by many new Web 2.0 applications (e.g. social networks) which could be more tolerant with a wider window of data staleness (replication delay).However, unfortunately, there are no generic applicat\n",
      "\n",
      "3. id: 5390ada620f70186a0ec3a68   score: 0.79645085   abstract: We propose a consistency model for a data store in the Cloud and develop a framework towards the goal of deploying Database as a Service over the Cloud. This includes consistency across the data partitions and consistency of any replicas that exist across different nodes in the system. We target applications which need stronger consistency guarantees than the applications currently supported by the data stores on the Cloud. We propose a cost-effective algorithm that ensures distributed consistency of data without really compromising on availability for fully replicated data. This paper describes a design in progress, presents the consistency and recovery algorithms for relational data, highlights the guarantees provided by the system and presents future research challenges. We believe that the current notions of consistency for databases might not be applicable over the Cloud and a new f\n",
      "\n",
      "4. id: 558adf12612c41e6b9d3bf01   score: 0.790692   abstract: Modern transactional processing systems need to be fast and scalable, but this means many such systems settled for weak consistency models. It is however possible to achieve all of strong consistency, high scalability and high performance, by using fine-grained partitions and light-weight concurrency control that avoids superfluous synchronization and other overheads such as lock management. Independent transactions are one such mechanism, that rely on good partitions and appropriately defined transactions. On the downside, it is not usually straightforward to determine optimal partitioning schemes, especially when dealing with non-trivial amounts of data. Our work attempts to solve this problem by automating the partitioning process, choosing the correct transactional primitive, and routing transactions appropriately.\n",
      "\n",
      "5. id: 5390a63c20f70186a0e82c14   score: 0.78116375   abstract: Cloud computing platforms provide scalability and high availability properties for web applications but they sacrifice data consistency at the same time. However, many applications cannot afford any data inconsistency. We present a scalable transaction manager for cloud database services to execute ACID transactions of web applications, even in the presence of server failures. We demonstrate the scalability of our system using a prototype implementation, and show that it scales linearly to at least 40 nodes sustaining a maximum throughput of 7286 transactions per second.\n",
      "\n",
      "6. id: 539095ba20f70186a0df171e   score: 0.7350715   abstract: This paper describes the design of the weak consistency scheme used in ROSS, the EAN object store. ROSS supports nested atomic transactions on distributed and replicated objects. The weak consistency method falls into the family of optimistic protocols. After a partitioning, execution of transactions proceeds normally. If write-write conflicts are detected when partitions later merge, transactions may be rolled back to ensure consistency. One-copy serializability is not provided. This approach is particularly well suited to a common class of database applications where there is limited interdependency between objects. A distributed name service is one such application.\n",
      "\n",
      "7. id: 5390bae520f70186a0f3ac51   score: 0.72311395   abstract: In order to design distributed business applications or services, the common practice consists in setting up a multi-tier architecture on top of a relational database. Due to the recent evolution of the needs in terms of scalability and availability in cloud environments, the design of the data access layer got significantly more complicated because of the trade-off decisions between consistency, scalability and availability that have to be taken into account in accordance with the CAP theorem. An interesting compromise in this context consists in offering some flexibility at the consistency level, in order to allow multi-tier architectures to support partition tolerance flexibility while guaranteeing availability. This paper introduces a flexible data layer that guarantees availability and gives the ability to the developers to easily select the required execution context, by integratin\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3d327   score: 0.63238925   abstract: Many applications hosted on the cloud have sophisticated data management needs that are best served by a SQL-based relational DBMS. It is not difficult to run a DBMS in the cloud, and in many cases one DBMS instance is enough to support an application's workload. However, a DBMS running in the cloud (or even on a local server) still needs a way to persistently store its data and protect it against failures. One way to achieve this is to provide a scalable and reliable storage service that the DBMS can access over a network. This paper describes such a service, which we call DAX. DAX relies on multi-master replication and Dynamo-style flexible consistency, which enables it to run in multiple data centers and hence be disaster tolerant. Flexible consistency allows DAX to control the consistency level of each read or write operation, choosing between strong consistency at the cost of high l\n",
      "\n",
      "9. id: 5390a05a20f70186a0e4b64f   score: 0.5990161   abstract: Replication has been told to be a solution to provide scalability and high availability in databases. Unfortunately, the cost of ensuring isolated and consistent executions is sometimes too high. Weakest isolation models have proved to be a way to reduce this cost but they can violate some applications transactions isolation needs. In stand-alone systems, models supporting different isolation restrictions for concurrent transaction are used to avoid this dilemma. With this kind of protocols, applications can specify every transaction isolation requirements. However, how to extend these models to replicated systems is still an issue. In this paper we extend Adya's model based on serialization graphs as a first step to construct replication protocols with such a feature.\n",
      "\n",
      "10. id: 539087a520f70186a0d48fea   score: 0.5951397   abstract: Chopping transactions into pieces is good for performance but may lead to non-serializable executions. Many researchers have reacted to this fact by either inventing new concurrency control mechanisms, weakening serializability, or both. We adopt a different approach.We assume a user who• has only the degree 2 and degree 3 consistency options offered by the vast majority of conventional database systems; and •knows the set of transactions that may run during a certain interval (users are likely to have such knowledge for online or real-time transactional applications).Given this information, our algorithm finds the finest partitioning of a set of transactions TranSet with the following property; if the partitioned transactions execute serializably, then TranSet executes serializably. This permits users to obtain more concurrency while preserving correctness. Besides obtaining more inter-\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652082\n",
      "index                                        559167670cf2e89307ca9974\n",
      "title               Sound Modular Verification of C Code Executing...\n",
      "authors                     Pieter Agten, Bart Jacobs, Frank Piessens\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 42nd Annual ACM SIGPLAN-SIG...\n",
      "references          53908aac20f70186a0da7fef;5390b72e20f70186a0f21...\n",
      "abstract            Over the past decade, great progress has been ...\n",
      "id                                                            1652082\n",
      "clustered_labels                                                    3\n",
      "Name: 1652082, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a1bc20f70186a0e55840   score: 0.94767237   abstract: If program verification tools are ever to be used widely, it is essential that they work in a modular fashion. Otherwise, verification will not scale. This paper discusses the scientific challenges that this poses for research in program logic. Some recent work on separation logic is described, and test problems that would be useful in measuring advances on modular reasoning are suggested.\n",
      "\n",
      "2. id: 5390980720f70186a0e04181   score: 0.9466954   abstract: Traditional approaches to enforcing memory safety of programs rely heavily on run-time checks of memory accesses and on garbage collection, both of which are unattractive for embedded applications. The goal of our work is to develop advanced compiler techniques for enforcing memory safety with minimal run-time overheads. In this paper, we describe a set of compiler techniques that, together with minor semantic restrictions on C programs and no new syntax, ensure memory safety and provide most of the error-detection capabilities of type-safe languages, without using garbage collection, and with no run-time software checks, (on systems with standard hardware support for memory management). The language permits arbitrary pointer-based data structures, explicit deallocation of dynamically allocated memory, and restricted array operations. One of the key results of this paper is a compiler te\n",
      "\n",
      "3. id: 5390aeba20f70186a0ec9c96   score: 0.91920847   abstract: We present a work-in-progress proof system and tool, based on separation logic, for analysing memory safety of multicore programs that use asynchronous memory operations.\n",
      "\n",
      "4. id: 5390a1bc20f70186a0e55863   score: 0.9043131   abstract: After some general remarks about program verification, we introduce separation logic, a novel extension of Hoare logic that can strengthen the applicability and scalability of program verification for imperative programs that use shared mutable data structures or shared-memory concurrency.\n",
      "\n",
      "5. id: 5390aa0e20f70186a0ea7b4e   score: 0.89010334   abstract: Separation logic is a popular approach for specifying properties of recursive mutable data structures. Several existing systems verify a subclass of separation logic specifications using static analysis techniques. Checking data structure specifications during program execution is an alternative to static verification: it can enforce the sophisticated specifications for which static verification fails, and it can help debug incorrect specifications and code by detecting concrete counterexamples to their validity. This paper presents Separation Logic Invariant ChecKer (SLICK), a runtime checker for separation logic specifications. We show that, although the recursive style of separation logic predicates is well suited for runtime execution, the implicit footprint and existential quantification make efficient runtime checking challenging. To address these challenges we introduce a coloring\n",
      "\n",
      "6. id: 5390a7f520f70186a0e93125   score: 0.87568074   abstract: C is the most widely used imperative system's implementation language. While C provides types and high-level abstractions, its design goal has been to provide highest performance which often requires low-level access to memory. As a consequence C supports arbitrary pointer arithmetic, casting, and explicit allocation and deallocation. These operations are difficult to use, resulting in programs that often have software bugs like buffer overflows and dangling pointers that cause security vulnerabilities. We say a C program is memory safe, if at runtime it never goes wrong with such a memory access error. Based on standards for writing \"good\" C code, this paper proposes strong memory safety as the least restrictive formal definition of memory safety amenable for runtime verification. We show that although verification of memory safety is in general undecidable, even when restricted to clos\n",
      "\n",
      "7. id: 5390a6b120f70186a0e85efd   score: 0.8679338   abstract: This paper presents a methodology for automated modular verification of C programs against specifications written in separation logic. The distinguishing features of the approach are representation of the C memory model in separation logic by means of ...\n",
      "\n",
      "8. id: 5390a6b120f70186a0e85efe   score: 0.8671483   abstract: This paper presents a methodology for automated modular verification of C programs against specifications written in separation logic. The distinguishing features of the approach are representation of the C memory model in separation logic by means of ...\n",
      "\n",
      "9. id: 5390a6b120f70186a0e85efc   score: 0.8671483   abstract: This paper presents a methodology for automated modular verification of C programs against specifications written in separation logic. The distinguishing features of the approach are representation of the C memory model in separation logic by means of ...\n",
      "\n",
      "10. id: 5390a6b120f70186a0e85f01   score: 0.8671483   abstract: This paper presents a methodology for automated modular verification of C programs against specifications written in separation logic. The distinguishing features of the approach are representation of the C memory model in separation logic by means of ...\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1724762\n",
      "index                                        55323d1245cec66b6f9dd799\n",
      "title               Classification and indexing scheme of large-sc...\n",
      "authors             Daehoon Kim, Seungmin Rho, Sanghoon Jun, Eenju...\n",
      "year                                                           2015.0\n",
      "venue                           Integrated Computer-Aided Engineering\n",
      "references                                   558c30500cf20e727d0f6e1b\n",
      "abstract            In this paper, we propose a classification and...\n",
      "id                                                            1724762\n",
      "clustered_labels                                                    0\n",
      "Name: 1724762, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac5720f70186a0eb5765   score: 0.9761292   abstract: In this paper, the performance of several visual features is evaluated in automatically recognizing landmarks (monuments, statues, buildings, etc.) in pictures. A number of landmarks were selected for the test. Pictures taken from a test set were classified automatically trying to guess which landmark they contained. We evaluated both global and local features. As expected, local features performed better given their capability of being less affected to visual variations and given that landmarks are mainly static objects that generally also maintain static local features. Between the local features, SIFT outperformed SURF and ColorSIFT.\n",
      "\n",
      "2. id: 5390bb7b20f70186a0f40113   score: 0.96771675   abstract: This demo shows our system that takes a landmark image as input, recognizes the landmark from the image and returns historical events of the landmark with related photos. Different from existing landmark related researches, we focus on the temporal dimension of a landmark. Our system automatically recognizes the landmark, shows historical events chronologically and provides detailed photos for the events. To build these functions, we fuse information from multiple online resources.\n",
      "\n",
      "3. id: 5390a8b220f70186a0e9bd9c   score: 0.96378   abstract: One commonly used approach to scene localization and landmark recognition is to match an input image against a large annotated database of images using local image features. However problems exist with these approaches relating to memory constraints and the processing time required to compare high dimensional image feature vectors in a very large scale database.\\par We investigate a new landmark classification technique which takes advantage of the fact that there is considerable overlap in visually similar images of landmarks in any large public photo repository. A large number of images containing landmarks are clustered into visually similar clusters. Classification models are then implemented and trained based on global histograms of interest point features from these clusters to create models which can be used for robust real-time accurate classification of images containing these l\n",
      "\n",
      "4. id: 5390a40520f70186a0e6fc33   score: 0.93721074   abstract: Many people take pictures of different city landmarks and post them to photo-sharing systems like Flickr. They also add tags and place photos in Flickr groups, created around particular themes. Using tags, other people can search for representative landmark images of places of interest. Searching for landmarks using tags results into many non-landmark photos and provides poor landmark summary for a city. In this paper we propose a new method to identify landmark photos using tags and social Flickr groups. In contrast to similar modern systems, our approach is also applicable when GPS-coordinates for photos are not available. Presented user study shows that the proposed method outperforms state-of-the-art systems for landmark finding.\n",
      "\n",
      "5. id: 5390aefc20f70186a0ecd409   score: 0.93721074   abstract: An image analysis scheme can automate the detection of landmarks and events in large image collections, significantly improving the content-consumption experience. The Web extra includes a video that illustrates a new approach for mining landmarks and events in large, tagged photo collections. View the same Web extra online at http://www.youtube.com/watch?v=UbZpj9BjfMg.\n",
      "\n",
      "6. id: 55922eb90cf2c3a0875c9e69   score: 0.93546456   abstract: The task of a visual landmark recognition system is to identify photographed buildings or objects in query photos and to provide the user with relevant information on them. With their increasing coverage of the world's landmark buildings and objects, Internet photo collections are now being used as a source for building such systems in a fully automatic fashion. This process typically consists of three steps: clustering large amounts of images by the objects they depict; determining object names from user-provided tags; and building a robust, compact, and efficient recognition index. To this date, however, there is little empirical information on how well current approaches for those steps perform in a large-scale open-set mining and recognition task. Furthermore, there is little empirical information on how recognition performance varies for different types of landmark objects and where\n",
      "\n",
      "7. id: 5390a40520f70186a0e6e5b6   score: 0.93158704   abstract: We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks\n",
      "\n",
      "8. id: 5390b7fe20f70186a0f26037   score: 0.9296961   abstract: In this paper, we propose a location-based large-scale landmark image recognition scheme for mobile devices such as smart phones. To achieve this goal, we collected landmark images all around the world, which were available on the web. For each landmark, we detected interest points and constructed their feature descriptors using SURF. Next, we performed a statistical analysis on the local features to select representative points among them. Intuitively, the representative points of an object are the interest points that best characterize the object. Similar representative points are merged for filtering and fast matching purposes. These points are indexed using an R-tree based on GPS information. Our scheme is based on client-server architecture. When the user takes a picture of a landmark using a mobile device, the client module on the mobile device extracts the local features from the \n",
      "\n",
      "9. id: 5390b72e20f70186a0f21723   score: 0.90829307   abstract: The recognition of a place depicted in an image typically adopts methods from image retrieval in large-scale databases. First, a query image is described as a \"bag-of-features\" and compared to every image in the database. Second, the most similar images are passed to a geometric verification stage. However, this is an inefficient approach when considering that some database images may be almost identical, and many image features may not repeatedly occur. We address this issue by clustering similar database images to represent distinct scenes, and tracking local features that are consistently detected to form a set of real-world landmarks. Query images are then matched to landmarks rather than features, and a probabilistic model of landmark properties is learned from the cluster to appropriately verify or reject putative feature matches. We present novelties in both a bag-of-features retr\n",
      "\n",
      "10. id: 558c43af0cf25dbdbb052c81   score: 0.8868131   abstract: Landmark image classification is a challenging task due to the various circumstances, e.g., illumination, viewpoint, zoom in/out and occlusion under which landmark images are taken. Most existing approaches utilize features extracted from the whole image including both landmark and non-landmark areas. However, non-landmark areas introduce redundant and noisy information. In this paper, we propose a novel approach to improve landmark image classification consisting of three steps. First, an attention-based 3-D reconstruction method is proposed to reconstruct sparse 3-D landmark models. Second, the sparse 3-D models are projected onto iconic images in order to identify images of the hot regions. For a landmark, hot regions are parts of a landmark which attract photographers' attention and are popularly captured in photos. These hot region images are later used to enhance reconstructed spar\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652334\n",
      "index                                        559155840cf232eb904fbcbd\n",
      "title               The Emerging Role of Self-Perception in Studen...\n",
      "authors             Jennifer Dempsey, Richard T. Snodgrass, Isabel...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 46th ACM Technical Symposiu...\n",
      "references          5390882420f70186a0d890b7;5390a01420f70186a0e47...\n",
      "abstract            Recruitment and retention of women has been a ...\n",
      "id                                                            1652334\n",
      "clustered_labels                                                    0\n",
      "Name: 1652334, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a25820f70186a0e60639   score: 0.9923934   abstract: Women are underrepresented in computing careers. This study investigated factors that influence women to choose a computing career and to persist in it. The purposely chosen participants for this study were computing women with at least a bachelor's degree and at least five years experience in the field. Experienced computing women were chosen because of the belief that the lives of women who chose the career, persist in it, and are successful in their career paths contain important information to enlighten the study of women's choices in this field and to make recommendations for the recruitment of more women into the computing/information technology field. This study used a mixed methods design. Fifty computing women completed a 60-question online survey. Twenty-two of the survey participants then completed follow-up telephone interviews of a semi-structured format. The participants an\n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3e1b4   score: 0.99155444   abstract: The underrepresentation of women in computing is an enduring problem in the U.S. This work seeks to increase women's retention in computing, to change the culture of introductory computer science classes to make them comfortable for diverse students, and to understand what factors are affiliated with interest and persistence in computing.\n",
      "\n",
      "3. id: 539087e720f70186a0d68f43   score: 0.9910493   abstract: The recruiting and retention of women in the computing sciences has been an area of study for many years. In 1992, 49% of all high school graduates were women prepared and interested in the computer science and engineering disciplines. Of the bachelor of science degrees awarded, only 31% went to women in these fields of study. Women represented only 28% of the master's degrees and 11% of the Ph.D.s awarded during that time. The following year, 1993, reported a drop of women earning B.S. degrees to 28%, with 27% and 14% of master's and Ph.Ds degrees awarded, respectively, to women.A panel of six discuss why women who are initially attracted to computer science bail out without completing degree requirements, most in the first two years of undergraduate study. The panelists present diverse positions as to why fewer women persevere and experimental efforts for increasing the retention rate \n",
      "\n",
      "4. id: 539087e720f70186a0d68b2c   score: 0.98975134   abstract: Worldwide, there is a significant discrepancy between the numbers of male and female graduates from computer science programs. SUNY Geneseo offers no exception. The literature cites a number of plausible explanations for the problem, but no definitive answers. We conducted a study to determine why few women complete our own computer science major. Our major finding is that (at least on our campus) the problem is not actually one of retention. Few women---even those in the introductory computer science courses---actually plan to major in computer science to begin with. Although some barriers suggested in the literature do operate within the major, they seem much less significant than the low entry rates. Retention of women once they enter the major is important, but it is secondary to getting women into the major initially. This suggests that the most effective solutions will be those tha\n",
      "\n",
      "5. id: 5390882420f70186a0d890c6   score: 0.9832145   abstract: The under-representation of women in the computing profession in many parts the western world has received our attention through numerous publications, the noticeable low representation of women at computer science conferences and in the lecture halls. Over the past two decades, the situation had become worse. Please refer to the other articles in this special issue for detailed statistics, a discussion of factors that contribute to the low participation rate by women, and for suggestions on how to reverse the current trend.This paper seeks to add to the dialogue by presenting preliminary findings from a research project conducted in four countries. The aim of this research was to gain an insight into the perceptions future computer professionals hold on the category of employment loosely defined under the term of \"a computer professional.\" One goal was to get insight into whether or not\n",
      "\n",
      "6. id: 5390aca820f70186a0eb7c87   score: 0.98141783   abstract: The problem of women not entering the science fields is persistent. The proportion of women in computer science fields have actually decreased over the past 30 years while it has increased or reached parity in other technical fields. Researchers have proposed solutions to this problem over many years but there is still much progress to be made. This paper looks at several studies that address this problem and address the findings of those studies. In addition, this paper will relate these findings to personal experiences. This paper will suggest giving more attention to girls in grade school (K-12) and producing more powerful media to attract additional women to the Computer Science field.\n",
      "\n",
      "7. id: 5390a54620f70186a0e77525   score: 0.9804314   abstract: An overview of recent research concerning the underrepresentation of women in computer science studies indicates that this problem might be more complex than previously assumed. The percentage of female computer science students varies from country to country, and there is also some indication that gender stereotypes are defined differently in different cultures. Gender stereotypes concerning technology are deeply embedded in the dominant culture and often contradictory. Only a few general assertions can be made about the development of the inclusion or exclusion of women from computer science. In addition, there does not seem to be a specific female style of computer usage. Concepts of diversity and ambivalence seem to be more appropriate but difficult to realize. All this makes the development of appropriate interventions for overcoming the underrepresentation of women in computer scie\n",
      "\n",
      "8. id: 5390882420f70186a0d890c3   score: 0.97838473   abstract: At a cost to both their own opportunities and society's ability to produce people with much-needed technical skills, women continue to be underrepresented in computer science degree programs at both the undergraduate and graduate level. Although some of the barriers that women face have their foundations in cultural expectations established well before the college level, we believe that departments can take effective steps to increase recruitment and retention of women students. This paper describes several strategies we have adopted at Stanford over the past decade.\n",
      "\n",
      "9. id: 539098dc20f70186a0e0d70d   score: 0.971779   abstract: The current paper presents research that investigates those factors that affect the recruitment and retention of women and African Americans in the computing sciences. We begin by discussing the data that describes the historic participation of women and African American students in the computing sciences. We then discuss the social, cultural, and financial factors that have traditionally affected these students and their progression in the field. Finally, we present an outline of recommendations that can be used to develop new policies and learning programs that can be used to increase the number of students from these groups. Though there is much literature on this topic, the current paper suggests that further investigation is needed to understand the extent to which existing statistical data can be interpreted to aid in the recruitment and retention of these underrepresented groups i\n",
      "\n",
      "10. id: 5390b44620f70186a0ef8490   score: 0.9680205   abstract: As percentages of women in computing jobs and university programs decline, recruiting and retaining women in the field of Computer Science (CS) becomes increasingly important. Undergraduate CS programs, and more specifically, introductory-level CS courses, offer an opportunity to introduce women to CS studies. Furthermore, learning experiences in introductory CS courses can be pivotal in shaping female students' perceptions of CS. Collaborative learning, in various forms, is an instructional construct that has been shown to be effective in recruiting and retaining women in undergraduate CS programs. In this paper we present an exploratory study on formal learning groups and their potential to attract and maintain female students' interest in CS studies.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1682516\n",
      "index                                        558e36860cf2c779a6477d70\n",
      "title               Endoscopic Image Retrieval System Using Multi-...\n",
      "authors                           Manish Chowdhury, Malay Kumar Kundu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2nd International Conferenc...\n",
      "references          5390b64020f70186a0f19cfa;5390a01420f70186a0e46...\n",
      "abstract            We present a novel Content Based Medical Image...\n",
      "id                                                            1682516\n",
      "clustered_labels                                                    2\n",
      "Name: 1682516, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b7ff20f70186a0f279f1   score: 0.99549675   abstract: A new algorithm for medical image retrieval is presented in the paper. An 8-bit grayscale image is divided into eight binary bit-planes, and then binary wavelet transform (BWT) which is similar to the lifting scheme in real wavelet transform (RWT) is performed on each bitplane to extract the multi-resolution binary images. The local binary pattern (LBP) features are extracted from the resultant BWT sub-bands. Three experiments have been carried out for proving the effectiveness of the proposed algorithm. Out of which two are meant for medical image retrieval and one for face retrieval. It is further mentioned that the database considered for three experiments are OASIS magnetic resonance imaging (MRI) database, NEMA computer tomography (CT) database and PolyU-NIRFD face database. The results after investigation shows a significant improvement in terms of their evaluation measures as comp\n",
      "\n",
      "2. id: 5390b4c420f70186a0efee0e   score: 0.9850429   abstract: In this article, a novel content based image retrieval (CBIR) system based on a new Multiscale Geometric Analysis (MGA)-tool, called Ripplet Transform Type-I (RT) is presented. To improve the retrieval result, a fuzzy relevance feedback mechanism (F-RFM) is also implemented. Fuzzy entropy based feature evaluation mechanism is used for automatic computation of revised feature's importance and similarity distance at the end of each iteration. Experimental results on a large image database demonstrate the efficiency and effectiveness of the proposed CBIR system in the image retrieval paradigm\n",
      "\n",
      "3. id: 558b8625612c6b62e5e8aa73   score: 0.9452985   abstract: Content Based Image Retrieval (CBIR) is a system that searches for similar images from a large image database according to users' specification. Digital image database is growing rapidly in size as a result of technological advancements. Traditional systems like text based image retrieval faces too many problems like human perception, deeper needs, image annotation and etc. Thus it is difficult to support a variety of task-dependent queries for traditional systems. CBIR systems comprehend a wide areas, viz. image segmentation, image feature extraction, representation, feature matching, storage and indexing, image retrieval - making CBIR system development a challenging task. The visual content of an image is analyzed features (i.e., color, shape, texture) extracted from the image. CBIR system is used in Medical Imagery Retrieval, Finger Print Retrieval, Satellite Imagery Retrieval, Inter\n",
      "\n",
      "4. id: 5390bda020f70186a0f458bb   score: 0.9401336   abstract: Content-based image retrieval (CBIR) is a technique that uses visual contents such as color, shape, texture and spatial layout to retrieve image from large scale image database, given a query image. To automate CBIR system for medical images, algorithms are to be developed that can automatically detect regions of interest (ROIs) and retrieve semantically and perceptually similar images to provide evidence-based decision support. Developing automated algorithms for retrieval of biomedical images by structural contents is a significant research challenge, because ROIs are commonly irregular, overlapping, partially occluded or highly localized. Our work is based on content based retrieval of cervical spine images and mammographic mass. Extensive research work is going on at National Library of Medicine (NLM) on indexing and content based retrieval of spine x-ray images, collected in the sec\n",
      "\n",
      "5. id: 5390a30b20f70186a0e69392   score: 0.9190633   abstract: In this paper, we propose a novel algorithm for the efficient classification and retrieval of medical images, especially X-ray images. Since medical images have bright foreground against dark background, we extract MPEG-7 visual descriptor from only salient parts of foreground. For color descriptor, Color Structure Descriptor (H-CSD) is extracted from salient points, which are detected by Harris corner detector. For texture descriptor, Edge Histogram Descriptor (EHD) is extracted from global and local parts of images. Then extracted feature vector is applied to multi-class Support Vector Machine (SVM) to give membership scores for each image. From the membership scores of H-CSD and EHD, two membership scores are combined as one ensemble feature and it is used for similarity matching of our retrieval system, MISS (Medical Information Searching System). The experimental results using CLEF-\n",
      "\n",
      "6. id: 539090c420f70186a0dddc2f   score: 0.91891783   abstract: This technical report presents an introduction to content-based information retrieval (CBIR) in the domain of medical imaging. CBIR is a very actively researched area in recent years, however, utilising it in the healthcare community is still relatively new and unexplored. This report provides a survey of current CBIR research, with special emphasis on medical imaging. Research has also been done in the MPEG-7 area, especially on the Contour Shape Descriptor. An implementation and experimental results of the Contour Shape Descriptor using Curvature Scale Space (CSS) are also discussed.\n",
      "\n",
      "7. id: 5390a55520f70186a0e7b7fc   score: 0.91181   abstract: Content-based image retrieval techniques have extensively been studied for past few years. However; few systems are dedicated to medical images today while demands for content-based analysis and retrieval tools increases with growth of digital medical image databases. A prototype of content-based image retrieval system is built to investigate performance of descriptors for blood cell image retrieval.Here, traditional global color histogram and wavelet-based method is investigated. In addition, performance of indexing method for the aforementioned descriptors is analyzed. The prototype system allows users to search by providing a query image and selecting one of four implemented methods.Research goal is enhancing current content-based image retrieval techniques. Proposed method is able to perform clinically relevant queries on image databases without user supervision.\n",
      "\n",
      "8. id: 5390a96e20f70186a0ea2c90   score: 0.87621135   abstract: This paper presents an original system for interactive content-based image retrieval (CBIR). A novel approach for searching by similarity is introduced. It is based on a classification of the index database using mixture models and the EM algorithm. The presented retrieval system is evaluated and validated using a medical image database and the Washington University heterogeneous database (ANN).\n",
      "\n",
      "9. id: 5390a77d20f70186a0e8f86c   score: 0.87557435   abstract: Content-based medical image retrieval is getting more and more importance in aspect of clinical assistant diagnose. In this paper a new method based on the characters of color change is proposed. First a color clustering technique is used for image segmentation in CIE L*a*b* color space. And then color change feature is extracted from the binary edge image. Kullback-Leibler distance is used to calculate the dissimilarity. Meanwhile，a method combining both color change feature and dominant color information is proposed to carry out integrate retrieval. Finally, a system for gastroscopic image retrieval is developed which is available to support clinical decision making. Some contrast experiments are designed in the retrieval accuracies，the rank and the execution time. The comparison of the experimental results shows that the approach proposed in this paper is effective.\n",
      "\n",
      "10. id: 5390adfc20f70186a0ec4798   score: 0.8631611   abstract: Aiming at improving the retrieval rate of the original contour let transform based texture image retrieval system, a non-sub sampled contour let transform based texture image retrieval system was proposed. Generalized Gaussian Density (GGD) model parameters were cascaded to form feature vectors and Kullback-Leibler distance (KLD) function was used for similarity measure. Experimental results on 640 texture images from Vistex texture image database indicate that non-sub sampled contour let transform based image retrieval system is superior to that of the original contour let transform under the same system structure with almost same length of feature vectors, retrieval time and memory needed. Furthermore, GGD combined with KLD method has higher retrieval rates than energy based features combined with Euclidean distance under comparable levels of computational complexity, decomposition par\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674273\n",
      "index                                        559255440cf205530abc96a9\n",
      "title               The relationship between experimentally valida...\n",
      "authors                             Xiaofeng Song, Yan Jing, Ping Han\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Data Mining and Bioin...\n",
      "references          5390b8d720f70186a0f2b06e;539098dc20f70186a0e0d...\n",
      "abstract            Protein degradation is critical for most cellu...\n",
      "id                                                            1674273\n",
      "clustered_labels                                                    1\n",
      "Name: 1674273, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390882120f70186a0d866ad   score: 0.9439706   abstract: From the Publisher:Modern computer graphics transforms protein structures into visually exciting images. 'Protein Architecture: A Practical Approach' shows the reader how to visualize protein structures, and how to design an illustration to help understand and appreciate the variety of protein folding patterns.\n",
      "\n",
      "2. id: 5390bd1520f70186a0f44ad1   score: 0.92590266   abstract: Protein is a linear chain of amino acids bonded by peptide bonds. Protein plays a vital role in almost every biological process. For most proteins, they need to fold into a stable 3D structure (native state) in order to function properly. This process that proteins fold from the sequence of amino acids to the 3D structure is known as protein folding. The relations among protein sequence, structure and function have been studied for many years in lab by expensive experimental methods such as X-ray crystallography and NMR spectroscopy. The emerging high performance microprocessors enable efficient and accurate protein structure simulation. Despite extensive studies previously, the factors behind the protein folding and their cooperative effects are still not completely understood. This dissertation presents the methods and results about investigating the determining factors for protein fol\n",
      "\n",
      "3. id: 53909ed120f70186a0e2fb0d   score: 0.91490096   abstract: Motivation: The underlying assumption of many sequence-based comparative studies in proteomics is that different aspects of protein structure and therefore functionality may be linked to particular sequence motifs. This holds true if sequence similarity is sufficiently high, but in general the relationship between protein sequence and structure appears complex and is not well understood. Results: Statistical analysis of multiple and pairwise structural alignments of protein structures within SCOP folds is performed. The results indicate that multiple conservation of residue identity is not common and that relationship between sequence and structure may be explained by a model based on the assumption that protein structure is tolerant to residue substitutions preserving hydropathic profile of the sequence. This model also explains the origin and specific value of the sequence similarity t\n",
      "\n",
      "4. id: 539098dc20f70186a0e0cf4b   score: 0.88161486   abstract: Summary: Absence of any regular structure is increasingly being observed in structural studies of proteins. These disordered regions or random coils, which have been observed under physiological conditions, are indicators of protein plasticity. The wide variety of interactions possible due to the flexibility of these 'natively disordered' regions confers functional advantage to the protein and the organism in general. This concept is underscored by the increasing proportion of intrinsically unstructured proteins seen with the ascension in the complexity of the organisms. The 'natively unfolded/disordered' state of the protein can be predicted utilizing Uversky's or Dunker's algorithm. We utilized Uversky's prediction scheme and based on the unique position of a protein in the charge--hydrophobicity plot, a derived net score was used to predict the overall disorder of the human housekeepi\n",
      "\n",
      "5. id: 53909fbd20f70186a0e4317f   score: 0.86838096   abstract: We analyzed the structural properties and the local sur- face environment of surface amino acid residues of pro- teins using a large, non-redundant dataset of 2383 pro- tein chains in dimeric complexes from PDB. We compared the interface residues and non-interface residues based on six properties: side chain orientation, surface roughness, solid angle, cx value, hydrophobicity and interface cluster size. The results of our analysis show that interface residues have side chains pointing inward; interfaces are rougher, tend to be flat, moderately convex or concave and protrude more relative to non-interface surface residues. Interface residues tend to be surrounded by hydrophobic neighbors and tend to form clusters consisting of three or more in- terfaces residues. These findings are consistent with pre- vious published studies using much smaller datasets, while allowing for more qualitati\n",
      "\n",
      "6. id: 539098dc20f70186a0e0d206   score: 0.85145944   abstract: Summary: Along with their mutating sequences, protein structures change in time. Analyzing a formate dehydrogenase domain that is evolutionarily related to ferredoxin, but simultaneously contains all the structural elements of a β-Grasp fold, we illustrate here a mechanism termed as structural drift, by which changes to a protein fold can occur. Contact: grishin@chop.swmed.edu\n",
      "\n",
      "7. id: 539098dc20f70186a0e0cebd   score: 0.8499712   abstract: Motivation: The solubility of a protein is crucial for its function and is therefore an evolutionary constraint. As the solubility of a protein is related to the distribution of polar and hydrophobic residues on its solvent accessible surface, such a constraint should provide a valuable insight into the evolution of protein surfaces. We examine how the surfaces of proteins have evolved by considering how the average hydrophobicities of patches of surface residues vary across homologous proteins. We derive distributions for the average hydrophobicity/philicity of surface patches at a residue-based level---which we refer to as the residue hydrophobic density. This is computed for a set of 28 monomeric proteins and their homologues. The resulting distributions are compared with a set of randomized sequences, with the same residue content. Results: We find that the patches, involving typical\n",
      "\n",
      "8. id: 539098dc20f70186a0e0d3ab   score: 0.8345418   abstract: Summary: Utilizing the user-supplied coordinates of a protein structure, the COREX/BEST Server generates a structural thermodynamic ensemble. This conformational ensemble can then be used to calculate the regional variations in stability of a protein structure, and the stabilities are presented in units of energy (kcal/mol). The regional stabilities, which are calculated at the resolution of individual residues, can be mapped onto the protein structure for visual representation and downloaded from the site in the form of tab delimited text. The site provides an easy to follow summary of the theoretical and algorithmic approaches and provides links to references for more detailed descriptions. Availability: The COREX/BEST Server may be accessed through a typical web browser by visiting http://best.utmb.edu/BEST/ Contact: vjhilser@utmb.edu\n",
      "\n",
      "9. id: 5390a01420f70186a0e46b00   score: 0.8288003   abstract: Motivation: Understanding the basis of protein stability in thermophilic organisms raises a general question: what structural properties of proteins are responsible for the higher thermostability of proteins from thermophilic organisms compared to proteins from mesophilic organisms? Results: A unique database of 373 structurally well-aligned protein pairs from thermophilic and mesophilic organisms is constructed. Comparison of proteins from thermophilic and mesophilic organisms has shown that the external, water-accessible residues of the first group are more closely packed than those of the second. Packing of interior parts of proteins (residues inaccessible to water molecules) is the same in both cases. The analysis of amino acid composition of external residues of proteins from thermophilic organisms revealed an increased fraction of such amino acids as Lys, Arg and Glu, and a decreas\n",
      "\n",
      "10. id: 5390b8d720f70186a0f2c81b   score: 0.82375413   abstract: The most recent methods and strategies for the elucidation of protein and peptide structures, e.g. sensitive techniques for the analysis of primary structure and amino acid sequences as well as secondary and tertiary structures are discussed in detail. They include sophisticated crystallization procedures for proteins and cell organelles, folding mechanisms of proteins, strategies for domain structure and binding site evaluation and advanced immunological methods. Analyses of structure-function correlations important for genetic engineering and the design of new drugs or proteins are also reported. A special chapter addresses the documentation of protein sequences, the organization of protein data bases and their use in the search and comparison of proteins.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691335\n",
      "index                                        5592559b0cf2aff368683bfb\n",
      "title               ANN and multiple regression method-based model...\n",
      "authors                                F. Kara, K. Aslantas, A. Çiçek\n",
      "year                                                           2015.0\n",
      "venue                               Neural Computing and Applications\n",
      "references          5390ba0a20f70186a0f330a8;5390afc920f70186a0ed1...\n",
      "abstract            In this study, predictive modelling was perfor...\n",
      "id                                                            1691335\n",
      "clustered_labels                                                    2\n",
      "Name: 1691335, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909f6a20f70186a0e3bfac   score: 0.9853843   abstract: A reliable prediction of cutting forces is the aim of many researchers. In this study cutting forces prediction was modeled using back propagation (BP) neural network with an enhancement by differential evolution (DE) algorithm. Experimental machining data is used in this study to train and evaluate the model. The data includes speed, feed rate, depth of cut, nose wear, flank wear, notch wear, feed force, vertical force, and radial force. A graphical study of the data reveals high non-linearity and early experiments carried out in this study using simple back propagation network gave marginally acceptable results. The results have shown an obvious improvement in the reliability of predicting the cutting forces over the previous work.\n",
      "\n",
      "2. id: 5390b64020f70186a0f19307   score: 0.93674946   abstract: In the present investigation, three different type of support vector machines (SVMs) tools such as least square SVM (LS-SVM), Spider SVM and SVM-KM and an artificial neural network (ANN) model were developed to estimate the surface roughness values of AISI 304 austenitic stainless steel in CNC turning operation. In the development of predictive models, turning parameters of cutting speed, feed rate and depth of cut were considered as model variables. For this purpose, a three-level full factorial design of experiments (DOE) method was used to collect surface roughness values. A feedforward neural network based on backpropagation algorithm was a multilayered architecture made up of 15 hidden neurons placed between input and output layers. The prediction results showed that the all used SVMs results were better than ANN with high correlations between the prediction and experimentally measu\n",
      "\n",
      "3. id: 5390ac5720f70186a0eb4e5c   score: 0.9145963   abstract: The experimental program was designed according to the character of metallic material machining, and lots of data were acquired through experiment. Then the corresponding connection of input and output parameters were building based on model constructed by artificial neural network predictive theory. Radial error after machining of metallic material can be forecasted accurately in the predictive model. Lastly, predictive value and measured value of radial error after machining were compared and analyzed. The results indicated the availability and validity of artificial neural network predictive theory.\n",
      "\n",
      "4. id: 5390a25820f70186a0e5fe11   score: 0.91228   abstract: In this study the machining of AISI 1030 steel (i.e. orthogonal cutting) uncoated, PVD- and CVD-coated cemented carbide insert with different feed rates of 0.25, 0.30, 0.35, 0.40 and 0.45mm/rev with the cutting speeds of 100, 200 and 300m/min by keeping depth of cuts constant (i.e. 2mm), without using cooling liquids has been accomplished. The surface roughness effects of coating method, coating material, cutting speed and feed rate on the workpiece have been investigated. Among the cutting tools-with 200mm/min cutting speed and 0.25mm/rev feed rate-the TiN coated with PVD method has provided 2.16@mm, TiAlN coated with PVD method has provided 2.3@mm, AlTiN coated with PVD method has provided 2.46@mm surface roughness values, respectively. While the uncoated cutting tool with the cutting speed of 100m/min and 0.25mm/rev feed rate has yielded the surface roughness value of 2.45@mm. Afterwa\n",
      "\n",
      "5. id: 5390ac1820f70186a0eb33d0   score: 0.9113377   abstract: In this study, fuzzy expert system (FES) and artificial neural network (ANN) models are designed for the estimation of cutting forces in turning operations. On designed models, cutting forces and experimental temperature data obtained from different cutting conditions were used in process of turning. Cutting forces at different cutting conditions and temperature values can be estimated with the help of developed models. The results obtained with these models, compared with the experimental data. The regression values were found as 0.99505 between the Experiment-FES and, 0.9888 between Experiment-ANN in the analysis. As a result, the both artificial intelligence (AI) methods have made successful modeling, but it's seen that, realized FES model has more successful results than the ANN model in the process of estimation of cutting forces.\n",
      "\n",
      "6. id: 5390b52620f70186a0f039a4   score: 0.9014011   abstract: Nowadays, artificial neural networks (ANN) are often applied in solving numerous problems in machining processes. A tool life prediction of coated and uncoated cutting tools proves to be significant. In this study, a feed forward back propagation neural network with a Levenberg-Marquard (L-M) training algorithm is used in modeling the tool life of a PVD insert cutting tool when end milling of Ti6Al4V under dry cutting conditions. The objective of this study is to apply ANN in the prediction of the tool life of PVD cutting tools using low experimental data sets. One hundred and ten (110) models were designed, trained and tested using Matlab neural network tool box. Good agreement was obtained between the ANN model and the experimental data.\n",
      "\n",
      "7. id: 5390a79f20f70186a0e91408   score: 0.8045595   abstract: The endpoint temperature and carbon content of molten steel cannot be measured timely or accurately due to the extremely high temperature in BOF, so it is very important to establish an accurate predictive model for them. Steelmaking process is a very complex nonlinear process, and therefore it is very difficult to build up an accurate math model for it.The precision of traditional models based on oxygen balance and thermal equilibrium theory or based on reproducibility theory is low, and hit rate for prediction is low too. In this paper, the method that combines neural network technique with traditional modeling technology is adopted to build up static and dynamic models for steelmaking process. On this basis, presetting model is modified by using neural network technique to implement optimal setting control for steelmaking endpoint.\n",
      "\n",
      "8. id: 55323cad45cec66b6f9dc9fb   score: 0.76960015   abstract: This paper presents a comparison of experimental results and a fuzzy rule based system model for calculating the cutting force in the turning operation. A full bridge dynamometer was used to measure the cutting forces over the mild steel work piece and Cemented Carbide Insert tool for different combinations of cutting velocity, feed rate and depth of cut. The rake angle, approach angle and nose radius of the cutting tool insert is kept constant throughout the experiment. This fuzzy model consists of 27 rules and Mamdani Max-min inference mechanism was used. The Taguchi designs of experiments were used to determine the number of experiments. Also, an attempt had been made to analyze the influence of the parameters using the regression analysis which yields a maximum error of 3.214% at the time of prediction which was smaller. The experiments are planned based on Taguchi's design and the m\n",
      "\n",
      "9. id: 5390bded20f70186a0f495cd   score: 0.74372685   abstract: This work primarily aims to develop an expert system based on the artificial neural network ANN to predict the tensile behaviour of tailor welded blanks TWBs made of dual-phase DP 590 steel. The work also aims to compare the predictions by ANN models with empirical models and the size of the training data set of the prediction accuracy of these models. The strain hardening exponent ‘n’ and strength coefficient ‘K’ are predicted. The results obtained from expert system and empirical models are validated by comparing them with the results obtained from finite element simulations and experiments. It is observed that expert system/ANN predictions based on the full factorial design of experiments DOE is better than the ANN predictions based on the orthogonal array and predictions based on the empirical models. With the reduced orthogonal training data, ANN model-based predictions are more acc\n",
      "\n",
      "10. id: 5390ae2e20f70186a0ec7375   score: 0.70456684   abstract: Machine parts during their useful life are significantly influenced by surface roughness quality. The machining process is more complex, and therefore, it is very hard to develop a comprehensive model involving all cutting parameters. In this study, the surface roughness is measured during turning at different cutting parameters such as speed, feed, and depth of cut. Full factorial experimental design is implemented to increase the confidence limit and reliability of the experimental data. Artificial neural networks (ANN) and multiple regression approaches are used to model the surface roughness of AISI 1040 steel. Multiple regression and neural network-based models are compared using statistical methods. It is clearly seen that the proposed models are capable of prediction of the surface roughness. The ANN model estimates the surface roughness with high accuracy compared to the multiple\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1703669\n",
      "index                                        55323b1e45cec66b6f9d95c6\n",
      "title               A bidirectional neural interface SoC with an i...\n",
      "authors             Kanokwan Limnuson, Hui Lu, Hillel J. Chiel, Pe...\n",
      "year                                                           2015.0\n",
      "venue                Analog Integrated Circuits and Signal Processing\n",
      "references          5390aa0f20f70186a0ea90d0;5390962020f70186a0df5...\n",
      "abstract            This paper presents a neural interface system-...\n",
      "id                                                            1703669\n",
      "clustered_labels                                                    2\n",
      "Name: 1703669, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390c04520f70186a0f55a23   score: 0.71243227   abstract: This dissertation presents the system architecture and implementation of two wireless systems-on-chip (SoCs) for diagnostics and treatment of neurological disorders. It also validates the SoCs as an electronic implant for preoperative monitoring and treatment of intractable epilepsy.The first prototype SoC is a neural recording interface intended for wireless monitoring of intractable epilepsy. The 0.13µm CMOS SoC has 64 recording channels, 64 programmable FIR filters and an integrated 915MHz FSK PLL-based wireless transmitter. Each channel contains a low-noise amplifier and a modified 8-bit SAR ADC that and can provide analog-digital multiplication by modifying the ADC sampling phase. It is used in conjunction with 12-bit digital adders and registers to implement 64 16-tap FIR filters with a minimal area and power overhead. In vivo measurement results from freely moving rodents demonstr\n",
      "\n",
      "2. id: 5390bded20f70186a0f4a51e   score: 0.66104823   abstract: We propose an implant able CMOS micro system for detection of neural spike signals from complex brain neural potentials which achieves the characteristics of ultra low-power and high-precision. The neural recording micro system consists of a low-noise bioamplifier, a neural spike detector based on nonlinear energy operator (NEO) and a precision hysteresis comparator. The DC offset in the bioamplifier is rejected by introducing a new active feedback configuration instead of the large capacitors, the NEO algorithm is implemented through simple analog circuits operating in sub-threshold region, and the hysteresis comparator is added to determine the location of neural spike. The current consumption of the recording system is 3.02μA with 3.3V supply. The proposed system has been implemented in 0.35μm CMOS process and precisely detects neural spike signals from extra cellular recording.\n",
      "\n",
      "3. id: 55922db70cf2c3a0875c9dbf   score: 0.65599746   abstract: This paper presents a combined fabrication technique that is based on some recent advances in silicon microengineering. Buried microchannels in ultralong silicon microelectrodes thinned by etching-before grinding technology offers novel functional microdevices in the field of neural interfaces. Providing injection, sampling and electrical recording--all integrated monolithically in a long and subsequently thinned silicon microelectrode--extends translational research in fundamental neuroscience due to reduced microelectrode dimensions and functionality like stimulation and recording in deep brain region of cats or apes.\n",
      "\n",
      "4. id: 5390ab8820f70186a0eb0089   score: 0.6550051   abstract: We present the design of an integrated neural interface intended for multi-channel neural recording. The design features a mixed-signal part that handles neural signal conditioning, digitization, time-division multiplexing, and a digital module providing control, absolute threshold detection, extraction of spikes, and serial communications towards a host interface. The detection and extraction strategy preserves the entire neuronal spike waveshapes by means of synchronized internal data buffering. This bandwidth reduction scheme prompts for better waveform sorting results and improved performance in prosthetic applications. Both parts of the presented neural interface were fabricated separately in a CMOS 0.18 μm process. The whole neural interface features 16 channels for validation, but, the proposed approach is scalable to larger channel counts. The performance of the implemented neura\n",
      "\n",
      "5. id: 5390a6b120f70186a0e85dbf   score: 0.65367985   abstract: To date, spike sorting for neural processing in brain-computer interfaces has been performed using off-chip analysis or custom ASIC designs. In this paper, we propose and test the feasibility of performing on-chip, real-time spike sorting on a commercially available and programmable smartdust wireless sensor network mote since such a platform is currently the closest to an implantable chip due to its small size and similar physical and environmental constraints.\n",
      "\n",
      "6. id: 5390aeba20f70186a0ec9d3b   score: 0.649912   abstract: This book will describe ultra low-power, integrated circuits and systems designed for the emerging field of neural signal recording and processing, and wireless communication. Since neural interfaces are typically implanted, their operation is highly energy-constrained. This book introduces concepts and theory that allow circuit operation approaching the fundamental limits. Design examples and measurements of real systems are provided. The book will describe circuit designs for all of the critical components of a neural recording system, including: Amplifiers which utilize new techniques to improve the trade-off between good noise performance and low power consumption. Analog and mixed-signal circuits which implement signal processing tasks specific to the neural recording application: Detection of neural spikes Extraction of features that describe the spikes Clustering, a machine learni\n",
      "\n",
      "7. id: 553e6ea30cf2cadf4a0696cf   score: 0.647018   abstract: This book will describe ultra low-power, integrated circuits and systems designed for the emerging field of neural signal recording and processing, and wireless communication. Since neural interfaces are typically implanted, their operation is highly energy-constrained. This book introduces concepts and theory that allow circuit operation approaching the fundamental limits. Design examples and measurements of real systems are provided. The book will describe circuit designs for all of the critical components of a neural recording system, including: Amplifiers which utilize new techniques to improve the trade-off between good noise performance and low power consumption. Analog and mixed-signal circuits which implement signal processing tasks specific to the neural recording application: Detection of neural spikes Extraction of features that describe the spikes Clustering, a machine learni\n",
      "\n",
      "8. id: 5390a2be20f70186a0e65070   score: 0.6159545   abstract: Parallel recording of micro-scale signals using an integrated system approach has become feasible with recent advances in technology. Practical applications include the recording of neural-signals in a brain-computer interface or in prosthetic implants. In an integrated circuit implementation the restriction in size and available power pose considerable challenges, especially in implanted devices. Furthermore, the provision of both high gain and excellent noise performance in the presence of input offset voltages are mandatory. The presented tutorial highlights design strategies for recording system optimization and compares the performance of actual system implementations with the best-case performance achievable in theory. Special consideration is given to the noise vs. power and offset-tolerance vs. noise trade-offs. An application dependent design strategy is proposed.\n",
      "\n",
      "9. id: 53909eef20f70186a0e35ce0   score: 0.5912513   abstract: We have developed a single-chip neural recording system with wireless power delivery and telemetry. The 0.5-脢m CMOS IC is designed to be bonded to the back of a 100-channel Utah Electrode Array. A pad near each amplifier allows connection of the chip to the MEMS electrode array. The complete Integrated Neural Interface will receive power wirelessly through a 2.64-MHz inductive link. A clock, regulated supply, and commands are derived from the power signal .The neural amplifiers each have a gain of 60 dB. A 10-bit charge-redistribution ADC is used to digitize the signal from one amplifier selected with an analog MUX. Digitizing all channels simultaneously would generate prohibitively high data rates; therefore, we perform data reduction by incorporating one-bit gspike detectors h into each amplifier. Neural data is transmitted off chip using an -integrated 433-MHz FSK transmitter. The ch\n",
      "\n",
      "10. id: 55323ba445cec66b6f9da503   score: 0.42048058   abstract: Few-unit recordings were obtained using metal microelectrodes. Separation into single-unit spike trains was based on differences in spike amplitude and spike waveform. For that purpose a hardware microprocessor based spike waveform analyser was designed and built. Spikes are filtered by four matched filters and filter outputs at the moments of spike occurrence are read by a computer and used for off-line separation and spike waveform reconstruction. Thirthy-one double unit recordings were obtained and correlation between the separated spike trains was determined. After stimulus correction correlation remained in only 8 of the double unit records. It appeared that in most cases this neural correlation was stimulus dependent. Continuous noise stimulation resulted in the strongest neural correlation remaining after correction for stimulus coupling, stimulation with 48 ms duration tonepips p\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672330\n",
      "index                                        55913d410cf232eb904fb678\n",
      "title               Understanding Data Providers in a Global Scien...\n",
      "authors             Yurong He, Jennifer Preece, Jen Hammock, Brian...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference Compani...\n",
      "references          5390b56a20f70186a0f0696a;5390aefb20f70186a0eccf1d\n",
      "abstract            In the absence of systematic knowledge about t...\n",
      "id                                                            1672330\n",
      "clustered_labels                                                    3\n",
      "Name: 1672330, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390adfc20f70186a0ec5147   score: 0.6773673   abstract: We present a general conceptual framework that maps relationships and dependencies among scientific data practices, types of data produced and used, and associated curation activities. As part of the Data Conservancy initiative, the framework is being elaborated through empirical studies of data practices in the earth sciences and life science and validated against use cases as curatorial services are developed around data being prepared for ingest into the repository. The framework can be applied more broadly for identifying and representing curation requirements and to support description and assessment of existing or planned curation infrastructure and services. It will support full accounts of the data products and workflows required to maintain the coherence and context of complex data collections.\n",
      "\n",
      "2. id: 5390bded20f70186a0f49e23   score: 0.51734227   abstract: Collaboration across research, government, academic, and private sectors is integrating more than 70 scientific computing libraries and applications through a tailorable provenance framework, empowering scientists to exchange and examine data in novel ways.\n",
      "\n",
      "3. id: 5390aefc20f70186a0ecd901   score: 0.45126605   abstract: In this paper we argue that how scientific collaborations share data is bound up in the ways in which they produce and acquire that data. We draw on ethnographic work with two robotic space exploration teams to show how each community's norms of \"data-sharing\" are best understood as arising not from the context of the use or exchange of data, but from the context of data production. Shifting our perspective back to the point of production suggests that digital artifacts are embedded in a broader data economy. We present implications for analysis of data in interactional context, and for introducing systems or policies that conflict with the value of data in its context of production.\n",
      "\n",
      "4. id: 5390a4cc20f70186a0e74b59   score: 0.355663   abstract: We report on the exploratory stages of multi-university, multi-research-site, multi-year effort to investigate and compare data practices in multiple cyberinfrastructure projects and their emerging virtual organizations. Our long-term goal is to understand the data practices and data management requirements of virtual organizations and their implications for the design and development of data digital libraries. We have constructed our own virtual organization as a participant-observer approach to the research. Results to date suggest that collaborative technologies are emergent and that defining and scoping the data products of collaborations continues to be problematic.\n",
      "\n",
      "5. id: 5390a37f20f70186a0e6d83b   score: 0.22695492   abstract: The management of globally distributed data requires capabilities that have been developed in the data grid, digital library, and archivist communities. The storage resource broker incorporates essential features from each of these communities to support international collaborations that share data collections, provide publication environments for scientific data collections, and sustain preservation environments.\n",
      "\n",
      "6. id: 55323c5545cec66b6f9dbd42   score: 0.22236224   abstract: This paper reports our ongoing work investigating the structural features of scientific collaboration based on metadata collected from a scientific data repository (SDR). The background literature is reviewed in supporting our claim that metadata collected from SDRs offer a complimentary data source to traditional publication metadata collected from digital libraries. Methodological considerations are discussed in association with using metadata from SDRs, including author name disambiguation and data parsing. Initial findings show that the network has some unique macro-level structural features while also in agreement with existing networks theories. Challenges due to inconsistent metadata quality control procedures are also discussed in an attempt to reinforce claims that metadata should be designed to support both domain specific retrieval and evaluation and assessment needs.\n",
      "\n",
      "7. id: 5390b3da20f70186a0ef5a95   score: 0.20561503   abstract: Scientific communities have long been concerned with the design and implementation of effective infrastructures for data access and collaborative scientific work. Recent studies have shown an increase in collaborative data generation and reuse. However, further improvements require a deeper understanding of the social and technological circumstances under which they emerge. To that effect we conduct in-situ observation study of a Nano-photonics Research Centre. We consider the artifact ecology that evolved from the Centre's common experimentation and data platform, the scientific practices, and the intricate interactions with digital artifacts that arise from the researchers' activities. We uncover the use of progress summaries for collaborative data interpretation and knowledge sharing. By studying this reputable collaborative scientific environment we (1) identified the factors that le\n",
      "\n",
      "8. id: 5390bded20f70186a0f49f94   score: 0.19253936   abstract: This special issue presents researchers' latest efforts to help the scientific community manage increasingly large repositories of data.\n",
      "\n",
      "9. id: 5390b2fc20f70186a0eee0df   score: 0.18862271   abstract: As data become scientific capital, digital libraries of data become more valuable. To build good tools and services, it is necessary to understand scientists’ data practices. We report on an exploratory study of habitat ecologists and other participants in the Center for Embedded Networked Sensing. These scientists are more willing to share data already published than data that they plan to publish, and are more willing to share data from instruments than hand-collected data. Policy issues include responsibility to provide clean and reliable data, concerns for liability and misappropriation of data, ways to handle sensitive data about human subjects arising from technical studies, control of data, and rights of authorship. We address the implications of these findings for tools and architecture in support of digital data libraries.\n",
      "\n",
      "10. id: 5390a6d920f70186a0e86590   score: 0.16211456   abstract: Powerful computers and high-speed networks are posing new challenges to scientific data management and are changing the way the scientists use computers today. Meanwhile, scientific communities are typically geographically distributed and the Internet is the main tool used to coordinate scientific research and share data from networked laboratories. The contribution of this paper is the identification and testing of factors important for the success of such Collaborative Scientific Environments (CSEs). We present an analysis of CSEs as virtual organizations whose participants must marshal competencies to manage scientific experiments. Focusing on data integration, we propose a framework that tries to model transparent integration of data and experiments across research institutions. The framework provides a general meta-model to capture the basic aspects of a CSE and a dataspace that mod\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1747949\n",
      "index                                        554e3f090cf22ca2c80f9ca7\n",
      "title                          An Arabic handwriting synthesis system\n",
      "authors                                                           NaN\n",
      "year                                                           2015.0\n",
      "venue                                             Pattern Recognition\n",
      "references                                   558c14a30cf20e727d0f5f85\n",
      "abstract            In this paper, we present an Arabic handwritin...\n",
      "id                                                            1747949\n",
      "clustered_labels                                                    2\n",
      "Name: 1747949, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390879220f70186a0d3d6f9   score: 0.98998636   abstract: Arabic characters are always in cursive script. Handwritten words were entered into an IBM PC via a graphics tablet and a segmentation process applied to the points; the length and the slope of each segment was then found, and the slope categorized into one of four directions. In the learning process, specifications on the strokes of each character are fed to the computer. In the recognition process, the parameters of each stroke are found and special rules applied to select the collection of strokes which best matches the features of one of the stored characters. The results are promising, and suggestions for improvements leading to 100% recognition are proposed.\n",
      "\n",
      "2. id: 558ae9dd612c41e6b9d3d418   score: 0.98835784   abstract: Graphical abstractDisplay Omitted A spatial representations for Arabic characters are proposed.A new approach to the skeletonization of handwriting images documents is introduced.Experiments were performed on the IFN/ENIT databases.The approach is successful even when using handwritten upper case English characters. Segmentation is the most challenging part of Arabic handwriting recognition due to the unique characteristics of Arabic writing that allow the same shape to denote different characters. An Arabic handwriting recognition system cannot be successful without using an appropriate segmentation method. In this paper, a very effective and efficient off-line Arabic handwriting recognition approach is proposed. The proposed approach has three stages. Firstly, all characters are simplified to single-pixel-thin images that preserve the fundamental writing characteristics. Secondly, the \n",
      "\n",
      "3. id: 5390a6d920f70186a0e87105   score: 0.9872773   abstract: The recognition of handwritten characters, words, and text arouses great interest today. To develop the best working system is subject of many papers published. With this paper, methods to improve the performance of existing word recognition systems are discussed. The availability of a sufficient data sets for training and testing the system assumed, optimization algorithms are presented. The usage of different feature sets and the combination of different recognizers are proposed. Tests with Arabic handwriting recognition systems using the reference IfN/ENIT-database show the usefulness of the proposed methods. An improvement of the recognition rate of up to 28% of the best single system is achieved.\n",
      "\n",
      "4. id: 5390b44620f70186a0ef843e   score: 0.97467697   abstract: Synthesizing handwritten-style characters is an interesting issue in today’s handwriting analysis field. The purpose of this study is to artificially generate training data, foster a deep understanding of human handwriting, and promote the use of the handwritten-style computer fonts, in which the individuality or variety of the synthesized characters is considered important. Research considering such two properties together, however, is very rare. In this paper, a handwriting model is proposed to synthesize various handwritten characters while preserving the writer’s individuality from a limited number of training data, using a statistical approach. The proposed model is verified in single- and multiple-stroke characters, such as Arabic numbers, small English letters, and Japanese Kanji letters. Synthesized characters are evaluated in three ways. First, they are analyzed visually using t\n",
      "\n",
      "5. id: 558bf5460cf2e30013db3af6   score: 0.9728308   abstract: This paper describes the Arabic handwriting recognition systems proposed by A2iA to the NIST OpenHaRT2013 evaluation. These systems were based on an optical model using Long Short-Term Memory (LSTM) recurrent neural networks, trained to recognize the different forms of the Arabic characters directly from the image, without explicit feature extraction nor segmentation.Large vocabulary selection techniques and n-gram language modeling were used to provide a full paragraph recognition, without explicit word segmentation. Several recognition systems were also combined with the ROVER combination algorithm. The best system exceeded 80% of recognition rate.\n",
      "\n",
      "6. id: 5390b72e20f70186a0f20b32   score: 0.9694979   abstract: The aim of this work is to fill a void in the literature of Arabic handwriting recognition by studying the performance of different feature extraction methods on online segmented Arabic characters. The contribution of this paper is to introduce a large database of segmented online handwritten Arabic characters and report the performance of various feature extraction techniques on the segmented characters to serve as a benchmark for any future work on the problem of online Arabic characters recognition.\n",
      "\n",
      "7. id: 539089d220f70186a0d9acf2   score: 0.96820134   abstract: An automatic off-line character recognition system for handwritten cursive Arabic characters is presented. A robust noise-independent algorithm is developed that yields skeletons that reflect the structural relationships of the character components. The character skeleton is converted to a tree structure suitable for recognition. A set of fuzzy constrained character graph models (FCCGM's), which tolerate large variability in writing, is designed. These models are graphs, with fuzzily labeled arcs used as prototypes for the characters. A set of rules is applied in sequence to match a character tree to an FCCGM. Arabic handwritings of four writers were used in the learning and testing stages. The system proved to be powerful in tolerance to variable writing, speed, and recognition rate.\n",
      "\n",
      "8. id: 539087ae20f70186a0d4cc82   score: 0.96411926   abstract: A personal computer-based Arabic character recognition system that performs three preprocessing stages sequentially, thinning, stroke segmentation, and sampling, is described. The eight-direction code used for stroke representation and classification, the character classification done at primary and secondary levels, and the contextual postprocessor used for error detection and correction are described. Experimental results obtained using samples of handwritten and typewritten Arabic words are presented.\n",
      "\n",
      "9. id: 5390878320f70186a0d3285f   score: 0.9533619   abstract: In spite of the progress of machine recognition techniques of Latin, Kana, and Chinese characters over the two past decades, the machine recognition of Arabic characters has remained almost untouched. In this correspondence, a structural recognition method of Arabic cursively handwritten words is proposed. In this method, words are first segmented into strokes. Those strokes are then classified using their geometrical and topological properties. Finally, the relative position of the classified strokes are examined, and the strokes are combined in several steps into a string of characters that represents the recognized word. Experimental results on texts handwritten by two persons showed high recognition accuracy.\n",
      "\n",
      "10. id: 5390ba0a20f70186a0f32863   score: 0.9488225   abstract: This paper proposes and contributes towards designing a complete system for off-line Arabic character recognition. The proposed system is specifically meant for Arabic handwriting recognition, but it equally works for the typed character recognition. It has various phases including preprocessing and segmentation. It also includes thinning phase and finds vertical and horizontal projection profiles. The recognition phase is managed by genetic algorithm. The genetic algorithm stands on feature extraction algorithm that defines six features for each segment. The algorithm, for Arabic handwriting recognition, obtained 90.46 recognition rate. The proposed system has been compared with other systems in the literature. It has achieved the second best recognition rate.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1712309\n",
      "index                                        559148230cf232eb904fb973\n",
      "title               Screen dynamics analysis-based adaptive frame ...\n",
      "authors             Puleum Bae, JeongGil Ko, Jaehong Ryu, Young-Ba...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 14th International Conferen...\n",
      "references                                   5390b64020f70186a0f19aaf\n",
      "abstract            The introduction of ubiquitously connected mob...\n",
      "id                                                            1712309\n",
      "clustered_labels                                                    3\n",
      "Name: 1712309, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a7f620f70186a0e9526e   score: 0.9231778   abstract: The significant increase in data transmission rates of mobile networks, improvement in camera/display technologies and the continuous rise in computational power of mobile devices has stimulated and supported the wide deployment of multimedia services to mobile platforms. However the wide diversity of these devices creates a need for efficient and seamless adaptation scheme that provides for the transparent and augmented use of these multimedia services.\n",
      "\n",
      "2. id: 5390ad5620f70186a0ebe2a9   score: 0.57075006   abstract: The network convergence of wired, wireless, and mobile systems creates a ubiquitous network environment where modern networking devices feature multiple networking interfaces and can connect to different networks simultaneously. Mobile users in ubiquitous networks expect to access information services anytime, and from anywhere. This paper presents a mobile content sharing scenario in which a networked device can discover neighboring devices and share multimedia content in a convenient, networked manner. This ideal scenario differs from the traditional usage which requires the tedious manual operations of connection setup and file transfer. To achieve this goal, this study proposes a user-provided multimedia content distribution architecture for a mobile and ubiquitous network environment. The proposed architecture integrates several specific mechanisms, including device discovery, async\n",
      "\n",
      "3. id: 539099a220f70186a0e18ef6   score: 0.38306415   abstract: Customized multimedia content delivery has become one of the most desirable applications to mobile device users. However its intense usage of wireless and user interfaces poses a great challenge to device usability and battery lifetime. In this paper, we describe our design and implementation of an energy-efficient multimedia messaging system to address this challenge. We construct a hierarchical system for users to access multimedia content, leveraging widely available short message service (SMS), an embedded system-based new interfacing device, and the Internet capability of mobile devices. Being the first of its type, the new system not only reduces energy overhead but also improves the usability of the service.\n",
      "\n",
      "4. id: 5390bded20f70186a0f487b7   score: 0.34037554   abstract: The availability of heterogeneous devices has rapidly changed the way people access the World Wide Web that includes rich content applications such as video streaming, 3D games, video conferencing, and mobile TV. However, most of these devices' (i.e., mobile phone, PDA, smartphone, and tablet) capabilities differ in terms of built-in software and library (what they can display), display size (how the content appears), and battery supply (how long the content can be displayed). In order for the digital contents to fit the target device, content adaptation is required. There have been many projects focused on energy-aware-based content adaptation that have been designed with different goals and approaches. This paper reviews some of the representative content adaptation solutions that have been proposed during the last few years, in relation to energy consumption focusing on wireless multi\n",
      "\n",
      "5. id: 5390b19020f70186a0ede6b0   score: 0.2791402   abstract: This paper presents a unique media content sharing ecosystem based on seamlessly established communication sessions between a device such as cellular phone and a remote web-enabled device with a larger display.\n",
      "\n",
      "6. id: 53908af920f70186a0daf250   score: 0.2656899   abstract: According to current trends in the field of mobile communication, image utilization for services that can be accessed from mobile devices using a wireless connection will likely become very common. The limitations in the mobile networks and mobile devices' capabilities, as well as the differences in their features, require specific modalities for transmitting and representing the visual information. In this paper we investigate the issues involved in image utilization within mobile e-services, and propose a method to manage the visual data utilization within WAP adaptive mobile e-services. We also present a method for delivering and representing the visual information at the end-user device that compensates for the limitations and enhances the mobile e-services' usability.\n",
      "\n",
      "7. id: 5390bf1320f70186a0f50cac   score: 0.21733753   abstract: Mobile users are continuously surrounded by heterogeneous (mobile) devices that provide a basis for direct multimedia interaction within the immediate physical context. However, current multimedia approaches are restricted to sense the availability of devices and their respective multimedia application content through Internet services that manage the locality of devices and content. Each service thereby manages only the narrow context of each device according to the respective application, preventing devices and approaches from sensing the diversity, spontaneity, and dynamics of mobile contexts and the physical interaction scope. Orthogonally, we facilitate direct and ubiquitous mobile multimedia sensing and interaction in 802.11 and 802.15.4 within the unrestricted physical context of mobile devices. Removing the need for Internet services or wireless networks, we derive a wireless ser\n",
      "\n",
      "8. id: 558b174c612c41e6b9d42e5d   score: 0.18653922   abstract: Mobile apps connect with counterpart cloud services and pursue data transfer over wireless networks. The amount of mobile data transferred will only increase as app data needs and usage expand. Energy expenditure during periods of data transfer constitutes a significant portion of a mobile's battery usage. At the same time, due to the evolving nature of wireless technologies, there is a proliferation of networks (such as public Wifi hotspots, access points, and cellular technologies) that exhibit diverse energy and performance characteristics. As a user moves around (even within the same logical network) the hardware serving data transfer varies, often offering opportunities for data transfer with distinct bandwidth and latency capabilities. Apps and data services in smartphones however are oblivious to this diversity and the energy impact of using one opportunity vs. others. In this pap\n",
      "\n",
      "9. id: 5390b29820f70186a0ee9be3   score: 0.16940588   abstract: The extensive coverage of wireless networks brings tremendous opportunities for messaging services to satisfy the demands of accessing multimedia and time-critical information from mobile users. This trend makes energy efficiency an increasing challenge on mobile devices. In this paper, we propose a novel solution toward meeting this challenge. We explore communications utilities along both \"cross-network\" and \"cross-device\" dimensions for managing multimedia messaging services. Along the cross-network dimension, we consider the multitude of networking capabilities at a mobile terminal and select the most energy-efficient communication medium to maintain the connectivity or accomplish data transfer. Along the cross-device dimension, we consider the multitude of user interface devices by which mobile users can further improve overall energy efficiency in consuming multimedia messages. In \n",
      "\n",
      "10. id: 5390b0ca20f70186a0eda2ca   score: 0.14535783   abstract: Modern mobile devices have become an important part of our daily life but the performance of multimedia applications still suffers from the constrained energy supply and communication bandwidth of the mobile devices. In this work, we develop an energy-efficient streaming system for mobile hotspots to achieve better Quality-of-Experience. Our main idea is (a) to avoid redundant 3G transmissions as well as reduce the usage of 3G links for those low residual-energy users, and (b) to enable nearby mobile users cooperatively to share the downloaded data via short-range interfaces. The experiment results shows our scheme can improve the system lifetime by 27%, and provide better throughput as well as lower loss rate than conversional 3G systems do.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1667978\n",
      "index                                        55915c8c0cf232eb904fbeaa\n",
      "title               Perfect Reconstructability of Control Flow fro...\n",
      "authors             Helge Bahmann, Nico Reissmann, Magnus Jahre, J...\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Architecture and Code Opti...\n",
      "references          53909a0220f70186a0e1f3bc;539087a520f70186a0d48...\n",
      "abstract            Demand-based dependence graphs (DDGs), such as...\n",
      "id                                                            1667978\n",
      "clustered_labels                                                    3\n",
      "Name: 1667978, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390882d20f70186a0d8e003   score: 0.92206234   abstract: Program analysis methods, especially those which support automatic vectorization, are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other. Powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept [AllK 82, KKLW 80].The dependence analysis in these systems is based on data dependence. In the presence of complex control flow, data dependence is not sufficient to transform programs because of the introduction of control dependences. A control dependence exists between two statements when the execution of one statement can prevent the execution of the other. Control dependences do not fit conveniently into dependence-based program translators.One solution is to convert all co\n",
      "\n",
      "2. id: 5390879220f70186a0d3cbde   score: 0.86647177   abstract: Previous attempts at vectorizing programs written in a sequential high level language focused on converting control dependences to data dependences using a mechanism known as IF-conversion. After IF-conversion vector optimizations are performed on a data dependence graph. However, IF-conversion is an irrevocable process which can introduce high run-time overhead if the input program is not amenable to vectorization.This paper uses a program dependence graph as the intermediate representation for a vectorizing compiler. A program dependence graph explicitly represents both control and data dependences, allowing guard values to be generated for vectorized statements. Techniques have been developed to perform code motion on vectorization candidates, to validly eliminate all unnecessary control and data dependence cycles, and to regenerate the newly vectorized program consistent with a topol\n",
      "\n",
      "3. id: 539087cb20f70186a0d59481   score: 0.7826625   abstract: Sparse program representations allow inter-statement dependences to be represented explicitly, enabling dataflow analyzers to restrict the propagation of information to paths where it could potentially affect the dataflow solution. This paper describes the use of a single sparse program representation, the value dependence graph, in both general and analysis-specific contexts, and demonstrates its utility in reducing the cost of dataflow analysis. We find that several semantics-preserving transformations are beneficial in both contexts.\n",
      "\n",
      "4. id: 53908bde20f70186a0dc784a   score: 0.7622471   abstract: We present a method to handle data- and control-flow information, represented as simplified system dependence graphs. As soon as a program is bigger than a few dozens of lines of code, its dependence graph becomes unreadable with standard drawing tools, since it contains far too many nodes and edges. In our approach, we propose to decompose the program into a hierarchy of groups that are likely to be of manageable size. We implemented a tool that first builds this hierarchy and stores it in a data base. A graphical interface allows then to browse this hierarchy to visualize the dependences of each group, to annote the nodes or groups and possibly to refine the proposed hierarchy.This paper introduces our approach for program decomposition, it describes our tool for dependence exploration and discusses the preliminary results we obtained with a few sample programs.\n",
      "\n",
      "5. id: 539087f920f70186a0d73690   score: 0.7599387   abstract: The program dependence graph, PDG, is used to represent the data and control dependencies between the statements of some program. The data dependencies between the statements are fully understood and they correspond to the definition-use chain. On the other hand the concept of control dependence is not fully explored. In this paper we formalize and analyze the concept or control dependence and present algorithms to find directly the control dependence subgraph of PDG in &Ogr;(N &agr;(N)), where N is the number of statements in the program.\n",
      "\n",
      "6. id: 5390a96e20f70186a0ea324a   score: 0.7440989   abstract: We define the Value State Dependence Graph (VSDG). The VSDG is a form of the Value Dependence Graph (VDG) extended by the addition of state dependence edges to model sequentialised computation. These express store dependencies and loop termination dependencies of the original program. We also exploit them to express the additional serialization inherent in producing final object code. The central idea is that this latter serialization can be done incrementally so that we have a class of algorithms which effectively interleave register allocation and code motion, thereby avoiding a well-known phase-order problem in compilers. This class operates by first normalizing the VSDG during construction, to remove all duplicated computation, and then repeatedly choosing between: (i) allocating a value to a register, (ii) spilling a value to memory, (iii) moving a loop-invariant computation within \n",
      "\n",
      "7. id: 539098dc20f70186a0e0cc53   score: 0.73866904   abstract: Scientific source code for high performance computers is extremely complex containing irregular control structures with complicated expressions. This complexity makes it difficult for compilers to analyze the code and perform optimizations. In particular with regard to program parallelization, complex expressions are often not taken intro consideration during the data dependence analysis phase. In this work we propose new data dependence analysis techniques to handle such complex instances of the dependence problem and increase program parallelization. Our method is based on a set of polynomial time techniques that can prove or disprove dependences in the presence of non-linear expressions, complex loop bounds, arrays with coupled subscripts, and if-statement constraints. In addition our algorithm can produce accurate and complete direction vector information enabling the compiler to app\n",
      "\n",
      "8. id: 5390877f20f70186a0d31723   score: 0.68899184   abstract: In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control a\n",
      "\n",
      "9. id: 53908cde20f70186a0dcf586   score: 0.6645403   abstract: The topic of intermediate languages for optimizing and parallelizing compilers has received much attention lately. In this paper, we argue that any good representation must have two crucial properties: first, the representation of a program must be a data structure that can be rapidly traversed to determine dependence information; second, the representation must be a program in its own right, with a parallel, local, model of execution. In this paper, we illustrate the importance of these points by examining algorithms for standard optimization-global constant propagation. We discuss the problems in working with current representations. Then, we propose a novel representation called the dependence flow graph which has each of the properties mentioned above. In this representation, dependencies are part of the computational mode, in that there is an algebra of operators over dependencies. \n",
      "\n",
      "10. id: 539087a520f70186a0d48e00   score: 0.66377795   abstract: A single method for normalizing the control-flow of programs to facilitate program transformations, program analysis, and automatic parallelization is presented. While previous methods result in programs whose control flowgraphs are reducible, programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods. In particular, all control-flow cycles are normalized into single-entry, single-exit while loops and all GOTOs are eliminated. Furthermore, the method avoids problems of code replication that are characteristic of node-splitting techniques. This restructuring obviates the control dependence graph, since afterwards control dependence relations are manifest in the syntax tree of the program. Transformations that effect this normalization are presented, and the complexity of the method\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1694774\n",
      "index                                        559247ee0cf28b1a968ff600\n",
      "title               On Second-Order Necessary Conditions for Broke...\n",
      "authors                                        Nikolai P. Osmolovskii\n",
      "year                                                           2015.0\n",
      "venue                 Journal of Optimization Theory and Applications\n",
      "references          5390b9d520f70186a0f315b8;5390af8820f70186a0ecf44b\n",
      "abstract            We consider optimal control problems with init...\n",
      "id                                                            1694774\n",
      "clustered_labels                                                    1\n",
      "Name: 1694774, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539087cb20f70186a0d5a02b   score: 0.98039377   abstract: This paper deals with state-constrained optimal control problems governed by a quasilinear elliptic equation. These constraints are given in an integral form and depend on the state and its gradient. Equality and inequality constraints are simultaneously considered. Existence of a solution is investigated and some optimality conditions are obtained with the aid of Ekeland's variational principle.\n",
      "\n",
      "2. id: 5390958920f70186a0dee22c   score: 0.9775   abstract: In this paper a general optimal control problem with pure state and mixed control-state constraints is considered. These constraints are of the form of set-inclusions. Second-order necessary optimality conditions for weak local minimum are derived for this problem in terms of the original data. In particular the nonemptiness of the set of critical directions and the evaluation of its support function are expressed in terms of the given functions and set-valued maps. In order that the Lagrange multiplier corresponding to the mixed control-state inclusion constraint be represented via an integrable function, a strong normality condition involving the notion of the critical tangent cone is introduced.\n",
      "\n",
      "3. id: 539087c320f70186a0d54e49   score: 0.9628831   abstract: The goal of this paper is to conduct a complete study of second-order conditions for the optimal control problem with mixed state-control constraints. The conjugate point theory is presented and a necessary condition in terms of the corresponding Riccati equation is obtained. Sufficiency criteria are developed in terms of strengthened necessary conditions, including the Riccati equation. The results generalize the known ones for pure control constraints as well as for the mixed state-control constraints.\n",
      "\n",
      "4. id: 539087d920f70186a0d607e7   score: 0.9590777   abstract: This paper is devoted to the study of necessary or sufficient second-order conditions for a weak local minimum in an optimal control problem. The problem is stated in the Mayer form and includes equality constraints both on the endpoints and on the state-control trajectory. The second-order conditions are stated through an associated linear--quadratic problem.\n",
      "\n",
      "5. id: 5390b19020f70186a0ededfb   score: 0.9563905   abstract: The main purpose of this paper is to show how the theory of augmentability in optimization can be applied to problems involving equality and inequality constraints. The problems considered need not be finite dimensional so that the theory can be generalized to calculus of variations or optimal control problems. In particular, an assumption of augmentability is used in place of normality to establish first and second order necessary conditions for a minimum for optimal control problems involving mixed equality constraints.\n",
      "\n",
      "6. id: 558b1efc612c41e6b9d43ff8   score: 0.9383503   abstract: Standard second order sufficient conditions in optimal control theory provide not only the information that an extremum is a weak local minimizer, but also tell us that the extremum is locally unique. It follows that such conditions will never cover problems in which the extremum is continuously embedded in a family of constant cost extrema. Such problems arise in periodic control, when the cost is invariant under time translations, in shape optimization, where the cost is invariant under Euclidean transformations (translations and rotations of the extremal shape), and other areas where the domain of the optimization problem does not really comprise elements in a linear space, but rather an equivalence class of such elements. We supply a set of sufficient conditions for minimizers that are not locally unique, tailored to problems of this nature. The sufficient conditions are in the spiri\n",
      "\n",
      "7. id: 5390a25820f70186a0e60586   score: 0.9326989   abstract: We investigate optimal control problems subject to mixed control-state constraints. The necessary conditions are stated in terms of a local minimum principle. By use of the Fischer-Burmeister function the minimum principle is transformed into an equivalent nonlinear and nonsmooth equation in appropriate Banach spaces. This nonlinear and nonsmooth equation is solved by a nonsmooth Newton's method. We will show the local quadratic convergence under certain regularity conditions and suggest a globalization strategy based on the minimization of the squared residual norm. A numerical example for the Rayleigh problem concludes the article.\n",
      "\n",
      "8. id: 5390893e20f70186a0d9380a   score: 0.9313377   abstract: This paper deals with necessary and sufficient optimality conditions for control problems governed by semilinear elliptic partial differential equations with finitely many equality and inequality state constraints. Some recent results on this topic for optimal control problems based upon results for abstract optimization problems are compared with some new results using methods adapted to the control problems. Meanwhile, the Lagrangian formulation is followed to provide the optimality conditions in the first case; the Lagrangian and Hamiltonian functions are used in the second statement. Finally, we prove the equivalence of both formulations.\n",
      "\n",
      "9. id: 53909fbd20f70186a0e424a3   score: 0.9293122   abstract: In this paper we derive second-order necessary conditions for optimality for an optimization problem with abstract constraints in Banach spaces. Results for the nondegenerate case derived earlier [H. Gfrerer, SIAM J. Control Optim., 45 (2006), pp. 972-997] are extended to the degenerate case. For the mathematical programming problem, where the constraints are given by equality and finitely many inequality constraints, our approach applies to the degenerate case, when the equality constraints are not regular; our results appear to be new even in this special case. Our second-order necessary conditions are contained in the gap between the standard necessary and sufficient conditions, where the only difference is the change from a nonstrict to a strict inequality. Our results are formulated in such a way to be applicable also to vector optimization problems.\n",
      "\n",
      "10. id: 539087eb20f70186a0d69c7a   score: 0.92696756   abstract: For a nonlinear optimal control problem with state constraints, we give conditions under which the optimal control depends Lipschitz continuously in the L2 norm on a parameter. These conditions involve smoothness of the problem data, uniform independence of active constraint gradients, and a coercivity condition for the integral functional. Under these same conditions, we obtain a new nonoptimal stability result for the optimal control in the $L^\\infty$ norm. And under an additional assumption concerning the regularity of the state constraints, a new tight $L^\\infty$ estimate is obtained. Our approach is based on an abstract implicit function theorem in nonlinear spaces.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1682533\n",
      "index                                        55916e9d0cf2e89307ca9c33\n",
      "title               Adaptive Analog Disparity Model using Ocular D...\n",
      "authors                      Sheena Sharma, Priti Gupta, C. M. Markan\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2nd International Conferenc...\n",
      "references          539089ab20f70186a0d95fa9;5390bae620f70186a0f3c...\n",
      "abstract            Biological vision system extracts depth from t...\n",
      "id                                                            1682533\n",
      "clustered_labels                                                    2\n",
      "Name: 1682533, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b72e20f70186a0f208e8   score: 0.81960475   abstract: Most biological approaches to disparity extraction rely on the disparity energy model (DEM). In this paper we present an alternative approach which can complement the DEM model. This approach is based on the multiscale coding of lines and edges, because surface structures are composed of lines and edges and contours of objects often cause edges against their background. We show that the line/edge approach can be used to create a 3D wireframe representation of a scene and the objects therein. It can also significantly improve the accuracy of the DEM model, such that our biological models can compete with some state-of-the-art algorithms from computer vision.\n",
      "\n",
      "2. id: 5390a05920f70186a0e49725   score: 0.6972998   abstract: Many models for stereo disparity computation have been proposed, but few can be said to be truly biological. There is also a rich literature devoted to physiological studies of stereopsis. Cells sensitive to binocular disparity have been found in the visual cortex, but it is not clear whether these cells could be used to compute disparity maps from stereograms. Here we propose a model for biological stereo vision based on known receptive field profiles of binocular cells in the visual cortex and provide the first demonstration that these cells could effectively solve random dot stereograms. Our model also allows a natural integration of stereo vision and motion detection. This may help explain the existence of units tuned to both disparity and motion in the visual cortex.\n",
      "\n",
      "3. id: 5390a6d920f70186a0e869ce   score: 0.59649193   abstract: Duplicating the full dynamic capabilities of the human eye-brain combinationis a difficult task but an important goal because of the wideapplication for a system which can acquire {\\it accurate} 3D models of a scene in realtime.Such a system must be able to correct images to remove lens distortionand camera misalignments from high resolution images at video framerates - 30 fps or better.The images then need to be matched todetermine the distance to scene objects.We have constructed a system whichuses reconfigurable hardware (FPGAs) to handle the very large number ofcalculations required and is capable of processing 1 Mpixel imagesfor disparity ranges of $\\sim 100$ (allowing $\\sim 1\\%$ depth accuracy)at 30 fps.This paper focuses on the use of \\LUT s in the hardwareto correct images in real time with latencies that are determined more bythe quality of the optics and mechanical alignment th\n",
      "\n",
      "4. id: 5390b8d720f70186a0f2ad69   score: 0.49299663   abstract: This paper presents a new hardware-oriented approach for the extraction of disparity maps from stereo images. The proposed method is based on the herein named Adaptive Census Transform that exploits adaptive support weights during the image transformation; the adaptively weighted sum of SADs is then used as the dissimilarity metric. Quality tests show that the proposed method reaches significantly better accuracy than alternative hardware-oriented approaches. To demonstrate the practical hardware feasibility, a specific architecture has been designed and its implementation has been carried out using a single FPGA chip. Such a VLSI implementation allows a frame rate up to 68fps to be reached for 640x480 stereo images, using just 80,000 slices and 32 RAM blocks of a Virtex6 chip.\n",
      "\n",
      "5. id: 5390a8dc20f70186a0e9edd1   score: 0.46144146   abstract: This paper studies the relevance of the conformal camera to computational vision with a particular focus on stereopsis. First we review projective Fourier analysis of the conformal camera and point to its unique attributes for modeling physiological aspects of perception. Then we design the head-eye-visual cortex integrated system with each eye modeled by the conformal camera. It provides a biologically realistic computational approach to the process of stereoscopic depth perception.\n",
      "\n",
      "6. id: 53908e0020f70186a0dd4cde   score: 0.3935598   abstract: Abstract: We present an architectural description of a visual sensor capable of extracting a dense depth map of the scene under consideration. The goal of the stereo depth perception is twofold: find an estimation of distance for each point in the image and a confidence measure of this operation. Stereoscopic depth estimation is based on disparity measurements. At functional level, in the flow of processing, we distinguish: 1) adaptive convolution with Gabor fillers for measuring phase, 2) regularization process based on local context information, 3) a comparison between left and right phase maps to determine disparity. The architecture has been validated with respect to both artificial and real-world images.\n",
      "\n",
      "7. id: 5390b4da20f70186a0f00448   score: 0.33720353   abstract: Depth information using the biological Disparity Energy Model can be obtained by using a population of complex cells. This model explicitly involves cell parameters like their spatial frequency, orientation, binocular phase and position difference. However, this is a mathematical model. Our brain does not have access to such parameters, it can only exploit responses. Therefore, we use a new model for encoding disparity information implicitly by employing a trained binocular neuronal population. This model allows to decode disparity information in a way similar to how our visual system could have developed this ability, during evolution, in order to accurately estimate disparity of entire scenes.\n",
      "\n",
      "8. id: 5390a01420f70186a0e47173   score: 0.3331777   abstract: We designed a VLSI binocular vision system that emulates the disparity computation in the primary visual cortex (V1). The system consists of two silicon retinas, orientation chips, and field programmable gate array (FPGA), mimicking a hierarchical architecture of visual information processing in the disparity energy model. The silicon retinas emulate a Laplacian-Gaussian-like receptive field of the vertebrate retina. The orientation chips generate an orientation-selective receptive field by aggregating multiple pixels of the silicon retina, mimicking the Hubel-Wiesel-type feed-forward model in order to emulate a Gabor-like receptive field of simple cells. The FPGA receives outputs from the orientation chips corresponding to the left and right eyes and calculates the responses of the complex cells based on the disparity energy model. The system can provide the responses of complex cells t\n",
      "\n",
      "9. id: 5390975920f70186a0dfca30   score: 0.24202643   abstract: The relative depth of objects causes small shifts in the left and right retinal positions of these objects, called binocular disparity. This letter describes an electronic implementation of a single binocularly tuned complex cell based on the binocular energy model, which has been proposed to model disparity-tuned complex cells in the mammalian primary visual cortex. Our system consists of two silicon retinas representing the left and right eyes, two silicon chips containing retinotopic arrays of spiking neurons with monocular Gabor-type spatial receptive fields, and logic circuits that combine the spike outputs to compute a disparity-selective complex cell response. The tuned disparity can be adjusted electronically by introducing either position or phase shifts between the monocular receptive field profiles. Mismatch between the monocular receptive field profiles caused by transistor m\n",
      "\n",
      "10. id: 5390a1e620f70186a0e59efd   score: 0.23952718   abstract: It might be a very effective way for computational vision model to utilize the productions of neurophysiology and biology. Retina does astonishing work in the early vision. In this paper, according to anatomic structure, a multi-layer digital retina model is presented to simulate biological retina and it is placed in a physical visual field of a reduced eye to analyze why the retina can be capable of fulfilling all tasks, and further more, to analyze the characteristics of every layer cell in retina in information processing. The model is designed to achieve a kind of balance among hardware complexity, computing load and performance on the condition of sufficient information collection. This research also contributes to the design and implementation of artificial retina chips to improve perception of visually impaired patients.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1727989\n",
      "index                                        55323d5145cec66b6f9de17f\n",
      "title               Efficient multiple time-stepping algorithms of...\n",
      "authors             Abdullah Demirel, Jens Niegemann, Kurt Busch, ...\n",
      "year                                                           2015.0\n",
      "venue                                Journal of Computational Physics\n",
      "references                                   558f81af0cf2cb5aa7674381\n",
      "abstract            Multiple time-stepping (MTS) algorithms allow ...\n",
      "id                                                            1727989\n",
      "clustered_labels                                                    1\n",
      "Name: 1727989, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390962020f70186a0df5a82   score: 0.86703575   abstract: Time integration of ODEs or time-dependent PDEs with required resolution of the fastest time scales of the system, can be very costly if the system exhibits multiple time scales of different magnitudes. If the different time scales are localised to different components, corresponding to localisation in space for a PDE, efficient time integration thus requires that we use different time steps for different components.We present an overview of the multi-adaptive Galerkin methods meG(q) and mdG(q) recently introduced in a series of papers by the author. In these methods, the time step sequence is selected individually and adaptively for each component, based on an a posteriori error estimate of the global error.The multi-adaptive methods require the solution of large systems of nonlinear algebraic equations which are solved using explicit-type iterative solvers (fixed point iteration). If t\n",
      "\n",
      "2. id: 53909e8b20f70186a0e2df89   score: 0.84645087   abstract: The numerical solution of time-dependent ordinary and partial differential equations presents a number of well known difficulties--including, possibly, severe restrictions on time-step sizes for stability in explicit procedures, as well as need for solution of challenging, generally nonlinear systems of equations in implicit schemes. In this note we introduce a novel class of explicit methods based on use of one-dimensional Padé approximation. These schemes, which are as simple and inexpensive per time-step as other explicit algorithms, possess, in many cases, properties of stability similar to those offered by implicit approaches. We demonstrate the character of our schemes through application to notoriously stiff systems of ODEs and PDEs. In a number of important cases, use of these algorithms has resulted in orders-of-magnitude reductions in computing times over those required by lead\n",
      "\n",
      "3. id: 5390880d20f70186a0d7b7f1   score: 0.8134608   abstract: In the numerical solution of ordinary differential equations, certain implicit linear multistep formulas, i.e. formulas of type ∑kj=0 &agr;jxn+j - h ∑kj=0 &bgr;jxn+j = 0, (1) with &bgr;k ≠ 0, have long been favored because they exhibit strong (fixed-h) stability. Lately, it has been observed [1-3] that some special methods of this type are unconditionally fixed-h stable with respect to the step size. This property is of great importance for the efficient solution of stiff [4] systems of differential equations, i.e. systems with widely separated time constants. Such special methods make it possible to integrate stiff systems using a step size which is large relative to the rate of change of the fast-varying components of the solution.\n",
      "\n",
      "4. id: 5390a06e20f70186a0e4d203   score: 0.79787195   abstract: In this paper, we present some improvements, in terms of accuracy and speed-up, for a particular well adapted Discontinuous Galerkin method devoted to the time-domain Maxwell equations. First, to reduce spurious modes on very distorted meshes, the addition of dissipative terms as penalization in the numerical scheme is studied and compared on examples. Second, in order to increase the efficiency of the method, a multi-class local time-stepping strategy is presented and its validation and advantages are highlighted on different examples.\n",
      "\n",
      "5. id: 53909f2d20f70186a0e39257   score: 0.7826625   abstract: Despite the popularity of high-order explicit Runge-Kutta (ERK) methods for integrating semi-discrete systems of equations, ERK methods suffer from severe stability-based time step restrictions for very stiff problems. We implement a discontinuous Galerkin finite element method (DGFEM) along with recently introduced high-order implicit-explicit Runge-Kutta (IMEX-RK) schemes to overcome geometry-induced stiffness in fluid-flow problems. The IMEX algorithms solve the non-stiff portions of the domain using explicit methods, and isolate and solve the more expensive stiff portions using an L-stable, stiffly-accurate explicit, singly diagonally implicit Runge-Kutta method (ESDIRK). Furthermore, we apply adaptive time-step controllers based on the embedded temporal error predictors. We demonstrate in a number of numerical test problems that IMEX methods in conjunction with efficient preconditio\n",
      "\n",
      "6. id: 558fa6360cf23638afbe6d52   score: 0.72865176   abstract: A new approach to the problem of numerically integrating stiff differential systems is described. In this approach a linear multistep method (the basic method) is split into a kind of predictor-corrector scheme, where the predictor is also implicit. If this splitting is done in an appropriate manner, the modified method has considerably better stability properties than the basic method. As a result, splitting methods are particularly useful for problems where conventional integration methods experience stability difficulties. In particular some highly stable split linear multistep methods based on backward differentiation formulae are derived and a highly stable variable step implementation is proposed.\n",
      "\n",
      "7. id: 5390a72320f70186a0e8b25f   score: 0.70344764   abstract: Locally refined meshes impose severe stability constraints on explicit time-stepping methods for the numerical simulation of time dependent wave phenomena. To overcome that stability restriction, local time-stepping methods are developed, which allow arbitrarily small time steps precisely where small elements in the mesh are located. When combined with a symmetric finite element discretization in space with an essentially diagonal mass matrix, the resulting discrete numerical scheme is explicit, is inherently parallel, and exactly conserves a discrete energy. Starting from the standard second-order “leap-frog” scheme, time-stepping methods of arbitrary order of accuracy are derived. Numerical experiments illustrate the efficiency and usefulness of these methods and validate the theory.\n",
      "\n",
      "8. id: 53909f2c20f70186a0e3700d   score: 0.6136997   abstract: In this dissertation, we introduce a new class of spectral time stepping methods for efficient and accurate solutions of ordinary differential equations (ODEs), differential algebraic equations (DAEs), and partial differential equations (PDEs). The methods are based on applying spectral deferred correction techniques as preconditioners to Picard integral collocation formulations, least squares based orthogonal polynomial approximations are computed using Gaussian type quadratures, and spectral integration is used instead of numerically unstable differentiation. For ODE problems, the resulting Krylov deferred correction (KDC) methods solve the preconditioned nonlinear system using Newton-Krylov schemes such as Newton-GMRES method. For PDE systems, method of lines transpose (MoLT) couples the KDC techniques with fast elliptic equation solvers based on integral equation formulations and fas\n",
      "\n",
      "9. id: 5390b20120f70186a0ee5c9a   score: 0.60159355   abstract: A variety of numerical calculations, especially when considering wave propagation, are based on the method-of-lines, where time-dependent partial differential equations (PDEs) are first discretized in space. For the remaining time-integration, low-storage Runge-Kutta schemes are particularly popular due to their efficiency and their reduced memory requirements. In this work, we present a numerical approach to generate new low-storage Runge-Kutta (LSRK) schemes with optimized stability regions for advection-dominated problems. Adapted to the spectral shape of a given physical problem, those methods are found to yield significant performance improvements over previously known LSRK schemes. As a concrete example, we present time-domain calculations of Maxwell's equations in fully three-dimensional systems, discretized by a discontinuous Galerkin approach.\n",
      "\n",
      "10. id: 5390b86b20f70186a0f2a281   score: 0.5961981   abstract: Locally refined meshes impose severe stability constraints on explicit time-stepping methods for the numerical simulation of time dependent wave phenomena. Local time-stepping methods overcome that bottleneck by using smaller time-steps precisely where the smallest elements in the mesh are located. Starting from classical Adams-Bashforth multi-step methods, local time-stepping methods of arbitrarily high order of accuracy are derived for damped wave equations. When combined with a finite element discretization in space with an essentially diagonal mass matrix, the resulting time-marching schemes are fully explicit and thus inherently parallel. Numerical experiments with continuous and discontinuous Galerkin finite element discretizations corroborate the expected rates of convergence and illustrate the usefulness of these local time-stepping methods.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1713920\n",
      "index                                        55323c3245cec66b6f9db790\n",
      "title               Patterns and evolution of coauthorship in Chin...\n",
      "authors                                          Jiang Li, Yueting Li\n",
      "year                                                           2015.0\n",
      "venue                                                  Scientometrics\n",
      "references          558b3d74612c41e6b9d475d9;558ae259612c41e6b9d3c...\n",
      "abstract            This paper examined the coauthorship patterns ...\n",
      "id                                                            1713920\n",
      "clustered_labels                                                    2\n",
      "Name: 1713920, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a79f20f70186a0e913e3   score: 0.98486924   abstract: Based on the data covered by China Social Science Citation Index (CSSCI), two scientometric models-rank-frequency distribution-exponential function and negative power function, concentration measurement are used. Quantitative characteristics of social science papers, citation were analyzed through distribution of region, institution and discipline in China. The results concern (a) No matter the index of papers upon social science, or the citation index reflects paper's influence, or the input index effects the output of papers upon social science, according to regions, organizations or academic discipline, their rank-frequency distribution modes are stable and unified, all of which are negative exponential distributions; (b) from the comparison of Beijing, Shanghai and from economics, politics and the region distribution of social science papers, it can find the self similarity phenomeno\n",
      "\n",
      "2. id: 5390a80f20f70186a0e974a7   score: 0.9743861   abstract: Collaboration patterns among scientists are becoming more and more complicated. Even sophisticated methods for taking the number of co-authors into account do not solve all problems related to the calculation of citation measures such as the h -index. In this article we introduce role-based h-indices and in particular the major contribution h-index, denoted as h-maj, which takes only those articles into account in which the scientist plays a major or core role. As an example we provide major contribution indices for scientists in the health sciences in China. Differences between the h-index and h-maj are shown for data based on the Web of Science (WoS), and separately, based on the China National Knowledge Infrastructure (CNKI) database. It is suggested to use the major contribution h-index as a supplementary index, especially in those fields where multiple â聙聵first authorsâ聙聶 and/or cor\n",
      "\n",
      "3. id: 558b2d84612c41e6b9d45c43   score: 0.97428846   abstract: This paper probes into the current status of collaboration regarding the field of the Chinese humanities and social sciences in respects of the degree of collaboration and the status of the relationships. It researches the status quo in humanities, the growth of social development science and cross-disciplinary social science, and the maturity of applied social science. In addition, it further highlights the important roles of economics, management, and library and information science in the collaboration network of humanities and social science with their extensive intra-disciplinary cooperation and crucial roles in the whole collaboration network.\n",
      "\n",
      "4. id: 5390bb7b20f70186a0f40650   score: 0.97334224   abstract: In 2010 and this for the first time in 20years the number of Chinese publications included in the Web of science dropped significantly. During the same period the USA kept a steady growth, although with a low rate of increase. This might indicate that China has reached a point of change in its growing speed in producing scientific publications. Why this could be the case is discussed in this short note.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f519f3   score: 0.97140163   abstract: The paper studied 211,946 articles indexed in Thomson Reuters's Web of Science from January 1st 2002 to December 31st 2011, in order to describe the growth and distribution of Chinese international research collaboration (IRC), from the perspective of amount, authors, countries, discipline fields and journals. By applying bibliometric and social network methods, this study provided the collaboration network of countries and fields. The main results were as follow: the number of article increased faster comparing with the stable growth in average annual of IRC degree; the articles collaborated with SAC are 80 % more than all IRC's; as to the fields, collaboration in Social science is at disadvantage, while the largest field is physics and the fastest field is molecular biology and genetics; mathematics, physics, multidisciplinary and space science had more in fluencies than others in corr\n",
      "\n",
      "6. id: 5390b0ca20f70186a0edb4fb   score: 0.9601375   abstract: International collaborative papers are increasingly common in journals of many disciplines. These types of papers are often cited more frequently. To identify the coauthorship trends within Library and Information Science (LIS), this study analyzed 7,489 papers published in six leading publications (ARIST, IP&M, JAMIA, JASIST, MISQ, and Scientometrics) over the last three decades. Logistic regression tested the relationships between citations received and seven factors: authorship type, author's subregion, country income level, publication year, number of authors, document type, and journal title. The main authorship type since 1995 was national collaboration. It was also the dominant type for all publications studied except ARIST, and for all regions except Africa. For citation counts, the logistic regression analysis found all seven factors were significant. Papers that included intern\n",
      "\n",
      "7. id: 5390adfc20f70186a0ec5103   score: 0.937669   abstract: In this work, a bibliometric research method was used where co-authorships are regarded as an indicator of international research collaboration of Turkish, Greek, Polish, and Portuguese scientists; for comparison purposes scientists from the mainstream countries such as Belgium, Denmark, Switzerland, Norway, Sweden, and the Netherlands. The seventeen years (1990--2006) of scientific research collaboration of the aforementioned countries with the G7 nations (France, Germany, Italy, Japan, UK, USA, and Canada) was examined by using ISI Web of Science Database. Findings reveal that, Turkey clearly is experiencing a remarkable proportional co-authorship growth rate with the G7 countries in comparison to Greece, Poland, Portugal, Denmark, Norway, Netherlands, Belgium, Switzerland, and Sweden, for the period of 1990--2006.\n",
      "\n",
      "8. id: 5390aefc20f70186a0ecdfa4   score: 0.9360516   abstract: We study global and local Q-measures, as well as betweenness centrality, as indicators of international collaboration in research. After a brief review of their definitions, we introduce the concepts of external and internal inter-group geodesics. These concepts are applied to a collaboration network of 1129 researchers from different countries, which is based on publications in bibliometrics, informetrics, webometrics, and scientometrics (BIWS in short) from the period 1990---2009. It is thus illustrated how international collaboration (among authors from different countries) in BIWS is carried out. Our results suggest that average scores for local Q-measures are typically higher, indicating a relatively low degree of international collaboration in BIWS. The dominating form of international collaboration is bilateral, whereas multilateral collaboration is relatively rare in the field of\n",
      "\n",
      "9. id: 5390ad0720f70186a0ebad97   score: 0.9339146   abstract: The issue of primary interest to this study is the collaboration that has taken place in science and technology (S&T) research in China. Due to our empirical evidences, the regions with higher relationship (network) capital enjoy higher knowledge productivity in terms of published articles. Our purpose in this paper is to investigate the relationships that exist between regional published articles and co-authorship in China covering the period from 1998 to 2007 by using Stata to investigate the relation between the regional publications and co-authored published articles. As main findings, the greater the number of co-authored articles that a region has, the greater their success, in terms of the number of articles published. Indeed, both domestic and international co-authorship have had positive effects on published article levels in China.\n",
      "\n",
      "10. id: 5390ac5720f70186a0eb6b91   score: 0.9314625   abstract: We argue that the communication structures in the Chinese social sciences have not yet been sufficiently reformed. Citation patterns among Chinese domestic journals in three subject areas—political science and Marxism, library and information science, and economics—are compared with their counterparts internationally. Like their colleagues in the natural and life sciences, Chinese scholars in the social sciences provide fewer references to journal publications than their international counterparts; like their international colleagues, social scientists provide fewer references than natural sciences. The resulting citation networks, therefore, are sparse. Nevertheless, the citation structures clearly suggest that the Chinese social sciences are far less specialized in terms of disciplinary delineations than their international counterparts. Marxism studies are more established than politi\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1721777\n",
      "index                                        55323cc045cec66b6f9dcce9\n",
      "title               Banach space projections and Petrov---Galerkin...\n",
      "authors                                                     Ari Stern\n",
      "year                                                           2015.0\n",
      "venue                                           Numerische Mathematik\n",
      "references                                   558ff6e90cf28af999b581e2\n",
      "abstract            We sharpen the classic a priori error estimate...\n",
      "id                                                            1721777\n",
      "clustered_labels                                                    2\n",
      "Name: 1721777, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558d7b720cf2c779a6476224   score: 0.5791624   abstract: We consider some extensions of the classical error estimates for mixed problems. We take into account, in particular, nonconforming approximations and the splitting of the bilinear formb(·,·), intob 1(·,·) andb 2(·,·). We use this last fact to sharpen some estimates in a case where the forma(·,·) does not have coercivity in the right norm. We also consider duality results in the nonconforming case.Nous considérons quelques extensions des résultats classiques d'estimation d'erreur pour les problèmes mixtes. Nous étudions en particulier l'effet de l'emploi d'une approximation non conforme et celui de la décomposition de la formeb(·,·) enb 1(·,·) etb 2(·,·). Nous employons cette dćomposition pour affiner certaines estimations dans le cas où la formea(·,·) n'est pas coercive dans la bonne norme. Nous considérons aussi des résultats de dualité dans le cas non conforme.\n",
      "\n",
      "2. id: 558ff7510cf2351542718cca   score: 0.409634   abstract: Let us suppose that Ω is an open \\\"regular\\\" subset ¿ n which canbe triangulated by finite elements.Letu be an element of the Sobolev spaceW k+1,p (Ω),k+1¿n/p>0 and ¿u an interpolant ofu such that ¿u¿¿¿ m, p, Ω , ¿C m h k+1-m |u| k+1,p,Ω , 0¿m¿k+1 (cf. [3]).Using the theory of Kernels of Banach spaces we show thatC m can be exactly evaluated.Soientu¿W k+1,p (Ω),k+1¿n/p>0, Ω ouert ¿régulier¿ de ¿ n admettant une triangulationC et ¿u une interpolation deu sur Ω par la méthode des éléments finis telle que: $$\\\\left\\\\| {u - \\\\pi u} \\\\right\\\\|_{m,p,\\\\Omega } \\\\leqq C_m h^{k + 1 - m} \\\\left| u \\\\right|_{k + 1,p,\\\\Omega } ,0 \\\\leqq m \\\\leqq k + 1(cf.[3]).$$ En utilisant la théorie des noyaux d'espaces de Banach on fournit des explicitations précises deC m qui permettent d'évaluer cette constante.\n",
      "\n",
      "3. id: 53909eef20f70186a0e362cc   score: 0.39082456   abstract: In this paper we consider new error estimates under the weighted Chebyshev norm for Galerkin method to an airfoil equation. Galerkin discretisation is discussed. The smoothness conditions of the input functions are improved, i.e., they need to be Holder continuous with 12\n",
      "\n",
      "4. id: 53908d6520f70186a0dd0b9c   score: 0.17695588   abstract: In this paper we propose quasi-optimal error estimates, in various norms, for the Streamline-Upwind Petrov-Galerkin (SUPG) method applied to the linear one-dimensional advection-diffusion problem. We follow the classical argument due to Babuska and Brezzi, therefore the goal of this work is the proof of the inf-sup and of the continuity conditions for the bilinear stabilized variational form, with respect to suitable norms. These norms are suggested by our previous work, in which we analyze the continuous multi-dimensional advection-diffusion operator. We obtain these results by means of functional spaces interpolation.\n",
      "\n",
      "5. id: 5390ad5620f70186a0ebe9a5   score: 0.1732882   abstract: An a priori error analysis of discontinuous Galerkin methods for a general elliptic problem is derived under a mild elliptic regularity assumption on the solution. This is accomplished by using some techniques from a posteriori error analysis. The model problem is assumed to satisfy a Gårding type inequality. Optimal order L 2 norm a priori error estimates are derived for an adjoint consistent interior penalty method.\n",
      "\n",
      "6. id: 53909e8a20f70186a0e2d4ed   score: 0.17189364   abstract: In this paper, we provide a priori and a posteriori error analyses of an augmented mixed finite element method with Lagrange multipliers applied to elliptic equations in divergence form with mixed boundary conditions. The augmented scheme is obtained by including the Galerkin least-squares terms arising from the constitutive and equilibrium equations. We use the classical Babuska-Brezzi theory to show that the resulting dual-mixed variational formulation and its Galerkin scheme defined with Raviart-Thomas spaces are well posed, and also to derive the corresponding a priori error estimates and rates of convergence. Then, we develop a reliable and efficient residual-based a posteriori error estimate and a reliable and quasi-efficient Ritz projection-based one, as well. Finally, several numerical results illustrating the performance of the augmented scheme and the associated adaptive algori\n",
      "\n",
      "7. id: 558fea110cf28fa910316d14   score: 0.1482937   abstract: On établit des majorations explicites de I'erreur de meilleure approximation polynomiale ainsi que des majorations explicites et nonexplicites de I'erreur d'interpolation de Lagrange, lorsque la fonction considérée appartient à un espace de Sobolev d'ordre non entier défini sur un ouvert borné de ¿ n .Les résultats obtenus généralisent les résultats connus dans le cas des espaces de Sobolev d'ordre entier.Explicit bounds for the best polynomial approximation error, explicit and non-explicit bounds for the Lagrange interpolation error are derived for functions belonging to fractional order Sobolev spaces defined over a bounded open set in ¿ n .Thus the classical results of the integer order Sobolev spaces are extended.\n",
      "\n",
      "8. id: 53909fbd20f70186a0e424cf   score: 0.14402843   abstract: It is shown that the interelement discontinuities in a discontinuous Galerkin finite element approximation are subordinate to the error measured in the broken $H^1$-seminorm. One consequence is that the DG-norm of the error is equivalent to the broken energy seminorm. Computable a posteriori error bounds are obtained for the error measured in both the DG-norm and the broken energy seminorm and are shown to be efficient in the sense that they also provide lower bounds up to a constant and higher order data oscillation terms. The estimators are completely free of unknown constants and provide guaranteed numerical bounds for the error.\n",
      "\n",
      "9. id: 55922f9e0cf2ceaae74c8f14   score: 0.14151853   abstract: This paper introduces a new locking---free formulation that combines the discontinuous Galerkin methods with weakly over-penalized techniques for Reissner---Mindlin plates. We derive optimal a priori error estimates in both the energy norm and $$L^2$$L2 norm for polynomials of degree $$k=2$$k=2, and we extend the results concerning the energy norm to higher-order polynomial degrees. Numerical tests confirm our theoretical predictions.\n",
      "\n",
      "10. id: 5390adfd20f70186a0ec5df1   score: 0.13117322   abstract: In Guo (2005) [9], the a posteriori error estimators for Galerkin spectral methods andp-version finite element methods in the one-dimensional case and their analysis as regards reliable and efficient properties were given. In this paper, some improved a posteriori error estimators are proposed which have simpler forms than the ones given in [9], and so can be easily calculated in practical applications. In particular, these a posteriori error estimates are independent of any information about solutions of Galerkin spectral methods or p-version finite element methods, and are determined solely by the known right-hand side term. It is proved that these a posteriori error estimators have the same reliability and efficiency as the ones in [9].\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1651084\n",
      "index                                        559133a00cf232eb904fb388\n",
      "title               Authenticity, Relatability and Collaborative A...\n",
      "authors             John Vines, Peter C. Wright, David Silver, Mag...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390b64020f70186a0f192ca;5390b86b20f70186a0f29...\n",
      "abstract            Health and care providers are increasingly loo...\n",
      "id                                                            1651084\n",
      "clustered_labels                                                    3\n",
      "Name: 1651084, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390995c20f70186a0e143e9   score: 0.3220994   abstract: This paper describes changes in the use of e-health services over a 5-year period. It compares findings from two large-scale questionnaire studies undertaken in 2000 and 2005. Key changes in usage and trust practices are noted with patients \"acting as scientists\" using web sites to test out theories regarding their health. Future analyses regarding trust and identity markers are discussed.\n",
      "\n",
      "2. id: 558b796f612c6b62e5e8969b   score: 0.26855758   abstract: We report findings from a study that set out to explore the experience of older people living with assisted living technologies and care services. We find that successful `ageing in place' is socially and collaboratively accomplished --- `co-produced' --- day-to-day by the efforts of older people, and their formal and informal networks of carers (e.g. family, friends, neighbours). First, we reveal how `bricolage' allows care recipients and family members to customise assisted living technologies to individual needs. We argue that making customisation easier through better design must be part of making assisted living technologies `work'. Second, we draw attention to the importance of formal and informal carers establishing and maintaining mutual awareness of the older person's circumstances day-to-day so they can act in a concerted and coordinated way when problems arise. Unfortunately, \n",
      "\n",
      "3. id: 5390b7fe20f70186a0f26881   score: 0.22151907   abstract: This paper introduces the peer-supported design process undertaken in the creation of a novel online knowledge sharing application called the Haystack Exchange. Along with seven undergrad and graduate students involved in a course research group, the authors of this paper presented a fully functional online prototype of an application designed to connect those seeking knowledge work with those willing to do that work, creating an outlet for knowledge workers to share and contribute effort. Students were engaged as active co-designers in the system, examining existing applications online offering similar services, discussing relevant research in building online communities, and ultimately re-designing the system to make it context-appropriate for different deployment scenarios. This paper reports on the details of this unique design process, discussing its merits, implications, and the pr\n",
      "\n",
      "4. id: 5390b78a20f70186a0f23360   score: 0.15662019   abstract: In order to foster the relationship between geographically distant grandparents and grandchildren, a prototype of an online platform is developed in an Ambient Assisted Living project. After identifying relevant attributes in the requirements analysis together with older adults and experts for children, we conducted two rounds of user studies in a laboratory setting with older adults. In the studies we were not only interested in the usability of the platform and the older participants' computer skills, but especially in the experiences the older users have when interacting with and via the platform. As expected, we found a relation between self-rated computer skills and the usability problems. However, the skills were not decisive for experiencing the interaction regarding curiosity, engagement, social connectedness and social presence. Finally, implications for the design of socially c\n",
      "\n",
      "5. id: 55914ab80cf232eb904fb9f6   score: 0.15165494   abstract: This paper reports on a study of motivations for the use of peer-to-peer or sharing economy services. We interviewed both users and providers of these systems to obtain different perspectives and to determine if providers are matching their system designs to the most important drivers of use. We found that the motivational models implicit in providers' explanations of their systems' designs do not match well with what really seems to motivate users. Providers place great emphasis on idealistic motivations such as creating a better community and increasing sustainability. Users, on the other hand are looking for services that provide what they need whilst increasing value and convenience. We discuss the divergent models of providers and users and offer design implications for peer system providers.\n",
      "\n",
      "6. id: 5390a63c20f70186a0e818a1   score: 0.12002562   abstract: This review provides a snapshot of the literature in online communities for healthcare consumers. It summarizes the features commonly used by healthcare consumers in online communities: seeking and sharing personal experiences, opinions and answers, and exchanging social support. This review also identifies behaviors that are commonly practiced by healthcare consumers but are not readily supported in current online communities. These include collaborative healthcare decision-making, conducting social comparison, and lurking in online communities. This review concludes by emphasizing the importance of trust, privacy and safety when designing an online community for healthcare consumers, particularly in the age of Web 2.0.\n",
      "\n",
      "7. id: 5390b52620f70186a0f0447d   score: 0.11940814   abstract: The aging population is growing rapidly and technology has great potential to meet older adults' healthcare needs. However, the technologies being developed must take into account older adults' needs and related interaction issues. This workshop explores interaction issues such as accepting, integrating, efficacy, sharing, and privacy for emerging health technologies, including tablets, ambient systems, robotics, electronic medical records, mobile systems, and tracking and monitoring devices. We also consider the user characteristics of care recipients, informal caregivers, and formal caregivers. An outcome of this workshop will be the development of use cases to provide guidance for designing technologies for older adults and their caregivers.\n",
      "\n",
      "8. id: 5390a88c20f70186a0e994e7   score: 0.110279374   abstract: We performed a content analysis of the information shared in a locally and culturally focused health application, EatWell. In EatWell, information is shared via the creation of audio recordings. Our results highlight the reflective nature of these recordings, in particular, 1) the topics discussed in these reflections as well as their tone, 2) how these reflections were contextualized (locally and culturally) and 3) how system users addressed one another in their reflections. We compare our findings with the dominant technological approach to supporting health information exchange amongst lay people: online support groups. In particular, we reflect upon why, though many of the community-building features of online support groups did not translate into EatWell, our users felt a sense of community empowerment. Based on our results, we discuss implications for designing locally and cultural\n",
      "\n",
      "9. id: 5390a7f520f70186a0e93bc9   score: 0.06500873   abstract: In this research we examine the generation specific acceptance motives of eHealth technologies in order to assess the likelihood of success for these new technologies. 280 participants (14 - 92 years of age) volunteered to participate in a survey, in which using motives and barriers toward smart home technologies were explored. The scenario envisaged was the use of a medical stent implemented into the body, which monitors automatically the health status and which is able to remotely communicate with the doctor. Participants were asked to evaluate the pros and cons of the usage of this technology, their acceptance motives and potential utilization barriers. In order to understand the complex nature of acceptance, personal variables (age, technical expertise, health status), individual's cognitive concepts toward ageing as well as perceived usefulness were related. Outcomes show that trust\n",
      "\n",
      "10. id: 5390b95420f70186a0f2df37   score: 0.05572029   abstract: In this paper we describe a multi-faceted approach for engaging older adults in the design and evaluation of a tablet application. Our approach consisted of five key elements: 1) involving care providers in the research, 2) conducting social events, 3) supporting use of the technology through scaffolding, 4) providing multiple channels of communication between participants and researchers, and 5) progressively revealing the technology functions. We explored this approach with seven participants (aged 71-92) who used a prototype iPad application to engage in peer-to-peer communication. We found that each of the five elements contributed to successfully engaging older people in the development and evaluation process. This paper provides insights into suitable approaches for designing technologies with older users, illustrating the importance of creating a supportive environment and employi\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1663791\n",
      "index                                        559120e70cf232eb904fae27\n",
      "title               Rapid Prototyping of Wireless Physical Layer M...\n",
      "authors             James Chacko, Cem Sahin, Douglas Pfiel, Nagara...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 ACM/SIGDA Internationa...\n",
      "references                                   5390a4cc20f70186a0e7569f\n",
      "abstract            This paper describes a step by step approach i...\n",
      "id                                                            1663791\n",
      "clustered_labels                                                    3\n",
      "Name: 1663791, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908d6520f70186a0dd21c9   score: 0.98083925   abstract: Abstract: In this paper, we provide an overview of the Hardware-Software design flow that has been applied to the design of a platform based SoC for the HiperLAN/2 and IEEE 802.11a wideband wireless communication standards. Starting from a high-level description in MATLAB, the design framework is used to gain important information for the HW/SW partitioning. Step by step the MATLAB description is refined down to an embedded assembler implementation. Simultaneously, the framework is used to generate cycle true reference data for simulations on several abstraction levels. A universal interface concept allows the exchange of modules of different abstraction levels in a cosimulation. This way, a high confidence level for the design verification is achieved, and design and verification time is substantially reduced.\n",
      "\n",
      "2. id: 53909eef20f70186a0e34938   score: 0.89478946   abstract: We provide an overview of a MATLAB/sup TM/ based hardware-software design flow that has been applied to the design of a platform based SoC for the HiperLAN/2 and IEEE 802.11 wideband wireless communication standards. Starting from a high-level algorithmic description, the MATLAB/sup TM/ environment serves as a \"golden model\" and universal, cycle-true testbench for the embedded hardware and software implementation. A universal interface concept allows the exchange of modules with different abstraction levels in a cosimulation. This way, a high confidence level for the design validation is achieved, and both design and validation time is substantially reduced.\n",
      "\n",
      "3. id: 5390ad0720f70186a0ebb445   score: 0.84221536   abstract: With the multiplication of mobile and wireless communication networks and standards, the physical layer of communication systems (i.e. the modem part of the system) has to be completely flexible. This assumption leads to the well known Software Defined Radio concept which enables the implementation and the deployment of different waveforms on the same platform. This concept has been widely investigated since the early 2000's mainly for processors and Sw approach but less for reconfigurable Hw or DSP implementation. This paper deals with a specific architecture and an innovative design methodology which were designed within the framework of a fully flexible high data rate Software Defined Radio wireless modem. This approach is focused on the waveform part of the system and its goal is to reach a fully flexible physical layer. In case of modem evolutions or upgrades, it enables to avoid si\n",
      "\n",
      "4. id: 53909f6920f70186a0e3acaa   score: 0.5794599   abstract: To assess the performance of forthcoming 4th generation wireless local area networks, the algorithmic functionality is usually modelled using a high-level mathematical software package, for instance, Matlab. In order to validate the modelling assumptions against the real physical world, the high-level functional model needs to be translated into a prototype. A systematic system design methodology proves very valuable, since it avoids, or, at least reduces, numerous design iterations. In this paper, we propose a novel Matlab-to-hardware design flow, which allows to map the algorithmic functionality onto the target prototyping platform in a systematic and reproducible way. The proposed design flow is partly manual and partly tool assisted. It is shown that the proposed design flow allows to use the same testbench throughout the whole design flow and avoids time-consuming and error-prone in\n",
      "\n",
      "5. id: 5390baa120f70186a0f399c1   score: 0.56997234   abstract: This paper using hierarchical design method completes the OLT/ONU physical layer design and Implementation according to the EPON agreement requirements. Compared with the traditional EPON physical layer chip it has many advantages, such as low price, adjustable rate, low bit error rate and it can be reprogrammed. This flexible modular design methods can be used in the design of XPON system.\n",
      "\n",
      "6. id: 5390ad8920f70186a0ec06ff   score: 0.5306774   abstract: This paper presents design and verification methods of Toshiba's wireless LAN (WLAN) baseband SoCs. An FPGA-based high-speed and reliable verification environment for physical layer (PHY), a new SDL-based hardware design method for media access control layer (MAC), and an ultra low power design resulting in power consumption of 22 uW in the deep-sleep mode are described.\n",
      "\n",
      "7. id: 5390b95420f70186a0f2e6c6   score: 0.5192623   abstract: Verification and design-space exploration of today's embedded systems require the simulation of heterogeneous aspects of the system, i.e., software, hardware, communications. This work shows the use of SystemC to simulate a model-driven specification of the behavior of a networked embedded system together with a complete network scenario consisting of the radio channel, the IEEE~802.15.4 protocol for wireless personal area networks and concurrent traffic sharing the medium. The paper describes the main issues addressed to generate SystemC modules from Matlab/Stateflow descriptions and to integrate them in a complete network scenario. Simulation results on a healthcare wireless sensor network show the validity of the approach.\n",
      "\n",
      "8. id: 5390aca820f70186a0eb7f9a   score: 0.5054319   abstract: In this paper, we present WARPLab, a framework for the rapid prototyping and implementation of physical layer algorithms for wireless LANs. WARPLab is based on WARP, an FPGA-based, wireless experimental platform. We first give an overview of WARPLab and then present an example of a physical layer algorithm that we implemented on the framework for our work. We then discuss the features, limitations, and future modifications as they pertain to our research.\n",
      "\n",
      "9. id: 53908bcc20f70186a0dc5ad2   score: 0.4946291   abstract: The increasing complexity of modern wireless basebandsystems demands the use of rapid prototyping methodologiesto provide an early estimation of system functionalityand performance. In order to achieve an efficient hardwareemulation, the complete system including the analog partshould be prototyped. In this work we present synthesizabledescriptions of a communication channel module anda sigma delta modulator, suitable for fundamental emulationof wireless baseband environments. By avoiding timeintensivehardware/software cosimulation, a great speedupof the system verification can be attained. In order to assistthe designer and speed up the design process we alsocreated an environment for automatically customizing themodules according to a specific scenario.\n",
      "\n",
      "10. id: 5390af8920f70186a0ed10b6   score: 0.4649623   abstract: In this paper, we introduce an efficient design flow for Globally Asynchronous Locally Synchronous systems, which can be used by designers without prior knowledge of asynchronous circuits. The design flow starts with a high-level description model of the system in Simulink and ends with a hardware implementation in an FPGA or a standard-cell ASIC. We have developed a tool in MATLAB, so that the designer easily can include asynchronous logic in a conventional synchronous design flow. The proposed design flow is currently used for implementing the physical layer for a wireless LAN.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674813\n",
      "index                                        558da0480cf222bc17bbfdc2\n",
      "title                                   Why Do Children Abuse Robots?\n",
      "authors             Tatsuya Nomura, Takayuki Uratani, Takayuki Kan...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Tenth Annual ACM/IEEE Inter...\n",
      "references          559150940cf232eb904fbb7f;5390a30b20f70186a0e6aeb2\n",
      "abstract            We found that children sometimes abuse a socia...\n",
      "id                                                            1674813\n",
      "clustered_labels                                                    2\n",
      "Name: 1674813, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 559150940cf232eb904fbb7f   score: 0.9993175   abstract: Social robots working in public space often stimulate children's curiosity. However, sometimes children also show abusive behavior toward robots. In our case studies, we observed in many cases that children persistently obstruct the robot's activity. Some actually abused the robot by saying bad things, and at times even kicking or punching the robot. We developed a statistical model of occurrence of children's abuse. Using this model together with a simulator of pedestrian behavior, we enabled the robot to predict the possibility of an abuse situation and escape before it happens. We demonstrated that with the model the robot successfully lowered the occurrence of abuse in a real shopping mall.\n",
      "\n",
      "2. id: 558c2cbf0cf25dbdbb05268f   score: 0.97112906   abstract: Studies of different robot features and behaviors are leading to the development of models for more effective peer-based interaction between robots and children.\n",
      "\n",
      "3. id: 5390a8b220f70186a0e9cb6c   score: 0.9507779   abstract: This study examined the interactions between children and robots by observing whether children help a robot complete a task under five conditions to determine which elicited the most help. Each condition had an experimental and control group, with 20-32 children (even number of boys and girls) in each group. Visitors to a science centre located in a major Western Canadian city were invited to participate in an experiment set up at the centre. Their behaviors with a robot, a small 5 degree of freedom robot arm, were observed. Results of chi-square analyses indicated that children are most likely to help a robot after being introduced to it, X2(1) = 4.15, p = .04. This condition was the only one of five tested that demonstrated a significant increase in children's helping behaviors. These results suggest that an adult's demonstrated positive introduction to a robot impacts children's helpi\n",
      "\n",
      "4. id: 5390aefc20f70186a0ecd4c6   score: 0.91398406   abstract: This paper seeks to describe the knowledge and perceptions of robots that have a group of 19 children under 11 years old in Mexico City has, interacting with a teleoperated anthropomorphic robot compared with a group of 18 children with the same characteristics that have not interacted with same robot. We seek to focus a precedent in the Mexican children in their context for future development of social robots.\n",
      "\n",
      "5. id: 5390aefc20f70186a0ecd509   score: 0.88700897   abstract: In this study we have tried to determine if the cultural background of children has an influence on how they interact with robots. Children of different age groups and cultures played a card guessing game with a robot (iCat). By using perception tests to evaluate the children's emotional response it was revealed that children from South Asia (Pakistani) were much more expressive than European children (Dutch) and younger children were more expressive than the older ones in the context of child robot interaction.\n",
      "\n",
      "6. id: 5390b20120f70186a0ee5615   score: 0.8161134   abstract: We present insights gleaned from a series of child-robot interaction experiments carried out in a hospital paediatric department. Our aim here is to share good practice in experimental design and lessons learned about the implementation of systems for social HRI with child users towards application in \"the wild\", rather than in tightly controlled and constrained laboratory environments: a trade-off between the structures imposed by experimental design and the desire for removal of such constraints that inhibit interaction depth, and hence engagement, requires a careful balance.\n",
      "\n",
      "7. id: 53909e8a20f70186a0e2ce76   score: 0.6633419   abstract: Children's perceptions and evaluations of different robot designs are an important unexplored area within robotics research considering that many robots are specifically designed for children. To examine children's feelings and attitudes towards robots, a large sample of children (N=159) evaluated 40 robot images by completing a questionnaire for each image, which enquired about robot appearance, robot personality dimensions and robot emotions. Results showed that depending on a robot's appearance children clearly distinguished robots in terms of their intentions (i.e. friendly vs. unfriendly), their capability to understand, and their emotional expression. Results of a principal components analysis of the children's ratings of the robots' personality attributes revealed two dimensions labelled 'Behavioural Intention' and 'Emotional Expression'. Robots were classified according to their \n",
      "\n",
      "8. id: 539096cb20f70186a0df726a   score: 0.66061044   abstract: This study examined preschool children's reasoning about and behavioral interactions with one of the most advanced robotic pets currently on the retail market, Sony's robotic dog AIBO. Eighty children, equally divided between two age groups, 34-50 months and 58-74 months, participated in individual sessions that included play with and an interview about two artifacts: AIBO and a stuffed dog. Results showed similarities in children's reasoning about the two artifacts, but differences in their behavioral interactions. Discussion focuses on how robotic pets, as representative of an emerging technological genre in HCI, may be (a) blurring foundational ontological categories, and (b) impacting children's social and moral development. More broadly, results inform on our understanding of the human-robotic relationship.\n",
      "\n",
      "9. id: 5390b13020f70186a0edc3bb   score: 0.6532375   abstract: This study addresses the attitudes of children toward robots displaying various degrees of anthropomorphic appearance. Understanding the means by which children perceive and evaluate robots across the spectrum of anthropomorphism is a crucial issue within the field of robotics research. This study conducted two experiments to understand children's attitudes toward robots with various degrees of realism and examine whether gender or age influences the social and physical attraction children feel toward humanoid robots. The results of the study suggest that when designing robots for children, designers need not focus on creating an authentic human-like appearance. In addition, the influence of children's age on their attitudes toward robots is less significant than that of gender. Generally, children aged from 8 to 14 years have similar attitudes to and perceptions of humanoid robots. An i\n",
      "\n",
      "10. id: 5390a54720f70186a0e78c6b   score: 0.648355   abstract: This article overviews the current and diverse paradigms about a social robot. We discuss and contextualize key aspects about the mechanisms used by this type of robot to relate with humans, other robots and its surrounded social environment: interaction, communication, physical and environmental embodiment and autonomy. We enrich current definitions and discuss about the crucial role of the purpose of this type of robots within nowadays society.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1688398\n",
      "index                                        559159050cf2127aa930cd2b\n",
      "title               Block Programs: Improving Efficiency of Verifi...\n",
      "authors                          Gang Xu, George Amariucai, Yong Guan\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 10th ACM Symposium on Infor...\n",
      "references          558b17b8612c41e6b9d42ef3;5390a17720f70186a0e51...\n",
      "abstract            In the cloud computing paradigm, clients outso...\n",
      "id                                                            1688398\n",
      "clustered_labels                                                    3\n",
      "Name: 1688398, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b7fe20f70186a0f2716f   score: 0.94853723   abstract: Outsourced computations (where a client requests a server to perform some computation on its behalf) are becoming increasingly important due to the rise of Cloud Computing and the proliferation of mobile devices. Since cloud providers may not be trusted, a crucial problem is the verification of the integrity and correctness of such computation, possibly in a public way, i.e., the result of a computation can be verified by any third party, and requires no secret key -- akin to a digital signature on a message. We present new protocols for publicly verifiable secure outsourcing of Evaluation of High Degree Polynomials and Matrix Multiplication. Compared to previously proposed solutions, ours improve in efficiency and offer security in a stronger model. The paper also discusses several practical applications of our protocols.\n",
      "\n",
      "2. id: 5390bf1320f70186a0f504e7   score: 0.9400236   abstract: As the cloud computing paradigm has gained prominence, the need for verifiable computation has grown urgent. Protocols for verifiable computation enable a weak client to outsource difficult computations to a powerful, but untrusted, server. These protocols provide the client with a (probabilistic) guarantee that the server performed the requested computations correctly, without requiring the client to perform the computations herself.Surprisingly powerful protocols for verifying outsourced computations were discovered within the computer science theory community in the 1980s and 1990s, in the form of interactive proofs and their brethren. However, these protocols were considered to be primarily of theoretical interest, far too inefficient for actual deployment. This thesis seeks to overturn this viewpoint.We make progress along two interrelated directions. The first seeks to render inter\n",
      "\n",
      "3. id: 5390bae620f70186a0f3c4ea   score: 0.92007446   abstract: In the Cloud Computing paradigm, a user often reduces financial, personnel, and computational burdens by outsourcing computation and other IT services to a professional service provider. However, to be able to assure the correctness of the result, the user still needs to perform the verification himself. Such verification may be tedious and expensive. Consequently, users are likely to outsource (again) the verification workload to a third party. Other scenarios such as auditing and arbitrating may also require the use of thirdparty verification. Outsourcing verification will introduce new security challenges. One such challenge is to protect the computational task and the results from the untrusted third party verifier. In this work, we address this problem by proposing an efficient verification outsourcing scheme. To our knowledge, this is the first solution to the verification outsourc\n",
      "\n",
      "4. id: 5390b71120f70186a0f1ed0c   score: 0.89982784   abstract: As the cloud computing paradigm has gained prominence, the need for verifiable computation has grown increasingly urgent. Protocols for verifiable computation enable a weak client to outsource difficult computations to a powerful, but untrusted server, in a way that provides the client with a guarantee that the server performed the requested computations correctly. By design, these protocols impose a minimal computational burden on the client, but they require the server to perform a very large amount of extra bookkeeping to enable a client to easily verify the results. Verifiable computation has thus remained a theoretical curiosity, and protocols for it have not been implemented in real cloud computing systems. In this paper, we assess the potential of parallel processing to help make practical verification a reality, identifying abundant data parallelism in a state-of-the-art general-\n",
      "\n",
      "5. id: 558b0e3d612c41e6b9d41b2a   score: 0.8293538   abstract: In the new cloud computing paradigm, outsourcing computation is a fundamental principle. Among its various aspects, the correctness of the computation result remains paramount. This motivates the birth of verifiable computation, which aims at efficiently checking the result for general-purpose computation. Although significant progress has been made in verifiable computation towards practice, the verifier's workload still remains too high. Only through batching or amortizing the very expensive investment over a large number of computation instances, can the verifiers cost be less than re-computing the computation task from the scratch. In the work of delegation of verification (PODC'13), Xu et al. proposes that the client can also outsource (again) the verification to a third party. However, whether this idea is feasible in large scale network is not clear. In this paper, we propose to a\n",
      "\n",
      "6. id: 5390b29820f70186a0ee98ef   score: 0.82796735   abstract: When delegating computation to a service provider, as in the cloud computing paradigm, we seek some reassurance that the output is correct and complete. Yet recomputing the output as a check is inefficient and expensive, and it may not even be feasible to store all the data locally. We are therefore interested in what can be validated by a streaming (sublinear space) user, who cannot store the full input, or perform the full computation herself. Our aim in this work is to advance a recent line of work on \"proof systems\" in which the service provider proves the correctness of its output to a user. The goal is to minimize the time and space costs of both parties in generating and checking the proof. Only very recently have there been attempts to implement such proof systems, and thus far these have been quite limited in functionality. Here, our approach is two-fold. First, we describe a ca\n",
      "\n",
      "7. id: 558b1b07612c41e6b9d436a6   score: 0.8230441   abstract: With the tremendous growth of cloud computing, verifiable computation has been firstly formalized by Gennaro et al. and then studied widely to provide integrity guarantees in the outsourced computation. However, existing verifiable computation protocols either work in the secret key setting or in the public key setting, namely, work either for single client or for all clients, which rules out some practical applications with access control policies. In this paper, we introduce and formalize the notion of verifiable computation with access control (AC-VC), in which only the computationally weak clients with necessary access control permissions can be allowed by a trusted source to apply the outsourced computation of a function to a server. We present a formal security definition and a proved secure black-box construction for AC-VC. This construction is built based on any verifiable comput\n",
      "\n",
      "8. id: 5390bf1320f70186a0f50b24   score: 0.7634837   abstract: Computation outsourcing to the cloud has become a popular application in the age of cloud computing. Recently, two protocols for secure outsourcing scientific computations, i.e., linear equation solving and linear programming solving, to the cloud were proposed. In this paper, we improve the work by proposing new protocols that achieve significant performance gains. For linear equation solving outsourcing, we achieve the improvement by proposing a completely new protocol. The new protocol employs some special linear transformations and there are no homomorphic encryptions and interactions between the client and the cloud, compared with the previous protocol. For linear programming outsourcing, we achieve the improvement by reformulating the linear programming problem in the standard and natural form. We also introduce a method to reduce the key size by using a pseudorandom number generat\n",
      "\n",
      "9. id: 554a4a4e0cf2c69e964873fc   score: 0.7479843   abstract: Cloud computing offers a great opportunity to bridge the gap between the fast growing computation needs and limited local resources. However, without the adequate trust and strong integrity assurance, it would be difficult to expect clients to completely turn over control of their computation to the cloud. Hence, securing cloud computation becomes an imperative and challenging task, especially in the aspect of integrity verification. To address the challenge, we propose a hassle-free, fixed-rate, and job-based software as a service cloud model along with the integrity verification mechanisms, with particular focus on outsourcing the widely applicable engineering optimization problem, i.e., convex optimization. We aim to construct efficient integrity verification mechanisms using application-specific techniques. Our security design does not require the use of heavy cryptographic tools. In\n",
      "\n",
      "10. id: 5390ad0720f70186a0ebb742   score: 0.70385486   abstract: Secure outsourcing of computation to an untrusted (cloud) service provider is becoming more and more important. Pure cryptographic solutions based on fully homomorphic and verifiable encryption, recently proposed, are promising but suffer from very high latency. Other proposals perform the whole computation on tamper-proof hardware and usually suffer from the the same problem. Trusted computing (TC) is another promising approach that uses trusted software and hardware components on computing platforms to provide useful mechanisms such as attestation allowing the data owner to verify the integrity of the cloud and its computation. However, on the one hand these solutions require trust in hardware (CPU, trusted computing modules) that are under the physical control of the cloud provider, and on the other hand they still have to face the challenge of run-time attestation. In this paper we f\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1655910\n",
      "index                                        55912d4b0cf232eb904fb1d8\n",
      "title                      Interaction Design as a Bricolage Practice\n",
      "authors                                 Anna Vallgårda, Ylva Fernaeus\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Ninth International Confere...\n",
      "references          558bd2c60cf20e727d0f2433;558bd2b70cf2e30013db1...\n",
      "abstract            With this paper we propose bricolage as an int...\n",
      "id                                                            1655910\n",
      "clustered_labels                                                    0\n",
      "Name: 1655910, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ab8820f70186a0eaf85e   score: 0.8267118   abstract: Interaction design is the study of devices with which a user can interact, in particular computer users. The practice typically centers around embedding information technology into the ambient social complexities of the physical world.It can also apply to other types of non-electronic products and services, and even organizations. Interaction design defines the behavior of an artifact or system in response to its users. Malcolm McCullough has written, As a consequence of pervasive computing, interaction design is poised to become one of the main liberal arts of the twenty-first century\n",
      "\n",
      "2. id: 5390a80f20f70186a0e971f2   score: 0.5854567   abstract: We describe an approach for a class of tangible interaction elements that are applicable across a broad variety of interactive systems. These tangibles share certain physical, visual, tagging, and software conventions, while fostering diversity in many aspects of design and function. Building on related techniques using paper and graspable artifacts as interactive embodiments of digital information, we propose several fixed and free parameters, present illustrative examples and applications, and discuss the resulting design space.\n",
      "\n",
      "3. id: 558bd2b70cf2e30013db17da   score: 0.55615854   abstract: Ajna is a musical cabinet made from a rich composition of acoustic materials and designed to perform digitally composed music. In this paper, we aim to unpack the design as well as key aspects of the design process that lead up to this unique artwork. We base our analysis on interviews with its two creators as well as on observations of Ajna performing in different contexts. From the perspective of interaction design, we first analyse the process of its making through the negotiations between physical form, temporal from, and the interactive gestalts. Lastly, we place these negotiations in a larger picture of bricolage as a design approach. Based on this we then discuss the qualities of bricolage in interaction design.\n",
      "\n",
      "4. id: 558e35e50cf2c779a6477d26   score: 0.5343695   abstract: Tangible interactions have incorporated new materials into interaction design as well as new perspectives on how those materials can be made useful. The dissertation research I discuss here examines materials and forms in terms of how they both mediate information but also can represent meanings in their own right. My work emphasizes these meanings as an important aspect of the experience of tangible interactions.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f52070   score: 0.5292178   abstract: The computer is no longer the center of attention. Thus, what we design is no longer the interface to the computer. Rather, what we design is a thing or an environment in which a computer might be used to create certain desired effects. Indeed, interaction design in a sense becomes the practice of giving form to artifacts or environments rather like any of the other design disciplines that we have know for centuries. However, giving form to computational things is highly complex and somewhat different than most other form-giving practices due to its temporal form element--its ability to change between states. Thus, an interaction design practice needs to encompass this temporal form giving in combination with physical form giving and performances of the interaction gestalt. In this paper, I propose this trinity of forms as a framework to unfold the practice of interaction design. I furth\n",
      "\n",
      "6. id: 539098dc20f70186a0e0d4b3   score: 0.4933018   abstract: Computational technology is, in interaction design practice, not just a medium providing neutral technical implementations; it is an expressive design material. In all design practice we need to understand and know how to handle our basic means of expression. The purpose of this paper is to sketch the outline of a methodology for describing and defining basic expressional properties of computational technology. Through stories of interaction, and on basis of these stories we outline exercises aiming at a methodology for aesthetic practice --- in interaction design practice as well as in interaction design teaching.\n",
      "\n",
      "7. id: 5390881220f70186a0d7e69c   score: 0.4758184   abstract: This paper proposes a `bricolage' approach to designing systems forcooperative work. This involves users, participatory designers andethnographers in a continuing cycle of design and revised work practice,often in settings where resources are limited and short-term results arerequired. If exploits the flood to market of hardware, software and services.The approach is illustrated with results from a project with a practice oflandscape architects. Their work is analysed in terms of communities ofpractice and actor networks. These perspectives help to identify the`socilities' of people and technologies and of the relationships betweenthem. They help to distinguish different forms of cooperation with differingsupport needs, opportunities and vulnerabilities. They inform the designof technical support, the assessment of outcomes, and the design of furthersolutions, in a cycle of `situated exp\n",
      "\n",
      "8. id: 5390a37f20f70186a0e6c341   score: 0.46386927   abstract: We propose a new perspective, seeing interactivity that is the immaterial part of an interactive artifact as something concretely describable and perceivable as we do with physical materials. In order to examine the validity of this proposal, we extracted a set of interactivity attributes to be used as a design language for thinking and describing interactivity in a new way, and conducted an online survey with 14 Flash prototypes representing pairs of values of 7 interactivity attributes we extracted. The result showed that all the interactivity attributes were significant, and participants experienced distinctive and meaningful emotional effects for different interactivity attributes.\n",
      "\n",
      "9. id: 5390a63c20f70186a0e82a7c   score: 0.46280688   abstract: This research investigates novel methods and techniques along with tool support that result from a conceptual blend of human-computer interaction with design practice. Using blending theory with material anchors as a theoretical framework, we frame both input spaces and explore emerging structures within technical, cognitive, and social aspects. Based on our results, we will describe a framework of the emerging structures and will design and evaluate tool support within a spatial, studio-like workspace to support collaborative creativity in interaction design.\n",
      "\n",
      "10. id: 5390ba3820f70186a0f373f2   score: 0.4344013   abstract: Tangible user interfaces utilize our ability to interact with everyday objects in order to manipulate virtual data. Designers and engineers usually follow the rule \"form follows function\", they support an existing interaction with a purpose-built interface. Still, we do not fully exploit the expressiveness of forms, materials and shapes of the nondigital objects we interact with. Therefore, we propose to invert the design process: we empower materiality to inspire the implementation of tangible interactions. Glass objects were chosen as an example of culturally and structurally rich objects: in a three-month workshop, these glass objects were transformed into interactive artefacts. In the paper, we present three resulting contributions: First, we describe our inverted design process as a tool for the stimulation of multidisciplinary development. Second, we derive a list of material-induc\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672412\n",
      "index                                        559158070cf2127aa930cce0\n",
      "title                      Distributed Search over Encrypted Big Data\n",
      "authors             Mehmet Kuzu, Mohammad Saiful Islam, Murat Kant...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 5th ACM Conference on Data ...\n",
      "references          5390b3ae20f70186a0ef3599;5390bfa220f70186a0f53...\n",
      "abstract            Nowadays, huge amount of documents are increas...\n",
      "id                                                            1672412\n",
      "clustered_labels                                                    3\n",
      "Name: 1672412, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b63320f70186a0f177ce   score: 0.99021614   abstract: In recent years, due to the appealing features of cloud computing, large amount of data have been stored in the cloud. Although cloud based services offer many advantages, privacy and security of the sensitive data is a big concern. To mitigate the concerns, it is desirable to outsource sensitive data in encrypted form. Encrypted storage protects the data against illegal access, but it complicates some basic, yet important functionality such as the search on the data. To achieve search over encrypted data without compromising the privacy, considerable amount of searchable encryption schemes have been proposed in the literature. However, almost all of them handle exact query matching but not similarity matching, a crucial requirement for real world applications. Although some sophisticated secure multi-party computation based cryptographic techniques are available for similarity tests, th\n",
      "\n",
      "2. id: 5390be6620f70186a0f4bd38   score: 0.9635748   abstract: The advent of cloud computing has dramatically changed the IT scene, as it offers cost savings and improvements to major operations. Nevertheless, the major obstacle relies on the effort on how to secure sensitive data files that are outsourced to the cloud environment. To ensure confidentiality, the sensitive data are usually encrypted prior to being outsourced. Nevertheless, effective data utilization remains a challenging task and there is a clear need for a secure and efficient searching mechanism over the encrypted data in the cloud, to increase the us-ability of the secure cloud environment. Unfortunately, existing work in the area of secure searching in the outsourcing scenario usually incur high computational complexity, which makes the approach impractical. In this paper, we take one step ahead by proposing an efficient keyword search scheme for cloud computing. Our solution is \n",
      "\n",
      "3. id: 558b2df0612c41e6b9d45d16   score: 0.9487276   abstract: The remote storage service has been one of the most popular cloud services. However, outsourcing in the remote storage causes a privacy issue such as the exposure of personal data and the leakage of private information. In this paper, we focus on the encrypted keyword search problem, called searchable symmetric encryption in cryptography, to reserve the user privacy and the data confidentiality in cloud storage services. Even though many works have been introduced, they still suffer from limitations in their practical use due to high search costs and huge index size. We present a new keyword search mechanism based on the bitmap index that represents a set of files in the inverted index form. The proposed mechanism has fast search time and small index size compared with the previous works in practical use. We show that it can be directly applied to practical services by testing the known \n",
      "\n",
      "4. id: 5390bae620f70186a0f3c51b   score: 0.9470882   abstract: With the increasing popularity of cloud computing, huge amount of documents are outsourced to the cloud for reduced management cost and ease of access. Although encryption helps protecting user data confidentiality, it leaves the well-functioning yet practically-efficient secure search functions over encrypted data a challenging problem. In this paper, we present a privacy-preserving multi-keyword text search (MTS) scheme with similarity-based ranking to address this problem. To support multi-keyword search and search result ranking, we propose to build the search index based on term frequency and the vector space model with cosine similarity measure to achieve higher search result accuracy. To improve the search efficiency, we propose a tree-based index structure and various adaption methods for multi-dimensional (MD) algorithm so that the practical search efficiency is much better than\n",
      "\n",
      "5. id: 5390be6620f70186a0f4c6da   score: 0.937783   abstract: In recent years, database as a service (DAS) model where data management is outsourced to cloud service providers has become more prevalent. Although DAS model offers lower cost and flexibility, it necessitates the transfer of potentially sensitive data to untrusted cloud servers. To ensure the confidentiality, encryption of sensitive data before its transfer to the cloud emerges as an important option. Encrypted storage provides protection but it complicates data processing including crucial selective record retrieval. To achieve selective retrieval over encrypted collection, considerable amount of searchable encryption schemes have been proposed in the literature with distinct privacy guarantees. Among the available approaches, oblivious RAM based ones offer optimal privacy. However, they are computationally intensive and do not scale well to very large databases. On the other hand, al\n",
      "\n",
      "6. id: 5390ba3820f70186a0f3683b   score: 0.92206234   abstract: Advances in cloud computing and Internet technologies have pushed more and more data owners to outsource their data to remote cloud servers to enjoy with huge data management services in an efficient cost. However, despite its technical advances, cloud computing introduces many new security challenges that need to be addressed well. This is because, data owners, under such new setting, loss the control over their sensitive data. To keep the confidentiality of their sensitive data, data owners usually outsource the encrypted format of their data to the untrusted cloud servers. Several approaches have been provided to enable searching the encrypted data. However, the majority of these approaches are limited to handle either a single keyword search or a Boolean search but not a multikeyword ranked search, a more efficient model to retrieve the top documents corresponding to the provided key\n",
      "\n",
      "7. id: 5390b0ca20f70186a0ed963d   score: 0.9026096   abstract: In cloud computing, clients usually outsource their data to the cloud storage servers to reduce the management costs. While those data may contain sensitive personal information, the cloud servers cannot be fully trusted in protecting them. Encryption is a promising way to protect the confidentiality of the outsourced data, but it also introduces much difficulty to performing effective searches over encrypted information. Most existing works do not support efficient searches with complex query conditions, and care needs to be taken when using them because of the potential privacy leakages about the data owners to the data users or the cloud server. In this paper, using on line Personal Health Record (PHR) as a case study, we first show the necessity of search capability authorization that reduces the privacy exposure resulting from the search results, and establish a scalable framework f\n",
      "\n",
      "8. id: 5390b29820f70186a0eea422   score: 0.9007046   abstract: The advent of cloud computing has ushered in an era of mass data storage in remote servers. Remote data storage offers reduced data management overhead for data owners in a cost effective manner. Sensitive documents, however, need to be stored in encrypted format due to security concerns. But, encrypted storage makes it difficult to search on the stored documents. Therefore, this poses a major barrier towards selective retrieval of encrypted documents from the remote servers. Various protocols have been proposed for keyword search over encrypted data (commonly referred to as searchable encryption) to address this issue. Oblivious RAM type protocols offer secure search over encrypted data, but are too expensive to be used in practical applications. Unfortunately, all of the symmetric key based encryption protocols leak data access patterns due to efficiency reasons. In this poster, we are\n",
      "\n",
      "9. id: 5390bf1320f70186a0f4feda   score: 0.8908652   abstract: With the rise of cloud computing, cloud storage has been widely supported and applied. But the security and search problems of encryption data in cloud storage imperil the wide promotion of cloud storage. And users may not fully trust that Cloud Providers can be reluctant to store their files on Cloud Storage Services. This paper discusses several encryption algorithms that have been proposed in areas of cipher text search and gives a solution of encrypted cloud storage based on present research. The solution allows users to store their data on a public cloud securely, while also allows for searching through the user's encrypted data. Users are able to submit encrypted keyword queries and the system will find all files that contain such keywords through different algorithms schemes. The system is designed based on that trust from a public cloud provider is not required. A further advanta\n",
      "\n",
      "10. id: 5390bd1520f70186a0f439a2   score: 0.8889517   abstract: Cloud computing technologies become more and more popular every year, as many organizations tend to outsource their data utilizing robust and fast services of clouds while lowering the cost of hardware ownership. Although its benefits are welcomed, privacy is still a remaining concern that needs to be addressed. We propose an efficient privacy-preserving search method over encrypted cloud data that utilizes minhash functions. Most of the work in literature can only support a single feature search in queries which reduces the effectiveness. One of the main advantages of our proposed method is the capability of multi-keyword search in a single query. The proposed method is proved to satisfy adaptive semantic security definition. We also combine an effective ranking capability that is based on term frequency-inverse document frequency (tf-idf) values of keyword document pairs. Our analysis \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1703674\n",
      "index                                        55323b1e45cec66b6f9d95c4\n",
      "title               A low distortion CMOS analogue switch with hig...\n",
      "authors             Maliang Liu, Jingyu Wang, Zhangming Zhu, Wei G...\n",
      "year                                                           2015.0\n",
      "venue                Analog Integrated Circuits and Signal Processing\n",
      "references                                   5390b56a20f70186a0f06eb4\n",
      "abstract            A novel bootstrapped analogue switch with high...\n",
      "id                                                            1703674\n",
      "clustered_labels                                                    2\n",
      "Name: 1703674, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a6d920f70186a0e87905   score: 0.89478946   abstract: This paper presents a novel low distortion CMOS bootstrapped switch that adopts a “source track” technique to track the real source terminal of the sampling switch. This technique improves nonlinear distortion due to variation of the gate overdrive and the threshold voltage in conventional switches, acquiring a precise sampling of signal. Based on the SMIC 0.18μm standard CMOS process, the proposed switch has been simulated at 200MSPS. The simulation results show the spurious-free dynamic range (SFDR) is 73.6 dB and total harmonic distortion (THD) is -79.8 dB.\n",
      "\n",
      "2. id: 558ad58a612c41e6b9d3b250   score: 0.82810634   abstract: A phase-dependent gain-transition correlated double-sampling technique is proposed and applied to a 10-bit 75-MS/s pipelined analog-to-digital converter. This reduces the accumulation of predictive error of each multiplying digital-to-analog converter stage due to the finite gain of the operational amplifiers, without the need for additional capacitors and switches at the input. With a 10-MHz sinusoidal input, a prototype fabricated in a 0.13-$\\\\mu{\\\\rm m}$ CMOS process has a 56.90 dB signal-to-noise plus distortion ratio (SNDR) and a 64.57 dB spurious-free dynamic range (SFDR) at 75 MS/s. For a 37 MHz input at full sampling rate, the SNDR and SFDR are 55.01 and 60.77 dB, respectively. The IC has an active area of 0.65 ${\\\\rm mm}^{2}$ and consumes 32 mW with a 1.2 V supply.\n",
      "\n",
      "3. id: 539096cb20f70186a0df6dea   score: 0.7018153   abstract: In this paper a new 8-bit 50-M sample/s CMOS digital-to-analog converter (DAC) is presented. The circuit employs 9 operational transconductance amplifiers (OTAs) and CMOS transistors as switching circuit. The proposed DAC is simulated using SPICE simulation program with 1.2 μm CMOS technology. Simulation results verify good performance of the circuit.\n",
      "\n",
      "4. id: 5390aaf920f70186a0eaec99   score: 0.70069003   abstract: This brief presents an 8-bit 120-MS/s time-interleaved pipeline analog-to-digital converter (ADC) fully based on MOS discrete-time parametric amplification. The ADC, fabricated in a 130-nm CMOS logic process, features an active area below 0.12mm2, where only MOS devices are used. Measurement results for a 20-MHz input signal shows that the ADC achieves 39.7 dB of signal-to-noise ratio, 49.3 dB of spurious-free dynamic range, -47.5 dB of total harmonic distortion, 39.1 dB of signal-to-noise-plus-distortion ratio, and 6.2 bits of peak effective number of bits while consuming less than 14 mW from a 1.2-V supply.\n",
      "\n",
      "5. id: 53908b9320f70186a0dc0a33   score: 0.6139312   abstract: A modified architecture of the second-order switched-capacitor modulator is proposed. A simple four-transistor variable gain attenuator is included in the architecture which continuously adjusts the reference voltage of the quantizer feedback. This improves the output SNR for small signal input and reduces the harmonic distortion for large signal input. Simulation results show that it achieves higher dynamic range and lower harmonic distortions compared with the traditional architecture.\n",
      "\n",
      "6. id: 5390a8b220f70186a0e9c0fa   score: 0.6095242   abstract: A new power reduction technique for analog-to-digital converters is proposed in this paper. A novel current-mode algorithm which uses time to perform analog-to-digital conversion has been described and a 12 bit 100-ksample/s time-based pipeline analog to digital converter has been designed and simulated in standard 90-nm CMOS technology based on introduced structure. Employed circuit techniques include a continues-time comparator, bottom plate sampling, digital correction and a state machine. A time based-mechanism has been used for subtraction and amplification. Simulation results show that the pipelined ADC achieves a peak signal-to-noise-and-distortion ratio of 69.8 dB, a peak spurious-free dynamic range of 75 dB, a total harmonic distortion of 73 dB, and a peak integral nonlinearity of 0.85 least significant bits. The total power dissipation is 90 μW from a 3-V supply.\n",
      "\n",
      "7. id: 5390b0ca20f70186a0ed9d34   score: 0.60929173   abstract: In this paper, we describe a cyclic ADC to adopt the comparator-based switched-capacitor (CBSC) technique, for the first time, so as to compensate for the technology scaling and reduce power consumption by eliminating the need for high gain opamps. A boosted preset voltage is also introduced to improve the conversion rate without consuming more power. The ADC operates at 2.5MS/s, and near the Nyquist-rate, a prototype has a signal-to-noise and distortion ratio (SNDR) of 55.99 dB and a spurious-free dynamic-range (SFDR) of 66.85 dB. The chip was fabricated in 0.18um CMOS and it has an active area of 0.146mm2 and consumes 0.74mW from a 1.8V supply.\n",
      "\n",
      "8. id: 5390a05920f70186a0e499ac   score: 0.60369825   abstract: A high efficiency and low distortion and switching power amplifier is proposed. We use feedback to reduce the total harmonic distortion (THD) and the DC output bias current. The experimental results show that the proposed circuit has 0.24% total harmonic distortion and 91% power efficiency while the DC output bias current is 12 μA at 1.5 V supply voltage.\n",
      "\n",
      "9. id: 5390975920f70186a0dfdc45   score: 0.5994265   abstract: A compact, four-quadrant analog CMOS multiplier featuring wide dynamic range is presented. The capacitive voltage division obtained by the use of Floating-Gate MOS (FGMOS) transistors, and an accurate wide-swing current mirror based on active bootstrapping, allow a wide input range, low harmonic distortion, and high linearity. Simulation and measurement results for a 0.8 μm CMOS prototype demonstrate the validity of the proposed approach.\n",
      "\n",
      "10. id: 5390afc920f70186a0ed2614   score: 0.5599218   abstract: In this paper, an all NMOS voltage-mode four quadrant analog multiplier based on a basic NMOS differential amplifier that can produce the output signal in voltage form without using resistors is presented. The proposed circuit has been fabricated and simulated with .35 micron technology. The power consumption is about 3.6mW from a ±2.5V power supply voltage, and the total harmonic distortion is 0.85% with a 1V input signal.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672423\n",
      "index                                        559258c30cf205530abc98ac\n",
      "title               Database Fragmentation with Confidentiality Co...\n",
      "authors                             Xiaofeng Xu, Li Xiong, Jinfei Liu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 5th ACM Conference on Data ...\n",
      "references          53908a9620f70186a0da4bb0;5390bda020f70186a0f46...\n",
      "abstract            Database fragmentation is a promising approach...\n",
      "id                                                            1672423\n",
      "clustered_labels                                                    3\n",
      "Name: 1672423, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba3820f70186a0f366e7   score: 0.9662198   abstract: Ensuring confidentiality of outsourced data continues to be an area of active research in the field of privacy protection. Almost all existing privacy-preserving approaches to address this problem rely on heavyweight cryptographic techniques with a large computational overhead that makes inefficient on large databases. In this paper, we address this problem by improving on an existing approach based on a combination of fragmentation and encryption. We present a method for optimizing and executing queries over distributed fragments stored in different Cloud storage service providers. We then extend this approach by presenting a Private Information Retrieval (PIR) based query technique to enforce data confidentiality under a collaborative Cloud storage service providers model.\n",
      "\n",
      "2. id: 5390b71120f70186a0f1f3b1   score: 0.9568766   abstract: With the increasing cost of maintaining IT centers, there is a trend among organizations to outsource data management functions to a third-party service provider to reduce storage and computational cost. However, this opens the door for privacy violations. Existing approaches for protecting data confidentiality are based on encryption or a combination of encryption and fragmentation. In this paper, we propose an approach based only on fragmentation. In particular, we discuss the issue of employing both vertical and horizontal fragmentation to a database relation so that a minimum amount of data is stored at the owner. We represent the privacy (confidentiality) constraints as a graph. The constraint graph may have some cycles. We employ the two-coloring technique for the acyclic portion of the graph. We propose some heuristic algorithms to eliminate cycles and complete the coloring of all\n",
      "\n",
      "3. id: 5390b86b20f70186a0f297b7   score: 0.9557341   abstract: While the idea of database outsourcing is becoming increasingly popular, the associated security risks still prevent many potential users from deploying it. In particular, the need to give full access to one's data to a third party, the database service provider, remains a major obstacle. A seemingly obvious solution is to encrypt the data in such a way that the service provider retains the ability to perform relational operations on the encrypted database. In this paper we present a model and an encryption scheme that solves this problem at least partially. Our approach represents the provably secure solution to the database outsourcing problem that allows operations exact select, Cartesian product, and projection, and that guarantees the probability of erroneous answers to be negligible. Our scheme is simple and practical, and it allows effective searches on encrypted tables: For a tab\n",
      "\n",
      "4. id: 5390bfa220f70186a0f5395c   score: 0.95447797   abstract: Data outsourcing has recently emerged as a successful solution allowing individuals and organizations to delegate data and service management to external third parties. A major challenge in the data outsourcing scenario is how to guarantee proper privacy protection against the external server. Recent promising approaches rely on the organization of data in indexing structures that use encryption and the dynamic allocation of encrypted data to physical blocks for destroying the otherwise static relationship between data and the blocks in which they are stored. However, dynamic data allocation implies the need to re-write blocks at every read access, thus requesting exclusive locks that can affect concurrency. Also, these solutions only support search conditions on the values of the attribute used for building the indexing structure.In this paper, we present an approach that overcomes such\n",
      "\n",
      "5. id: 5390b04120f70186a0ed89e5   score: 0.94407386   abstract: Existing approaches for protecting sensitive information outsourced at external “honest-but-curious” servers are typically based on an overlying layer of encryption applied to the whole database, or on the combined use of fragmentation and encryption. In this paper, we put forward a novel paradigm for preserving privacy in data outsourcing, which departs from encryption. The basic idea is to involve the owner in storing a limited portion of the data, while storing the remaining information in the clear at the external server. We analyze the problem of computing a fragmentation that minimizes the owner's workload, which is represented using different metrics and corresponding weight functions, and prove that this minimization problem is NP-hard. We then introduce the definition of locally minimal fragmentation that is used to efficiently compute a fragmentation via a heuristic algorithm. \n",
      "\n",
      "6. id: 5390af8920f70186a0ed0657   score: 0.90958625   abstract: Database outsourcing has become an important research topic in recent years because of the large volume of data that needs to be distributed or hosted to serve client applications. This dissertation studies data integrity, privacy and key management problems in the context of outsourced databases. Most of our research work is related to certain kinds of range query problems in database indexing. The goal of the research is to develop efficient algorithms and data structures to satisfy data security and privacy requirements when performing such range queries.In the outsourced database model, the owner of the data uses third party databases to distribute the data or allow clients to query the databases. There are many challenges in this model. For example, how does the client verify the completeness and correctness of the query results when the third parties are untrusted? How to protect t\n",
      "\n",
      "7. id: 5390bda020f70186a0f46be4   score: 0.87663436   abstract: Database outsourcing to semi-honest servers raises concerns against the confidentiality of sensitive information. To hide such information, an existing approach splits data among two supposedly mutually isolated servers by means of fragmentation and encryption. This approach is modelled logic-orientedly and then proved to be confidentiality preserving, even if an attacker employs some restricted but nevertheless versatile class of a priori knowledge to draw inferences. Finally, a method to compute a secure fragmentation schema is developed.\n",
      "\n",
      "8. id: 5390b8d720f70186a0f2cef5   score: 0.81405276   abstract: Security of database outsourcing, due to the untrustworthiness of service provider, is a basic challenge to have Database As a Service in a cloud computing environment. Having disparate assumptions to solve different aspects of security such as confidentiality and integrity is an obstacle for an integrated secure solution through the combination of existing approaches. Concentrating on confidentiality and integrity aspects of database outsourcing, this paper proposes an approach in which each attribute value is split up between several data servers using a customized threshold secret sharing scheme. Our approach preserves data confidentiality and at the same time provides the correctness verifiability of query results for clients. The distribution algorithm and redundant shares in the secret sharing scheme are the basis of correctness verification for query results.\n",
      "\n",
      "9. id: 5390aaf920f70186a0ead0b0   score: 0.8071568   abstract: We put forward a novel paradigm for preserving privacy in data outsourcing which departs from encryption. The basic idea behind our proposal is to involve the owner in storing a limited portion of the data, and maintaining all data (either at the owner or at external servers) in the clear. We assume a relational context, where the data to be outsourced is contained in a relational table. We then analyze how the relational table can be fragmented, minimizing the load for the data owner. We propose several metrics and present a general framework capturing all of them, with a corresponding algorithm finding a heuristic solution to a family of NP-hard problems.\n",
      "\n",
      "10. id: 5390b63320f70186a0f1777b   score: 0.7444706   abstract: For cloud-based outsourcing of confidential data, various techniques based on cryptography or data-fragmentation have been proposed, each with its own tradeoff between confidentiality, performance, and the set of supported queries. However, it is complex and error-prone to select appropriate techniques to individual scenarios manually. In this paper, we present a policy-based approach consisting of a domain specific language and a policy-transformator to automatically generate scenario-specific software adapters called mediators that set up data outsourcing and govern data access. Mediators combine state-of-the-art confidentiality techniques to ensure a user-specified level of confidentiality while still offering efficient data access. Thus, our approach simplifies data outsourcing by decoupling policy decisions from their technical implementation and realizes appropriate tradeoffs betwe\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1724674\n",
      "index                                        55323d1045cec66b6f9dd731\n",
      "title               A multiobjective optimization based framework ...\n",
      "authors             Zhiwei Feng, Qingbin Zhang, Qingfu Zhang, Qian...\n",
      "year                                                           2015.0\n",
      "venue                                  Journal of Global Optimization\n",
      "references                                   558b07f2612c41e6b9d40d35\n",
      "abstract            In many engineering optimization problems, obj...\n",
      "id                                                            1724674\n",
      "clustered_labels                                                    1\n",
      "Name: 1724674, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b1f02612c41e6b9d4400b   score: 0.9374402   abstract: The use of surrogate based optimization (SBO) is widely spread in engineering design to reduce the number of computational expensive simulations. However, \\\"real-world\\\" problems often consist of multiple, conflicting objectives leading to a set of competitive solutions (the Pareto front). The objectives are often aggregated into a single cost function to reduce the computational cost, though a better approach is to use multiobjective optimization methods to directly identify a set of Pareto-optimal solutions, which can be used by the designer to make more efficient design decisions (instead of weighting and aggregating the costs upfront). Most of the work in multiobjective optimization is focused on multiobjective evolutionary algorithms (MOEAs). While MOEAs are well-suited to handle large, intractable design spaces, they typically require thousands of expensive simulations, which is pr\n",
      "\n",
      "2. id: 5390b56a20f70186a0f06566   score: 0.89029425   abstract: This paper concerns multiobjective optimization in scenarios where each solution evaluation is financially and/or temporally expensive. We make use of nine relatively low-dimensional, nonpathological, real-valued functions, such as arise in many applications, and assess the performance of two algorithms after just 100 and 250 (or 260) function evaluations. The results show that NSGA-II, a popular multiobjective evolutionary algorithm, performs well compared with random search, even within the restricted number of evaluations used. A significantly better performance (particularly, in the worst case) is, however, achieved on our test set by an algorithm proposed herein-ParEGO-which is an extension of the single-objective efficient global optimization (EGO) algorithm of Jones et al. ParEGO uses a design-of-experiments inspired initialization procedure and learns a Gaussian processes model o\n",
      "\n",
      "3. id: 5390ac1820f70186a0eb3516   score: 0.8862236   abstract: Solving real-life engineering problems requires often multiobjective, global, and efficient (in terms of objective function evaluations) treatment. In this study, we consider problems of this type by discussing some drawbacks of the current methods and then introduce a new population-based multiobjective optimization algorithm UPS-EMOA which produces a dense (not limited to the population size) approximation of the Pareto-optimal set in a computationally effective manner.\n",
      "\n",
      "4. id: 5390a79f20f70186a0e92a18   score: 0.82417905   abstract: Multi-objective optimization is an essential and challenging topic in the domains of engineering and computation because real-world problems usually include several conflicting objectives. Current trends in the research of solving multi-objective problems (MOPs) require that the adopted optimization method provides an approximation of the Pareto set such that the user can understand the tradeoff between objectives and therefore make the final decision. Recently, an efficient framework, called MOEA/D, combining decomposition techniques in mathematics and optimization methods in evolutionary computation was proposed. MOEA/D decomposes a MOP to a set of single-objective problems (SOPs) with neighborhood relationship and approximates the Pareto set by solving these SOPs. In this paper, we attempt to enhance MOEA/D by proposing two mechanisms. To fully employ the information obtained from nei\n",
      "\n",
      "5. id: 5390a1d420f70186a0e589d9   score: 0.8162599   abstract: Real-world optimization problems often require the consideration of multiple contradicting objectives. These multiobjective problems are even more challenging when facing a limited budget of evaluations due to expensive experiments or simulations. In these cases, a specific class of multiobjective optimization algorithms (MOOA) has to be applied. This paper provides a review of contemporary multiobjective approaches based on the singleobjective meta-model-assisted 'Efficient Global Optimization' (EGO) procedure and describes their main concepts. Additionally, a new EGO-based MOOA is introduced, which utilizes the $\\mathcal{S}$-metric or hypervolume contribution to decide which solution is evaluated next. A benchmark on recently proposed test functions is performed allowing a budget of 130 evaluations. The results point out that the maximization of the hypervolume contribution within a re\n",
      "\n",
      "6. id: 5390b3ae20f70186a0ef3b7e   score: 0.7902067   abstract: Due to the vagaries of optimization problems encountered in practice, users resort to different algorithms for solving different optimization problems. In this paper, we suggest an optimization procedure which specializes in solving multi-objective, multi-global problems. The algorithm is carefully designed so as to degenerate to efficient algorithms for solving other simpler optimization problems, such as single-objective uni-global problems, single-objective multi-global problems and multi-objective uni-global problems. The efficacy of the proposed algorithm in solving various problems is demonstrated on a number of test problems. Because of it's efficiency in handling different types of problems with equal ease, this algorithm should find increasing use in real-world optimization problems.\n",
      "\n",
      "7. id: 5390b68720f70186a0f1bf66   score: 0.78249633   abstract: Many optimization problems are multiobjective in nature in the sense that multiple, conflicting criteria need to be optimized simultaneously. Due to the conflict between objectives, usually, no single optimal solution exists. Instead, the optimum corresponds to a set of so-called Pareto-optimal solutions for which no other solution has better function values in all objectives. Evolutionary Multiobjective Optimization (EMO) algorithms are widely used in practice for solving multiobjective optimization problems due to several reasons. As randomized blackbox algorithms, EMO approaches allow to tackle problems with nonlinear, nondifferentiable, or noisy objective functions. As set-based algorithms, they allow to compute or approximate the full set of Pareto-optimal solutions in one algorithm run---opposed to classical solution-based techniques from the multicriteria decision making (MCDM) fi\n",
      "\n",
      "8. id: 5390b04120f70186a0ed6c62   score: 0.78249633   abstract: Many optimization problems are multiobjective in nature in the sense that multiple, conflicting criteria need to be optimized simultaneously. Due to the conflict between objectives, usually, no single optimal solution exists. Instead, the optimum corresponds to a set of so-called Pareto-optimal solutions for which no other solution has better function values in all objectives. Evolutionary Multiobjective Optimization (EMO) algorithms are widely used in practice for solving multiobjective optimization problems due to several reasons. As randomized blackbox algorithms, EMO approaches allow to tackle problems with nonlinear, nondifferentiable, or noisy objective functions. As set-based algorithms, they allow to compute or approximate the full set of Pareto-optimal solutions in one algorithm run---opposed to classical solution-based techniques from the multicriteria decision making (MCDM) fi\n",
      "\n",
      "9. id: 5390a0b720f70186a0e4f063   score: 0.7687332   abstract: One of the major difficulties when applying Multiobjective Evolutionary Algorithms (MOEA) to real world problems is the large number of objective function evaluations. Approximate (or surrogate) methods offer the possibility of reducing the number of evaluations, without reducing solution quality. Artificial Neural Network (ANN) based models are one approach that have been used to approximate the future front from the current available fronts with acceptable accuracy levels. However, the associated computational costs limit their effectiveness. In this work, we introduce a simple approach that has comparatively smaller computational cost and we have developed this model as a variation operator that can be used in any kind of multiobjective optimizer. When designing this model, we have considered the whole search procedure as a dynamic system that takes available objective values in curre\n",
      "\n",
      "10. id: 558bd1c40cf23f2dfc593ac7   score: 0.76629364   abstract: Many optimization problems are multiobjective in nature in the sense that multiple, conflicting criteria need to be optimized simultaneously. Due to the conflict between objectives, usually, no single optimal solution exists. Instead, the optimum corresponds to a set of so-called Pareto-optimal solutions for which no other solution has better function values in all objectives. Evolutionary Multiobjective Optimization (EMO) algorithms are widely used in practice for solving multiobjective optimization problems due to several reasons. As stochastic blackbox algorithms, EMO approaches allow to tackle problems with nonlinear, nondifferentiable, or noisy objective functions. As set-based algorithms, they allow to compute or approximate the full set of Pareto-optimal solutions in one algorithm run---opposed to classical solution-based techniques from the multicriteria decision making (MCDM) fi\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717674\n",
      "index                                        5591329e0cf232eb904fb341\n",
      "title               Spatio-temporal analysis of state-of-charge st...\n",
      "authors             Junghoon Lee, Gyung-Leen Park, Yumin Cho, Suna...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 14th International Conferen...\n",
      "references          5390bb7b20f70186a0f40181;5390b86b20f70186a0f2a...\n",
      "abstract            This paper collects the SoC (State-of-Charge) ...\n",
      "id                                                            1717674\n",
      "clustered_labels                                                    3\n",
      "Name: 1717674, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390adfc20f70186a0ec4b33   score: 0.84972197   abstract: The battery state of charge (SOC) is an important parameter for the electrical vehicle. Its estimation is an important function of the electric vehicle battery management system. In this paper, definition of SOC is discussed; fundamental characteristics are also introduced. Many kinds of estimation tactics of SOC are described at the moment. The advantages and disadvantages of these methods during applications are given. Influence factors are analyzed at last.\n",
      "\n",
      "2. id: 53909f8220f70186a0e3df6c   score: 0.74186105   abstract: This paper presents a new method to estimate the battery state of charge (SOC) in electric vehicles (EVs). The key of the proposed method is to establish the relationship of the SOC to the battery current, voltage and temperature by using least squares support vector machine (LS-SVM). For ease of practical application, the pruning procedure is developed to reduce the number of support vectors in terms of their significance. The results show that the proposed method can simulate the battery dynamics for the accurate estimation of the SOC in EVs.\n",
      "\n",
      "3. id: 5390a88c20f70186a0e9a51d   score: 0.68836373   abstract: State of charge (SOC) estimation is an increasingly important issue in battery management system (BMS) and has become a core factor to promote the development of electric vehicle (EV). In addition to offering the real time display of battery parameters to user, the accurate SOC information would exert some controls over the charging and discharging process that in turn reduces the risk of cell over voltage. Considering the shortcoming of traditional estimation methods and the harsh requirements in EV environment, a new method named combination algorithm is proposed in this paper in accordance with the characteristics of lithium-ion power battery. Some capacity effect factors, such as current rate and cell temperature, are also taken into consideration in the algorithm. The dynamic discharge test shows that the maximal SOC estimation error is less than 5%, which validates the feasibility \n",
      "\n",
      "4. id: 55323c2c45cec66b6f9db67e   score: 0.6585273   abstract: Graphical abstractDisplay Omitted HighlightsIntroduction of a context-aware routing methodology.Ability to sense the environment and to perceive FEV's special characteristics.A rather unbiased (MPE=0.85%) and accurate (MAPE=10.56%) consumption model.Proposed consumption model vs. a reference model: MPE 20x better, MAPE 2x better.Energy efficient vs. fastest routes: save 21% energy, only 10% longer in time. This paper proposes an alternative approach for determining the most energy efficient route towards a destination. An innovative mesoscopic vehicular consumption model that is based on machine learning functionality is introduced and its application in a case study involving Fully Electric Vehicles (FEVs) is examined. The integration of this model in a routing engine especially designed for FEVs is also analyzed and a software architecture for implementing the proposed routing methodol\n",
      "\n",
      "5. id: 5390a5dc20f70186a0e805db   score: 0.6036398   abstract: The accurate estimation of lead-acid batteries state of charge (SOC) is very important for electric vehicles (EVs).However electrochemical reaction is a very complex process; it is difficult to simulate its dynamic behavior. Various equivalent circuit models have been studied, but can not show a good compromise between real-time and precision. In this paper, a revised model considering temperatures, discharge rates and different conditions is proposed based on a large number of experiments. These data are obtained from ARBIN, a battery testing device, and EVs. The simulated values are in good agreement with experimental data. And the associated error has been considered acceptable from an engineering point of view. It provided a model support for high accurate SOC estimation.\n",
      "\n",
      "6. id: 5390a88c20f70186a0e9a6c2   score: 0.57618433   abstract: One of the most essential and significant aspect of the battery management systems (BMS) is to estimate the state of charge (SOC) accurately, which can provide the judgment basis to system control strategy. In view of the lithium-ion power battery’ s properties and its operation condition in electric vehicles, a new method named EKF-Ah that derives from the extended Kalman filtering (EKF) algorithm and ampere hour counting method is proposed, which has a good performance on SOC estimation in complicated environment and is able to accomplish the requirements on power batteries. Results of tests show that the maximal SOC estimation error is fewer than 6.5%, which validates the feasibility and reliability of the proposed method.\n",
      "\n",
      "7. id: 5390a79f20f70186a0e91ef1   score: 0.55244917   abstract: The state of charge (SOC) of battery is a import thing in battery manage system. In order to predict the SOC of battery during discharge, by analyzing the factors influenced the SOC of battery, a new method investigates the SOC of battery with fuzzy neural network which can be used to approach non-linear function with many input/output parameters. Simulated results and experimental results show the advantages of the new technique, and a novel method is presented for estimation modeling of the SOC of Battery.\n",
      "\n",
      "8. id: 5390ac1720f70186a0eb2d8c   score: 0.36060163   abstract: State-of-charge (SOC) measures energy left in a battery, and it is critical for modeling and managing batteries. Developing efficient yet accurate SOC algorithms remains a challenging task. Most existing work uses regression based on a time-variant circuit model, which may be hard to converge and often does not apply to different types of batteries. Knowing open-circuit voltage (OCV) leads to SOC due to the well known mapping between OCV and SOC. In this paper, we propose an efficient yet accurate OCV algorithm that applies to all types of batteries. Using linear system analysis but without a circuit model, we calculate OCV based on the sampled terminal voltage and discharge current of the battery. Experiments show that our algorithm is numerically stable, robust to history dependent error, and obtains SOC with less than 4% error compared to a detailed battery simulation for a variety of\n",
      "\n",
      "9. id: 5390bded20f70186a0f49826   score: 0.31174105   abstract: For the study of optimal control problems of battery power in electric vehicle, accurately estimating the state of charge (SOC) of the battery is a non-negligible part. This paper proposes a prediction model for state of charge of batteries Based on least squares support vector machine. It was with battery terminal voltage, temperature, electric current as inputs, state of charge as output. After gaining data samples through experiment platform, least squares support vector machine was established, and state of charge can be predicted by the model. The experimental results show that the prediction accuracy of the method Based on LS - SVM significantly better than BP neural network, so it can be used to predict battery SOC values.\n",
      "\n",
      "10. id: 5390a80e20f70186a0e95be9   score: 0.28487435   abstract: Plug-in hybrid electric vehicles (PHEV) are widely received as a promising means of green mobility by utilizing more battery power. Recently, we have proposed a scheme of two-scale spatial-domain dynamic programming (DP) as a nearly global optimization approach to trip based optimal power management for PHEV through the combination with traffic data and trip modeling. Previously, the segment-wise power demand and SOC change was calculated through numerical integration based on the average speed and acceleration of the segment, and lookup tables were obtained. When more parameters are involved into power management, such as road grade and load change, such process becomes very tedious. In this paper, the spatial-domain DP is improved by calculating the power demand and SOC change in an analytical manner. The power demand is first calculated based on length, initial speed, acceleration, ro\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1748225\n",
      "index                                        554e3f0f0cf22ca2c80f9d71\n",
      "title               Scale- and rotation-invariant texture descript...\n",
      "authors             Reza Davarzani, Saeed Mozaffari, Khashayar Yag...\n",
      "year                                                           2015.0\n",
      "venue                                               Signal Processing\n",
      "references          558febbc612c29c89cd7c3cd;558fef77612c29c89cd7c5f3\n",
      "abstract            Local Binary Pattern (LBP) is an effective ima...\n",
      "id                                                            1748225\n",
      "clustered_labels                                                    2\n",
      "Name: 1748225, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b95520f70186a0f2ec8c   score: 0.9874236   abstract: The local binary pattern (LBP) is a simple effective local texture feature and widely used in the fields of face analysis, motion analysis, image segmentation, and image & video retrieval et al. However, there is less considering of color information and descriptor shapes in traditional LBP. In this paper, we investigate the performance of LBP feature in six color spaces which are invariant to illumination condition changes. In additional, we extent the LBP to larger area with more pixels and descriptor shapes involved and expect to improve its performance while keeping the computational complexity. Experimental results on the well-known PASCAL VOC 2011 dataset give special preference to the C-LBP, Opponent-LBP and the diamond descriptor with 8 neighbor pixels around center.\n",
      "\n",
      "2. id: 558b539e612c41e6b9d494b3   score: 0.97962874   abstract: The Local Binary Pattern (LBP) is an effective image descriptor. However, this descriptor has limitations in some of challenging issues in texture analysis, such as invariance to scaling, rotation, viewpoint variations and non-rigid deformations. In order to overcome these demerits of LBP, the paper proposed a weighted and adaptive LBP-based texture descriptor. Adaptive definition of circular neighboring set in LBP descriptor is very effective to achieve scale invariance features [1]. In our proposed method, both circular neighboring radius and orientation of sampling in LBP descriptor are defined in an adaptive manner. We used the radius of blob-like structures to determine the radius of sampling, similar to [1]. Definition of LBP operator with respect to dominant orientation of each pixel can guarantee the rotation invariance of LBP features. The original LBP operator discards the magn\n",
      "\n",
      "3. id: 5390ba3820f70186a0f35290   score: 0.9701835   abstract: Original Local Binary Pattern (LBP) descriptor has two obvious demerits, i.e., it is sensitive to noise, and sometimes it tends to characterize different structural patterns with the same binary code which will reduce its discriminability inevitably. In order to overcome these two demerits, this paper proposes a robust framework of LBP, named Completed Robust Local Binary Pattern (CRLBP), in which the value of each center pixel in a 3x3 local area is replaced by its average local gray level. Compared to the center gray value, average local gray level is more robust to noise and illumination variants. To make CRLBP more robust and stable, Weighted Local Gray Level (WLG) is introduced to take place of the traditional gray value of the center pixel. The experimental results obtained from four representative texture databases show that the proposed method is robust to noise and can achieve i\n",
      "\n",
      "4. id: 5390aa7620f70186a0eac06d   score: 0.9622497   abstract: Objective: This paper focuses on the use of image-based machine learning techniques in medical image analysis. In particular, we present some variants of local binary patterns (LBP), which are widely considered the state of the art among texture descriptors. After we provide a detailed review of the literature about existing LBP variants and discuss the most salient approaches, along with their pros and cons, we report new experiments using several LBP-based descriptors and propose a set of novel texture descriptors for the representation of biomedical images. The standard LBP operator is defined as a gray-scale invariant texture measure, derived from a general definition of texture in a local neighborhood. Our variants are obtained by considering different shapes for the neighborhood calculation and different encodings for the evaluation of the local gray-scale difference. These sets of\n",
      "\n",
      "5. id: 5390ada620f70186a0ec1b6d   score: 0.9614615   abstract: The Local Binary Pattern (LBP) operator is a computationally efficient yet powerful feature for analyzing local texture structures. While the LBP operator has been successfully applied to tasks as diverse as texture classification, texture segmentation, face recognition and facial expression recognition, etc., it has been rarely used in the domain of Visual Object Classes (VOC) recognition mainly due to its deficiency of power for dealing with various changes in lighting and viewing conditions in real-world scenes. In this paper, we propose six novel multi-scale color LBP operators in order to increase photometric invariance property and discriminative power of the original LBP operator. The experimental results on the PASCAL VOC 2007 image benchmark show significant accuracy improvement by the proposed operators as compared with both the original LBP and other popular texture descriptor\n",
      "\n",
      "6. id: 5390b00c20f70186a0ed60b9   score: 0.9603612   abstract: Local binary pattern (LBP) is an effective texture descriptor which has successful applications in texture classification and face recognition. Many extensions are made for conventional LBP descriptors. One of the extensions is dominant local binary patterns which aim at extracting the dominant local structures in texture images. The second extension is representing LBP descriptors in Gabor transform domain (LGBP). The third extension is multi-resolution LBP (MLBP). Another extension is dynamic LBP for video texture extraction. In this paper, we extend the conventional local binary pattern to pyramid transform domain (PLBP). By cascading the LBP information of hierarchical spatial pyramids, PLBP descriptors take texture resolution variations into account. PLBP descriptors show their effectiveness for texture representation. Comprehensive comparisons are made for LBP, MLBP, LGBP, and PLBP\n",
      "\n",
      "7. id: 5390b52620f70186a0f0305b   score: 0.95961094   abstract: The local binary pattern (LBP) operator is a very effective multi-resolution texture descriptor that can be applied in many image processing applications. However, existing LBP operators can not use the information of non-uniform patterns efficiently and they are also sensitive to noise. This paper proposes a noise tolerant extension of LBP operators to extract statistical and structural image features for efficient texture analysis. The proposed LBP operator uses a circular majority voting filter and suitable rotation-invariant labeling scheme to obtain more regular uniform and non-uniform patterns that have better discrimination ability and more robustness against noise. Experimental results on the Brodatz, CUReT and MeasTex databases show the improvement of the proposed LBP operator performance, especially when a large number of neighbors are used for extracting texture patterns.\n",
      "\n",
      "8. id: 5390b24420f70186a0ee86c9   score: 0.9587699   abstract: Local Binary Pattern (LBP) algorithm is a typical texture analysis method combined with structural and statistical texture. LBP has a drawback of losing global spatial information, while global features preserving little local texture information. By extension of the standard LBP algorithm, this paper focuses on Completed modeling of Local Binary Pattern (CLBP), which is composed by the center gray level, sign components and magnitude components. Two experiments were carried out to test the classification ability of CLBP by Brodatz and UIUC image database. Results show that CLBP algorithm has the highest average classification accuracy of 86.63% (Brodatz) and 83.29% (UIUC). But the standard LBP only obtained the highest average classification accuracy of 79.97% (Brodatz) and 57.59% (UIUC). So the CLBP has a better texture feature extraction capabilities than standard LBP, and the differe\n",
      "\n",
      "9. id: 5390bed320f70186a0f4f17b   score: 0.9518632   abstract: Conventional binary descriptor only uses limited spatial information from original image patch, such as BRIEF, which will result in limited discriminative power. We settle this problem through further excavating feature information and propose a binary descriptor encoding not only intensity comparison information but also intensity order information. Results based on experiments of performance evaluation have shown that the proposed binary descriptor outperforms other binary descriptors under rotation and scale changes.\n",
      "\n",
      "10. id: 5390ae2e20f70186a0ec737b   score: 0.94994867   abstract: This paper focuses on the combination of variants of local binary patterns (LBP), widely considered the state of the art among texture descriptors, using the same radius and the same number of neighborhoods. We report new experiments exploring several LBP-based descriptors and propose a set of variants for the representation of images. Our experiments are of two main types. In the first set, the Fourier transform is used to extract features starting from the histogram of uniform patterns. In these experiments we test different methods of extracting features from the histogram and each method is used to train a set of support vector machines (SVMs) which are then combined. In the second set of experiments, features are extracted from histograms using different definitions of uniform patterns. These are used to train SVMs, and the results are then combined. Our results show that descriptor\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1743228\n",
      "index                                        554505650cf21e970c06b319\n",
      "title               Towards improving statistical modeling of soft...\n",
      "authors             Nicolas Bettenburg, Meiyappan Nagappan, Ahmed ...\n",
      "year                                                           2015.0\n",
      "venue                                  Empirical Software Engineering\n",
      "references                                   5590a2950cf2ce4b6f39e627\n",
      "abstract            Much research in software engineering (SE) is ...\n",
      "id                                                            1743228\n",
      "clustered_labels                                                    0\n",
      "Name: 1743228, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b0ca20f70186a0eda916   score: 0.96557647   abstract: In past and recent years---also thanks to the availability of data from software repositories---several kinds of models have been built to characterize and, in some cases, to predict software change- and fault-proneness. While a wide variety of change- and fault-proneness models have been built so far, and although software repositories have opened the road towards promising research directions, there are several issues still to be solved. First, we need to carefully assess and validate the quality of data sets used for our models. Second, although predictive or explanatory models do not allow to claim for causation, we need to better exploit software repositories with the aim of providing qualitative, credible explanations to the statistical correlations captured by the models. Third, and most important, when building predictive models we often tend to forget what would be their ultimat\n",
      "\n",
      "2. id: 558bcb62612cf64242758554   score: 0.92956835   abstract: Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motiv\n",
      "\n",
      "3. id: 558be9030cf2e30013db2f82   score: 0.8680457   abstract: We welcome you to the 5th Workshop on Models in Software Engineering---MiSE'013! The purpose of this workshop is to study and advance the effective use of models in the engineering of software systems. In particular, we are interested in the exchange of experiences, challenges and promising technologies related to modeling. The goals of the software modeling community are to improve the productivity of software developers and to improve the quality of the resulting software products. Models are useful in all phases and activities surrounding software development and deployment. Thus, workshop topics range from requirements modeling, to runtime models, to models for assessing software quality, and to the pragmatics of how to manage large collections of models.\n",
      "\n",
      "4. id: 5590a2950cf2ce4b6f39e627   score: 0.83413684   abstract: Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into\n",
      "\n",
      "5. id: 5390ba3820f70186a0f37466   score: 0.82318634   abstract: Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to Sept\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3cf55   score: 0.79358625   abstract: Target audience: Software practitioners and researchers wanting to understand the state of the art in using data science for software engineering (SE). Content: In the age of big data, data science (the knowledge of deriving meaningful outcomes from data) is an essential skill that should be equipped by software engineers. It can be used to predict useful information on new projects based on completed projects. This tutorial offers core insights about the state-of-the-art in this important field. What participants will learn: Before data science: this tutorial discusses the tasks needed to deploy machine-learning algorithms to organizations (Part1: Organization Issues). During data science: from discretization to clustering to dichotomization and statistical analysis. And the rest: When local data is scarce, we show how to adapt data from other organizations to local problems. When priva\n",
      "\n",
      "7. id: 5390995d20f70186a0e1544c   score: 0.78825736   abstract: A predictive software model (PSM) is any model extracted from software engineering data that can be readily used to make a prediction regarding some aspect of a software system. In this paper we present some well known applications of predictive software models, and propose new potential applications for PSMs. We also introduce the PROMISE Software Engineering Repository of public datasets. The purpose of this repository is to promote repeatable, verifiable and refutable research in the area of predictive software models. We conclude the paper with our observations about the software engineering datasets used in building PSMs.\n",
      "\n",
      "8. id: 5390a2e920f70186a0e67bd5   score: 0.7439129   abstract: When building software quality models, the approach often consists of training data mining learners on a single fit dataset. Typically, this fit dataset contains software metrics collected during a past release of the software project that we want to predict the quality of. In order to improve the predictive accuracy of such quality models, it is common practice to combine the predictive results of multiple learners to take advantage of their respective biases. Although multi-learner classifiers have been proven to be successful in some cases, the improvement is not always significant because the information in the fit dataset sometimes can be insufficient. We present an innovative method to build software quality models using majority voting to combine the predictions of multiple learners induced on multiple training datasets. To our knowledge, no previous study in software quality has \n",
      "\n",
      "9. id: 5390be6620f70186a0f4be20   score: 0.7010995   abstract: Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset \n",
      "\n",
      "10. id: 539098dc20f70186a0e0cb23   score: 0.6734065   abstract: To aid software analysis and maintenance tasks, a number of software clustering algorithms have been proposed to automatically partition a software system into meaningful subsystems or clusters. However, it is unknown whether these algorithms produce similar meaningful clusterings for similar versions of a real-life software system under continual change and growth. This paper describes a comparative study of six software clustering algorithms. We applied each of the algorithms to subsequent versions from five large open source systems. We conducted comparisons based on three criteria respectively: stability (Does the clustering change only modestly as the system undergoes modest updating?), authoritativeness (Does the clustering reasonably approximate the structure an authority provides?) and extremity of cluster distribution (Does the clustering avoid huge clusters and many very small \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674801\n",
      "index                                        559121790cf232eb904fae68\n",
      "title               Command Robots from Orbit with Supervised Auto...\n",
      "authors             Neal Y. Lii, Daniel Leidner, André Schiele, Pe...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Tenth Annual ACM/IEEE Inter...\n",
      "references                                   5390a80e20f70186a0e95e8c\n",
      "abstract            The on-going work at German Aerospace Center (...\n",
      "id                                                            1674801\n",
      "clustered_labels                                                    2\n",
      "Name: 1674801, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539089bb20f70186a0d98028   score: 0.8547634   abstract: The paper briefly outlines DLR's experience with real space robot missions (ROTEX and ETS VII). It then discusses forthcoming projects, e.g., free-flying systems in low or geostationary orbit and robot systems around the space station ISS, where the telerobotic system MARCO might represent a common baseline. Finally it describes our efforts in developing a new generation of “mechatronic” ultra-light weight arms with multifingered hands. The third arm generation is operable now (approaching present-day technical limits). In a similar way DLR's four-fingered hand II was a big step towards higher reliability and yet better performance. Artificial robonauts for space are a central goal now for the Europeans as well as for NASA, and the first verification tests of DLR's joint components are supposed to fly already end of 93 on the space station.\n",
      "\n",
      "2. id: 5390b36120f70186a0ef17b4   score: 0.5343999   abstract: Cette thèse a pour objectif de développer un simulateur tutoriel intelligent pour l'apprentissage de manipulations robotisées, applicable au bras robot canadien sur la station spatiale internationale. Le simulateur appelé Roman Tutor est une preuve de concept de simulateur d'apprentissage autonome et continu pour des manipulations robotisées complexes. Un tel concept est notamment pertinent pour les futures missions spatiales sur Mars ou sur la Lune, et ce en dépit de l'inadéquation du bras canadien pour de telles missions en raison de sa trop grande complexité. Le fait de démontrer la possibilité de conception d'un simulateur capable, dans une certaine mesure, de donner des rétroactions similaires à celles d'un enseignant humain, pourrait inspirer de nouvelles idées pour des concepts similaires, applicables à des robots plus simples, qui seraient utilisés dans les prochaines missions sp\n",
      "\n",
      "3. id: 539099b320f70186a0e1b5ea   score: 0.48736838   abstract: This paper describes the ongoing development of a teleoperation platform for space robotic missions. The proposed platform consists of a ground control station, an offline simulation environment, a full-scale working model of the robot and the actual robot on the mission. From the simulation environment a path strategy is obtained and a set of commands is generated and sent to the ground control station using standardized files (XML) to describe how commands and data move between functional units, this way it is no longer needed to create custom software, and/or distributed objects to communicate with new systems. Instead, only providing an XML description of how the communication takes place will be required. Then the systems can parse and interpret the XML to generate the required commands to control the unit, interface with it, construct messages and display the data that is returned.\n",
      "\n",
      "4. id: 5390981d20f70186a0e057a5   score: 0.43698242   abstract: In this paper we describe work being done at our department to make the robotics laboratory accessible to students and colleagues to execute and watch real-time experiments at any time and from anywhere. We describe a few different installations and highlight the underlying philosophy, which is aimed at enlarging the lab in all the dimensions of space, time, and available resources, through the use of Internet technologies. In particular, four experimental setups with hardware and software architecture description are presented: the dc motor, the magnetic levitator, the nonholomonic motion planner (NHMP), and the graphic environment tool. © 2005 Wiley Periodicals, Inc.\n",
      "\n",
      "5. id: 53908b2a20f70186a0db92a7   score: 0.43116498   abstract: This paper presents a new style of human-robot collaboration in a teleoperation system where the robot has the autonomy to control its behavior. Our model provides the \"shared communicational modality\" between the human operator and the robot autonomy to promote their mixed-initiative interactions. This paper describes the results of experiments using our developing system to evaluate our model, and discusses the interactions between the two autonomies based upon the Lens model framework known as a judgment analysis method.\n",
      "\n",
      "6. id: 5390baa120f70186a0f3989d   score: 0.3706809   abstract: As we move along the scale of adjustable autonomy for the control of robots from direct teleoperation at one extreme to full automation at the other, several opportunities for improvement in control quality, user feedback and machine learning suggest themselves. We describe three experiments, in telerobotics, the provision of situational awareness, and the acquisition of knowledge for automation from the human operator, and explain our concept of explicit, assigned responsibility as an organising principle for flexible work-sharing between humans and robots. A novel design for an interface based on this principle is outlined.\n",
      "\n",
      "7. id: 5390ada620f70186a0ec23a5   score: 0.3671568   abstract: Simulated operations during a recent NASA robotic field test demonstrated real-time computation of performance metrics for human-robot interaction that includes adjustable autonomy.\n",
      "\n",
      "8. id: 5390893e20f70186a0d94494   score: 0.35131106   abstract: Despite advances in autonomy, there will always be a need for human involvement in vehicle teleoperation. In particular, tasks such as exploration, reconnaissance and surveillance will continue to require human supervision, if not guidance and direct control. Thus, it is critical that the operator interface be as efficient and as capable as possible. In this paper, we provide an overview of vehicle teleoperation and present a summary of interfaces currently in use.\n",
      "\n",
      "9. id: 5390985e20f70186a0e08ab2   score: 0.31184584   abstract: Telerobotics denotes the technology of robotics controlled at a distance by human beings. When a task involving physical exploration, manipulation, and sampling is too dangerous or impractical to be performed directly by a human, it may be suited to a telerobot. In such a system, the human operator is physically removed from the task, sends commands to the robot over a telecommunication system, and receives information about the status of the task and its environment. Teleoperation therefore involves augmenting, supervising, or substituting artificial intelligence (q.v.) and control functions of the robot with the intelligence and pattern recognition abilities of the human operator.\n",
      "\n",
      "10. id: 5390b5ed20f70186a0f0e0ab   score: 0.18359363   abstract: The deployment of space robots for servicing and maintenance operations that are teleoperated from the ground is a valuable addition to existing autonomous systems, because it will provide flexibility and robustness in mission operations. In this connection, not only robotic manipulators are of great use, but also free-flying inspector satellites supporting the operations through additional feedback to the ground operator. The manual control of such an inspector satellite at a remote location is challenging, because navigation in three-dimensional space is unfamiliar and large time delays can occur in the communication channel. This paper shows a series of robotic experiments, in which free flyers are controlled by astronauts aboard the International Space Station (ISS). The Synchronized Position Hold Engage Reorient Experimental Satellites (SPHERES) were utilized to study several aspect\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696280\n",
      "index                                        55909b5b0cf28af999b589e4\n",
      "title               A Vision System for Robotic Ultrasound Guided ...\n",
      "authors             P. J. Gonçalves, P. M. Torres, F. Santos, R. A...\n",
      "year                                                           2015.0\n",
      "venue                      Journal of Intelligent and Robotic Systems\n",
      "references          539087a620f70186a0d4961e;53908b2a20f70186a0db9...\n",
      "abstract            Surgical navigation is a crucial concept in co...\n",
      "id                                                            1696280\n",
      "clustered_labels                                                    1\n",
      "Name: 1696280, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558e34670cf2c779a6477cdd   score: 0.952131   abstract: In recent years, many new tools and techniques have been developed in computer assisted orthopaedic surgery primarily with an industry led effort in software innovation and development. Only a few research and clinical projects have focused on intraoperative difficulties. A common operative challenge in computer assisted orthopaedic surgery is the positioning of the reference base. Rigid fixation of a dynamic reference base is essential in navigated surgery of the extremities. The aim of this study was to develop a minimal-invasive screw which could be placed effectively and efficiently with rotational stability during computer assisted orthopaedic surgery. The minimal-invasive screw was initially evaluated in an artificial bone experiment. After successful results with the artificial bone experiment, it underwent testing in seven human cadaver thighs with ISO-C^{3D} navigated drilling. \n",
      "\n",
      "2. id: 5390882820f70186a0d8b56d   score: 0.831006   abstract: From the Publisher:This is a review of the rapidly emerging computer assisted technologies that are starting to fundamentally influence the way that we design, plan, simulate, and carry out orthopedic surgery. This volume provides an overview of the state-of-the-art of the development of CAOS and its clinical applications. Topics covered range from basic concepts to planning, simulation and execution of surgery in different anatomical areas, including the spine, hip and knee. Various surgical techniques, such as joint reconstruction and replacement, trauma fixation, and minimally invasive approaches are also presented.\n",
      "\n",
      "3. id: 539089ab20f70186a0d9568a   score: 0.70659554   abstract: Surgical robots assisting surgeons during operations are being used in selected medical fields like neurosurgery, orthopaedics and endoscopy. In an introductory part, the authors present a workflow for robot assisted surgery, which includes the steps: image data acquisition, image processing, surgical planning and the actual robot assisted intraoperative transposition. Each step of the workflow comprises different computer aided methods and apparatuses, which are presented in this paper. The following part focuses on different mechanical set-ups and various application fields for surgical robots. They include neurosurgery, orthopaedic surgery, radiosurgery and radiotherapy, prostatectomy, endoscopy, laparoscopy, cardiac surgery and craniofacial surgery.\n",
      "\n",
      "4. id: 5390a96e20f70186a0ea23bc   score: 0.64400125   abstract: In this paper we introduce the extendible and cross-platform software framework Julius. Julius combines both pre-operative planning and intraoperative assistance within one single environment. In this paper we discuss three aspects of Julius: the medical data processing, the visualization pipeline and the interaction. Each aspect provides interfaces that allow to extend the application with own algorithms and to build complex applications. We believe that this approach facilitates the development of image guided navigation and simulation procedures for computer-aided-surgery.\n",
      "\n",
      "5. id: 5390a8b220f70186a0e9ce17   score: 0.6199316   abstract: A brief history of robotic surgery is provided, which describes the transition from autonomous robots to hands-on systems that are under the direct control of the surgeon. An example of the latter is the Acrobot (for active-constraint robot) system used in orthopaedics, whilst soft-tissue surgery is illustrated by the daVinci telemanipulator system. Non-technological aspects of robotic surgery have often been a major impediment to their widespread clinical use. These are discussed in detail, together with the role of navigation systems, which are considered a major competitor to surgical robots. A detailed description is then given of a registration method for robots to achieve improved accuracy. Registration is a major source of error in robotic surgery, particularly in orthopaedics. The paper describes the design and clinical implementation of a novel method, coined the bounded registr\n",
      "\n",
      "6. id: 5390995d20f70186a0e1680f   score: 0.52594715   abstract: This paper is to highlight a concept for computer aided navigation system that uses fluoroscopic image as a basis for real-time surgical tools navigation. Unfortunately, the inherent distortion of the fluoroscopic image deteriorates the quality of surgical navigation. Combing with global and local correction methods, a novel approach for distortion correction is proposed which allows good image quality in relatively acceptable time. A linear cone beam projection model is setup, and the associate transformation that models the actual X-ray projection is calculated.\n",
      "\n",
      "7. id: 5390bae520f70186a0f3b8df   score: 0.4993763   abstract: This paper presents the integration between the Image Guided Surgery Toolkit (IGSTK) and the Point Cloud Library (PCL) to deliver a software tool (SWT) to robotic orthopedic surgery. The application that demonstrates the usefulness of the tool was developed for the registration of 2D ultrasound (US) images of the femur to 3D point clouds obtained from CT or US images, i.e., the 3D bone model. The surgery tackled is Hip Resurfacing of the femur, which needs a precise knowledge of the bone precision and orientation. The SWT developed takes advantage of the well known characteristics of IGSTK and the libraries from which it depends, e.g., OpenCV, ITK, VTK, etc, and the capabilities of the PCL library for point cloud processing and integration to OROCOS/ROS, the Open Robot Control Software Project. For the registration, during surgery, the SWT acquires a bone surface from a sequence of US im\n",
      "\n",
      "8. id: 5390aaf920f70186a0eae9ca   score: 0.49299663   abstract: Recent research in orthopedic surgeries indicates that computer-assisted robotic systems can improve the precision and accuracy of the surgery which in turn leads to better long-term outcomes. Kinematic and dynamic analysis of an orthopedic robot called OrthoRoby, which will be used in bone cutting operations, are derived. Computed-torque and disturbance based observer low-level control methods have been evaluated for OrthoRoby to track a desired bone cutting trajectory. Low-level controllers are evaluated and the results are presented to demonstrate the feasibility of the controllers.\n",
      "\n",
      "9. id: 5390994d20f70186a0e138aa   score: 0.47426593   abstract: Recently there have been many efforts to investigate navigation technology and apply it in various clinical fields in which the target position in the surgical region is indicated during surgery. The objective is to facilitate an intuitive understanding of the surgical region by the surgeon, so that the accuracy of the surgery can be improved. Currently, the position of the surgical area is usually measured by a magnetic sensor or a marker-type optical 3D position sensor. In navigation of hard tissue such as bone, the target is rigid, and the position of the target can be measured from several discrete point markers. In navigation of soft tissue such as the body surface and the liver, where the shape and the position can change easily, a position sensor which can measure the state of modification in the form of time-series surface data is required. In the method proposed here in order to\n",
      "\n",
      "10. id: 539099b320f70186a0e1b86d   score: 0.465266   abstract: To design and evaluate a novel computer-assisted, fluoroscopy-based planning and navigation system for minimally invasive ventral spondylodesis of thoracolumbar fractures.materials and Methods.\" Instruments and an image intensifier are tracked with the SurgiGATE® navigation system (Praxim-Medivision). Two fluoroscopic images, one acquired from anterior-posterior (AP) direction and the other from lateral-medial (LM) direction, are used for the complete procedure of planning and navigation. Both of them are calibrated with a custom-made software to recover their projection geometry and to co-register them to a common patient reference coordinate system, which is established by attaching an opto-electronically trackable dynamic reference base (DRB) on the operated vertebra. A biplanar landmark reconstruction method is used to acquire deep-seated anatomical landmarks such that an intraoperat\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1714796\n",
      "index                                        55323c4845cec66b6f9dbaaa\n",
      "title               How small are shifts required in optimal preem...\n",
      "authors                 E. G. Coffman, Jr., C. T. Ng, V. G. Timkovsky\n",
      "year                                                           2015.0\n",
      "venue                                           Journal of Scheduling\n",
      "references                                   5590c9d90cf237666fc28a1a\n",
      "abstract            An event in a schedule is a job start, interru...\n",
      "id                                                            1714796\n",
      "clustered_labels                                                    1\n",
      "Name: 1714796, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a74f20f70186a0e8d26d   score: 0.6280653   abstract: Ways of decreasing the number of operations needed to compute the lower bounds of optimal schedules, by reducing the number of time intervals that must be considered, are presented. The bounds apply to a system of identical processors executing a partially ordered set of tasks, with known execution times, using a non-preemptive scheduling strategy. In one approach we find that the required number of intervals depends on the graph. In our other approach, which subsumes the first, the number of intervals is decreased to at most min[D2/2, n2], where D is the deadline to complete the tasks and n is the number of tasks. The actual number of intervals for a particular graph can be considerably smaller than this worst case.\n",
      "\n",
      "2. id: 5390b63320f70186a0f1720c   score: 0.62658125   abstract: This paper addresses the problem of scheduling n unit length tasks on m identical machines under certain precedence constraints. The aim is to compute minimal length nonpreemptive schedules. We introduce a new order class which contains properly two rich families of precedence graphs: interval orders and a subclass of the class of series parallel orders. We present a linear time algorithm to find an optimal schedule for this new order class on any number of machines.\n",
      "\n",
      "3. id: 558af6ac612c41e6b9d3ec81   score: 0.5891255   abstract: We consider basic problems of non-preemptive scheduling on uniformly related machines. For a given schedule, defined by a partition of the jobs into m subsets corresponding to the m machines, $$C_i$$ C i denotes the completion time of machine i. Our goal is to find a schedule that minimizes or maximizes $$\\\\sum _{i=1}^m C_i^p$$ ¿ i = 1 m C i p for a fixed value of p such that $$0<p<\\\\infty $$ 0 < p < ¿ . For $$p>1$$ p > 1 the minimization problem is equivalent to the well-known problem of minimizing the $$\\\\ell _p$$ ℓ p norm of the vector of the completion times of the machines, and for $$0<p<1$$ 0 < p < 1 , the maximization problem is of interest. Our main result is an efficient polynomial time approximation scheme (EPTAS) for each one of these problems. Our schemes use a non-standard application of the so-called shifting technique. We focus on the work (total size of jobs) assigned to \n",
      "\n",
      "4. id: 554b9c2e0cf230e3b86432c3   score: 0.53239477   abstract: We consider the nonpreemptive scheduling of n â¥ 1 jobs on identical, parallel machines. With job running times all given by some constant, the objective is to minimize the expected number of machines needed throughout a schedule subject to the following waiting-time constraints. At time 0, a timer with random initial value W is started; after time W all jobs must either be finished or running on a machine. The value of W is not known in advance, but its distribution is made available to the scheduler. In general, if job running times are random, there might be situations in which, after running jobs on a given number of machines, it would be desirable to activate new machines, because the remaining times of unfinished jobs are stochastically too large compared to the time remaining on the timer. A principal result of this paper is that randomness of job running times is essential to su\n",
      "\n",
      "5. id: 5390b63320f70186a0f17109   score: 0.5128146   abstract: We investigate one-machine scheduling problems subject to generalized precedence constraints which specify that when job J\"j precedes job J\"j\"' (denoted by J\"j-J\"j\"'), the time between the end J\"j and the beginning of job J\"j\"' must be at least l\"j\"j\"', but no more than u\"j\"j\"', where 0=\n",
      "\n",
      "6. id: 53908f5b20f70186a0dda693   score: 0.49086097   abstract: WE CONSIDER PARALLEL EXECUTION OF STRUCTURED JOBS WITH REAL TIME CON- STRAINTS IN (POSSIBLY HETEROGENEOUS) MULTIPROCESSOR SYSTEMS. A JOB IS COM- POSED OF A SET OF TASKS AND A PARTIAL ORDER SPECIFYING THE PRECEDENCE CON- STRAINTS BETWEEN THE TASKS. THE TASK PROCESSING TIMES ARE RANDOM VARIABLES WITH KNOWN PROBABILITY DISTRIBUTION FUNCTIONS. THE INTERARRIVAL TIME OF THE JOBS ARE ALSO RANDOM VARIABLES WITH ARBITRARY DISTRIBUTIONS. THE REAL TIME CONSTRAINTS ARE SPECIFIED BY REFERENCE TIMES, ALSO CALLED SOFT REAL-TIME DEADLINES. IN THE DISCUSSION WE ASSUME FIRST THAT ALL THE JOBS HAVE THE SAME TASK GRAPH, I.E, THE SAME TASK SET AND THE SAME PARTIAL ORDER. WE ASSUME THAT THERE IS A PREDEFINED MAPPING FROM THE SET OF TASKS ONTO THE SET OF TASKS ONTO THE SET OF MACHINES, IDENTICAL FOR ALL JOBS, THAT ALLO- CATES TASKS TO MACHINES. WE FOCUS ON DYNAMIC SCHEDULING POLICIES WHICH DO NOT USE INFORMATI\n",
      "\n",
      "7. id: 5390b63320f70186a0f1712a   score: 0.47196853   abstract: Consider a scheduling problem where several classes of non-preemptive jobs with different priorities are to be processed on two processors with different processing speeds. Of the jobs waiting to be processed, jobs with the highest priority have the first option of being dispatched to the available processors. Let processing times have increasing hazard rate (IHR) distributions which depend on the processor and job class. It is shown that for each job class there exists a simple threshold policy which minimizes the expected makespan (the completion time of the last job) of the class, among all non-preemptive policies. This research extends the result of Coffman, Flatto, Garey and Weber to include several priority job classes and more general processing time distributions. The proof is presented in a more intuitive and simpler manner.\n",
      "\n",
      "8. id: 53909fca20f70186a0e4607b   score: 0.43494126   abstract: We consider Edmonds's model (1999) extended by precedence constraints. In our setting, a scheduler has to schedule non-clairvoyantly jobs consisting in DAGs of tasks arriving over time, each task going through phases of different degrees of parallelism, unknown to the scheduler. As in the original model without precedence constraints, the scheduler is only informed of the arrival and the completion of each task, at the time of these events, and nothing more. Furthermore, it is not aware of the DAG structure of each job beforehand neither of the precise characteristics of the phases of the tasks that compose each job. We consider the preemptive strategy Equi&cir;Equi, that divides the processors evenly among the alive jobs and then divides the processing power alloted to each job evenly among its alive tasks. We show that whatever how complex the precedences are, Equi&cir;Equi is (2 + ε)-\n",
      "\n",
      "9. id: 5390ae2e20f70186a0ec74e4   score: 0.4279943   abstract: In this work we show that certain classical preemptive shop scheduling problems with integral data satisfy the following integer preemption property: there exists an optimal preemptive schedule where all interruptions and all starting and completion times occur at integral dates. We also give new upper bounds on the minimal number of interruptions for various shop scheduling problems.\n",
      "\n",
      "10. id: 5390a2e920f70186a0e67474   score: 0.42119467   abstract: We study two closely related problems in non-preemptive scheduling of sequential jobs on identical parallel machines. In these two settings there are either fixed jobs or non-availability intervals during which the machines are not available; in either case, the objective is to minimize the makespan. Both formulations have different applications, e.g. in turnaround scheduling or overlay computing. For both problems we contribute approximation algorithms with an improved ratio of 3/2 + ε, respectively. For scheduling with fixed jobs, a lower bound of 3/2 on the approximation ratio has been obtained by Scharbrodt, Steger & Weisser; for scheduling with non-availability we provide the same lower bound. In total, our approximation ratio for both problems is essentially tight via suitable inapproximability results. We use dual approximation, creation of a gap structure and job configurations, \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696472\n",
      "index                                        559149a40cf2127aa930c8b9\n",
      "title                Streaming lower bounds for approximating MAX-CUT\n",
      "authors                 Michael Kapralov, Sanjeev Khanna, Madhu Sudan\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558ae173612c41e6b9d3c3a1;558ce5d40cf2b0acc6503...\n",
      "abstract            We consider the problem of estimating the valu...\n",
      "id                                                            1696472\n",
      "clustered_labels                                                    2\n",
      "Name: 1696472, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b33e2612c41e6b9d4679b   score: 0.9668514   abstract: We present a streaming algorithm that makes one pass over the edges of an unweighted graph presented in random order, and produces a polylogarithmic approximation to the size of the maximum matching in the graph, while using only polylogarithmic space. Prior to this work the only approximations known were a folklore Õ(√n) approximation with polylogarithmic space in an n vertex graph and a constant approximation with Ω(n) space. Our work thus gives the first algorithm where both the space and approximation factors are smaller than any polynomial in n. Our algorithm is obtained by effecting a streaming implementation of a simple \\\"local\\\" algorithm that we design for this problem. The local algorithm produces a O(k · n1/k) approximation to the size of a maximum matching by exploring the radius k neighborhoods of vertices, for any parameter k. We show, somewhat surprisingly, that our local \n",
      "\n",
      "2. id: 53909f2d20f70186a0e386a5   score: 0.9466954   abstract: We give a deterministic polynomial-time algorithm which for any given average degree d and asymptotically almost all random graphs G in G(n, m = [d/2 n]) outputs a cut of G whose ratio (in cardinality) with the maximum cut is at least 0.952. We remind the reader that it is known that unless P=NP, for no constant ε0 is there a Max-Cut approximation algorithm that for all inputs achieves an approximation ratio of (16/17) +ε (16/17\n",
      "\n",
      "3. id: 559155990cf232eb904fbcc6   score: 0.92121595   abstract: We consider the problem of estimating the size of a maximum matching when the edges are revealed in a streaming fashion. When the input graph is planar, we present a simple and elegant streaming algorithm that with high probability estimates the size of a maximum matching within a constant factor using [EQUATION](n2/3) space, where n is the number of vertices. The approach generalizes to the family of graphs that have bounded arboricity, which include graphs with an excluded constant-size minor. To the best of our knowledge, this is the first result for estimating the size of a maximum matching in the adversarial-order streaming model (as opposed to the random-order streaming model) in o(n) space. We circumvent the barriers inherent in the adversarial-order model by exploiting several structural properties of planar graphs, and more generally, graphs with bounded arboricity. We further r\n",
      "\n",
      "4. id: 5390b60d20f70186a0f128e7   score: 0.91550726   abstract: We study exact algorithms for the MAX-CUT problem. Introducing a new technique, we present an algorithmic scheme that computes a maximum cut in graphs with bounded maximum degree. Our algorithm runs in time O^*(2^(^1^-^(^2^/^@D^)^)^n). We also describe a MAX-CUT algorithm for general graphs. Its time complexity is O^*(2^m^n^/^(^m^+^n^)). Both algorithms use polynomial space.\n",
      "\n",
      "5. id: 5390893e20f70186a0d93950   score: 0.91228   abstract: We show that maximal matchings can be computed deterministically in O(log4 n) rounds in the synchronous, message-passing model of computation. This is one of the very few cases known of a nontrivial graph structure, and the only \"classical\" one, which can be computed distributively in polylogarithmic time without recourse to randomization.\n",
      "\n",
      "6. id: 5390adfc20f70186a0ec51eb   score: 0.8912444   abstract: We show that the exact computation of a minimum or a maximum cut of a given graph G is out of reach for any one-pass streaming algorithm, that is, for any algorithm that runs over the input stream of G's edges only once and has a working memory of o(n^2) bits. This holds even if randomization is allowed.\n",
      "\n",
      "7. id: 539087a120f70186a0d4599d   score: 0.891055   abstract: The maximum cut problem is known to be an important NP-complete problem with many applications. The authors investigate this problem (which they call the normal maximum cut problem) and a variant of it (which is referred to as the connected maximum cut problem). They show that any n-vertex e-edge graph admits a cut with at least the fraction 1/2+1/2n of its edges, thus improving the ratio 1/2+2/e known before. It is shown that it is NP-complete to decide if a given graph has a normal maximum cut with at least a fraction (1/2+ epsilon ) of its edges, where the positive constant epsilon can be taken smaller than any value chosen. The authors present an approximation algorithm for the normal maximum cut problem on any graph that runs in O((e log e+n log n)/p+log p*log n) parallel time using p(1or=por=e+n) processors that guarantees a ratio of at least (1/2+1/2n), given a matching of size e/\n",
      "\n",
      "8. id: 5390a40520f70186a0e7054a   score: 0.8872046   abstract: We describe a new approximation algorithm for Max Cut. Our algorithm runs in ~O(n2) time, where n is the number of vertices, and achieves an approximation ratio of .531. On instances in which an optimal solution cuts a 1-ε fraction of edges, our algorithm finds a solution that cuts a 1-4√ε + 8ε-o(1) fraction of edges. Our main result is a variant of spectral partitioning, which can be implemented in nearly linear time. Given a graph in which the Max Cut optimum is a 1-ε fraction of edges, our spectral partitioning algorithm finds a set S of vertices and a bipartition L,R=S-L of S such that at least a 1-O(√ε) fraction of the edges incident on S have one endpoint in L and one endpoint in R. (This can be seen as an analog of Cheeger's inequality for the smallest eigenvalue of the adjacency matrix of a graph.) Iterating this procedure yields the approximation results stated above. A differen\n",
      "\n",
      "9. id: 539099a220f70186a0e18833   score: 0.82190347   abstract: Streaming is an important paradigm for handling data sets that are too large to fit in main memory. In the streaming computational model, algorithms are restricted to using much less space than they would need to store the input. The massive data set is accessed in a sequential fashion and, therefore, can be viewed as a stream of data elements. The order of the data elements in the stream is not controlled by the algorithm. There are three important resources considered in the streaming model: the size of the workspace, the number of passes that the algorithm makes over the stream, and the time to process each data element in the stream. In this thesis, we study computational-geometry problems and graph problems in the streaming model. We design algorithms for computing diameter in the streaming and the sliding-window models and prove some corresponding lower bounds. The sliding-window m\n",
      "\n",
      "10. id: 5390995d20f70186a0e15b63   score: 0.799129   abstract: We show that the sparsest cut in graphs can be approximated within O(log2 n) factor in Õ(n3/2) time using polylogarithmic single commodity max-flow computations. Previous algorithms are based on multicommodity flows which take time Õ(n2). Our algorithm iteratively employs max-flow computations to embed an expander flow, thus providing a certificate of expansion. Our technique can also be extended to yield an O(log2 n) (pseudo) approximation algorithm for the edge-separator problem with a similar running time.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696099\n",
      "index                                        5591711f0cf2e89307ca9d4c\n",
      "title               The emergence of content strategy work and rec...\n",
      "authors                                              Rebekka Andersen\n",
      "year                                                           2015.0\n",
      "venue                           Communication Design Quarterly Review\n",
      "references          558b0322612c41e6b9d40550;5390b9d520f70186a0f31afd\n",
      "abstract            In my last column, I wrote about the need for ...\n",
      "id                                                            1696099\n",
      "clustered_labels                                                    0\n",
      "Name: 1696099, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390881720f70186a0d802cb   score: 0.93257624   abstract: In this panel session, the authors identify four different factors shaping the future of technical communication: user-centered design, corporate universities, cross-disciplinary collaboration, and knowledge management. The authors each address how factors once considered external to the field of technical communication are now becoming thoroughly integrated with it. These four studies, in conjunction, suggest how the field of technical communication is becoming increasingly complex and how participants (practitioners, researchers, and educators) will need to adapt to this new terrain.\n",
      "\n",
      "2. id: 53908cde20f70186a0dce74b   score: 0.83031917   abstract: From the Publisher:Content and Complexity: Information Design in Technical Communication explores both theoretical perspectives as well as the practicalities of information design in areas relevant to technical communicators. This integration of theoretical and applied components make it a practical resource for students, educators, academic researchers, and practitioners in the technical communication and information design fields.\n",
      "\n",
      "3. id: 53908cde20f70186a0dce75a   score: 0.83031917   abstract: From the Publisher:Content and Complexity: Information Design in Technical Communication explores both theoretical perspectives as well as the practicalities of information design in areas relevant to technical communicators. This integration of theoretical and applied components make it a practical resource for students, educators, academic researchers, and practitioners in the technical communication and information design fields.\n",
      "\n",
      "4. id: 5390bfa220f70186a0f54783   score: 0.7405498   abstract: For the past few years, I have attended a number of industry conferences focused on content management (CM); reviewed a wealth of CM-focused publications, including trade books, white papers, newsletters, and blogs; and followed numerous CM-focused online discussions. Through these experiences and readings I have learned a great deal about the affordances and challenges of CM. But the message that has most impacted my thinking about CM---and what it means for the field of Technical Communication (TC)---is this: the era of document-based information development (ID), which has shaped all aspects of TC research, training, and practice since the field's inception, is coming to an end.\n",
      "\n",
      "5. id: 5390bda020f70186a0f45cca   score: 0.42560524   abstract: We believe that one of the major research questions that will drive the field of technical communication during the next 5--10 years is, \"How can technical communication scholars navigate increasingly cross-cultural, cross-disciplinary, and cross-organizational contexts to support social justice through better communication?\"\n",
      "\n",
      "6. id: 5390bda020f70186a0f45ccd   score: 0.3747907   abstract: To identify some of the research questions and needs of most importance to industry professionals and academics, we conducted a Technical Communication Industry Research Survey that posed a common set of questions about research. Here we report the results, which suggest some differing priorities for academics and industry professionals, but also some shared priorities that might help guide disciplinary research, including content strategy, user behavior, metrics/measurements, and process/practices.\n",
      "\n",
      "7. id: 53909f8220f70186a0e3c70f   score: 0.3287449   abstract: In this paper, I suggest that granularized content management introduces as-yet-unexplored issues to genres of technical communication. I argue that content management, while it can, as advertised, free content and make it easy to reuse that content in multiple genres, that flexibility can create new problems for genres and genre systems, leading to problematic reuse, inflexible genre systems, rigid and proprietary genres, and uncritical internationalization.\n",
      "\n",
      "8. id: 55323d7745cec66b6f9de6a2   score: 0.26189685   abstract: Technical Communication in the Twenty-First Century (TCTC) preparesreaders to be successful writers and readers of technical communication, regardless of their career path. Featuring a wealth of examples and cases, it emphasizes problem-solving, collaboration, visual rhetoric and usability. Its approach analyzes why something worked or did not work, as well as how to produce the appropriate communication. Now available with the MyTechCommLab online learning tool, this edition features more focus on transnational communication, forty-five new case studies, and new information on the relationship between technology and communication.\n",
      "\n",
      "9. id: 539099ec20f70186a0e1d0f2   score: 0.13568956   abstract: Over the past ten years, technical communicators驴theorists and practitioners alike驴have been acknowledging change and growth in the field. For example, what once was considered merely writing about technical subjects or objects now encompasses writing the technology itself, in that we are designing complex systems to facilitate the work we do. As the technical communication discipline grows, we report on select trends in the SIGDOC proceedings in recent years to examine the widening of boundaries of the discipline. By doing so, we aim to provide context and situation for the ever-changing role of the technical communicator\n",
      "\n",
      "10. id: 5390b9d520f70186a0f31afd   score: 0.12678517   abstract: Technical content is often the last in line for investment and innovation, but poor content has profound effects inside and outside the organization--it damages your reputation, shrinks sales, and causes legal problems. Content Strategy 101 is an invaluable resource for transforming your technical content into a business asset.Join the conversation at contentstrategy101.com.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1651005\n",
      "index                                        5591512c0cf232eb904fbb9c\n",
      "title               Handheld or Handsfree?: Remote Collaboration v...\n",
      "authors                 Steven Johnson, Madeleine Gibson, Bilge Mutlu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          558b48c1612c41e6b9d4824d;539087ef20f70186a0d6d...\n",
      "abstract            Emerging wearable and mobile communication tec...\n",
      "id                                                            1651005\n",
      "clustered_labels                                                    3\n",
      "Name: 1651005, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558ae8c0612c41e6b9d3d1c6   score: 0.96498734   abstract: Many studies suggest that tangibles and digital tabletops have potential to support collaborative interaction. However, previous findings show that users often work in parallel with such systems. One design strategy that may encourage collaboration rather than parallel use involves creating a system that responds to co-dependent access points in which more than one action is required to create a successful system response. To better understand how co-dependent access points support collaboration, we designed a comparative study with 12 young adults using the same application with a co-dependent and an independent access point design. We collected and analyzed categories of both verbal and behavioural data in the two conditions. Our results show support for the co-dependent strategy and suggest ways that the co-dependent design can be used to support flexible collaboration on tangible tab\n",
      "\n",
      "2. id: 5390b3da20f70186a0ef5a5c   score: 0.93071055   abstract: Mobile and handheld devices have become platforms to support remote collaboration. But, their small form-factor may impact the effectiveness of the visual feedback channel often used to help users maintain an awareness of their partner's activities during synchronous collaborative tasks. We investigated how visual and tactile feedback affects collaboration on mobile devices, with emphasis on spatial coordination in a shared workspace. From two user studies, our results highlight different benefits of each feedback channel in collaborative handheld systems. Visual feedback can provide precise spatial information for collaborators, but degrades collaboration when the feedback is occluded, and sometimes can distract the user's attention. Spatial tactile feedback can reduce the overload of information in visual space and gently guides the user's attention to an area of interest. Our results \n",
      "\n",
      "3. id: 5537da0b0cf23ee1cc767a42   score: 0.906156   abstract: A key driver of enterprise adoption of consumer mobile devices is their potential to support collaboration. However, today there is little understanding of how well mobile devices support collaborative work activities in the enterprise. We present Ask an Expert, a tablet-based collaboration application for mobile industrial workers. Ask an Expert combines communication channels and content sharing tools in persistent 'spaces' to support collaborative troubleshooting between remote workers. We describe the user-centered design of Ask an Expert and provide design recommendations for mobile collaboration applications. We also discuss findings from preliminary user evaluations.\n",
      "\n",
      "4. id: 5390ba0a20f70186a0f33875   score: 0.8840393   abstract: Many real world scenarios involve a remote helper guiding a local worker performing manipulations of physical objects (physical tasks). Technologies and systems have been developed to support such collaborations. However, existing systems often confine collaborators in fixed desktop settings. Yet, there are many situations in which collaborators are mobile and/or desktop settings are not possible to set up. In this paper, we present HandsInAir, a real-time collaborative wearable system for remote collaboration. HandsInAir is designed to support mobility of both the worker and the helper and to provide easy access to remote expertise. In particular, this system implements a novel approach that allows helpers to perform hand gestures in the air and frees two hands of workers for object operations. We describe the system and an evaluation of it and envision future work.\n",
      "\n",
      "5. id: 5390995d20f70186a0e15bff   score: 0.8701566   abstract: After reminding the characteristics and the fundamental role of wearable computing and wearable computers for mobile task support, this paper presents the state of our research on three human-real world mediation levels: virtual alteration, virtualization and diffusion. To enhance a user's performances, the first mediation level investigates the virtual modification of existing interfaces while the second undertakes their abstraction in order to recreate them in a form of augmented reality. The last mediation level draws upon the task's artifacts and disseminates the interfaces' functionalities directly into the environment. We present for each of these mediation levels an analysis of their characteristics, an evaluation of their advantages for task support and our most recent experiments. Finally, we highlight the similarities and differences between these mediation levels to guide desi\n",
      "\n",
      "6. id: 5390b8d720f70186a0f2c616   score: 0.848847   abstract: There is currently a strong need for collaborative systems with which two or more participants interact over a distance on a task involving tangible artifacts (e.g., a machine, a patient, a tool). The present paper focuses on the specific category of remote-collaboration systems where hand gestures are used by a remote helper to assist a physically distant worker to perform manual tasks. Existing systems use a combination of video capturing, 2D monitors or 2D projectors, however displaying a video of the remote workspace and allowing helpers to gesture over the video does not provide helpers with sufficient understanding of the spatial relationships between remote objects and between their hands and the remote objects. In this paper we introduce our tele-presence Mixed Reality system for remote collaboration on physical tasks based on real-time capture and rendering of the remote workspa\n",
      "\n",
      "7. id: 5390b52620f70186a0f044c4   score: 0.84428066   abstract: Future mobile devices that feature a rollout display will be able to act as a relatively large interactive surface on-the-go. This will allow for novel collaborative usages in mobile settings. In this paper, we explore several dimensions of the design space of such \"handheld tabletop\" devices. We will illustrate our thoughts by means of a first prototype. Early evaluation results indicate that it effectively supports mobile social encounters.\n",
      "\n",
      "8. id: 5390b3ae20f70186a0ef4d83   score: 0.8441522   abstract: The primary contribution of my work is a methodology for the study of teamwork and taskwork during mixed-focus collaboration. I am currently completing the third experimental study in a series of three that investigate the role of shared and personal displays in supporting collaboration. The first study compared single- and multi-display configurations, and found that single display systems supported coordination, whereas multi-display systems improved collaborative outcomes. The second study compared different shared display types and identified how they support group synchronization, monitoring, and grounding behaviours. The final study investigates information sharing when groups are supported by interactive tabletops and tablets.\n",
      "\n",
      "9. id: 5390bded20f70186a0f49569   score: 0.78695047   abstract: Due to the rapid development in wearable computing, gestural interaction and augmented reality in recent years, remote collaboration has been seen as a fast growing field with many advanced designs and implementations for a wide range of applications. Most of existing remote guiding or collaboration solutions still rely on specifically designed hardware systems on both helper and worker side with limitations on usage, mobility, flexibility and portability. Considering widespread deployment of smart mobile devices such as smartphones and tablets in the past a few years, it already provides us numerous potentials of migrating conventional remote guiding solutions to such powerful platforms with the possibility of overcoming many existing issues and limits. In this paper, we introduce MobileHelper, a remote guiding prototype that is developed on a tablet device with the feature of allowing \n",
      "\n",
      "10. id: 5390afc920f70186a0ed2180   score: 0.77982527   abstract: In spite of all the attention paid to multi-touch tabletop displays, little is known about the collaborative tasks they are best suited for in comparison to alternatives such as multi-mouse Single-Display Groupware setups. In this paper, we share the results of a study we conducted comparing a multi-mouse Single-Display Groupware (SDG) setup (two mice, 15\" vertical display) to a multi-touch tabletop display (81cm by 61cm) for visual tasks that require coordination and collaboration. In the study, participants were more efficient when using the multi-mouse SDG setup, but preferred the multi-touch tabletop. We use the study as a platform for discussing how to interpret results from studies that compare an exciting technology to one that is not.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1650988\n",
      "index                                        559251750cf205530abc9530\n",
      "title               Collective Smile: Measuring Societal Happiness...\n",
      "authors             Saeed Abdullah, Elizabeth L. Murnane, Jean M.R...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390a63c20f70186a0e81ff4;5390ad0620f70186a0eba...\n",
      "abstract            The increasing adoption of social media provid...\n",
      "id                                                            1650988\n",
      "clustered_labels                                                    3\n",
      "Name: 1650988, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b4c420f70186a0efeb27   score: 0.6598436   abstract: Large scale analysis of social media content allows for real time discovery of macro-scale patterns in public opinion and sentiment. In this paper we analyse a collection of 484 million tweets generated by more than 9.8 million users from the United Kingdom over the past 31 months, a period marked by economic downturn and some social tensions. Our findings, besides corroborating our choice of method for the detection of public mood, also present intriguing patterns that can be explained in terms of events and social changes. On the one hand, the time series we obtain show that periodic events such as Christmas and Halloween evoke similar mood patterns every year. On the other hand, we see that a significant increase in negative mood indicators coincide with the announcement of the cuts to public spending by the government, and that this effect is still lasting. We also detect events such\n",
      "\n",
      "2. id: 5390b71120f70186a0f1e311   score: 0.45262668   abstract: Recent years have witnessed the explosive growth of online social media. Weibo, a Twitter-like online social network in China, has attracted more than 300 million users in less than three years, with more than 1000 tweets generated in every second. These tweets not only convey the factual information, but also reflect the emotional states of the authors, which are very important for understanding user behaviors. However, a tweet in Weibo is extremely short and the words it contains evolve extraordinarily fast. Moreover, the Chinese corpus of sentiments is still very small, which prevents the conventional keyword-based methods from being used. In light of this, we build a system called MoodLens, which to our best knowledge is the first system for sentiment analysis of Chinese tweets in Weibo. In MoodLens, 95 emoticons are mapped into four categories of sentiments, i.e. angry, disgusting, \n",
      "\n",
      "3. id: 5390bb7b20f70186a0f3fd9f   score: 0.38444984   abstract: Online social networks have attracted attention of people from both the academia and real world. In particular, the rich multimedia information accumulated in recent years provides an easy and convenient way for more active communication between people. This offers an opportunity to research people's behaviors and activities based on those multimedia content, which can be considered as social imagematics. One emerging area is driven by the fact that these massive multimedia data contain people's daily sentiments and opinions. However, existing sentiment analysis typically only pays attention to the textual information regardless of the visual content, which may be more informative in expressing people's sentiments and opinions. In this paper, we attempt to analyze the online sentiment changes of social media users using both the textual and visual content. In particular, we analyze the s\n",
      "\n",
      "4. id: 5390bb7b20f70186a0f4010f   score: 0.30208218   abstract: A picture is worth one thousand words, but what words should be used to describe the sentiment and emotions conveyed in the increasingly popular social multimedia? We demonstrate a novel system which combines sound structures from psychology and the folksonomy extracted from social multimedia to develop a large visual sentiment ontology consisting of 1,200 concepts and associated classifiers called SentiBank. Each concept, defined as an Adjective Noun Pair (ANP), is made of an adjective strongly indicating emotions and a noun corresponding to objects or scenes that have a reasonable prospect of automatic detection. We believe such large-scale visual classifiers offer a powerful mid-level semantic representation enabling high-level sentiment analysis of social multimedia. We demonstrate novel applications made possible by SentiBank including live sentiment prediction of social media and v\n",
      "\n",
      "5. id: 5390b4c420f70186a0efe624   score: 0.26284185   abstract: A convergence of emotions among people in social networks is potentially resulted by the occurrence of an unprecedented event in real world. E.g., a majority of bloggers would react angrily at the September 11 terrorist attacks. Based on this observation, we introduce a sentiment index, computed from the current mood tags in a collection of blog posts utilizing an affective lexicon, potentially revealing subtle events discussed in the blogosphere. We then develop a method for extracting events based on this index and its distribution. Our second contribution is establishment of a new bursty structure in text streams termed a sentiment burst. We employ a stochastic model to detect bursty periods of moods and the events associated. Our results on a dataset of more than 12 million mood-tagged blog posts over a 4-year period have shown that our sentiment-based bursty events are indeed meanin\n",
      "\n",
      "6. id: 5390bded20f70186a0f490da   score: 0.2155158   abstract: The dramatic rise in the use of social network platforms such as Facebook or Twitter has resulted in the availability of vast and growing user-contributed repositories of data. Exploiting this data by extracting useful information from it has become a great challenge in data mining and knowledge discovery. A recently popular way of extracting useful information from social network platforms is to build indicators, often in the form of a time series, of general public mood by means of sentiment analysis. Such indicators have been shown to correlate with a diverse variety of phenomena. In this article we follow this line of work and set out to assess, in a rigorous manner, whether a public sentiment indicator extracted from daily Twitter messages can indeed improve the forecasting of social, economic, or commercial indicators. To this end we have collected and processed a large amount of T\n",
      "\n",
      "7. id: 5390bb1d20f70186a0f3d371   score: 0.18982129   abstract: The explosion of social media services presents a great opportunity to understand the sentiment of the public via analyzing its large-scale and opinion-rich data. In social media, it is easy to amass vast quantities of unlabeled data, but very costly to obtain sentiment labels, which makes unsupervised sentiment analysis essential for various applications. It is challenging for traditional lexicon-based unsupervised methods due to the fact that expressions in social media are unstructured, informal, and fast-evolving. Emoticons and product ratings are examples of emotional signals that are associated with sentiments expressed in posts or words. Inspired by the wide availability of emotional signals in social media, we propose to study the problem of unsupervised sentiment analysis with emotional signals. In particular, we investigate whether the signals can potentially help sentiment ana\n",
      "\n",
      "8. id: 5390bd1520f70186a0f44369   score: 0.1328517   abstract: Social media offers a powerful outlet for people's thoughts and feelings—it is an enormous ever-growing source of texts ranging from everyday observations to involved discussions. This thesis contributes to the field of sentiment analysis, which aims to extract emotions and opinions from text. A basic goal is to classify text as expressing either positive or negative emotion. Sentiment classifiers have been built for social media text such as product reviews, blog posts, and even Twitter messages. With increasing complexity of text sources and topics, it is time to re-examine the standard sentiment extraction approaches, and possibly to re-define and enrich sentiment definition. Thus, this thesis begins by introducing a rich multi-dimensional model based on Affect Control Theory and showing its usefulness in sentiment classification. Next, unlike sentiment analysis research to date, we e\n",
      "\n",
      "9. id: 558b7bb7612c6b62e5e89a70   score: 0.13217813   abstract: We manually analyzed a corpus of Tumblr posts for sentiment, looking at images, text, and their combination. A dataset was constructed of posts with both text and images, as well as a dataset of posts containing only text, along with a codebook for classifying and counting the content in each. This paper reports on the construction of the overall corpus and the codebook, and presents the results of a preliminary analysis that focuses on emotion. Posts containing images expressed more emotion, more intense emotion, and were more positive in valence than posts containing only text. The study contributes a micro-level analysis of multimodal communication in a social media platform, as well as a gold standard corpus that can be used to train learning algorithms to identify sentiment in multimodal Tumblr data.\n",
      "\n",
      "10. id: 5390a93b20f70186a0ea0998   score: 0.124532074   abstract: I analyze the use of emotion words for approximately 100 million Facebook users since September of 2007. \"Gross national happiness\" is operationalized as a standardized difference between the use of positive and negative words, aggregated across days, and present a graph of this metric. I begin to validate this metric by showing that positive and negative word use in status updates covaries with self-reported satisfaction with life (convergent validity), and also note that the graph shows peaks and valleys on days that are culturally and emotionally significant (face validity). I discuss the development and computation of this metric, argue that this metric and graph serves as a representation of the overall emotional health of the nation, and discuss the importance of tracking such metrics.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691651\n",
      "index                                        559256200cf2aff368683c45\n",
      "title               The role eigenvalues play in forming GMRES res...\n",
      "authors                       Gérard Meurant, Jurjen Duintjer Tebbens\n",
      "year                                                           2015.0\n",
      "venue                                            Numerical Algorithms\n",
      "references          539087f320f70186a0d6fb27;5390a0b720f70186a0e4f...\n",
      "abstract            In this paper we give explicit expressions for...\n",
      "id                                                            1691651\n",
      "clustered_labels                                                    2\n",
      "Name: 1691651, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908cde20f70186a0dcf65c   score: 0.80193585   abstract: \\begin{abstract} \\noindent The GMRES algorithm minimizes $\\norm{p(A)b}$ over polynomials $p$ of degree $n$ normalized at $z=0$. The ideal GMRES problem is obtained if one considers minimization of $\\norm{p(A)}$ instead. The ideal problem forms an upper bound for the worst-case true problem, where the GMRES norm $\\norm{p_b(A)b}$ is maximized over $b$. In work not yet published, Faber, Joubert, Knill and Manteuffel have shown that this upper bound need not be attained, constructing a $4 \\times 4$ example in which the ratio of the true to ideal GMRES norms is $0.9999$. Here, we present a simpler $4 \\times 4$ example in which the ratio approaches zero when a certain parameter tends to zero. The same example also leads to the same conclusion for Arnoldi vs. ideal Arnoldi norms. \\end{abstract}\n",
      "\n",
      "2. id: 5390975920f70186a0dfe0d1   score: 0.7826625   abstract: We analyze the residuals of GMRES [Y. Saad and M. H. Schultz, SIAM J. Sci. Statist. Comput., 7 (1986), pp. 856--859], when the method is applied to tridiagonal Toeplitz matrices. We first derive formulas for the residuals as well as their norms when GMRES is applied to scaled Jordan blocks. This problem has been studied previously by Ipsen [BIT, 40 (2000), pp. 524--535] and Eiermann and Ernst [Private communication, 2002], but we formulate and prove our results in a different way. We then extend the (lower) bidiagonal Jordan blocks to tridiagonal Toeplitz matrices and study extensions of our bidiagonal analysis to the tridiagonal case. Intuitively, when a scaled Jordan block is extended to a tridiagonal Toeplitz matrix by a superdiagonal of small modulus (compared to the modulus of the subdiagonal), the GMRES residual norms for both matrices and the same initial residual should be close \n",
      "\n",
      "3. id: 558d8e620cf222bc17bbfc2a   score: 0.6101631   abstract: A new class of norms which generalize norms previously investigated by Young [9, 14], Sheldon [4, 5], Golub [1], Golub and Varga [2], Varga [6], Wachspress [7], Young and Kincaid [12], Young [14], and Kincaid [3] is introduced. Expressions for these norms applied to the matrices associated with various iterative methods are developed.\n",
      "\n",
      "4. id: 539087e720f70186a0d687e8   score: 0.59425694   abstract: The purpose of this paper is two-fold: to analyze the behavior of inverse iteration for computing a single eigenvector of a complex square matrix and to review Jim Wilkinson's contributions to the development of the method. In the process we derive several new results regarding the convergence of inverse iteration in exact arithmetic.In the case of normal matrices we show that residual norms decrease strictly monotonically. For eighty percent of the starting vectors a single iteration is enough.In the case of non-normal matrices, we show that the iterates converge asymptotically to an invariant subspace. However, the residual norms may not converge. The growth in residual norms from one iteration to the next can exceed the departure of the matrix from normality. We present an example where the residual growth is exponential in the departure of the matrix from normality. We also explain t\n",
      "\n",
      "5. id: 539087dd20f70186a0d63a62   score: 0.5824316   abstract: Residual norm estimates are derived for a general class of methods based on projection techniques on subspaces of the form $ K_m + {\\cal W}$, where $K_m$ is the standard Krylov subspace associated with the original linear system and ${\\cal W}$ is some other subspace. These \"augmented Krylov subspace methods\" include eigenvalue deflation techniques as well as block-Krylov methods. Residual bounds are established which suggest a convergence rate similar to one obtained by removing the components of the initial residual vector associated with the eigenvalues closest to zero. Both the symmetric and nonsymmetric cases are analyzed.\n",
      "\n",
      "6. id: 5390880720f70186a0d79d4e   score: 0.5461633   abstract: The generalized minimum residual method (GMRES) is well known for solving large nonsymmetric systems of linear equations. It generally uses restarting, which slows the convergence. However, some information can be retained at the time of the restart and used in the next cycle. We present algorithms that use implicit restarting in order to retain this information. Approximate eigenvectors determined from the previous subspace are included in the new subspace. This deflates the smallest eigenvalues and thus improves the convergence. The subspace that contains the approximate eigenvectors is itself a Krylov subspace, but not with the usual starting vector. The implicitly restarted FOM algorithm includes standard Ritz vectors in the subspace. The eigenvalue portion of its calculations is equivalent to Sorensen's IRA algorithm. The implicitly restarted GMRES algorithm uses harmonic Ritz vecto\n",
      "\n",
      "7. id: 53908d6520f70186a0dd0b91   score: 0.54274213   abstract: In most practical cases, the convergence of GMRES method applied to a linear algebraic system $Ax = b$ is determined by the distribution of eigenvalues of $A$. In theory, however, the information about the eigenvalues alone is not sufficient for determining the convergence. In this paper our previous work is extended in the following direction. It is given a complete parametrization of the set of all pairs $\\{ A,b \\}$ for which GMRES$(A,b)$ generates the prescribed convergence curve while the matrix $A$ has the prescribed eigenvalues. Moreover, a characterization of the right hand sides $b$ for which the GMRES$(A,b)$ converges exactly in $m$ steps, where $m$ is the degree of the minimal polynomial of $A$, is given. EMAIL:: ian@microian.ian.pv.cnr.it KEYWORDS:: Minimal polynomial, Krylov sequences, GMRES method, Convergence\n",
      "\n",
      "8. id: 5390a0b720f70186a0e4f23c   score: 0.54016596   abstract: In the present paper, we give some new convergence results of the global GMRES method for multiple linear systems. In the case where the coefficient matrix A is diagonalizable, we derive new upper bounds for the Frobenius norm of the residual. We also consider the case of normal matrices and we propose new expressions for the norm of the residual.\n",
      "\n",
      "9. id: 539087d920f70186a0d617ea   score: 0.5246078   abstract: Given a nonincreasing positive sequence $f(0) \\geq f(1) \\geq \\cdots \\geq f(n-1) 0$, it is shown that there exists an $n$ by $n$ matrix $A$ and a vector $r^0$ with $\\| r^0 \\| = f(0)$ such that $f(k) = \\| r^k \\|$, $k=1, \\ldots , n-1$, where $r^k$ is the residual at step $k$ of the GMRES algorithm applied to the linear system $Ax=b$, with initial residual $r^0 = b - A x^0$. Moreover, the matrix $A$ can be chosen to have any desired eigenvalues.\n",
      "\n",
      "10. id: 5390bf1320f70186a0f51777   score: 0.47257698   abstract: We generalize and extend results of the series of papers by Greenbaum and Strakoš (IMA Vol Math Appl 60:95---118, 1994), Greenbaum et al. (SIAM J Matrix Anal Appl 17(3):465---469, 1996), Arioli et al. (BIT 38(4):636---643, 1998) and Duintjer Tebbens and Meurant (SIAM J Matrix Anal Appl 33(3):958---978, 2012). They show how to construct matrices with right-hand sides generating a prescribed GMRES residual norm convergence curve as well as prescribed Ritz values in all iterations, including the eigenvalues, and give parametrizations of the entire class of matrices and right-hand sides with these properties. These results assumed that the underlying Arnoldi orthogonalization processes are breakdown-free and hence considered non-derogatory matrices only. We extend the results with parametrizations of classes of general nonsingular matrices with right-hand sides allowing the early termination\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696643\n",
      "index                                        559154980cf232eb904fbc76\n",
      "title                                 FPTAS for counting monotone CNF\n",
      "authors                                      Jingcheng Liu, Pinyan Lu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558fd66a612c29c89cd7b6f2;558ce6ea0cf2cffe760cd...\n",
      "abstract            A monotone CNF formula is a Boolean formula in...\n",
      "id                                                            1696643\n",
      "clustered_labels                                                    2\n",
      "Name: 1696643, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a63c20f70186a0e81c30   score: 0.9124362   abstract: We consider boolean formulas in conjunctive normal form (CNF). If all clauses are large, it needs many clauses to obtain an unsatisfiable formula; moreover, these clauses have to interleave. We review quantitative results for the amount of interleaving required, many of which rely on the Lovász Local Lemma, a probabilistic lemma with many applications in combinatorics.In positive terms, we are interested in simple combinatorial conditions which guarantee for a CNF formula to be satisfiable. The criteria obtained are nontrivial in the sense that even though they are easy to check, it is by far not obvious how to compute a satisfying assignment efficiently in case the conditions are fulfilled; until recently, it was not known how to do so. It is also remarkable that while deciding satisfiability is trivial for formulas that satisfy the conditions, a slightest relaxation of the conditions l\n",
      "\n",
      "2. id: 5390981d20f70186a0e04bd6   score: 0.89048487   abstract: We consider the satisfiability problem on Boolean formulas in conjunctive normal form. We show that a satisfying assignment of a formula can be found in polynomial time with a success probability of 2-n(1-1/(1+logm)), where n and m are the number of variables and the number of clauses of the formula, respectively. If the number of clauses of the formulas is bounded by nc for some constant c, this gives an expected run time of O(p(n).2n(1-1/(1+clogn))) for a polynomial p.\n",
      "\n",
      "3. id: 53908d6520f70186a0dd1df0   score: 0.86748546   abstract: We show that the number of satisfying assignments of a k-CNF formula is determined uniquely from the numbers of unsatisfying assignments for clause-sets of size up to ⌊log k⌋ + 2. This amount of information is also shown to be necessary.\n",
      "\n",
      "4. id: 558afca7612c41e6b9d3fa7b   score: 0.8662457   abstract: Let g: {-1, 1}^k to {-1, 1} be any Boolean function and q_1, dots, q_k be any degree-2 polynomials over {-1, 1}^n. We give a deterministic algorithm which, given as input explicit descriptions of g, q_1, dots, q_k and an accuracy parameter eps0, approximates [ Pr_{x sim {-1, 1}^n}[g(sign(q_ 1(x)), dots, sign(q_k(x)))=1] ] to within an additive pm eps. For any constant eps 0 and k geq 1 the running time of our algorithm is a fixed polynomial in n (in fact this is true even for some not-too-small eps = o_n(1) and not-too-large k = omega_n(1)). This is the first fixed polynomial-time algorithm that can deterministically approximately count satisfying assignments of a natural class of depth-3 Boolean circuits. Our algorithm extends a recent result DDS13:deg2count which gave a deterministic approximate counting algorithm for a single degree-2 polynomial threshold function sign(q(x)), correspo\n",
      "\n",
      "5. id: 55323d0d45cec66b6f9dd686   score: 0.8601343   abstract: A Boolean formula in conjunctive normal form (CNF) is called matched if the system of sets of variables which appear in individual clauses has a system of distinct representatives. Each matched CNF is trivially satisfiable (each clause can be satisfied by its representative variable). Another property which is easy to see, is that the class of matched CNFs is not closed under partial assignment of truth values to variables. This latter property leads to a fact (proved here) that given two matched CNFs it is co-NP complete to decide whether they are logically equivalent. The construction in this proof leads to another result: a much shorter and simpler proof of Σ2p-completeness of Boolean minimization for matched CNFs. The main result of this paper deals with the structure of clause minimum CNFs. We prove here that if a Boolean function f admits a representation by a matched CNF then ever\n",
      "\n",
      "6. id: 558b96e3612c6b62e5e8c328   score: 0.85989916   abstract: We study the hardness of approximation of clause minimum and literal minimum representations of pure Horn functions in n Boolean variables. We show that unless P=NP, it is not possible to approximate in polynomial time the minimum number of clauses and the minimum number of literals of pure Horn CNF representations to within a factor of 2 log 1 ¿ o ( 1 ) n $2^{\\\\log^{1-o(1)} n}$ . This is the case even when the inputs are restricted to pure Horn 3-CNFs with O(n 1+¿ ) clauses, for some small positive constant ¿. Furthermore, we show that even allowing sub-exponential time computation, it is still not possible to obtain constant factor approximations for such problems unless the Exponential Time Hypothesis turns out to be false.\n",
      "\n",
      "7. id: 5390a55520f70186a0e79538   score: 0.8446654   abstract: In this paper we consider the class of boolean formulas in Conjunctive Normal Form (CNF) where for each variable all but at most d occurrences are either positive or negative. This class is a generalization of the class of CNF formulas with at most d occurrences (positive and negative) of each variable which was studied in [Wahlström, 2005]. Applying complement search [Purdom, 1984], we show that for every d there exists a constant $\\gamma_d such that satisfiability of a CNF formula on n variables can be checked in runtime ${\\ensuremath{{O}}}(\\gamma_d^n)$ if all but at most d occurrences of each variable are either positive or negative. We thoroughly analyze the proposed branching strategy and determine the asymptotic growth constant *** d more precisely. Finally, we show that the trivial ${\\ensuremath{{O}}}(2^n)$ barrier of satisfiability checking can be broken even for a more general c\n",
      "\n",
      "8. id: 5390ba3820f70186a0f370d7   score: 0.84428066   abstract: We present an improvement on Thurley@?s recent randomized approximation scheme for #k-SAT where the task is to count the number of satisfying truth assignments of a Boolean function @F given as an n-variable k-CNF. We introduce a novel way to identify independent substructures of @F and can therefore reduce the size of the search space considerably. Our randomized algorithm works for any k. For #3-SAT, it runs in time O(@e^-^2@?1.51426^n), for #4-SAT, it runs in time O(@e^-^2@?1.60816^n), with error bound @e.\n",
      "\n",
      "9. id: 53909ee020f70186a0e32a02   score: 0.8233284   abstract: We construct a deterministic fully polynomial time approximationscheme (FPTAS) for computing the total number of matchings in abounded degree graph. Additionally, for an arbitrary graph, weconstruct a deterministic algorithm for computing approximately thenumber of matchings within running time exp(O(√n log2n)),where n is the number of vertices. Our approach is based on the correlation decay technique originating in statistical physics. Previously thisapproach was successfully used for approximately counting thenumber of independent sets and colorings in some classes of graphs [1, 24, 6].Thus we add another problem to the small, but growing, class of P-complete problems for whichthere is now a deterministic FPTAS.\n",
      "\n",
      "10. id: 53908cde20f70186a0dcf424   score: 0.8054792   abstract: We show that exact counting and approximate counting are polynomially equivalent. That is $P^{#P} = P^{Approx#P}$, where #$P$ is a function that computes the number of solutions to a given Boolean formula $f$ (denoted by $|| f ||$), and Approx#P computes a short list that contains $|| f ||$. It follows that if there is a good polynomial time approximator for #$P$ (i.e., one where the list has at most $O(|f|^{1-\\epsilon})$ elements), then $P = NP = P^{#P}$ and probabilistic polynomial time equals polynomial time. Thus we have strong evidence that #$P$ cannot be easily approximated.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652274\n",
      "index                                        5591518e0cf232eb904fbbba\n",
      "title                 Predicting Program Properties from \\\"Big Code\\\"\n",
      "authors                Veselin Raychev, Martin Vechev, Andreas Krause\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 42nd Annual ACM SIGPLAN-SIG...\n",
      "references          558b8093612c6b62e5e8a1d2;5390bb1d20f70186a0f3c...\n",
      "abstract            We present a new approach for predicting progr...\n",
      "id                                                            1652274\n",
      "clustered_labels                                                    3\n",
      "Name: 1652274, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558fedeb612c29c89cd7c519   score: 0.9717254   abstract: Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide vari\n",
      "\n",
      "2. id: 539087cb20f70186a0d5a451   score: 0.7310586   abstract: Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this paper, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction, evidence-based static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this paper, we use a neural network to map static features associated with each branch to the probability that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based \n",
      "\n",
      "3. id: 5390881720f70186a0d80dd0   score: 0.7180016   abstract: The need for accurate software prediction systems increases as software becomes much larger and more complex. A variety of techniques have been proposed; however, none has proven consistently accurate and there is still much uncertainty as to what technique suits which type of prediction problem. We believe that the underlying characteristics驴size, number of features, type of distribution, etc.驴of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. Also, in previous work, it has proven difficult to obtain significant results over small data sets. Consequently, it would be useful to have a large validation data set. Our solution is to simulate data allowing both con\n",
      "\n",
      "4. id: 539087d920f70186a0d61661   score: 0.64097303   abstract: Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this article, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction evidence-based static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this article, we use neural networks and decision trees to map static features associated with each branch to a prediction that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it i\n",
      "\n",
      "5. id: 5390bda020f70186a0f458ac   score: 0.6282934   abstract: Machine learning (ML) is the study of representations and algorithms used for building functions that improve their behavior with experience. Today, researchers in many domains are applying ML to solve their problems when conventional programming techniques have proven insufficient. The first such were simple, computing the setting of only a single output variable. More recently, structured learning is adding its own set of challenges, and the specifications designed for such learning based programs are not scaling well, nor are the programs they represent. This thesis introduces Learning Based Programming (LBP), the study of programming language formalisms that directly support programs that learn their representations from data. An LBP framework embodies a set of design principles that ensures learning based programs designed under its formalism are composable, prepared for an infinite\n",
      "\n",
      "6. id: 5390a6b120f70186a0e854c1   score: 0.5916053   abstract: This thesis shows how probabilistic graphical models may be applied in conjunction with static program analysis to automatically uncover software errors in large, complex, real-world systems. In particular, we show how probabilistic graphical models can be employed to tackle two critical problems in software bug-finding: (1) suppressing false error reports (false positives) emitted by bug-finding tools that arise due to analysis imprecision and (2) the automatic extraction of program-specific specifications directly from programs. For the first problem, we present two algorithms based on the use of statistical reasoning and Bayesian networks that are highly effective at suppressing false positives. Both algorithms are applicable to tools that check a variety of program properties, and we observe that they very often improve the effective precision of a bug-finding tool to the point where\n",
      "\n",
      "7. id: 53908bcc20f70186a0dc4ce3   score: 0.59136933   abstract: The need for accurate software prediction systems increases as software becomes much larger and more complex. A variety of techniques have been proposed, however, none has proved consistently accurate and there is still much uncertainty as to what technique suits which type of prediction problem. We believe that the underlying characteristics - size, number of features, type of distribution, etc. - of the dataset influence the choice of the prediction system to be used. In previous work, it has proved difficult to obtain significant results over small datasets. Consequently we required large validation datasets, moreover, we wished to control the characteristics of such datasets in order to systematically explore the relationship between accuracy, choice of prediction system and dataset characteristic. Our solution has been to simulate data allowing both control and the possibility of la\n",
      "\n",
      "8. id: 5390b78a20f70186a0f23406   score: 0.5070033   abstract: Web 2.0 applications rely heavily on JavaScript and client-side runtime manipulation of the DOM tree. One way to provide assurance about the correctness of such highly evolving and dynamic applications is through regression testing. However, JavaScript is loosely typed, dynamic, and notoriously challenging to analyze and test. We propose an automated technique for JavaScript regression testing, which is based on on-the-fly JavaScript source code instrumentation and dynamic analysis to infer invariant assertions. These obtained assertions are injected back into the JavaScript code to uncover regression faults in subsequent revisions of the web application under test. Our approach is implemented in a tool called Jsart. We present our case study conducted on nine open source web applications to evaluate the proposed approach. The results show that our approach is able to effectively generat\n",
      "\n",
      "9. id: 5390a63c20f70186a0e82799   score: 0.50625575   abstract: JavaScript is the main scripting language for Web browsers, and it is essential to modern Web applications. Programmers have started using it for writing complex applications, but there is still little tool support available during development. We present a static program analysis infrastructure that can infer detailed and sound type information for JavaScript programs using abstract interpretation. The analysis is designed to support the full language as defined in the ECMAScript standard, including its peculiar object model and all built-in functions. The analysis results can be used to detect common programming errors --- or rather, prove their absence, and for producing type information for program comprehension. Preliminary experiments conducted on real-life JavaScript code indicate that the approach is promising regarding analysis precision on small and medium size programs, which \n",
      "\n",
      "10. id: 5390b4c320f70186a0efdc1e   score: 0.49238646   abstract: We give an account of our experiences working at the intersection of two fields: program analysis and machine learning. In particular, we show that machine learning can be used to infer annotations for program analysis tools, and that program analysis techniques can be used to improve the efficiency of machine learning tools.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691382\n",
      "index                                        559190050cf2ff7b2eb9bcdb\n",
      "title                       On the Performance of the Spotify Backend\n",
      "authors             Rerngvit Yanggratoke, Gunnar Kreitz, Mikael Go...\n",
      "year                                                           2015.0\n",
      "venue                       Journal of Network and Systems Management\n",
      "references          5390bb7b20f70186a0f3f885;539087e120f70186a0d65...\n",
      "abstract            We model and evaluate the performance of a dis...\n",
      "id                                                            1691382\n",
      "clustered_labels                                                    1\n",
      "Name: 1691382, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bb7b20f70186a0f3f885   score: 0.9977216   abstract: We model and evaluate the performance of a distributed key-value storage system that is part of the Spotify backend. Spotify is an on-demand music streaming service, offering low-latency access to a library of over 16 million tracks and serving over 10 million users currently. We first present a simplified model of the Spotify storage architecture, in order to make its analysis feasible. We then introduce an analytical model for the distribution of the response time, a key metric in the Spotify service. We parameterize and validate the model using measurements from two different testbed configurations and from the operational Spotify infrastructure. We find that the model is accurate---measurements are within 11% of predictions---within the range of normal load patterns. We apply the model to what-if scenarios that are essential to capacity planning and robustness engineering. The main d\n",
      "\n",
      "2. id: 5390b0ca20f70186a0ed98fa   score: 0.83196384   abstract: With the advent of social networking and cloud data-stores, user data is increasingly being stored in large capacity and high performance storage systems, which account for a significant portion of the total cost of ownership of a datacenter (DC) [3]. One of the main challenges when trying to evaluate storage system options is the difficulty in replaying the entire application in all possible system configurations. Furthermore, code and datasets of DC applications are rarely available to storage system designers. This makes the development of a representative model that captures key aspects of the workload's storage profile, even more appealing. Once such a model is available, the next step is to create a tool that convincingly reproduces the application's storage behavior via a synthetic I/O access pattern.\n",
      "\n",
      "3. id: 53908a4020f70186a0d9d7b1   score: 0.77932185   abstract: Abstract In order to design a \"utility-aware\" streaming media service which automatically requests the necessary resources from Utility Data Center infrastructure, several classic performance questions should be answered: how to measure the basic capacity of a streaming media server? what is the set of basic benchmarks exposing the performance limits and main bottlenecks of a media server? In this paper, we propose a set of benchmarks for measuring the basic capacities of streaming media systems for different expected workloads, and demonstrate the results using an experimental testbed.\n",
      "\n",
      "4. id: 5390a1e620f70186a0e59af8   score: 0.7501869   abstract: A multimedia storage system plays a vital role for the performance and scalability of multimedia servers. To handle the server load imposed by increased user access to on-demand multimedia streaming applications, new storage system solutions are needed.\n",
      "\n",
      "5. id: 5390a1bc20f70186a0e54478   score: 0.53591824   abstract: In this paper, we study how to distribute storage capacity along a hierarchical system with cache-servers located at each node. This system is intended to deliver stored video streams in a video-on-demand way, ensuring that, once started, a transmission will be completed without any delay or quality loss. We use off-line smoothing for videos, dividing them into CBR video parts. Also, our request rates are distributed following a 24h audience curve. In this system, when a request is received, the server reserves the required bandwidth at the required time slots, trying to serve the video as soon as possible. We perform a detailed analysis by means of simulations of the start-up time delay for some storage distributions. It shows that an adequate storage distribution can increase performance about 25% with respect to a uniform distribution and about 47% with respect to one in which all the\n",
      "\n",
      "6. id: 5390bda020f70186a0f4787d   score: 0.5264036   abstract: The performance evaluation of large file systems, such as storage and media streaming, motivates scalable generation of representative traces. We focus on two key characteristics of traces, popularity and temporal locality. The common practice of using a system-wide distribution obscures per-object behavior, which is important for system evaluation. We propose a model based on delayed renewal processes which, by sampling interarrival times for each object, accurately reproduces popularity and temporal locality for the trace. A lightweight version reduces the dimension of the model with statistical clustering. It is workload-agnostic and object type-aware, suitable for testing emerging workloads and 'what-if' scenarios. We implemented a synthetic trace generator and validated it using: (1) a Big Data storage (HDFS) workload from Yahoo!, (2) a trace from a feature animation company, and (3\n",
      "\n",
      "7. id: 5390b4da20f70186a0f0051b   score: 0.4640514   abstract: Spotify is a popular music-streaming service which has seen widespread use across Europe. While Spotify's server-side behaviour has previously been studied, little is known about the client-side behaviour. In this paper, we describe an experimental study where we collect packet headers for Spotify traffic over multiple 24-hour time frames at a client host. Two distinct types of behaviour are observed, when tracks are being downloaded, and when the client is only serving requests from other peers. We also note wide variation in connection lifetimes, as seen in other studies of peer-to-peer systems. These findings are relevant for improving Spotify itself, and for the designers of other hybrid peer-to-peer and server-based distribution architectures.\n",
      "\n",
      "8. id: 5390b8d720f70186a0f2adb0   score: 0.447581   abstract: We design, implement and evaluate a global resource allocator to provide end-to-end quality of service in shared data centers and Clouds. Global resource allocation involves performance modeling for proportioning several levels of storage cache, and the storage bandwidth between applications according to overall performance goals. The problem is challenging due to the interplay between different resources, e.g., changing the client cache quota affects the access pattern at the cache/disk levels below it in the storage hierarchy. We use a combination of on-line modeling and sampling to arrive at near-optimal configurations within minutes. The key idea is to incorporate access tracking and known resource dependencies e.g., due to cache replacement policies, into our performance model. In our experimental evaluation, we use both micro-benchmarks and the industry standard benchmarks TPC-W an\n",
      "\n",
      "9. id: 558b59b3612c41e6b9d49deb   score: 0.43962705   abstract: Key-Value Stores (KVStore) are being widely used as the storage system for large-scale Internet services and cloud storage systems. However, they are rarely used in HPC systems, where parallel file systems (PFS) are the dominant storage systems. In this study, we carefully examine the architecture difference and performance characteristics of PFS and KVStore. We propose that it is valuable to utilize KVStore to optimize the overall I/O performance, especially for the workloads that PFS cannot handle well, such as the cases with hurtful data synchronization or heavy metadata operations. To verify this proposal, we conducted comprehensive experiments with several synthetic benchmarks, an I/O benchmark, and a real application. The results show that our proposal is promising.\n",
      "\n",
      "10. id: 5536867e0cf2dbb77a816a84   score: 0.43962705   abstract: Key-Value Stores (KVStore) are being widely used as the storage system for large-scale Internet services and cloud storage systems. However, they are rarely used in HPC systems, where parallel file systems (PFS) are the dominant storage systems. In this study, we carefully examine the architecture difference and performance characteristics of PFS and KVStore. We propose that it is valuable to utilize KVStore to optimize the overall I/O performance, especially for the workloads that PFS cannot handle well, such as the cases with hurtful data synchronization or heavy metadata operations. To verify this proposal, we conducted comprehensive experiments with several synthetic benchmarks, an I/O benchmark, and a real application. The results show that our proposal is promising.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1699676\n",
      "index                                        559168270cf2e89307ca99c7\n",
      "title               VoroGraph: Visualization Tools for Epidemic An...\n",
      "authors             Cody Dunne, Michael Muller, Nicola Perra, Maur...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390880220f70186a0d7704b;53909f8c20f70186a0e3f...\n",
      "abstract            Epidemiologists struggle to integrate complex ...\n",
      "id                                                            1699676\n",
      "clustered_labels                                                    0\n",
      "Name: 1699676, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539089bb20f70186a0d98c27   score: 0.23775287   abstract: The authors' clustering technique unambiguously locates, characterizes, and visualizes ecoregions and their borders. When coded with similarity colors, it can produce planar map views with sharpness contours that are visually rich in ecological information and represent integrated visualizations of complex and massive environmental data sets.\n",
      "\n",
      "2. id: 5390b72e20f70186a0f221bb   score: 0.16052906   abstract: Information visualisation methods can potentially be employed to assist the work of epidemiologists and other health care professionals in mapping the spread of communicable diseases in remote areas, where the task of disease surveillance encompasses temporal elements such as changes in climate, land use and population movements. This paper presents an investigation into the use of time-based visualisation techniques coupled with geographical maps and support for distributed mobile collection of patient data. This study has focused on the development of an information visualisation prototype designed for use by epidemiology researchers on mobile platforms (tablets and smart phones). The prototyping activity has involved the participation of prospective users working in the Amazon region. Initial results are presented and discussed.\n",
      "\n",
      "3. id: 5390bb1d20f70186a0f3e427   score: 0.11716747   abstract: Epidemiology requires the analysis and visualization of massive data sets. The field of cancer statistics in particular is facing the challenging task of visualizing a large data set that contains a wide range of available dimensions. The existing work of epidemiologists has been time-consuming because of visualization techniques that could not be scaled to support an unguided exploration process. This limitation has led to the inefficient use of data representations that are mainly used for detailed analysis. Our goal was to find a scalable visualization technique that focused on covering a wide range of categorical information. For this purpose, a task by data type taxonomy is used to analyze the existing data visualization techniques. The chosen representation was based on the implemented flow visualization and provided an overview for exploring the data by epidemiologists. In this wa\n",
      "\n",
      "4. id: 5390958a20f70186a0defce7   score: 0.11143445   abstract: Abstract- Cartograms are a well-known technique for showing geography-related statistical information, such as population demographics and epidemiological data. The basic idea is to distort a map by resizing its regions according to a statistical parameter, but in a way that keeps the map recognizable. In this study, we formally define a family of cartogram drawing problems. We show that even simple variants are unsolvable in the general case. Because the feasible variants are NP-complete, heuristics are needed to solve the problem. Previously proposed solutions suffer from problems with the quality of the generated drawings. For a cartogram to be recognizable, it is important to preserve the global shape or outline of the input map, a requirement that has been overlooked in the past. To address this, our objective function for cartogram drawing includes both global and local shape prese\n",
      "\n",
      "5. id: 5390a1bc20f70186a0e551d5   score: 0.097734325   abstract: Epidemiology, the study of disease risk factors in populations, emerged between the 16th and 19th centuries in response to terrifying epidemics of infectious diseases such as yellow fever, cholera and bubonic plague. Traditional epidemiological studies have led to modifications in hygiene, diet, and many other practices that have profoundly altered the dynamic between humans and diseases.In this thesis, we develop mathematical techniques to address modern challenges, including emerging diseases such as SARS and West Nile virus, the threat of bioterrorism, and stringent legislation protecting patient privacy. Within spatial epidemiology, one problem is to map the risk of disease across space (i.e., disease mapping), and another is to analyze the data for clustering. We propose a general technique, cartograms created from exact patient location data, that can address both of these problems\n",
      "\n",
      "6. id: 554a4a470cf2c69e96487318   score: 0.08882029   abstract: In this paper we propose a method for analysing and visualizing individual maps between shapes, or collections of such maps. Our method is based on isolating and highlighting areas where the maps induce significant distortion of a given measure in a multi-scale way. Unlike the majority of prior work, which focuses on discovering maps in the context of shape matching, our main focus is on evaluating, analysing and visualizing a given map, and the distortions it introduces, in an efficient and intuitive way. We are motivated primarily by the fact that most existing metrics for map evaluation are quadratic and expensive to compute in practice, and that current map visualization techniques are suitable primarily for global map understanding, and typically do not highlight areas where the map fails to meet certain quality criteria in a multi-scale way. We propose to address these challenges i\n",
      "\n",
      "7. id: 5390a5dc20f70186a0e7f9c7   score: 0.07682221   abstract: The pandemic spreading of a disease may be influenced in many ways. This paper focuses on simulation of the pandemic influenza and the graphical representation on the maps. For this reason we need to build maps by stitching the counties that contain cities and localities, the encoding of these elements using different colours on the map, the generation of the neighbour relationship, and finally the simulation.\n",
      "\n",
      "8. id: 5390a45620f70186a0e721d6   score: 0.07329728   abstract: There are many ways a pandemic spreading of a disease is influenced. This paper focuses on simulation of the pandemic influenza and the graphic representation on the maps. For this reason we need to build maps by stitching the counties that contain cities and localities, the encoding of these elements using different colours on the map, the generation of the neighbour relationship, and finally the simulation.\n",
      "\n",
      "9. id: 5390a6d920f70186a0e87dc1   score: 0.069668226   abstract: Spatial interactions (or flows), such as population migration and disease spread, naturally form a weighted location-to-location network (graph). Such geographically embedded networks (graphs) are usually very large. For example, the county-to-county migration data in the U.S. has thousands of counties and about a million migration paths. Moreover, many variables are associated with each flow, such as the number of migrants for different age groups, income levels, and occupations. It is a challenging task to visualize such data and discover network structures, multivariate relations, and their geographic patterns simultaneously. This paper addresses these challenges by developing an integrated interactive visualization framework that consists three coupled components: (1) a spatially constrained graph partitioning method that can construct a hierarchy of geographical regions (communities\n",
      "\n",
      "10. id: 5390b48420f70186a0efc03c   score: 0.058238626   abstract: The deaths of three children amid a series of recent influenza outbreaks in early March 2008 resulted in the immediate shut down of all kindergartens and primary schools in Hong Kong. While many parents welcome the decision, others queried the judgment given that citizens lack sufficient information to evaluate whether there is an outbreak and must follow actions prescribed by the government. We demonstrated in this paper various techniques to visualize disease distribution and present outbreak data for public consumption. Our analyses made use of affected (case) and non-affected (control) schools with influenza cases in March 2008. A series of maps were created to show disease spread and concentration by means of standard deviational ellipses, grid-based spatial autocorrelation, and kernel density. The generalized data did not permit statistical analysis other than the nearest neighbor \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1659483\n",
      "index                                        55914ae40cf2127aa930c910\n",
      "title               Delay-aware TDMA Scheduling for Multi-Hop Wire...\n",
      "authors                              Shanti Chilukuri, Anirudha Sahoo\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 International Conferen...\n",
      "references          5390a45620f70186a0e734a5;5390881720f70186a0d80...\n",
      "abstract            Time Division Multiple Access (TDMA)-based med...\n",
      "id                                                            1659483\n",
      "clustered_labels                                                    3\n",
      "Name: 1659483, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a54620f70186a0e78115   score: 0.9462997   abstract: Time division multiple access (TDMA) based medium access control (MAC) protocols can provide QoS with guaranteed access to the wireless channel. However, in multi-hop wireless networks, these protocols may introduce scheduling delay if, on the same path, an outbound link on a router is scheduled to transmit before an inbound link on that router. The total scheduling delay can be quite large since it accumulates at every hop on a path. This paper presents a method that finds conflict-free TDMA schedules with minimum scheduling delay. We show that the scheduling delay can be interpreted as a cost, in terms of transmission order of the links, collected over a cycle in the conflict graph. We use this observation to formulate an optimization, which finds a transmission order with the min-max delay across a set of multiple paths. The min-max delay optimization is NP-complete since the transmis\n",
      "\n",
      "2. id: 5390b63320f70186a0f180d3   score: 0.9081302   abstract: In this work, the region of achievable quality-of-service (QoS) is precisely described for a system of real-time heterogeneous variable bit rate (VBR) sources competing for slots (packet transmission times) of a time division multiple access (TDMA) frame. The QoS for each application is defined in terms of a maximum tolerable packet-dropping probability. Packets may be dropped due to delay violations and channel induced errors. The region of achievable QoS is precisely described for an interference/resource limited network by considering the underlying TDMA-multiple access control (TDMA-MAC) structure and the physical channel. A simple QoS-sensitive error-control protocol that combats the effects of the wireless channel while satisfying the real-time requirements is proposed and its impact on the region of achievable QoS is evaluated. The results presented here clearly illustrate the neg\n",
      "\n",
      "3. id: 5390aaf920f70186a0ead559   score: 0.9071478   abstract: Quality of Service (QoS) provision in wireless ad hoc networks requires the support of efficient MAC protocols and Radio Resource Management strategies to efficiently handle the access process and to achieve a high resource reuse. In this context, we present in this paper a MAC proposal capable of providing resource reservation and service differentiation in the medium access level as a basis for guaranteeing end-to-end QoS support in higher levels. We propose a TDMA MAC structure based on a frame subdivision consisting of a broadcast control phase and a data phase. We have designed an adaptive strategy to share the resources between both control and data services and a resource reservation strategy for data services that solves the problems stemmed from the contention for the medium, also adding a service differentiation mechanism based on priorities. In addition, we evaluate the impact\n",
      "\n",
      "4. id: 5390980720f70186a0e0233d   score: 0.8975158   abstract: The next generation of wireless networks are expected to support multimedia and real-time applications with quality of service (QoS) guaranteed. While bandwidth is a scarce resource in wireless and mobile communications, the demand for higher bandwidth is growing, particularly for multimedia applications that are delay-sensitive and/or loss-sensitive. In wireless networks with real-time traffic, there is a need for steady throughput with stable latency to satisfy the QoS requirements for real-time applications. In this paper, we provide the delay and jitter analysis of the demand assignment multiple access (DAMA) in closed form, which is based on a generalized time-division multiple access (G-TDMA) protocol, in which heterogenous stations with diverse demands for bandwidth use one or more time slots during a frame time. The number of assigned slots for a station is proportional to the st\n",
      "\n",
      "5. id: 5390b36120f70186a0ef1446   score: 0.8908652   abstract: Many scheduling techniques have been developed to solve the problem of sharing the common channel to multiple stations. TDMA has been increasingly used as a scheduling technique in ad-hoc networks. The current trend for QoS capable applications led to the deployment of numerous routing schemes that use TDMA. These schemes try to solve the problem of distributing the available slots among the wireless nodes and at the same time, to find paths within the network that fulfill some QoS related limitations, such as end-to-end delay. The exact way the slots are distributed among the transmitting nodes has an impact on the end-to-end delay and other performance parameters of the network, such as capacity. Therefore, the efficiency of the scheduling algorithms is closely related to the network topologies. In this paper, we propose two new end-to-end TDMA scheduling algorithms that try to enhance\n",
      "\n",
      "6. id: 53908e0020f70186a0dd471a   score: 0.88700897   abstract: In this paper, we propose a novel MAC protocol, called Quality-of-service Guarantee Multiple Access (QGMA), in wireless local area networks to support the quality of service required by embedded and multimedia-integrated real-time applications. As compared to other existing MAC protocols, QGMA achieves high system utilization, and provides a deterministic bound on the connection establishment delay and temporal QoS for real-time applications. In particular, by having the base station handle resource arbitration and schedule and utilize different scheduling algorithms to arbitrate access to data slots on a wireless channel, mobile hosts are relieved from the burden of coordinating all the ongoing activities, and moreover, different levels of temporal QoS can be easily achieved.To describe the key features of QGMA, we first present the base protocol, and then extend it to address several r\n",
      "\n",
      "7. id: 5390a25820f70186a0e606a5   score: 0.88161486   abstract: Current and future wireless standards use TDMA to provide guaranteed Quality-of-Service (QoS) in the network. While these standards specify how transmissions should occur, they do not discuss scheduling algorithms to find when transmissions should occur (transmission schedules). Despite the technological advances, the question of finding transmission schedules has existed for the past twenty years without a satisfactory answer. This thesis presents a new class of scheduling algorithms for Time Division Multiple Access (TDMA) wireless multihop networks. These algorithms have three major advantages. First, they take into account overhead and delay. With reduced overhead, transmission schedules have much higher throughput than what is possible with previous approaches. The algorithms can also be customized to produce schedules with specific delay properties. Scheduling to achieve a specific\n",
      "\n",
      "8. id: 5390a1e620f70186a0e59a50   score: 0.8728902   abstract: In the third-generation (and beyond) wireless communication systems, there will be a mixture of different traffic classes, each having its own transmission rate characteristics and quality-of-service (QoS) requirements. In this paper, a QoS-oriented medium access control (MAC) protocol with fair packet loss sharing (FPLS) scheduling is proposed for wireless code-division multiple access (CDMA) communications. The QoS parameters under consideration are the transmission bit error rate (BER), packet loss, and delay requirements. The MAC protocol exploits both time-division and code-division statistical multiplexing. The BER requirements are guaranteed by properly arranging simultaneous packet transmissions and controlling their transmit power levels, whereas the packet loss and delay requirements are guaranteed by proper packet scheduling. The basic idea of FPLS is to schedule the transmiss\n",
      "\n",
      "9. id: 5390a72220f70186a0e8989e   score: 0.86938226   abstract: As multimedia and high transmission of continuous media, such as live audio and video, with high quality services become more popular on wireless networks, the various traffic requiring different qualities of service (QoS) will co-exist. To this end, a request time division multiple access (TDMA)/code division multiple access (CDMA) protocol for supporting multimedia traffic in wireless networks, where CDMA is laid over TDMA has been proposed recently. In this paper, we wish to extend this scheme by introducing several QoS to the end-user, and present a generalized performance analysis of a request TDMA/CDMA QoS-based protocol. Quality of service factors includes customer retrial rates due to both user impatience and system timeouts. Our proposed analytical model allows us to understand how the end-to-end QoS guarantee can be obtained by analyzing the QoS requirements at every stage of t\n",
      "\n",
      "10. id: 5390ae2e20f70186a0ec75b8   score: 0.86165464   abstract: The design of a routing protocol for the provision of quality of service (QoS) in a mobile ad hoc network is a challenging task. In this paper, we propose a routing protocol that tries to satisfy QoS requirements in terms of bandwidth and end-to-end delays of a flow of packets in a time division multiple access (TDMA) based ad hoc network. Our protocol tries to identify a path that avoids hidden terminal and exposed terminal problems. In addition to that, our protocol tries to maximize the throughput of the network by minimizing the contention delays. We have carried out simulations to study the performance of our protocol. The proposed protocol performs better in terms of QoS satisfaction ratio and throughput as compared to the existing protocols.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1716149\n",
      "index                                        55323c6245cec66b6f9dbf40\n",
      "title               Vehicle to Vehicle GeoNetworking using Wireles...\n",
      "authors             José J. Anaya, Edgar Talavera, Felipe Jiménez,...\n",
      "year                                                           2015.0\n",
      "venue                                                 Ad Hoc Networks\n",
      "references          558c033c0cf23f2dfc596a1c;558be7e90cf20e727d0f3d34\n",
      "abstract            Vehicular communications will be the next qual...\n",
      "id                                                            1716149\n",
      "clustered_labels                                                    3\n",
      "Name: 1716149, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac5720f70186a0eb6c10   score: 0.86153823   abstract: A Vehicular Ad Hoc Network (VANET) is a non-infrastructure based network that does not rely on a central administration for communication between vehicles. The flexibility of VANETs opens the door to a myriad of applications; however, there are also a number of computer communication challenges that await researchers and engineers who are serious about their implementation and deployment. Advances in Vehicular Ad-Hoc Networks: Developments and Challenges tackles the prevalent research challenges that hinder a fully deployable vehicular network. This unique reference presents a unified treatment of the various aspects of VANETs and is essential for not only university professors, but also for researchers working in the automobile industry.\n",
      "\n",
      "2. id: 5390b61e20f70186a0f1453e   score: 0.85729086   abstract: There has been significant interest and progress in the field of vehicular ad hoc networks over the last several years. VANETs comprise vehicle-to-vehicle and vehicle-to-infrastructure communications based on wireless local area network technologies. The distinctive set of candidate applications (e.g., collision warning and local traffic information for drivers), resources (licensed spectrum, rechargeable power source), and the environment (e.g., vehicular traffic flow patterns, privacy concerns) make the VANET a unique area of wireless communication. This article gives an overview of the field, providing motivations, challenges, and a snapshot of proposed solutions.\n",
      "\n",
      "3. id: 5390c04520f70186a0f56889   score: 0.7856377   abstract: Vehicular communications are now the dominant mode of transferring information between automobiles. One of the most promising applications of vehicular communications is the vehicular ad hoc network (VANET), an approach to the intelligent transportation system (ITS). VANET is a subclass of the mobile ad hoc network, which does not depend on fixed infrastructure, in which the nodes are highly mobile. Therefore, the network topology changes rapidly. The design of routing protocols in VANETs is crucial in supporting the ITS. As a prerequisite to communication, the VANET routing protocols must establish an efficient route between network nodes. Furthermore, they should adjust efficiently to the quickly varying topology of moving vehicles. In this paper, we discuss the main characteristics and the research challenge of routing in VANETs, which may be considered in designing various routing pr\n",
      "\n",
      "4. id: 554cedc20cf21c5c67b8fec9   score: 0.7856377   abstract: Vehicular communications are now the dominant mode of transferring information between automobiles. One of the most promising applications of vehicular communications is the vehicular ad hoc network (VANET), an approach to the intelligent transportation system (ITS). VANET is a subclass of the mobile ad hoc network, which does not depend on fixed infrastructure, in which the nodes are highly mobile. Therefore, the network topology changes rapidly. The design of routing protocols in VANETs is crucial in supporting the ITS. As a prerequisite to communication, the VANET routing protocols must establish an efficient route between network nodes. Furthermore, they should adjust efficiently to the quickly varying topology of moving vehicles. In this paper, we discuss the main characteristics and the research challenge of routing in VANETs, which may be considered in designing various routing pr\n",
      "\n",
      "5. id: 5390ada620f70186a0ec2c87   score: 0.7676898   abstract: One of the notoriously difficult problems in vehicular ad-hoc networks (VANET) is to ensure that established routing paths do not break before the end of data transmission. This is a difficult problem because the network topology is constantly changing and the wireless communication links are inherently unstable, due to high node mobility. In this paper we classify existing VANET routing protocols into five categories: connectivity-based, mobility-based, infrastructure-based, geographic-location-based, and probability-model-based, according to their employed routing metrics. For each category, we present the general design ideas and state of the art. Our objective is to attract more attention to the VANET routing problem and encourage more research efforts on developing reliable solutions.\n",
      "\n",
      "6. id: 5390bf1320f70186a0f504e3   score: 0.765418   abstract: Vehicular networking is a new emerging wireless technology that supports the communication amongst vehicles and enables vehicles to connect with the Internet. This networking technology provides vehicles with endless possibility of applications, including safety, convenience, and entertainment applications. Examples for these applications are safety messaging, real-time traffic, route updates, and general purpose Internet access. The goal of vehicular networks is to provide an efficient, safe, and convenient environment for the vehicles. In vehicular networking technology, vehicles connect either through other vehicles in an ad-hoc multi-hop fashion or through road side units (infrastructure) which connects them to the Internet. Each approach has its own advantages and disadvantages. However, one of the main objectives of vehicular networking is to achieve a minimal delay for message del\n",
      "\n",
      "7. id: 5390b72e20f70186a0f20937   score: 0.75833166   abstract: Vehicular Ad-hoc Network (VANET) has become an active area of research due to its major role to improve vehicle and road safety, traffic efficiency, and convenience as well as comfort to both drivers and passengers. This paper thus addresses some of the attributes and challenging issues related to Vehicular Ad-hoc Networks (VANETs). A lot of VANET research work have focused on specific areas including routing, broadcasting, Quality of Service (QoS), and security. In this paper, a detailed overview of the current information gathering and data fusion capabilities and challenges in the context of VANET is presented. In addition, an overall VANET framework, an illustrative VANET scenario are provided in order to enhance safety, flow, and efficiency of the transportation system.\n",
      "\n",
      "8. id: 5390b20120f70186a0ee4abb   score: 0.7360213   abstract: Vehicular Ad-hoc Networks (VANETs) are currently in everyone's mouth when talking about future technologies that will be implemented in the automotive industry. In the last years the IEEE group has been working in the development of a standard for vehicular communications, this standard is the 802.11p. Most research work in this area has focused on vehicle-to-vehicle communication architecture, thus, more research is still necessary on the vehicle-to-infrastructure communication architecture. In this paper we present a performance study of the 802.11p technology for the development of applications in VANETs. This work simulates the IEEE 802.11p technology in a vehicular scenario. Results show that the IEEE 802.11p is an adequate technology for the development of several vehicular applications in terms of rate of packet loss, average end-to-end delay, and throughput.\n",
      "\n",
      "9. id: 5390a55520f70186a0e7b786   score: 0.695338   abstract: Vehicular Ad hoc Networks (VANETS) are self-organizing communities of wheeled mobile units consisting of large numbers of trucks, cars, buses and a small number of static infrastructure nodes such as traffic signals, highway rail grade crossings, and informational signage within radio communication range to each other. VANET is a promising approach for facilitating road safety, traffic management,and infotainment dissemination for drivers and passengers.Key characteristics that distinguish VANETs from other networks are time-varying nature of vehicle density, high mobility, and time-critical safety applications. Hence, devising protocols for VANETs may not be successfully accomplished by simple adaptation of protocols designed for wired networks and Mobile Ad hoc Networks (MANETs). In this paper, we compared and evaluated the performance of following routing protocols: AODV, DSR, and Swa\n",
      "\n",
      "10. id: 5390ad8920f70186a0ec0b79   score: 0.67800725   abstract: Vehicle to vehicle communication has strong potential to be a mechanism to improve driver’s safety and is now emerging as a prominent research area all over the world. At present, vehicular networks are still not considered to be very efficient because of their rapid topology changes and their highly dynamic structure. However, there has been ongoing and progressive research and development in Vehicular Ad hoc Networks (VANETs) [1] [2] [3] to support vehicle to vehicle communication, particularly in the area of routing in VANETs. In VANETs, routing schemes to reduced overhead and resource consumption is required to ensure successful message transfer within the network. The routing protocol proposed in this paper is based on a multi-hop transfer of a single message by discovering the most suitable vehicle within the transmission range instead of using single hop broadcast (flooding) schem\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691697\n",
      "index                                        559150f20cf232eb904fbb95\n",
      "title               BonnPlace: A Self-Stabilizing Placement Framework\n",
      "authors             Ulrich Brenner, Anna Hermann, Nils Hoppmann, P...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Symposium on Internati...\n",
      "references          558b530c612c41e6b9d49366;5390b61e20f70186a0f15...\n",
      "abstract            We present a new algorithm for VLSI placement....\n",
      "id                                                            1691697\n",
      "clustered_labels                                                    3\n",
      "Name: 1691697, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390981d20f70186a0e068ec   score: 0.95150405   abstract: We present BonnPlace, a new VLSI placement algorithm that combines the advantages of analytical and partitioning-based placers. Based on (non-disjoint) placements minimizing the total quadratic netlength, we partition the chip area into regions and assign the circuits to them (meeting capacity constraints) such that the placement is changed as little as possible. The core routine of our placer is a new algorithm for the Transportation Problem that allows to compute efficiently the circuit assignments to the regions. We test our algorithm on a set of industrial designs with up to 3.6 millions of movable objects and two sets of artificial benchmarks showing that it produces excellent results. In terms of wirelength, we can improve the results of leading-edge placement tools by about 5%.\n",
      "\n",
      "2. id: 53908bad20f70186a0dc3d82   score: 0.8934954   abstract: New placement algorithms have been developed which are suitable for the layout of Very Large Scale Integrated (VLSI) circuits Hierarchical decomposition is used to reduce the circuit function to a size that can be comprehended by the designer and is computationally feasible to layout. At each hierarchical level the problem consists of the placement of interconnected rectangular blocks of arbitrary size and shape such that the area occupied by the blocks and their interconnections is minimal. Constructive initial placement and iterative improvement algorithms are presented.\n",
      "\n",
      "3. id: 53908bad20f70186a0dc33df   score: 0.8677098   abstract: This paper presents a new algorithm for the initial placement of hierarchical VLSI circuits. The components to be placed are orthogonal macrocells of variable shape and size. This algorithm combines the advantages of force directed placement and min-cut algorithm. It provides an initial placement which avoids overlapping between cells and includes an estimation of routting area. This algorithm is suitable for regular cell arrangements, too.\n",
      "\n",
      "4. id: 5390994d20f70186a0e13338   score: 0.86488205   abstract: To achieve timing closure, one often has to run through several iterations of physical synthesis flows, for which placement is a critical step. During these iterations, one hopes to consistently move towards design convergence. A placement algorithm that is \"stable\" will consistently drive towards similar solutions, even with changes in the input netlist and placement parameters. Indeed, the stability of the algorithm is arguably as important a characteristic as the wirelength it achieves. However, currently there is no way to actually quantify the stability of a placement algorithm. This work seeks to address the issue by proposing metrics that measure the stability of a placement algorithm. Our experimental results examine the stability of three different placement algorithms with our proposed metrics and convincingly illustrate that some algorithms are quantifiably more stable than ot\n",
      "\n",
      "5. id: 5390962020f70186a0df56d9   score: 0.86223567   abstract: VLSI placement tools usually work in two steps: First, the cells that have to be placed are roughly spread out over the chip area ignoring disjointness (global placement). Then, in a second step, the cells are moved to their final position such that all overlaps are removed and all additional constraints are met (detailed placement or legalization).We consider algorithms for legalization. In particular, we analyze a generic legalization algorithm based on minimum cost flows and dynamic programming. Specializations are being used in industry for many years, and an improved version was proposed very recently in [2]. The objective of all these algorithms is to minimize the weighted sum of (squared) movements, i.e. they assume the placement to be already optimized except for not being legal.To evaluate results, we propose two different lower bounds for the legalization problem, one based on \n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3e0b7   score: 0.8578872   abstract: We present a new approach to VLSI placement legalization. Based on a minimum-cost flow algorithm that iteratively augments flows along paths, our algorithm ensures that only augmentations are considered that can be realized exactly by cell movements. Hence, the method avoids realization problems which are inherent to previous flow-based legalization algorithms. As a result, it combines the global perspective of minimum-cost flow approaches with the efficiency of local search algorithms. The tool is mainly designed to minimize total and maximum cell movement but it is flexible enough to optimize the effect on timing or netlength, too. We compare our approach to legalization tools from industry and academia by experiments on dense recent real-world designs and public benchmarks. The results show that we are much faster and produce significantly better results in terms of average (linear an\n",
      "\n",
      "7. id: 539087e120f70186a0d672b8   score: 0.8370879   abstract: This is a survey on the algorithms which are part ofa program for flat placement of large-scale VLSI processorchips. The basis is a quadratic optimization approachcombined with a new quadrisection algorithm.In contrast to most previous quadratic placement methods,no min-cut objective is used at all. Based on aquadratic placement, a completely new algorithm findsa four-way partitioning meeting capacity constraintsand minimizing the total movement.\n",
      "\n",
      "8. id: 558fe631612c29c89cd7c049   score: 0.8113773   abstract: Placement is the process of determining the exact locations of circuit elements within a chip. It is a crucial step in very large scale integration (VLSI) physical design, because it affects routability, performance, and power consumption of a design. In this paper, we develop a new analytical placer to solve the VLSI standard cell placement problem. The placer consists of two phases, multilevel global placement (GP) and detailed cell placement (DP). In the stage of GP, during the clustering stage, we use a nonlinear programming technique and a best-choice clustering algorithm to take a global view of the whole netlist and placement information, and then use an iterative local refinement technique during the declustering stage to further distribute the cells and reduce the wirelength. In the stage of DP, we develop a fast legalization algorithm to make the solution by global placement le\n",
      "\n",
      "9. id: 5390b61e20f70186a0f15b6b   score: 0.8089744   abstract: Most tools for the placement of very large scale integrated chips work in two steps. First, the cells that have to be placed are roughly spread out over the chip area, ignoring disjointness (global placement). Then, in a second step, the cells are moved to their final position such that all overlaps are removed and all additional constraints are met (detailed placement or legalization). In this paper, we describe new ideas for legalization. We divide the task into appropriate subproblems that can be solved optimality in polynomial time. For the most important parts, even a linear running time can be shown. Together, the solutions of the subproblems can be combined to an algorithm that legalizes a placement minimizing the total (linear or squared) movement of cells. The algorithm is tested on a set of recent application specific integrated circuits and the results are compared to lower bo\n",
      "\n",
      "10. id: 539087f820f70186a0d7256d   score: 0.67875296   abstract: This paper describes a study of placement procedures for VLSI Standard Cell Layout. The procedures studied are Simulated Annealing, Min Cut placement, and a number of improvements to Min Cut placement including a technique called Terminal Propagation which allows Min Cut to include the effect of connections to external cells. The Min Cut procedures are coupled with a Force Directed Pairwise Interchange (FDPI) algorithm for placement improvement. For the same problem these techniques produce a range of solutions with a typical standard deviation 4% for the total wire length and 3% to 4% for the routed area. The spread of results for Simulated Annealing is even larger. This distribution of results for a given algorithm implies that mean results of many placements should be used when comparing algorithms. We find that the Min Cut partitioning with simplified Terminal Propagation is the most\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710398\n",
      "index                                        55323be045cec66b6f9dac0c\n",
      "title               A novel estimator based learning automata algo...\n",
      "authors             Hao Ge, Wen Jiang, Shenghong Li, Jianhua Li, Y...\n",
      "year                                                           2015.0\n",
      "venue                                            Applied Intelligence\n",
      "references          5390b5c620f70186a0f0770a;5390bb1d20f70186a0f3e...\n",
      "abstract            Reinforcement learning is one of the subjects ...\n",
      "id                                                            1710398\n",
      "clustered_labels                                                    0\n",
      "Name: 1710398, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac5720f70186a0eb6b50   score: 0.9718859   abstract: Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of rein\n",
      "\n",
      "2. id: 5390bf1320f70186a0f51eef   score: 0.96138906   abstract: There are currently two fundamental paradigms that have been used to enhance the convergence speed of Learning Automata (LA). The first involves the concept of utilizing the estimates of the reward probabilities, while the second involves discretizing the probability space in which the LA operates. This paper demonstrates how both of these can be simultaneously utilized, and in particular, by using the family of Bayesian estimates that have been proven to have distinct advantages over their maximum likelihood counterparts. The success of LA-based estimator algorithms over the classical, Linear Reward-Inaction (L RI )-like schemes, can be explained by their ability to pursue the actions with the highest reward probability estimates. Without access to reward probability estimates, it makes sense for schemes like the L RI to first make large exploring steps, and then to gradually turn explo\n",
      "\n",
      "3. id: 558aec26612c41e6b9d3d95f   score: 0.94386727   abstract: The most difficult part in the design and analysis of Learning Automata (LA) consists of the formal proofs of their convergence accuracies. The mathematical techniques used for the different families (Fixed Structure, Variable Structure, Discretized etc.) are quite distinct. Among the families of LA, Estimator Algorithms (EAs) are certainly the fastest, and within this family, the set of Pursuit algorithms have been considered to be the pioneering schemes. Informally, if the environment is stationary, their ¿-optimality is defined as their ability to converge to the optimal action with an arbitrarily large probability, if the learning parameter is sufficiently small/large. The existing proofs of all the reported EAs follow the same fundamental principles, and to clarify this, in the interest of simplicity, we shall concentrate on the family of Pursuit algorithms. Recently, it has been re\n",
      "\n",
      "4. id: 5390958920f70186a0deee91   score: 0.87810534   abstract: Reinforcement Learning is the problem faced by a controller thatmust learn behavior through trial and error interactions with adynamic environment. The controller's goal is to maximize rewardover time, by producing an effective mapping of states to actionscalled policy. To construct the model of such systems, in thispaper, we present a generalized learning automaton approach withQ-learning behaviors. Comparing to Q-learning, the computationalexperiments of the pursuit problems show that proposedreinforcement scheme obtains better results in terms of convergencespeedand memory size.\n",
      "\n",
      "5. id: 539087c320f70186a0d54706   score: 0.82161736   abstract: In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework.In our model the learner does not have direct access to every state of the environment. Instead, every sequence of experiments starts in a fixed initial state and the learner is provided with a “reset” operation that interrupts the current sequence of experiments and starts a new one (from the initial state).We do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is &egr;-close to that of the optimal policy, with probability no less than 1−&dgr;.For this model, we describe an algorithm that produces such an (&egr;,&dgr;)-optimal policy for any environment, in time polynomial in N,K,1/&egr;,1/&dgr;,1/(1−&bgr;) and rma\n",
      "\n",
      "6. id: 5390a96f20f70186a0ea435d   score: 0.76454014   abstract: Learning Automata (LA) were recently shown to be valuable tools for designing Multi-Agent Reinforcement Learning algorithms. One of the principal contributions of LA theory is that a set of decentralized, independent learning automata is able to control a finite Markov Chain with unknown transition probabilities and rewards. We extend this result to the framework of Multi-Agent MDP's, a straightforward extension of single-agent MDP's to distributed cooperative multi-agent decision problems. Furthermore, we combine this result with the application of parametrized learning automata yielding global optimal convergence results.\n",
      "\n",
      "7. id: 53908a4020f70186a0d9d198   score: 0.7641884   abstract: An S-model absorbing learning automaton (LA) which is based on the use of a stochastic estimator is introduced. According to the proposed stochastic estimator scheme, the estimates of the mean rewards of actions are computed stochastically. Actions that have not been selected many times have the opportunity to be estimated as optimal, to increase their choice probabilities, and consequently, to be selected. In this way, the automaton's accuracy and speed of convergence are significantly improved.\n",
      "\n",
      "8. id: 53909ee020f70186a0e321e8   score: 0.7570767   abstract: We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/ε2)log(1/δ)) times to find an ε-optimal arm with probability of at least 1-δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and a\n",
      "\n",
      "9. id: 5390b5c620f70186a0f0770a   score: 0.745954   abstract: The fastest learning automata (LA) algorithms currently available fall in the family of estimator algorithms introduced by Thathachar and Sastry (1986). The pioneering work of these authors was the pursuit algorithm, which pursues only the current estimated optimal action. If this action is not the one with the minimum penalty probability, this algorithm pursues a wrong action. In this paper, we argue that a pursuit scheme that generalizes the traditional pursuit algorithm by pursuing all the actions with higher reward estimates than the chosen action, minimizes the probability of pursuing a wrong action, and is a faster converging scheme. To attest this, we present two new generalized pursuit algorithms (GPAs) and also present a quantitative comparison of their performance against the existing pursuit algorithms. Empirically, the algorithms proposed here are among the fastest reported L\n",
      "\n",
      "10. id: 5390b71120f70186a0f1f0ee   score: 0.73392886   abstract: In this paper we present a reinforcement learning technique based on Learning Automata (LA), more specific Continuous Action Reinforcement Learning Automaton (CARLA), introduced by Howell et. al. in [2]. LA are policy iterators, which have shown good convergence results in discrete action games with independent learners. The approach presented in this paper allows LA to deal with continuous action spaces. Recently, Rodríguez et al. [3] performed an analysis of the CARLA algorithm. The result of this analysis was an improvement of the CARLA method in terms of computation effort and local convergence properties. The improved automaton performs very well in single agent problems, but still has suboptimal performance with respect to global convergence in multi-agent settings. The CARLA algorithm has successfully been applied to control problems [2, 1]. However in real world applications syst\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1716098\n",
      "index                                        55323c5d45cec66b6f9dbe5c\n",
      "title               Independent domination in finitely defined cla...\n",
      "authors              Vadim Lozin, Raffaele Mosca, Christopher Purcell\n",
      "year                                                           2015.0\n",
      "venue                                    Discrete Applied Mathematics\n",
      "references                                   558fe5aa0cf23515427183d0\n",
      "abstract            We study the problem of finding in a graph an ...\n",
      "id                                                            1716098\n",
      "clustered_labels                                                    2\n",
      "Name: 1716098, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5590e12d0cf25001c36efa94   score: 0.98262435   abstract: We study the independent dominating set problem restricted to graph classes defined by finitely many forbidden induced subgraphs. The main result is two sufficient conditions for the problem to be NP-hard in a finitely defined class of graphs. We conjecture that those conditions are also necessary and describe several classes of graphs verifying the conjecture.\n",
      "\n",
      "2. id: 53908bfb20f70186a0dc9a0d   score: 0.98262435   abstract: We study the independent dominating set problem restricted to graph classes defined by finitely many forbidden induced subgraphs. The main result is two sufficient conditions for the problem to be NP-hard in a finitely defined class of graphs. We conjecture that those conditions are also necessary and describe several classes of graphs verifying the conjecture.\n",
      "\n",
      "3. id: 5390b56a20f70186a0f04e6b   score: 0.9666631   abstract: The maximum independent set problem is known to be NP-hard for graphs in general, but is solvable in polynomial time for graphs in many special classes. It is also known that the problem is generally intractable from a parameterized point of view. A simple Ramsey argument implies the fixed-parameter tractability of the maximum independent set problem in classes of graphs of bounded clique number. Beyond this observation very little is known about the parameterized complexity of the problem in restricted graph families. In the present paper we develop fpt-algorithms for graphs in some classes extending graphs of bounded clique number.\n",
      "\n",
      "4. id: 539099a220f70186a0e18133   score: 0.9657707   abstract: A graph G is called a satgraph if there exists a partition A ∪ B = V(G) such that • A induces a clique [possibly, A = 0], • B induces a matching [i.e., G(B) is a 1-regular subgraph, possibly, B = 0], and • there are no triangles (a, b, b'), where a ∈ A and b, b' ∈ B.We also introduce the hereditary closure of IAJ, denoted by HIAJ [hereditary satgraphs]. The class HIAJ contains split graphs. In turn, HIAJ is contained in the class of all (1, 2)-split graphs [A. Gyárfás, Generalized split graphs and Ramsey numbers, J. Combin. Theory Ser. A 81 (2) (1998) 255-261], the latter being still not characterized. We characterize satgraphs in terms of forbidden induced subgraphs.There exist close connections between satgraphs and the satisfiability problem [SAT]. In fact, SAT is linear-time equivalent to finding the independent domination number in the corresponding satgraph. It follows that the ind\n",
      "\n",
      "5. id: 5390b8d720f70186a0f2c225   score: 0.9530133   abstract: A dominating set of a graph is a set S of vertices such that every vertex in the graph is either in S or is adjacent to a vertex in S. The domination number of a graph G, denoted @c(G), is the minimum cardinality of a dominating set of G. We show that if G is an n-vertex maximal outerplanar graph, then @c(G)@?(n+t)/4, where t is the number of vertices of degree 2 in G. We show that this bound is tight for all t=2. Upper-bounds for @c(G) are known for a few classes of graphs.\n",
      "\n",
      "6. id: 5390b29820f70186a0eea1d4   score: 0.9524858   abstract: A dominating set of a graph G = (V,E) is a subset of vertices such that every vertex in has at least one neighbour in . Moreover if is an independent set, i.e. no vertices in are pairwise adjacent, then is said to be an independent dominating set. Finding a minimum independent dominating set in a graph is an NP-hard problem. We give an algorithm computing a minimum independent dominating set of a graph on n vertices in time O(1.3575n). Furthermore, we show that Ω(1.3247n) is a lower bound on the worst-case running time of this algorithm.\n",
      "\n",
      "7. id: 5590c6b80cf25001c36eea05   score: 0.94834626   abstract: We study two central problems of algorithmic graph theory: finding maximum and minimum maximal independent sets. Both problems are known to be NP-hard in general. Moreover, they remain NP-hard in many special classes of graphs. For instance, the problem of finding minimum maximal independent sets has been recently proven to be NP-hard in the class of so-called (1,2)-polar graphs. On the other hand, both problems can be solved in polynomial time for (1,1)-polar, also known as split graphs. In this paper, we address the question of distinguishing new classes of graphs admitting polynomial-time solutions for the two problems in question. To this end, we extend the hierarchy of (@a,@b)-polar graphs and study the computational complexity of the problems on polar graphs of special types.\n",
      "\n",
      "8. id: 5390afca20f70186a0ed3baa   score: 0.9465967   abstract: The maximum independent set problem is NP-complete for graphs in general, but becomes solvable in polynomial time when restricted to graphs in many special classes. The problem is also intractable from a parameterized point of view. However, very little is known about parameterized complexity of the problem in restricted graph classes. In the present paper, we analyse two techniques that have previously been used to solve the problem in polynomial time for graphs in particular classes and apply these techniques to develop fpt-algorithms for graphs in some classes where the problem remains NP-complete.\n",
      "\n",
      "9. id: 5390a0b720f70186a0e4f5c9   score: 0.93581736   abstract: Finding a dominating set of minimum cardinality is an NP-hard graph problem, even when the graph is bipartite. In this paper we are interested in solving the problem on graphs having a large independent set. Given a graph G with an independent set of size z, we show that the problem can be solved in time O^*(2^n^-^z), where n is the number of vertices of G. As a consequence, our algorithm is able to solve the dominating set problem on bipartite graphs in time O^*(2^n^/^2). Another implication is an algorithm for general graphs whose running time is O(1.7088^n).\n",
      "\n",
      "10. id: 5390a17720f70186a0e53a83   score: 0.91920847   abstract: We show that several problems that are hard for various parameterized complexity classes on general graphs, become fixed parameter tractable on graphs with no small cycles. More specifically, we give fixed parameter tractable algorithms for Dominating Set, t -Vertex Cover (where we need to cover at least t edges) and several of their variants on graphs with girth at least five. These problems are known to be W[i]-hard for some i≥1 in general graphs. We also show that the Dominating Set problem is W[2]-hard for bipartite graphs and hence for triangle free graphs. In the case of Independent Set and several of its variants, we show these problems to be fixed parameter tractable even in triangle free graphs. In contrast, we show that the Dense Subgraph problem where one is interested in finding an induced subgraph on k vertices having at least l edges, parameterized by k, is W[1]-hard even o\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691656\n",
      "index                                        5591628d0cf21fd553bfa69f\n",
      "title               A note on semi-convergence of generalized para...\n",
      "authors                                      Zhen Chao, Guoliang Chen\n",
      "year                                                           2015.0\n",
      "venue                                            Numerical Algorithms\n",
      "references          558b2e07612c41e6b9d45d56;5590d5fd0cf2ce4b6f3a0...\n",
      "abstract            Recently, Zhang and Wang studied the generaliz...\n",
      "id                                                            1691656\n",
      "clustered_labels                                                    2\n",
      "Name: 1691656, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390c04b20f70186a0f587dd   score: 0.97711015   abstract: In this paper, we further study the generalized parameterized inexact Uzawa method for solving singular saddle point problems, obtaining the generalized parameterized inexact Uzawa (GPIU) method. Theoretical analysis shows that the semi-convergence of this new method can be guaranteed by suitable choices of the iteration parameters. Numerical experiments are used to demonstrate the feasibility and effectiveness of the generalized parameterized inexact Uzawa method for solving singular saddle point problems.\n",
      "\n",
      "2. id: 558f35820cf2af9ee80eb8d9   score: 0.97090924   abstract: For the iterative solution of large sparse generalized saddle point problems, a class of new constraint preconditioners is presented, and the spectral properties and parameter choices are discussed. Numerical experiments are used to demonstrate the feasibility and effectiveness of the new preconditioners, as well as their advantages over the modified product-type skew-Hermitian triangular splitting (MPSTS) preconditioners.\n",
      "\n",
      "3. id: 5390b20120f70186a0ee5402   score: 0.96741027   abstract: A parameterized preconditioning framework is proposed to improve the conditions of the generalized saddle point problems. Based on the eigenvalue estimates for the generalized saddle point matrices, a strategy to minimize the upper bounds of the spectral condition numbers of the matrices is given, and the explicit expression of the quasi-optimal preconditioning parameter is obtained. In numerical experiment, parameterized preconditioning techniques are applied to the generalized saddle point problems derived from the mixed finite element discretization of the stationary Stokes equation. Numerical results demonstrate that the involved preconditioning procedures are efficient.\n",
      "\n",
      "4. id: 5390a0b720f70186a0e4fcf1   score: 0.96309197   abstract: The convergence of the inexact Uzawa method for stabilized saddle point problems was analysed in a recent paper by Cao, Evans and Qin. We show that this method converges under conditions weaker than those stated in their paper.\n",
      "\n",
      "5. id: 5390c04520f70186a0f57683   score: 0.9533619   abstract: We present new preconditioners based on matrix splittings for the saddle point problems. The spectral property of one of the preconditioned matrix is studied in detail. Numerical examples are used to illustrate the efficiency of the new preconditioners.\n",
      "\n",
      "6. id: 5390c04b20f70186a0f58446   score: 0.9265699   abstract: Recently, [Z.-Z. Bai, Z.-Q.Wang, On parameterized inexact Uzawa methods for generalized saddle point problems, Linear Algebra Appl. 428 (2008) 2900-2932] studied a class of parameterized inexact Uzawa (PIU) methods and proposed a generalized and modified accelerated parameterized inexact Uzawa (APIU) iteration method for solving nonsingular saddle point problems. In this paper, we further generalize this method to obtain the block-diagonally preconditioned accelerated parameterized inexact Uzawa (BDP-APIU) method for solving singular saddle point problems. Theoretical analysis shows that the semi-convergence of this new method can be guaranteed. In addition, the quasi-optimal parameters of the new method are discussed. Numerical example is given to show the feasibility and effectiveness of the new method for solving singular saddle point problems.\n",
      "\n",
      "7. id: 55323ca045cec66b6f9dc873   score: 0.91228   abstract: Recently, Z.-Z. Bai, Z.-Q.Wang, On parameterized inexact Uzawa methods for generalized saddle point problems, Linear Algebra Appl. 428 (2008) 2900-2932] studied a class of parameterized inexact Uzawa (PIU) methods and proposed a generalized and modified accelerated parameterized inexact Uzawa (APIU) iteration method for solving nonsingular saddle point problems. In this paper, we further generalize this method to obtain the block-diagonally preconditioned accelerated parameterized inexact Uzawa (BDP-APIU) method for solving singular saddle point problems. Theoretical analysis shows that the semi-convergence of this new method can be guaranteed. In addition, the quasi-optimal parameters of the new method are discussed. Numerical example is given to show the feasibility and effectiveness of the new method for solving singular saddle point problems.\n",
      "\n",
      "8. id: 5390a30b20f70186a0e69b1a   score: 0.9032942   abstract: The parameterized Uzawa preconditioners for saddle point problems are studied in this paper. The eigenvalues of the preconditioned matrix are located in (0, 2) by choosing the suitable parameters. Furthermore, we give two strategies to optimize the rate of convergence by finding the suitable values of parameters. Numerical computations show that the parameterized Uzawa preconditioners can lead to practical and effective preconditioned GMRES methods for solving the saddle point problems.\n",
      "\n",
      "9. id: 5390bb7b20f70186a0f40f58   score: 0.8975158   abstract: In this paper, for solving the singular saddle point problems, we present a new preconditioned accelerated Hermitian and skew-Hermitian splitting (AHSS) iteration method. The semi-convergence of this method and the eigenvalue distribution of the preconditioned iteration matrix are studied. In addition, we prove that all eigenvalues of the iteration matrix are clustered for any positive iteration parameters 驴 and β. Numerical experiments illustrate the theoretical results and examine the numerical effectiveness of the AHSS iteration method served either as a preconditioner or as a solver.\n",
      "\n",
      "10. id: 5590d5fd0cf2ce4b6f3a0504   score: 0.88563144   abstract: For large sparse saddle point problems, we firstly introduce the block diagonally preconditioned Gauss-Seidl method (PBGS) which reduces to the GSOR method [Z.-Z. Bai, B.N. Parlett, Z.-Q. Wang, On generalized successive overrelaxation methods for augmented linear systems, Numer. Math. 102 (2005) 1-38] and PIU method [Z.-Z. Bai, Z.-Q. Wang, On parameterized inexact Uzawa methods for generalized saddle point problems, Linear Algebra Appl. 428 (2008) 2900-2932] when the preconditioners equal to different matrices, respectively. Then we generalize the PBGS method to the PPIU method and discuss the sufficient conditions such that the spectral radius of the PPIU method is much less than one. Furthermore, some rules are considered for choices of the preconditioners including the splitting method of the (1,1) block matrix in the PIU method and numerical examples are given to show the superiority\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691552\n",
      "index                                        559254400cf2aff368683b28\n",
      "title               Quantum phase estimation with local amplified ...\n",
      "authors                                  Xue-Xiang Xu, Hong-Chun Yuan\n",
      "year                                                           2015.0\n",
      "venue                                  Quantum Information Processing\n",
      "references          5390bae520f70186a0f3bcc7;5390b9d520f70186a0f304f7\n",
      "abstract            We demonstrate the utility of Wigner-function ...\n",
      "id                                                            1691552\n",
      "clustered_labels                                                    2\n",
      "Name: 1691552, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909f6920f70186a0e3a8ce   score: 0.58195657   abstract: We present a review of the applications of the Wigner distribution function in various areas of signal processing: amplitude and phase retrieval, signal recognition, characterization of arbitrary signals, optical systems and devices, and coupling coefficient estimation in phase space. Although reference is made to specific signals and systems, the mathematical formulation is general and can be applied to either spatial, temporal, or spatio-temporal phase spaces, to coherent, partially coherent, or discrete signals. The universal and intuitive character of the Wigner distribution approach to signal characterization and processing and its simplicity in solving many issues are evidenced throughout the paper.\n",
      "\n",
      "2. id: 5390975920f70186a0dfc2b8   score: 0.45689437   abstract: Focusing particularly on one-qubit and two-qubit systems, I explain how the quantum state of a system of n qubits can be expressed as a real function--a generalized Wigner function--on a discrete 2n 脙聴 2n phase space. The phase space is based on the finite field having 2n elements, and its geometric structure leads naturally to the construction of a complete set of 2n + 1 mutually conjugate bases.\n",
      "\n",
      "3. id: 5390b5fa20f70186a0f10ab6   score: 0.33993718   abstract: The Wigner function was introduced as a generalization of the concept of distribution function for quantum statistics. The aim of this work is pushing further the formal analogy between quantum and classical approaches. The Wigner function is defined as an ensemble average, i.e., in terms of a mixture of pure states. From the point of view of basic physics, it would be very appealing to be able to define a Wigner function also for pure states and the associated expectation values for quantum observables, in strict analogy with the definition of mean value of a physical quantity in classical mechanics; then correct results for any quantum system should be recovered as appropriate superpositions of such ''pure-state'' quantities. We will show that this is actually possible, at the cost of dealing with generalized functions in place of proper functions.\n",
      "\n",
      "4. id: 5390bfa220f70186a0f54bdd   score: 0.3348069   abstract: We develop several algorithms for performing quantum phase estimation based on basic measurements and classical post-processing. We present a pedagogical review of quantum phase estimation and simulate the algorithm to numerically determine its scaling in circuit depth and width. We show that the use of purely random measurements requires a number of measurements that is optimal up to constant factors, albeit at the cost of exponential classical post-processing; the method can also be used to improve classical signal processing. We then develop a quantum algorithm for phase estimation that yields an asymptotic improvement in runtime, coming within a factor of log* of the minimum number of measurements required while still requiring only minimal classical post-processing. The corresponding quantum circuit requires asymptotically lower depth and width (number of qubits) than quantum phase \n",
      "\n",
      "5. id: 5390b5fa20f70186a0f0f1d2   score: 0.23248434   abstract: A quantum mechanical form of the Cramér-Rao inequality and a minimum-mcan-square-error quantum estimator for multiple parameters are derived, allowing all possible quantum measurements of the received field. The role of nonselfadjoint operators is emphasized in the formulation. Relations of our results to previous work on quantum estimation are discussed. For the estimation of complex mode amplitudes of coherent signals in Gaussian noise, it is shown that the optimal receiver measures the photon annihilation operator, which corresponds to optical heterodyning. This demonstrates the possible optimality of nonselfadjoint operators and clearly indicates the importance of considering more general quantum measurements in quantum signal detection.\n",
      "\n",
      "6. id: 53909f6920f70186a0e3a8cf   score: 0.18359363   abstract: It is shown how all global Wigner distribution moments of arbitrary order can be measured as intensity moments in the output plane of an appropriate number of separable first-order optical systems (generally anamorphic ones). The minimum number of such systems that are needed for the determination of these moments is derived.\n",
      "\n",
      "7. id: 5390b5fa20f70186a0f0f521   score: 0.16789974   abstract: A method is given for calculating the minimum average error probability attainable in deciding between a mixed state and a pure state of a quantum system. It is applied, first, to the detection of a noisy coherent pulse represented by a state with a Gaussian P-representation and, second, to the detection of a signal represented at the transmitter by a two-photon coherent state (TCS). In both problems the field of the receiver is free of background noise and, when no signal was transmitted, is in the ground state.\n",
      "\n",
      "8. id: 5390bae520f70186a0f3bcc7   score: 0.14523655   abstract: While Quantum phase estimation (QPE) is at the core of many quantum algorithms known to date, its physical implementation (algorithms based on quantum Fourier transform (QFT)) is highly constrained by the requirement of high-precision controlled phase shift operators, which remain difficult to realize. In this paper, we introduce an alternative approach to approximately implement QPE with arbitrary constant-precision controlled phase shift operators. The new quantum algorithm bridges the gap between QPE algorithms based on QFT and Kitaev's original approach. For approximating the eigenphase precise to the nth bit, Kitaev's original approach does not require any controlled phase shift operator. In contrast, QPE algorithms based on QFT or approximate QFT require controlled phase shift operators with precision of at least Pi/2n. The new approach fills the gap and requires only arbitrary con\n",
      "\n",
      "9. id: 5390b5fa20f70186a0f0f583   score: 0.1133831   abstract: Methods are presented for calculating the minimum attainable probability of error in detecting a coherent quantum signal received amid thermal noise in the limits of Iow and high signal-to-noise ratio. In the latter limit, quantum-mechanical perturbation theory is applied to solving the detection operator equation approximately. Graphical results are furnished for binary signals transmitted with equal prior probabilities. Previous work on this problem is thus extended to a broader range of signal and noise strengths. The results are applied to the detection of antipodal two-photon-coherent-state signals.\n",
      "\n",
      "10. id: 5390bb7b20f70186a0f41260   score: 0.11066323   abstract: A separable input state consisting of an $$n$$ -photon Fock state and a coherent state propagating through coupled waveguides is investigated in detail. We obtained the analytical solutions for the state vector evolution, the wavefunction or probability distribution in the quadrature space and the $$P$$ -function in the phase space. It is proved that the propagating states may evolve into quantum vortex states even for coupled lossy waveguides by appropriately selecting the propagation time. Based on the analytical $$P$$ -function in phase space and the relative linear entropy for the propagating state, it is found that the propagating state may be entangled and non-classical. Specially, in absence of loss, the degree of entanglement only depends on the photon number $$n$$ of the input Fock state but is independent of the displacement parameter $$\\alpha $$ associated with the input coher\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696362\n",
      "index                                        55922dd60cf244696a09d9da\n",
      "title               Efficient Nonsmooth Nonconvex Optimization for...\n",
      "authors                                    Miyoun Jung, Myungjoo Kang\n",
      "year                                                           2015.0\n",
      "venue                                 Journal of Scientific Computing\n",
      "references          5390b4da20f70186a0f00ed7;53909ce520f70186a0e28...\n",
      "abstract            In this article, we introduce variational imag...\n",
      "id                                                            1696362\n",
      "clustered_labels                                                    1\n",
      "Name: 1696362, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558cece40cf2b0acc6503707   score: 0.9778412   abstract: In image processing, nonconvex regularization has the ability to smooth homogeneous regions and sharpen edges but leads to challenging computation. We propose some iterative schemes to minimize the energy function with nonconvex edge-preserving potential. The schemes are derived from the duality-based algorithm proposed by Bermúdez and Moreno and the fixed point iteration. The convergence is proved for the convex energy function with nonconvex potential and the linear convergence rate is given. Applying the proposed schemes to Perona and Malik's nonconvex regularization, we present some efficient algorithms based on our schemes, and show the approximate convergence behavior for nonconvex energy function. Experimental results are presented, which show the efficiency of our algorithms, including better denoised performance of nonconvex regularization, faster convergence speed, higher calcu\n",
      "\n",
      "2. id: 5390ac5720f70186a0eb6187   score: 0.9347534   abstract: Variational models for image segmentation have many applications, but can be slow to compute. Recently, globally convex segmentation models have been introduced which are very reliable, but contain TV-regularizers, making them difficult to compute. The previously introduced Split Bregman method is a technique for fast minimization of L1 regularized functionals, and has been applied to denoising and compressed sensing problems. By applying the Split Bregman concept to image segmentation problems, we build fast solvers which can out-perform more conventional schemes, such as duality based methods and graph-cuts. The convex segmentation schemes also substantially outperform conventional level set methods, such as the Chan-Vese level set-based segmentation algorithm. We also consider the related problem of surface reconstruction from unorganized data points, which is used for constructing le\n",
      "\n",
      "3. id: 5390b64020f70186a0f19c10   score: 0.928668   abstract: Image restoration and segmentation are both classical problems, that are known to be difficult and have attracted major research efforts. This paper shows that the two problems are tightly coupled and can be successfully solved together. Mutual support of image restoration and segmentation processes within a joint variational framework is theoretically motivated, and validated by successful experimental results. The proposed variational method integrates semi-blind image deconvolution (parametric blur-kernel), and Mumford-Shah segmentation. The functional is formulated using the Γ-convergence approximation and is iteratively optimized via the alternate minimization method. While the major novelty of this work is in the unified treatment of the semi-blind restoration and segmentation problems, the important special case of known blur is also considered and promising results are obtained.\n",
      "\n",
      "4. id: 539087ef20f70186a0d6d381   score: 0.9280184   abstract: We analyze a variational approach to image segmentation that isbased on a strictly convex non-quadratic cost functional.The smoothness term combines a standard first-ordermeasure for image regions with a total-variation basedmeasure for signal transitions. Accordingly, the costs associatedwith “discontinuities” are givenby the length of level lines and local image contrast.For real images, this provides a reasonable approximation of thevariational model of Mumford and Shah that has been suggested asa generic approach to image segmentation.The global properties of the convex variational model are favorableto applications: Uniqueness of the solution, continuous dependenceof the solution on both data and parameters, consistent and efficientnumerical approximation of the solution with the FEM-method.Various global and local properties of the convex variational modelare analyzed and illustrat\n",
      "\n",
      "5. id: 5390aaf920f70186a0eae6ef   score: 0.9254997   abstract: This paper considers two approaches to perform image restoration while preserving the contrast. The first one is the Total Variation-based Bregman iterations while the second consists in the minimization of an energy that involves robust edge preserving regularization. We show that these two approaches can be derived form a common framework. This allows us to deduce new properties and to extend and generalize these two previous approaches.\n",
      "\n",
      "6. id: 5390ba3820f70186a0f356ec   score: 0.9230392   abstract: Image restoration problems, such as image denoising, are important steps in various image processing method, such as image segmentation and object recognition. Due to the edge preserving property of the convex total variation (TV), variational model with TV is commonly used in image restoration. However, staircase artifacts are frequently observed in restored smoothed region. To remove the staircase artifacts in smoothed region, convex higher-order TV (HOTV) regularization methods are introduced. But the valuable edge information of the image is also attenuated. In this paper, we propose non-convex hybrid TV regularization method to significantly reduce staircase artifacts while well preserving the valuable edge information of the image. To efficiently find a solution of the variation model with the proposed regularizer, we use the iterative reweighted method with the augmented Lagrangia\n",
      "\n",
      "7. id: 5390b36120f70186a0ef135d   score: 0.91715443   abstract: Recently total variation (TV) regularization has been proven very successful in image restoration and segmentation. In image restoration, TV based models offer a good edge preservation property. In image segmentation, TV (or vectorial TV) helps to obtain convex formulations of the problems and thus provides global minimizations. Due to these advantages, TV based models have been extended to image restoration and data segmentation on manifolds. However, TV based restoration and segmentation models are difficult to solve, due to the nonlinearity and non-differentiability of the TV term. Inspired by the success of operator splitting and the augmented Lagrangian method (ALM) in 2D planar image processing, we extend the method to TV and vectorial TV based image restoration and segmentation on triangulated surfaces, which are widely used in computer graphics and computer vision. In particular,\n",
      "\n",
      "8. id: 5390a6b120f70186a0e85446   score: 0.91625965   abstract: Image restoration models based on total variation (TV) have been popular and successful since their introduction by Rudin, Osher, and Fatemi (ROF) in 1992. The nonsmooth TV seminorm allows them to preserve sharp discontinuities (edges) in an image while removing noise and other unwanted fine scale detail. On the other hand, the TV term, which is the L1 norm of the gradient vector, poses computational challenge in solving those models efficiently. Furthermore, the global coupling of the gradient operator makes the problem extra harder than other L1 minimization problems where the variables under the L 1 norm are separable. In this paper we propose several new algorithms to tackle these difficulties from different perspectives. Numerical experiments show that they are competitive with the existing popular methods and some of them are significantly faster despite of their simplicity. The fi\n",
      "\n",
      "9. id: 558b1c81612c41e6b9d43975   score: 0.91505283   abstract: The problem of restoration of digital images plays a central role in multitude important applications. A particularly challenging instance of this problem occurs in the case when the degradation phenomenon is modelled by ill-conditional operator. In such situation, the presence of noise makes it impossible to recover a valuable approximation of the image of interest without using some priori information called as simply priors is essential for image restoration, rendering it stable and robust to noise. Particularly, if the original image is known to be a piecewise smooth function, one of the standard priors used Rudin-Osher-Fatemi model which results in total variation (TV) based image restoration. We proposed an algorithm for unconstrained optimization problem where the objective function includes a data fidelity term and a nonsmooth regulaizer.Total Variation method is employed to find\n",
      "\n",
      "10. id: 53908bde20f70186a0dc72f3   score: 0.9078038   abstract: We address the general problem of the recovery of an unknown image, x\\in {\\hbox{\\rlap{I}\\kern 2.0pt{\\hbox{R}}}}^p, from noisy data, y \\in {\\hbox{\\rlap{I}\\kern 2.0pt{\\hbox{R}}}}^q, by minimizing a regularized objective function {\\cal E}(x; y). We focus on typical situations when the objective function is {\\cal C}^m-smooth and is composed of a quadratic data-fidelity term and a general regularization term: {\\cal E}(x; y) = \\| Ax _ y\\|^2 +\\Phi(x), where A is a linear operator. Many authors have shown that especially nonconvex regularizers F allow the restoration of images involving both sharp edges and smoothly varying regions. The main limitation to use such regularizers is that, being highly nonconvex, the resultant objective functions are intricate to minimize. On the other hand, since very few facts are known about the minimizers of such functions, the properties and in particular the s\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1622431\n",
      "index                                        5591339a0cf232eb904fb386\n",
      "title                        Time Complexity of Link Reversal Routing\n",
      "authors             Bernadette Charron-Bost, Matthias Függer, Jenn...\n",
      "year                                                           2015.0\n",
      "venue                           ACM Transactions on Algorithms (TALG)\n",
      "references          5390adfd20f70186a0ec654c;5390879220f70186a0d3b...\n",
      "abstract            Link reversal is a versatile algorithm design ...\n",
      "id                                                            1622431\n",
      "clustered_labels                                                    3\n",
      "Name: 1622431, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b8d720f70186a0f2ba54   score: 0.96378   abstract: Link reversal is an algorithmic method with various applications. Originally proposed by Gafni and Bertsekas in 1981 for routing in radio networks, it has been later applied also to solve concurrency related problems as mutual exclusion, resource allocation, and leader election. For resource allocation, conflicts can be represented by conflict graphs, and link reversal algorithms work on these graphs to resolve conflicts. In this paper we establish that executions of link reversal algorithms on large graphs are similar (a notion which we make precise in the paper) to executions on smaller graphs. This similarity then allows to verify linear time temporal properties of large systems, by verifying a smaller one.\n",
      "\n",
      "2. id: 5390b13020f70186a0edd289   score: 0.953188   abstract: Link reversal is a versatile algorithm design paradigm, originally proposed by Gafni and Bertsekas in 1981 for routing, and subsequently applied to other problems includingmutual exclusion and resource allocation. Although these algorithms arewell-known, until nowthere have been only preliminary results on time complexity, even for the simplest link reversal scheme for routing, called FullReversal (FR). In this paper we tackle this open question for arbitrary communication graphs. Our central technical insight is to describe the behavior of FR as a dynamical system, and to observe that this systemis linear in the min-plus algebra. Fromthis characterization, we derive the first exact formula for the time complexity: Given any node in any (acyclic) graph, we present an exact formula for the time complexity of that node, in terms of some simple properties of the graph. These results for FR \n",
      "\n",
      "3. id: 5390b44620f70186a0ef9291   score: 0.93732566   abstract: Link reversal is a versatile algorithm design technique that has been used in numerous distributed algorithms for a variety of problems. The common thread in these algorithms is that the distributed system is viewed as a graph, with vertices representing the computing nodes and edges representing some other feature of the system (for instance, point-to-point communication channels or a conflict relationship). Each algorithm assigns a virtual direction to the edges of the graph, producing a directed version of the original graph. As the algorithm proceeds, the virtual directions of some of the links in the graph change in order to accomplish some algorithm-specific goal. The criterion for changing link directions is based on information that is local to a node (such as the node having no outgoing links) and thus this approach scales well, a feature that is desirable for distributed algori\n",
      "\n",
      "4. id: 5390b13020f70186a0edd28a   score: 0.92563426   abstract: Link reversal is the basis of several well-known routing algorithms [1,2,3]. In these algorithms, logical directions are imposed on the communication links and a node that becomes a sink reverses some of its incident links to allow the (re)construction of paths to the destination. In the Full Reversal (FR) algorithm [1], a sink reverses all its incident links. In other schemes, a sink reverses only some of its incident links; a notable example is the Partial Reversal (PR) algorithm [1]. Prior work [4] has introduced a generalization, called LR, of link-reversal routing, including FR and PR. In this paper, we show that every execution of LR on any link-labeled input graph corresponds, in a precise sense, to an execution of FR on a transformed graph. Thus, all the link reversal schemes captured by LR can be reduced to FR, indicating that \"partial is full.\" The correspondence preserves the \n",
      "\n",
      "5. id: 539098dc20f70186a0e0d177   score: 0.90481883   abstract: Link reversal algorithms provide a simple mechanism for routing in communication networks whose topology is frequently changing, such as in mobile ad hoc networks. A link reversal algorithm routes by imposing a direction on each network link such that the resulting graph is a destination oriented DAG. Whenever a node loses routes to the destination, it reacts by reversing some (or all) of its incident links. Link reversal algorithms have been studied experimentally and have been used in practical routing algorithms, including TORA [V. D. Park and M. S. Corson, A highly adaptive distributed routing algorithm for mobile wireless networks, in Proc. INFOCOM, IEEE, Los Alamitos, CA, 1997, pp. 1405--1413].This paper presents the first formal performance analysis of link reversal algorithms. We study these algorithms in terms of work (number of node reversals) and the time needed until the netw\n",
      "\n",
      "6. id: 5390b00c20f70186a0ed458d   score: 0.87212974   abstract: Although substantial analysis has been done on the Full Reversal (FR) routing algorithm since its introduction by Gafni and Bertsekas in 1981, a complete understanding of its functioning---especially its time complexity---has been missing until now. In this paper, we derive the first exact formula for the time complexity of FR: given any (acyclic) graph the formula provides the exact time complexity of any node in terms of some simple properties of the graph. Our major technical insight is to describe executions of FR as a dynamical system, and to observe that this system is linear in the min-plus algebra. As a consequence of the insight provided by the new formula, we are able to prove that FR is time-efficient when executed on tree networks. This result exposes an unstable aspect of the time complexity of FR that has not previously been reported. Finally, our results for FR are instrum\n",
      "\n",
      "7. id: 5390a55520f70186a0e7b3d5   score: 0.8512122   abstract: We analyze the correctness and the complexity of two well-known routing algorithms, introduced by Gafni and Bertsekas (1981): By reversing the directions of some edges, these algorithms transform an arbitrary directed acyclic input graph into an output graph with at least one route from each node to a special destination node (while maintaining acyclicity). The resulting graph can thus be used to route messages in a loop-free manner. Gafni and Bertsekas implement these algorithms by assigning to each node of the graph an unbounded \"height\" in some total order. The relative order of the heights of two neighboring nodes induces a logical direction on the edge between them; the direction of an edge is reversed by modifying the height of one endpoint. In this work, we present a novel formalization for these algorithms based only on directed graphs with binary labels for edges. Using this for\n",
      "\n",
      "8. id: 53908b4920f70186a0dbca81   score: 0.84479356   abstract: Link reversal algorithms provide a simple mechanism for routing in mobile ad hoc networks. These algorithms maintain routes to any particular destination in the network, even when the network topology changes frequently. In link reversal, a node reverses its incident links whenever it loses routes to the destination. Link reversal algorithms have been studied experimentally and have been used in practical routing algorithms, including [8].This paper presents the first formal performance analysis of link reversal algorithms. We study these algorithms in terms of work (number of node reversals) and the time needed until the network stabilizes to a state in which all the routes are reestablished. We focus on the full reversal algorithm and the partial reversal algorithm, both due to Gafni and Berstekas [5]; the first algorithm is simpler, while the latter has been found to be more efficient\n",
      "\n",
      "9. id: 5390b00c20f70186a0ed5155   score: 0.80332816   abstract: Partial Reversal (PR) is a link reversal algorithm which ensures that an initially directed acyclic graph (DAG) is eventually a destination-oriented DAG. While proofs exist to establish the acyclicity property of PR, they rely on assigning labels to either the nodes or the edges in the graph. In this work we show that such labeling is not necessary and outline a simpler direct proof of the acyclicity property.\n",
      "\n",
      "10. id: 5390a40520f70186a0e6f85e   score: 0.7111301   abstract: We first characterize the minimal link-sets L whose directions must be reversed for reestablishing one or more directed paths from each node x to a fixed destination node d in a network when a link fails. Then, we give a distributed O (|E |) algorithm for determining such a link-set L , where |E | = $\\sharp$(links in the network). This improves the previous lower bound O (n 2 ), where n = $\\sharp$(nodes in the network). The minimality of the reversed link-set L has other important consequences.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1715859\n",
      "index                                        55323c5845cec66b6f9dbdff\n",
      "title               Numerical solution of two-dimensional elliptic...\n",
      "authors                      Siraj-ul-Islam, Imran Aziz, Masood Ahmad\n",
      "year                                                           2015.0\n",
      "venue                       Computers & Mathematics with Applications\n",
      "references          558f6ebc0cf2cb5aa7673f93;558af7f0612c41e6b9d3ef84\n",
      "abstract            In the present paper, two numerical methods ar...\n",
      "id                                                            1715859\n",
      "clustered_labels                                                    2\n",
      "Name: 1715859, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bfa220f70186a0f54a5e   score: 0.98184055   abstract: The development of numerical methods for the solution of partial differential equations (PDEs) with nonlocal boundary conditions is important, since such type of problems arise as mathematical models of various real-world processes. We use radial basis function (RBF) collocation technique for the solution of a multidimensional linear elliptic equation with classical Dirichlet boundary condition and nonlocal integral conditions. RBF-based meshless methods are easily implemented and efficient, especially for multidimensional problems formulated on complexly shaped domains. In this paper, properties of the method are investigated by studying two- and three-dimensional test examples with manufactured solutions. We analyze the influence of the RBF shape parameter and the distribution of the nodes on the accuracy of the method as well as the influence of nonlocal conditions on the conditioning\n",
      "\n",
      "2. id: 5390ab8820f70186a0eb1880   score: 0.97792566   abstract: The numerical solution of the modified equal width equation is investigated by using meshless method based on collocation with the well-known radial basis functions. Single solitary wave motion, two solitary waves interaction and three solitary waves interaction are studied. Results of the meshless methods with different radial basis functions are presented.\n",
      "\n",
      "3. id: 5390aa7620f70186a0eab114   score: 0.9773276   abstract: The numerical solution of the regularized long wave equation is investigated by means of the meshless method based on collocation with the well-known radial basis functions (RBFs). Single solitary wave motion, two solitary wave interaction and wave undulation are studied. Results of the meshless method with different RBFs are compared.\n",
      "\n",
      "4. id: 5390bded20f70186a0f49342   score: 0.97640073   abstract: We present a simple discretization by radial basis functions for the Poisson equation with Dirichlet boundary condition. A Lagrangian multiplier using piecewise polynomials is used to accommodate the boundary condition. This simplifies previous attempts to use radial basis functions in the interior domain to approximate the solution and on the boundary to approximate the multiplier, which technically requires that the mesh norm in the interior domain is significantly smaller than that on the boundary. Numerical experiments confirm the theoretical results.\n",
      "\n",
      "5. id: 5390c04520f70186a0f57435   score: 0.97068775   abstract: The full multigrid (FMG) method is applied to the two dimensional Poisson equation with Dirichlet boundary conditions. This has been chosen as a relatively simple test case for examining the efficiency of fully vectorizing of the multigrid method. Data structure and programming considerations and techniques are discussed, accompanied by performance details.\n",
      "\n",
      "6. id: 5390aefc20f70186a0ece275   score: 0.9572778   abstract: Two-dimensional Haar wavelets are applied for solution of the partial differential equations (PDEs). The proposed method is mathematically simple and fast. To demonstrate the efficiency of the method, two test problems (solution of the diffusion and Poisson equations) are discussed. Computer simulation showed that the method guarantees the necessary exactness already for a small number of grid points.\n",
      "\n",
      "7. id: 5390a1f820f70186a0e5df3f   score: 0.9561454   abstract: This paper introduces a variant of direct and indirect radial basis function networks (DRBFNs and IRBFNs) for the numerical solution of Poisson's equation. We use transformation from Cartesian coordinates to polar ones and use DRBFN and IRBFN methods on the basis of a multiquadric approximation scheme. We have experienced that the result shows better accuracy than previously known ones. Also, our new way of solution does not influence the condition number.\n",
      "\n",
      "8. id: 53908b2120f70186a0db6e85   score: 0.9494823   abstract: The aim of this paper is twofold. First, two generalized (meshfree) finite difference methods (GFDM) for the Poisson equation are discussed. These are methods due to Liszka and Orkisz (1980) [10] and to Tiwari (2001) [7]. Both methods are based on using moving least squares (MLS) approach for deriving the discretization. The relative comparison shows, that the second method is preferable because it is less sensitive to the topological restrictions on the nodes distribution. Next, an extension of the second method is presented, which allows for accounting for internal interfaces, associated with discontinuous coefficients. Results from numerical experiments illustrate the second order convergence of the proposed GFDM for interface problems.\n",
      "\n",
      "9. id: 5390c04520f70186a0f57225   score: 0.94509614   abstract: A symmetric marching technique (SMT) for solving the discretized Poisson equation is developed. The effectiveness of the SMT when combined with mesh refinement technique has been demonstrated. The results of some numerical experiments are presented.\n",
      "\n",
      "10. id: 558af7f0612c41e6b9d3ef84   score: 0.9400236   abstract: A new numerical method based on Haar wavelet is proposed for two-dimensional nonlinear Fredholm, Volterra and Volterra-Fredholm integral equations of first and second kind. The proposed method is an extension of the Haar wavelet method Aziz and Siraj-ul-Islam (2013), Siraj-ul-Islam et al. (2013) and Siraj-ul-Islam et al. (2014) from one-dimensional nonlinear integral equations (Fredholm and Volterra) to two-dimensional nonlinear integral equations (Fredholm, Volterra and Volterra-Fredholm). The main characteristic of the method is that, unlike several other methods, it does not involve numerical integration which results in an improved accuracy of the method. In order to show the effectiveness of the method, it is applied to several benchmark problems. The numerical results are compared with other methods existing in the recent literature.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1634767\n",
      "index                                        55922c340cf2ceaae74c8c54\n",
      "title               Towards context aware data fusion: Modeling an...\n",
      "authors             Michael P. Jenkins, Geoff A. Gross, Ann M. Bis...\n",
      "year                                                           2015.0\n",
      "venue                                              Information Fusion\n",
      "references          53909e8a20f70186a0e2ce6c;5390a93b20f70186a0ea0...\n",
      "abstract            This paper presents a framework for characteri...\n",
      "id                                                            1634767\n",
      "clustered_labels                                                    0\n",
      "Name: 1634767, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b24320f70186a0ee6270   score: 0.9219218   abstract: The situation assessment problem is considered, in terms of object, condition, activity, and plan recognition, based on data coming from the realword via various sensors. It is shown that uncertainty issues are linked both to the models and to the matching algorithm. Three different types of uncertainties are identified, and within each one, the numerical and the symbolic cases are distinguished. The emphasis is then put on purely symbolic uncertainties: it is shown that they can be dealt with within a purely symbolic framework resulting from a transposition of classical numerical estimation tools.\n",
      "\n",
      "2. id: 5390afc920f70186a0ed38af   score: 0.920218   abstract: An overview of data fusion approaches is provided from the signal processing viewpoint. The general concept of data fusion is introduced, together with the related architectures, algorithms and performance aspects. Benefits of such an approach are highlighted and potential applications are identified. Case studies illustrate the merits of applying data fusion concepts in real world applications.\n",
      "\n",
      "3. id: 5390bd1520f70186a0f44c8e   score: 0.8879841   abstract: The recent experiences of asymmetric urban military operations have highlighted the pressing need for incorporation of soft data, such as informant statements, into the fusion process. Soft data are fundamentally different from hard data (generated by physics-based sensors), in the sense that the information they provide tends to be qualitative and subject to interpretation. These characteristics pose a major obstacle to using existing multi-sensor data fusion frameworks, which are quite well established for hard data. Given the critical and sensitive nature of intended applications, soft/hard data fusion requires a framework that allows for convenient representation of various data uncertainties common in soft/hard data, and provides fusion techniques that are robust, mathematically justifiable, and yet effective. This would allow an analyst to make decisions with a better understanding\n",
      "\n",
      "4. id: 539088b820f70186a0d906d5   score: 0.8812066   abstract: From the Publisher:Explains numeric and symbolic approaches to data association, tracking combination, classification, and situation assessment, and provides an overview of data fusion theory and mathematical formalisms.\n",
      "\n",
      "5. id: 5390b4da20f70186a0f0027b   score: 0.81640637   abstract: Considering the important role of data fusion evaluation system in testing, we propose a systematic auto-testing method to validate its function and performance. We also present some practical examples to illustrate the operation principle of the method. Based on abundant auto-testing, the experimental results show that the data fusion evaluation system can acquire a higher detecting precision and a superior validity when evaluating the target data fusion systems with our method.\n",
      "\n",
      "6. id: 5390ac5720f70186a0eb50b2   score: 0.80271024   abstract: Taking an intelligent rehabilitation training system device as an example, the specific method of segmentation and registration is proposed in the process of information fusion in this paper, and the result of actual running test illustrates the effectiveness of this method. For multi-sensor data fusion, this method has a certain universal significance.\n",
      "\n",
      "7. id: 5390882c20f70186a0d8c3fe   score: 0.7606507   abstract: From the Publisher:This dynamic new CD features a self-contained short course suitable for self-instruction or a classroom setting. Based on a popular data fusion course taught Dr. David Hall, an internationally recognized expert, this resource starts with introductory material and brings you to the leading-edge of multisensor data fusion and target tracking work currently underway. Comprising 10 learning modules, this unique software includes: 聲 An introduction to data fusion and target tracking 聲 Systems engineering for data fusion 聲 Examples of data fusion systems 聲 An overview of sensor technology and single sensor processing 聲 Target tracking and attribute fusion 聲 Target identity estimation 聲 Sensor management and control 聲 Automated reasoning systems for situation and threat assessment 聲 The role of man/human-in-the-loop 聲 An assessment of the state of the art Multisensor Data Fus\n",
      "\n",
      "8. id: 5390ac1820f70186a0eb4138   score: 0.755637   abstract: A comparative study on uncertainty analysis within some sensor fusion methods as evidential reasoning is performed in this paper. Analysis of occupied spaces by a small object in a chess plane and localizing that object in views of cameras at different positions is selected to compare sensor fusion methods according to available uncertainty measures.\n",
      "\n",
      "9. id: 5390882720f70186a0d8a51f   score: 0.7367796   abstract: From the Publisher:The purpose of this book is twofold: First, to point out present gaps in the way data fusion problems are conceptually treated. Second, to address this issue by exhibiting mathematical tools which treat combination of evidence in the presence of uncertainty in a more systematic and comprehensive way. These techniques are based essentially on two novel ideas relating to probability theory: the newly developed fields of random set theory and conditional and relational event algebra. This volume is intended to be both an update on research progress on data fusion and an introduction to potentially powerful new techniques: fuzzy logic, random set theory, and conditional and relational event algebra. This volume can be used as a reference book for researchers and practitioners in data fusion or expert systems theory, or for graduate students as text for a research seminar o\n",
      "\n",
      "10. id: 559172600cf2e89307ca9de9   score: 0.682891   abstract: This work considers the challenging problem of simultaneous modeling and fusion of 'soft data' generated by a network of 'human sensors' for spatial state estimation tasks, such as lost target search or large area surveillance. Human sensors can opportunistically provide useful information to constrain difficult state estimation problems, but are imperfect information sources whose reliability cannot be easily determined in advance. Formal observation likelihood models are derived for flexible sketch-based observations, but are found to lead to analytically intractable statistical dependencies between unknown sensor parameters and spatial states of interest that cannot adequately characterized by simple point estimates. Hierarchical Bayesian models and centralized inference strategies based on Gibbs sampling are proposed to address these issues, especially in cases of sparse, noisy, ambi\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691673\n",
      "index                                        558eb4fc0cf2af9ee80eb826\n",
      "title               Pseudospectral solutions to some singular nonl...\n",
      "authors                                          Călin-Ioan Gheorghiu\n",
      "year                                                           2015.0\n",
      "venue                                            Numerical Algorithms\n",
      "references          53908d6520f70186a0dd26ea;5390893e20f70186a0d93...\n",
      "abstract            We solve by Laguerre collocation method two se...\n",
      "id                                                            1691673\n",
      "clustered_labels                                                    2\n",
      "Name: 1691673, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539090c420f70186a0ddddd1   score: 0.97529614   abstract: A class of nonlinear singularly perturbed boundary value problems is considered, with restrictions which allow only well-posed problems with possible boundary layers, but no turning points. For the numerical solution of these problems, a close look is taken at a class of general purpose, symmetric finite difference schemes arising from collocation. .br It is shown that if locally refined meshes, whose practical construction is discussed, are employed, then high order uniform convergence of the numerical solution is obtained. Nontrivial examples are used to demonstrate that highly accurate solutions to problems with extremely thin boundary layers can be obtained in this way at a very reasonable cost.\n",
      "\n",
      "2. id: 5390c04b20f70186a0f5871a   score: 0.9724148   abstract: Analytical properties of a nonlinear singular second order boundary value problem in ordinary differential equations posed on an unbounded domain for the density profile of the formation of microscopic bubbles in a nonhomogeneous fluid are discussed. Especially, sufficient conditions for the existence and uniqueness of solutions are derived. Two approximation methods are presented for the numerical solution of the problem, one of them utilizes the open domain Matlab code bvpsuite. The results of numerical simulations are presented and discussed.\n",
      "\n",
      "3. id: 5590a8940cf25001c36ed863   score: 0.9703525   abstract: We frame a hierarchy of nonlinear boundary value problems which are shown to admit exponentially decaying exact solutions. We are able to convert the question of the existence and uniqueness of a particular solution to this nonlinear boundary value problem into a question of whether a certain polynomial has positive real roots. Furthermore, if such a polynomial has at least two distinct positive roots, then the nonlinear boundary value problem will have multiple solutions. In certain special cases, these boundary value problems arise in the self-similar solutions for the flow of certain fluids over stretching or shrinking sheets; examples given include the flow of first and second grade fluids over such surfaces.\n",
      "\n",
      "4. id: 5390a37f20f70186a0e6d443   score: 0.95565146   abstract: We consider a singularly perturbed boundary value problem with two small parameters. The problem is numerically treated by a quadratic spline collocation method. The suitable choice of collocation points provides the discrete minimum principle. Error bounds for the numerical approximations are established. Numerical results give justification of the parameter-uniform convergence of the numerical approximations.\n",
      "\n",
      "5. id: 539098b820f70186a0e0be16   score: 0.9537082   abstract: In this paper we show that a nonlinear boundary-value problem describing Blasius viscous flow of a kind of non-Newtonian fluid has an infinite number of explicit analytic solutions. These solutions are rather sensitive to the second-order derivative at the boundary, and the difference of the second derivatives of two obviously different solutions might be less than 10-1000. Therefore, it seems impossible to find out all of these solutions by means of current numerical methods. Thus, this nonlinear problem might become a challenge to current numerical techniques.\n",
      "\n",
      "6. id: 5390a88c20f70186a0e98d7e   score: 0.9500415   abstract: In this paper we show that a nonlinear boundary-value problem describing Blasius viscous flow of a kind of non-Newtonian fluid has an infinite number of explicit analytic solutions. These solutions are rather sensitive to the second-order derivative at the boundary, and the difference of the second derivatives of two obviously different solutions might be less than 10^-^1^0^0^0. Therefore, it seems impossible to find out all of these solutions by means of current numerical methods. Thus, this nonlinear problem might become a challenge to current numerical techniques.\n",
      "\n",
      "7. id: 558fe92c0cf23515427185ee   score: 0.94407386   abstract: The numerical solution of a nonlinear singularly perturbed two-point boundary value problem is studied. The developed method is based on Hermitian approximation of the second derivative on special discretization mesh. Numerical examples which demonstrate the effectiveness of the method are presented.\n",
      "\n",
      "8. id: 558d19bce4b0bb3102a0cf0c   score: 0.9405718   abstract: Solutions for the boundary value problem over an infinite domain have been obtained by first transforming the two-dimensional laminar boundary layer equations into an ordinary differential equation through similarity variables. The governing problem is the two-parameter Falkner-Skan equation with @b, the streamwise pressure gradient and @c the suction velocity. The closed form solution for @b=-1 obtained earlier is rewritten, which is then generalized for general@b. The same equation is also solved using method of stretching of variables. Different velocity profiles have been observed for both @b and @c. The results from both approaches are compared with that of direct numerical solutions, which agree very well.\n",
      "\n",
      "9. id: 5390a2e920f70186a0e6733d   score: 0.93721074   abstract: In the present paper we extend the fourth order method developed by Chawla et al. [M.M. Chawla, R. Subramanian, H.L. Sathi, A fourth order method for a singular two-point boundary value problem, BIT 28 (1988) 88-97] to a class of singular boundary value problems (p(x)y^')^'=p(x)f(x,y),0=0 is a non-negative function. The order of accuracy of the method is established under quite general conditions on f(x,y) and is also verified by one example. The oxygen diffusion problem in a spherical cell and a nonlinear heat conduction model of a human head are presented as illustrative examples. For these examples, the results are in good agreement with existing ones.\n",
      "\n",
      "10. id: 559015a6612c8aa08ca8ba3c   score: 0.9370958   abstract: A numerical method is treated for solving singular boundary value problems with solutions that can be represented as series expansions on a subinterval near the singularity. A regular boundary value problem is derived on the remaining interval, for which a difference method is used. Convergence theorems are given for general schemes and for schemes of positive type for second order equations.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1662616\n",
      "index                                        55913c980cf232eb904fb64c\n",
      "title               Measurable Decision Making with GSR and Pupill...\n",
      "authors             Jianlong Zhou, Jinjun Sun, Fang Chen, Yang Wan...\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Computer-Human Interaction...\n",
      "references          5390ac1720f70186a0eb29e6;5390aeba20f70186a0eca...\n",
      "abstract            This article presents a framework of adaptive,...\n",
      "id                                                            1662616\n",
      "clustered_labels                                                    0\n",
      "Name: 1662616, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909ed120f70186a0e30a40   score: 0.37868884   abstract: Multimodal user interfaces (MMUI) allow users to control computers using speech and gesture, and have the potential to minimise users. experienced cognitive load, especially when performing complex tasks. In this paper, we describe our attempt to use a physiological measure, namely Galvanic Skin Response (GSR), to objectively evaluate users. stress and arousal levels while using unimodal and multimodal versions of the same interface. Preliminary results show that users. GSR readings significantly increase when task cognitive load level increases. Moreover, users. GSR readings are found to be lower when using a multimodal interface, instead of a unimodal interface. Cross-examination of GSR data with multimodal data annotation showed promising results in explaining the peaks in the GSR data, which are found to correlate with sub-task user events. This interesting result verifies that GSR c\n",
      "\n",
      "2. id: 539099b320f70186a0e1b7e0   score: 0.33263552   abstract: Decision-making theories aiming at solving decision problems that involve multiple criteria have often been incorporated in knowledge-based systems for the improvement of these systems' reasoning process. However, multicriteria analysis has not been used adequately in intelligent user interfaces, even though user-computer interaction is, by nature, multicriteria-based. The actual process of incorporating multicriteria analysis into an intelligent user interface is neither clearly defined nor adequately described in the literature. It involves many experimental studies throughout the software life-cycle. Moreover, each multicriteria decision-making theory requires different kinds of experiments for the criteria to be determined and then for the proper respective weight of each criterion to be specified. In our research, we address the complex issue of developing intelligent user interface\n",
      "\n",
      "3. id: 5390880720f70186a0d79b03   score: 0.2941136   abstract: The information systems for decision-making must provide basic elements to the decision-maker in a synthetic and simple way. So, it is necessary to build Computer Human Interface (CHI) adapted to different user's perceptions as well as possible. Therefore, the necessity to design the application data processing providing an intelligent and self-adaptive CHI seems to be more and more necessary. An essential and necessary characteristic of this kind of CHI is the capacity to adapt it to the environment and to the user's behavior, and to permit the addition of components without putting back in question the design of this CHI. In order to have these characteristics in an application, a modelling using intelligent agents seems to be well adapted because it permits to take into account the complexe interaction present in the CHI.\n",
      "\n",
      "4. id: 5390a37f20f70186a0e6cdd0   score: 0.16885695   abstract: Any architectural optimization aims at satisfying the end user. However, modern architectures execute with little to no knowledge about the individual user. If architectures could determine whether their users are satisfied, they could provide higher efficiency; improved reliability, reduced power consumption, increased security, and a better user experience. A major reason for this limitation is their input devices. Specifically, the traditional input devices (e.g., the mouse and keyboard) provide limited information about the user. In this paper, we make a case for the addition of new biometric input devices for providing the computer information about the user’s physiological traits. We explore three biometric devices as potential sensors: an eye tracker, a galvanic skin response (GSR) sensor, and force sensors. We first present two user studies that explore the link between the senso\n",
      "\n",
      "5. id: 5390a01420f70186a0e46780   score: 0.16198196   abstract: The paper deals with a complex decision-making problem, the selection and evaluation of Learning Management Systems (LMS) in which several objectives - referring to the definite group of users - like social, technical, environmental, and economic impacts, must be simultaneously taken into account. We introduce Evaluation Cycle Management (ECM), a support methodology aimed at the evaluation of options that occur in the decision-making processes. ECM is based on Multi-attribute decision making (Criteria Evaluation) and Usability Testing (Usability Evaluation). The Multi-attribute decision making in the first phase of ECM presents an approach to the development of a qualitative hierarchical decision model that is based on DEX, an expert system shell for multiattribute decision support. The second phase of ECM is aimed at Usability Testing on end users. ECM illustrates its usefulness by show\n",
      "\n",
      "6. id: 5390ad0720f70186a0ebbf3c   score: 0.1526628   abstract: In multiple attribute decision making (MADM) problem, a decision maker (DM) has to choose the best alternative that satisfies the evaluation criteria among a set of candidate solutions. It is generally hard to find an alternative that meets all the criteria simultaneously, so a good compromise solution is preferred. The VIKOR method was developed for multi-criteria optimization of complex systems. This method focuses on ranking and selecting from a set of alternatives in the presence of conflicting criteria. It introduces the multi-criteria ranking index based on the particular measure of ''closeness'' to the ''ideal'' solution. To deal with the uncertainty and vagueness from humans' subjective perception and experience in decision process, this paper presents an evaluation model based on deterministic data, fuzzy numbers, interval numbers and linguistic terms. Combination of analytic hi\n",
      "\n",
      "7. id: 5390a9a520f70186a0ea6c7b   score: 0.14104463   abstract: Physiological measures have found reliable sensitivity to the variation of mental efforts to tasks of different difficulty levels. The sensitivity needs to be enhanced for further application. This paper proposed a composite measure consisting of three physiological measures, facial skin temperature, eye blinks and pupil dilation. The facial skin temperature will be measured by an infrared camera. One dimensional iris image will be used for the measurement of eye activities. All measurement will be done in real-time and unobtrusively. A preliminary experiment will be conducted for each measure to demonstrate their sensitivity. The combination then will be accomplished by factor analysis and regression analysis. Last, the analysis will focus on the improvement in sensitivity from the combination of individual measures.\n",
      "\n",
      "8. id: 5390a63c20f70186a0e81863   score: 0.11797801   abstract: This paper reviews some exploratory research focused on developing a usability methodology based on objective biometrics computing using physiological data (ECG, respiration, and GSR sensors, as well as an infrared eye tracker) as well as behavior data (mouse and keystroke input). Following a high level literature review, various biometrics are discussed with the goal of motivating further study into the development of a methodology for usability testing, including the assessment of user satisfaction. Lessons learned and suggestions for future work were also discussed.\n",
      "\n",
      "9. id: 5390a05a20f70186a0e4a73a   score: 0.09170696   abstract: This pilot study explores the use of combining multiple data sources (subjective, physical, physiological, and eye tracking) in understanding user cost and behavior. Specifically, we show the efficacy of such objective measurements as heart rate variability (HRV), and pupillary response in evaluating user cost in game environments, along with subjective techniques, and investigate eye and hand behavior at various levels of user cost. In addition, a method for evaluating task performance at the micro-level is developed by combining eye and hand data. Four findings indicate the great potential value of combining multiple data sources to evaluate interaction: first, spectral analysis of HRV in the low frequency band shows significant sensitivity to changes in user cost, modulated by game difficulty-the result is consistent with subjective ratings, but pupillary response fails to accord with\n",
      "\n",
      "10. id: 539095ba20f70186a0df206e   score: 0.07250525   abstract: A human-machine interface framework provides general guidelines for what information should be put on an interface display screen. The framework is thus a first step towards the design of an effective and efficient interface. This paper reports on an experimental study of two proposed frameworks: the ecological interface design framework and the function behaviour-state framework. In order to provide an unbiased comparative evaluation for both interfaces, the same application problem is used. The interfaces, based on each of the two frameworks, are implemented with as similar look-and-feel forms as possible in the presentation of information contents. Only the normal control operation and fault detection situations are considered at this stage of the study. In addition, in this study three categories of measures are used, namely: the performance measure; the physiological measure (the ey\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1712238\n",
      "index                                        55323c1245cec66b6f9db1d4\n",
      "title               Filtering of the Markov jump process given the...\n",
      "authors                 A. V. Borisov, B. M. Miller, K. V. Semenikhin\n",
      "year                                                           2015.0\n",
      "venue                                   Automation and Remote Control\n",
      "references          5390881720f70186a0d80a78;53908b4920f70186a0dbc...\n",
      "abstract            The problem of optimal filtering of the Markov...\n",
      "id                                                            1712238\n",
      "clustered_labels                                                    0\n",
      "Name: 1712238, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909ed120f70186a0e2fcfa   score: 0.79755676   abstract: A solution to the filtering problem of states of special Markov jump processes that is optimal in the mean-square sense at the class of polynomial observation functions is presented. A comparison of the proposed estimates with the known estimates of optimal linear and nonlinear filtering is given.\n",
      "\n",
      "2. id: 5390af8920f70186a0ed032c   score: 0.79117644   abstract: The paper is devoted to a state filtering problem of Markov jump processes given the continuous and/or counting observations. All the transition intensity matrix, observation plan and counting intensity are parameterized by a random vector with uncertain distribution on a known support set. The estimation problem is formulated in minimax settings with a conditional optimality criterion. We reduce the initial minimax problem to a dual problem of constrained quadratic optimization. The corresponding numerical algorithm of minimax filtering is presented as well as its illustrative implementation in the monitoring of a TCP link status under uncertainty.\n",
      "\n",
      "3. id: 5390a72220f70186a0e8936a   score: 0.5441959   abstract: Abstract: This paper formulates and analyzes a generalized, finite-state version of the linear-quadratic-Gaussian control problem with interrupted observations. The mechanism that varies observation quality is allowed to be partially observed and subject to control. The new problem formulation is noted to be a special case of a vector version of a partially observed Markov optimization problem. Upper and lower bounds on optimal cost are determined for the case where the mechanism that varies observation quality is completely observed. Numerical approaches for the partially observed Markov optimization problem that are applicable to the newly formulated optimization problem are discussed.\n",
      "\n",
      "4. id: 5390a01420f70186a0e4773e   score: 0.52879196   abstract: Consideration was given to some problems of estimation (filtering and identification) in the observation systems describing the Markov processes with finite state spaces. The transition intensity matrices and the observation plan are random and have unknown distributions of some class. The conditional expectations of the accessible observations of some quadratic functions of the estimate errors are used as the performance criteria. The estimation problems under study lie in constructing estimates minimizing the conditional mean losses corresponding to the least favorable distribution of the \"transition intensity matrix-observation plan matrix\" pair from the set of permissible distributions. For the corresponding minimax problems, existence of the saddle points was proved, and the form of the corresponding minimax estimates was established.\n",
      "\n",
      "5. id: 539087dd20f70186a0d64a49   score: 0.51377517   abstract: Consider the Wonham optimal filtering problem for a finite state ergodic Markov process in both discrete and continuous time, and let $\\sigma$ be the noise intensity for the observation. We examine the sensitivity of the solution with respect to the filter's initial conditions in terms of the gap between the first two Lyapunov exponents of the Zakai equation for the unnormalized conditional probability. This gap is studied in the limit as $\\sigma\\to 0$ by techniques involving considerations of nonlinear filtering and the stochastic Feynman--Kac formula. Conditions are given for the limit to be either negative or $-\\infty$. Asymptotic bounds are derived in the latter case.\n",
      "\n",
      "6. id: 5390a06e20f70186a0e4c2da   score: 0.46711904   abstract: The problem of the optimal estimation of continuous processes by discrete measurements in the presence of time lag (delay) is considered. On the basis of the theory of parametric transfer functions, an optimal, periodically nonstationary filter is developed, which affords a minimum of the estimation error variance at any instant of time. The comparison is performed of the obtained solution with the optimal stationary filter, which ensures a minimum of the mean (by continuous time) error variance. It is shown that in the problem of estimation of the Markov process of the first order, a simpler stationary filter with the fixer of order zero is insignificantly inferior to the optimal filter.\n",
      "\n",
      "7. id: 5390b4da20f70186a0f00fef   score: 0.42835295   abstract: Derives finite-dimensional discrete-time filters for estimating the parameters of discrete-time finite-state Markov chains imbedded in a mixture of Gaussian white noise and deterministic signals of known functional form with unknown parameters. The filters that are derived estimate quantities used in the expectation-maximization (EM) algorithm for maximum likelihood (ML) estimation of the Markov chain parameters (transition probabilities and state levels) as well as the parameters of the deterministic interference. Two types of deterministic signals are considered: periodic or almost periodic signals with unknown frequency components, amplitudes, and phases, and polynomial drift in the states of the Markov process with the coefficients of the polynomial unknown. The filter-based EM algorithm has negligible memory requirements. In comparison, implementing the EM algorithm using smoothed v\n",
      "\n",
      "8. id: 55323d1645cec66b6f9dd855   score: 0.42811382   abstract: <P>The purpose of this paper is two fold. First, bounds on the rate of convergence of empirical measures in controlled Markov chains are obtained under some recurrence conditions. These include bounds obtained through large deviations and central limit theorem arguments. These results are then applied to optimal control problems. Bounds on the rate of convergence of the empirical measures that are uniform over different sets of policies are derived, resulting in bounds on the rate of convergence of the costs. Finally, new optimal control problems that involve not only average cost criteria but also measures on the transient behavior of the cost, namely the rate of convergence, are introduced and applied to a problem in telecommunications. The solution to these problems rely on the bounds introduced in previous sections.</P>\n",
      "\n",
      "9. id: 5390b5fa20f70186a0f10566   score: 0.39828944   abstract: The problem of estimating the number of transitions of finite-state continuous-time Markov processes observed by a noisy sensor is considered. A finite-dimensional exact filter is derived, and using the EM algorithm (an extension of the Baum-Welch algorithm for the discrete-time case), an application is made to the problem of estimating the unknown transition matrix of a finite-state continuous-time Markov process\n",
      "\n",
      "10. id: 5390b5fa20f70186a0f109aa   score: 0.3761647   abstract: This paper discusses finite-dimensional optimal filters for partially observed Markov chains. A model for a system containing a finite number of components where each component behaves like an independent finite state continuous-time Markov chain is considered. Using measure change techniques various estimators are derived.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696319\n",
      "index                                        55922d8e0cf244696a09d996\n",
      "title               A Localization Method Based on Map-Matching an...\n",
      "authors             Andry M. Pinto, António P. Moreira, Paulo G. C...\n",
      "year                                                           2015.0\n",
      "venue                      Journal of Intelligent and Robotic Systems\n",
      "references          5390a74f20f70186a0e8d154;5390b13020f70186a0edd...\n",
      "abstract            This paper presents a novel localization metho...\n",
      "id                                                            1696319\n",
      "clustered_labels                                                    1\n",
      "Name: 1696319, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a74f20f70186a0e8c47b   score: 0.9850429   abstract: This paper presents a novel approach to mobile robot localization. This approach is basically different from other methods developed so far, because the sensors are not mounted on the mobile robot. In fact, they are statically located in the environment. These sensors, which could be for instance SICK lasers embedded in the walls of a museum, constantly measure the distance to points on the surface of the mobile robot in a plane. The only further assumption we have to make is that the robot has a circular shape in this plane. This supposition is feasible, since most indoor service robots have a circular shape. We then use a simple tracking algorithm based on the Kalman filter and geometrical considerations to localize the robot with a very high precision. In simulations it is shown, that our method is able to localize the robot with an error in the range of one centimeter.\n",
      "\n",
      "2. id: 5390ab8820f70186a0eb09a9   score: 0.9832145   abstract: The conventional approach to nonlinear state estimation, the Extended Kalman Filter (EKF), is quantitatively compared to the performance of the relative newcomer, the Sigma-Point Kalman Filter (SPKF). These approaches are applied to the problem of localization of a mobile robot using a known map, and compared under the context of the practical best performance of a Bayes Filter-type method using a particle filter with a very large number of particles.\n",
      "\n",
      "3. id: 5390ae2e20f70186a0ec8351   score: 0.9756698   abstract: Real mobile robots should be able to build an abstract representation of the physical environment, in order to navigate and work in such environment. We propose a method for integrated exploration, where mobile robot incrementally build a map of this environment while simultaneously use this map to compute the absolute robot localization, and make local decisions on where to move next in order to minimize the error in the estimation of the mobile pose and the configuration locations. The continuous localization process is based on the extended Kalman filter. We present simulated and experimental results on the Pionner 3DX robot to show the performance of the proposed strategy. In this methodology the robot uses only range sensors.\n",
      "\n",
      "4. id: 53909a0220f70186a0e1f31e   score: 0.9604354   abstract: Localization, i.e., estimating a robot pose relative to a map of an environment, is one of the most relevant problems in mobile robotics. The research community has devoted a big effort to provide solutions for the localization problem. Several methodologies have been proposed, among them the Kalman filter and Monte Carlo Localization filters. In this paper, the Clustered Evolutionary Monte Carlo filter (CE-MCL) is presented. This algorithm, taking advantage of an evolutionary approach along with a clusterization method, is able to overcome classical MCL filter drawbacks. Exhaustive experiments, carried on the robot ATRV-Jr manufactured by iRobot, are shown to prove the effectiveness of the proposed CE-MCL filter.\n",
      "\n",
      "5. id: 5390adfc20f70186a0ec47f9   score: 0.93195957   abstract: In this paper, the method of mobile robot localization based on Monte Carlo algorithm (MCL) is proposed. The method is the probability distribution of mobile robot position in the moving environment is expressed using a series of particles with weights. The step of this algorithm is predicting particle position, followed by the calculation of particle weight, then updating the particle distribution, and finally estimating the robot position. The results show the localization effect based on Monte Carlo algorithm is better than Markov algorithm, and the localization precision can be improved by increasing the number of sensors and enhancing the frequency of sampling.\n",
      "\n",
      "6. id: 5390a54620f70186a0e77891   score: 0.9203613   abstract: In this paper we present a mobile robot localization system that integrates Monte-Carlo localization with an active action-selection approach based on an aliasing map.The main novelty of the approach is in the off-line evaluation of the perceptual aliasing of the environment and in the use of this knowledge to perform localization processes faster and better. Preliminary results show improved performances compared with the classic Monte-Carlo localization approach.\n",
      "\n",
      "7. id: 5390aaf920f70186a0eae8d6   score: 0.9199307   abstract: Mobile robot localization is the problem of determining the position of a mobile robot from sensor data. Active localization provides setting the robot's motion direction and determining the pointing direction of the sensors during localization so as to most efficiently localize the robot. This paper proposes an active localization approach that employs Monte Carlo Localization, which is based on particle filters. The technique offers two main advantages. 1) The framework applies a different way of initializing the particles that helps to reduce some steps of localization, and 2) a new resampling scheme is used to reduce the cost of localization and solve the kidnapped robot problem. Experimental results show that the probability of robot successfully localize itself is considerably high, i.e. robot can recover from failure and localize itself based on new sensor data and reduction of co\n",
      "\n",
      "8. id: 5390990f20f70186a0e10722   score: 0.91891783   abstract: An autonomous mobile robot must be able to elaborate the measures provided by the sensor equipment to localize itself with respect to a coordinate system. The precision of the location estimate depends on the sensor accuracy and on the reliability of the measure processing algorithm. The purpose of this article is to propose a low cost positioning system using internal sensors like odometers and optical fiber gyroscopes. Three simple localization algorithms based on different sensor data processing procedures are presented. Two of them operate in a deterministic framework, the third operates in a stochastic framework where the uncertainty is induced by sensing and unmodeled robot dynamics. The performance of the proposed localization algorithms are tested through a wide set of laboratory experiments and compared in terms of localization accuracy and computational cost. © 2005 Wiley Perio\n",
      "\n",
      "9. id: 5390a45620f70186a0e72f20   score: 0.9175984   abstract: This paper presents a novel approach for real-time mobile robot localization in structured indoor environments. The proposed method takes advantage of the available structural information by implementing a Monte Carlo Localization strategy over a map of line segments rather than a grid-based map, thus allowing for speed improvements. Another novel aspect is in the likelihood function, which is based on a Modified Hausdorff Distance between the expected line segments the robot should sense and the line segments extracted from actual measurements using a range finder. Additionally, the number of particles of the Monte Carlo Localization method is automatically adjusted, using a large number of particles in the global localization phase, where the position of the robot is unknown, and a reduced number of particles during the state tracking phase, where uncertainty about the robot position i\n",
      "\n",
      "10. id: 5390a1d420f70186a0e58c96   score: 0.91006696   abstract: The visual localization problem in robotics poses a dynamically changing environment due to the movement of the robot compared to a static image set serving as environmental map. We develop a particle swarm method adapted to this task and apply elements from dynamic optimization research. We show that our algorithm is able to outperform a Particle Filter, which is a standard localization approach in robotics, in a scenario of two visual outdoor datasets, being computationally more effective and delivering a better localization result.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675372\n",
      "index                                        559152fd0cf232eb904fbc0c\n",
      "title                          NailO: Fingernails as an Input Surface\n",
      "authors             Hsin-Liu (Cindy) Kao, Artem Dementyev, Joseph ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558b1e90612c41e6b9d43eb2;5390afc920f70186a0ed2...\n",
      "abstract            We present NailO, a nail-mounted gestural inpu...\n",
      "id                                                            1675372\n",
      "clustered_labels                                                    0\n",
      "Name: 1675372, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bded20f70186a0f49e36   score: 0.90665317   abstract: Gesture-based interfaces--which let users control devices with,for example, hand or finger motions--are becoming increasingly popular.\n",
      "\n",
      "2. id: 5390995d20f70186a0e157c8   score: 0.5497917   abstract: This paper presents a novel input device design for capturing gestures. The system is based on commodity components and combines accelerometers, gyroscopes and bend sensors. It is a low-power, low-cost hand device, characterized by extreme wearability thanks to wireless communication support and small form-factor. It can be used as a stand-alone platform or combined with other wireless sensor nodes in a body area network. The system has been tested as input interface for moving a virtual three-dimensional hand in real-time.\n",
      "\n",
      "3. id: 5390a1f820f70186a0e5c896   score: 0.48463923   abstract: We present Scratch Input, an acoustic-based input technique that relies on the unique sound produced when a fingernail is dragged over the surface of a textured material, such as wood, fabric, or wall paint. We employ a simple sensor that can be easily coupled with existing surfaces, such as walls and tables, turning them into large, unpowered and ad hoc finger input surfaces. Our sensor is sufficiently small that it could be incorporated into a mobile device, allowing any suitable surface on which it rests to be appropriated as a gestural input surface. Several example applications were developed to demonstrate possible interactions. We conclude with a study that shows users can perform six Scratch Input gestures at about 90% accuracy with less than five minutes of training and on wide variety of surfaces.\n",
      "\n",
      "4. id: 5390bb1d20f70186a0f3e528   score: 0.24889915   abstract: This paper proposes an intuitive input interface that can handle various operations based on finger image recognition. It receives continuous analog input by detecting a knuckle of the user's clenched fist. In contrast to the conventional wireless mouse, whose sensitivity cannot be changed dynamically, the proposed method brings not only stable positioning but also quick clicking with a small finger gesture. In order to evaluate operability, we conducted a user experiment: a time trial for target selection. The subjects completed the task with the proposed controller in 44% less time than with a conventional wireless mouse. We confirmed that the proposed method can reliably follow finger gestures.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f5185f   score: 0.13637818   abstract: The current trend towards smaller and smaller mobile devices may cause considerable difficulties in using them. In this paper, we propose an interface called Anywhere Surface Touch, which allows any flat or curved surface in a real environment to be used as an input area. The interface uses only a single small camera and a contact microphone to recognize several kinds of interaction between the fingers of the user and the surface. The system recognizes which fingers are interacting and in which direction the fingers are moving. Additionally, the fusion of vision and sound allows the system to distinguish the contact conditions between the fingers and the surface. Evaluation experiments showed that users became accustomed to our system quickly, soon being able to perform input operations on various surfaces.\n",
      "\n",
      "6. id: 5390b3ae20f70186a0ef4d9b   score: 0.112600096   abstract: The recent success of Nintendo's Wii and multi-touch input devices like the Apple iPhone clearly shows that people are more willing to accept new input device-technologies based on intuitive forms of interaction. Gesture-based input is thus becoming important and even relevant in specific application scenarios. A sensor type especially suited for natural gesture recognition is the capacitive proximity sensor that allows the detection of objects without any physical contact. In this paper we extend the input device taxonomy by Card et al to include this detector category and allow modeling of devices based on advanced sensor units that involve data processing. We have created a prototype based on this modeling and evaluated its use regarding several application scenarios, where such a device might be useful. The focus of this evaluation was to determine the suitability of the device for d\n",
      "\n",
      "7. id: 5390a63c20f70186a0e82a47   score: 0.11008789   abstract: We discuss the benefits of using a mobile device to expand and improve the interactions on a large touch-sensitive surface. The mobile device's denser arrangement of pixels and touch-sensor elements, and its rich set of mechanical on-board input controls, can be leveraged for increased expressiveness, visual feedback and more precise direct-manipulation. We also show how these devices can support unique input from multiple simultaneous users in collaborative scenarios. Handheld mobile devices and large interactive surfaces can be mutually beneficial in numerous ways, while their complementary nature allows them to preserve the behavior of the original user interface.\n",
      "\n",
      "8. id: 558adcdd612c41e6b9d3bd51   score: 0.10248422   abstract: This paper presents a remote gesture input solution for interacting indirectly with user interfaces on mobile and wearable devices. The proposed solution uses a wearable ring platform worn on users index finger. The ring detects and interprets various gestures performed on any available surface, and wirelessly transmits the gestures to the remote device. The ring opportunistically harvests energy from an NFC-enabled phone for perpetual operation without explicit charging. We use a finger-tendon pressure-based solution to detect touch, and a light-weight audio based solution for detecting finger motion on a surface. The two level energy efficient classification algorithms identify 23 unique gestures that include tapping, swipes, scrolling, and strokes for hand written text entry. The classification algorithms have an average accuracy of 73% with no explicit user training. Our implementati\n",
      "\n",
      "9. id: 5390881720f70186a0d80495   score: 0.072374016   abstract: A technique for creating a touch-sensitive input device is proposed which allows multiple, simultaneous users to interact in an intuitive fashion. Touch location information is determined independently for each user, allowing each touch on a common surface to be associated with a particular user. The surface generates location dependent, modulated electric fields which are capacitively coupled through the users to receivers installed in the work environment. We describe the design of these systems and their applications. Finally, we present results we have obtained with a small prototype device.\n",
      "\n",
      "10. id: 5390baa120f70186a0f3862a   score: 0.068288535   abstract: This work presents a novel and always-available nail mounted display known as NailDisplay. The proposed display augments the use of a finger by allowing for always-available visual feedback owing to its fast accessibility and binding user controls with the display, i.e. what you control is what you see (through the display). Potential benefits of NailDisplay are demonstrated in three applications: from displaying to combining it with user controls. In the first application, NailDisplay can reveal what is occluded under a finger touch, making it a solution to operate small UI elements. In the second application, NailDisplay is complementary to an imaginary interface, helping users to learn an imaginary interface (e.g., on the users' arms) and allowing them to reassure the interface when their memory of it becomes unclear. In the third application, NailDisplay is integrated with rich finge\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672454\n",
      "index                                        55912a010cf232eb904fb0e2\n",
      "title               SemaDroid: A Privacy-Aware Sensor Management F...\n",
      "authors                                            Zhi Xu, Sencun Zhu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 5th ACM Conference on Data ...\n",
      "references          558b13f2612c41e6b9d42556;558bd2020cf25dbdbb04d...\n",
      "abstract            While mobile sensing applications are booming,...\n",
      "id                                                            1672454\n",
      "clustered_labels                                                    3\n",
      "Name: 1672454, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b86b20f70186a0f2892d   score: 0.91505283   abstract: Smart phones with increased computation and sensing capabilities have spurred the growth of context-aware apps. In current mobile platforms, these apps have direct access to raw sensor data streams, and can use the sensor data to infer a user's personal context. However, the sharing of raw sensor data poses a privacy risk because a malicious app can easily extract sensitive information about the user. We argue that a user can employ preventative measures to limit the sensitive information disclosed to apps. Current approaches amount to sensor data access control: trusted apps are trusted to not misuse the sensor data, and untrusted apps are simply not allowed access to sensor data. However, such simple static policies are too conservative because there is a sharp decline in the usefulness of untrusted apps. We propose Override: a mobile privacy framework that empowers users to specify co\n",
      "\n",
      "2. id: 5390c04520f70186a0f579aa   score: 0.906156   abstract: Smart phones are used to collect and share personal data with untrustworthy third-party apps, often leading to data misuse and privacy violations. Unfortunately, state-of-the-art privacy mechanisms on Android provide inadequate access control and do not address the vulnerabilities that arise due to unmediated access to so-called innocuous sensors on these phones. We present ipShield, a framework that provides users with greater control over their resources at runtime. ipShield performs monitoring of every sensor accessed by an app and uses this information to perform privacy risk assessment. The risks are conveyed to the user as a list of possible inferences that can be drawn using the shared sensor data. Based on user-configured lists of allowed and private inferences, a recommendation consisting of binary privacy actions on individual sensors is generated. Finally, users are provided w\n",
      "\n",
      "3. id: 558cee120cf23fdd601e128a   score: 0.8373541   abstract: Current smartphone systems allow the user to use only marginally contextual information to specify the behavior of the applications: this hinders the wide adoption of this technology to its full potential. In this paper, we fill this gap by proposing CRêPE, a fine-grained Context-Related Policy Enforcement System for Android. While the concept of context-related access control is not new, this is the first work that brings this concept into the smartphone environment. In particular, in our work, a context can be defined by: the status of variables sensed by physical (low level) sensors, like time and location; additional processing on these data via software (high level) sensors; or particular interactions with the users or third parties. CRêPE allows context-related policies to be set (even at runtime) by both the user and authorized third parties locally (via an application) or remotel\n",
      "\n",
      "4. id: 5390a5b020f70186a0e7d72b   score: 0.799129   abstract: Modern mobile phones possess three types of capabilities: computing, communication, and sensing. While these capabilities enable a variety of novel applications, they also raise serious privacy concerns. We explore the vulnerability where attackers snoop on users by sniffing on their mobile phone sensors, such as the microphone, camera, and GPS receiver. We show that current mobile phone platforms inadequately protect their users from this threat. To provide better privacy for mobile phone users, we analyze desirable uses of these sensors and discuss the properties of good privacy protection solutions. Then, we propose a general framework for such solutions and discuss various possible approaches to implement the framework's components.\n",
      "\n",
      "5. id: 5390bb1d20f70186a0f3e4de   score: 0.77982527   abstract: Today's smartphone operating systems frequently fail to provide users with adequate control over and visibility into how third-party applications use their privacy-sensitive data. We address these shortcomings with TaintDroid, an efficient, systemwide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid provides real-time analysis by leveraging Android's virtualized execution environment. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, we found 68 instances of misappropriation of users' location and device identification information across 20 applications. Monitoring sensitive data with TaintDroid provides informed use of third-party applications for phone users and valuable input for smartphone security service firms seeking to identify misbehaving applications.\n",
      "\n",
      "6. id: 5390adfd20f70186a0ec5ec4   score: 0.6931614   abstract: Today's smartphone operating systems frequently fail to provide users with adequate control over and visibility into how third-party applications use their private data. We address these shortcomings with TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid provides realtime analysis by leveraging Android's virtualized execution environment. TaintDroid incurs only 14% performance overhead on a CPU-bound micro-benchmark and imposes negligible overhead on interactive third-party applications. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, we found 68 instances of potential misuse of users' private information across 20 applications. Monitoring sensitive data with TaintDroid provides informed use of third-party applications for phone users a\n",
      "\n",
      "7. id: 5390bded20f70186a0f49aaa   score: 0.6629056   abstract: This paper describes the framework of Android mobile applications security system and the security threats, and puts forward some corresponding preventive measures.\n",
      "\n",
      "8. id: 5390bfa220f70186a0f5371f   score: 0.6205641   abstract: In these days there are many malicious applications that collect sensitive information owned by third-party applications by escalating their privileges to the higher level on the Android operating system. An attack of obtaining the root-level privilege in the Android operating system can be a serious threat to users because it can break down the whole system security. This paper proposes a new Android security framework that can meet the following three goals: (1) preventing privilege escalation attacks, (2) maintaining system integrity, and (3) protecting users' personal information. To achieve these goals, our proposed framework introduces three mechanisms: Root Privilege Protection (RPP), Resource Misuse Protection (RMP), and Private Data Protection (PDP). RPP keeps track of a list of trusted programs with root-level privileges and can detect and respond to malware that illegally trie\n",
      "\n",
      "9. id: 5390bae520f70186a0f3aa60   score: 0.524075   abstract: Private data stored on smartphones is a precious target for malware attacks. A constantly changing environment, e.g. switching network connections, can cause unpredictable threats, and require an adaptive approach to access control. Context-based access control is using dynamic environmental information, including it into access decisions. We propose an “ecosystem-in-an-ecosystem” which acts as a secure container for trusted software aiming at enterprise scenarios where users are allowed to use private devices. We have implemented a proof-of-concept prototype for an access control framework that processes changes to low-level sensors and semantically enriches them, adapting access control policies to the current context. This allows the user or the administrator to maintain fine-grained control over resource usage by compliant applications. Hence, resources local to the trusted container\n",
      "\n",
      "10. id: 5390b72e20f70186a0f222a6   score: 0.5236031   abstract: In the last decade, smartphones have gained widespread usage. Since the advent of online application stores, hundreds of thousands of applications have become instantly available to millions of smart-phone users. Within the Android ecosystem, application security is governed by digital signatures and a list of coarse-grained permissions. However, this mechanism is not fine-grained enough to provide the user with a sufficient means of control of the applications' activities. Abuse of highly sensible private information such as phone numbers without users' notice is the result. We show that there is a high frequency of privacy leaks even among widely popular applications. Together with the fact that the majority of the users are not proficient in computer security, this presents a challenge to the engineers developing security solutions for the platform. Our contribution is twofold: first,\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674944\n",
      "index                                        55913fbb0cf232eb904fb747\n",
      "title               Investigating the Dexterity of Multi-Finger In...\n",
      "authors             Srinath Sridhar, Anna Maria Feit, Christian Th...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558af095612c41e6b9d3dff4;5390b72e20f70186a0f21...\n",
      "abstract            This paper investigates an emerging input meth...\n",
      "id                                                            1674944\n",
      "clustered_labels                                                    0\n",
      "Name: 1674944, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4c5d2   score: 0.8056322   abstract: We present TouchSense, which provides additional touchscreen input vocabulary by distinguishing the areas of users' finger pads contacting the touchscreen. It requires minimal touch input area and minimal movement, making it especially ideal for wearable devices such as smart watches and smart glasses. For example, users of a calculator application on a smart watch could tap normally to enter numbers, and tap with the right side of their fingers to enter the operators (e.g. , -, =). Results from two human-factor studies showed that users could tap a touchscreen with five or more distinct areas of their finger pads. Also, they were able to tap with more distinct areas closer to their fingertips. We developed a TouchSense smart watch prototype using inertial measurement sensors, and developed two example applications: a calculator and a text editor. We also collected user feedback via an e\n",
      "\n",
      "2. id: 5390b13020f70186a0ede345   score: 0.8047131   abstract: We introduce an interaction technique that increases the touch screen input vocabulary by distinguishing a strong tap from a gentle tap without the use of additional hardware. We have designed and validated an algorithm that detects different types of screen touches by combining data from the built-in accelerometer with position data from the touch screen. The proposed technique allows a touch screen input to contain not only the position of a finger contact, but also its type, i.e., whether the contact is a 'Tap' or a 'ForceTap.' To verify the feasibility of the proposed technique we have implemented our detection algorithm in experiments that test cases of single-handed, two-handed, immersive, and on the move usage. Based on the experimental results, we investigate the advantages of using two types of touch inputs and discuss emerging issues. Finally, we suggest a design guideline for \n",
      "\n",
      "3. id: 5390a6d920f70186a0e8773b   score: 0.7733876   abstract: In a mobile environment, the visual attention a person can devote to a computer is often limited. In such situations, a manual interface should be “gropable,” that is, the user should be able to access and use the interface with little to no visual attention. We compare stationary and mobile input on two embroidered textile interfaces; a single touch three button interface and a multitouch four button interface that is activated by pressing two buttons at the same time. Sixteen participants completed 480 trials while walking a path and sitting. While multitouch increases the expressiveness of gestures that can be performed, our user study only shows a slight, not statistically signiﬁcant, increase in accuracy and an understandable decrease in speed for simple selection tasks.\n",
      "\n",
      "4. id: 5390b44620f70186a0efa372   score: 0.7574357   abstract: We demonstrate our airwriting interface for mobile hands-free text entry. The interface enables a user to input text into a computer by writing in the air like on an imaginary blackboard. Hand motion is measured by an accelerometer and a gyroscope attached to the back of the hand and data is sent wirelessly to the processing computer. The system can continuously recognize arbitrary sentences based on a predefined vocabulary in real-time. The recognizer uses Hidden Markov Models (HMM) together with a statistical language model. We achieve a user-independent word error rate of 11% for a 8K vocabulary based on an experiment with nine users.\n",
      "\n",
      "5. id: 5390b0ca20f70186a0ed984b   score: 0.6834195   abstract: Gesture input has an advantage of rich expressive power over conventional input devices, but it is difficult to share the gesture trajectory with other people through writing or verbally. We obtained acceleration data for 10 kinds of gestures instructed through text, figures, and movies, totaling 44 patterns from 13 test subjects, for a total of 2630 data samples. We then examined the differences in the gestures.\n",
      "\n",
      "6. id: 5390baa120f70186a0f39459   score: 0.68236214   abstract: We show that large consensus exists among users in the way they articulate stroke gestures at various scales (i.e., small, medium, and large), and formulate a simple rule that estimates the user-intended scale of input gestures with 87% accuracy. Our estimator can enhance current gestural interfaces by leveraging scale as a natural parameter for gesture input, reflective of user perception (i.e., no training required). Gesture scale can simplify gesture set design, improve gesture-to-function mappings, and reduce the need for users to learn and for recognizers to discriminate unnecessary symbols.\n",
      "\n",
      "7. id: 558bce5f612cf6424275898d   score: 0.66671383   abstract: We show that users are consistent in their assessments of the articulation difficulty of multi-touch gestures, even under the many degrees of freedom afforded by multi-touch input, such as (1) various number of fingers touching the surface, (2) various number of strokes that structure the gesture shape, and (3) single-handed and bimanual input. To understand more about perceived difficulty, we characterize gesture articulations captured under these conditions with geometric and kinematic descriptors computed on a dataset of 7,200 samples of 30 distinct gesture types collected from 18 participants. We correlate the values of the objective descriptors with users' subjective assessments of articulation difficulty and report path length, production time, and gesture size as the highest correlators (max Pearson's r=.95). We also report new findings about multi-touch gesture input, e.g., gestu\n",
      "\n",
      "8. id: 5390baa120f70186a0f38cb2   score: 0.664758   abstract: Recent technologies in vision sensors are capable of capturing 3D finger positions and movements. We propose a novel way to control and interact with computers by moving fingers in the air. The positions of fingers are precisely captured by a computer vision device. By tracking the moving patterns of fingers, we can then recognize users' intended control commands or input information. We demonstrate this human input approach through an example application of handwriting recognition. By treating the input as a time series of 3D positions, we propose a fast algorithm using dynamic time warping to recognize characters in online fashion. We employ various optimization techniques to recognize in real time as one writes. Experiments show promising recognition performance and speed.\n",
      "\n",
      "9. id: 5390bed320f70186a0f4faeb   score: 0.62977463   abstract: The main obstructions of making hand gesture recognition methods robust in real-world applications are the challenges from the uncontrolled environments, including: gesturing hand out of the scene, pause during gestures, complex background, skin-coloured regions moving in background, performers wearing short sleeve and face overlapping with hand. Therefore, a framework for real-time hand gesture recognition in uncontrolled environments is proposed in this paper. A novel tracking scheme is proposed to track multiple hand candidates in unconstrained background, and a weighting model for gesture classification based on Hidden Conditional Random Fields which takes trajectories of multiple hand candidates under different frame rates into consideration is also introduced. The framework achieved invariance under change of scale, speed and location of the hand gestures. The Experimental results \n",
      "\n",
      "10. id: 5390baa120f70186a0f3861f   score: 0.619759   abstract: Fitts' law has proven to be a strong predictor of pointing performance under a wide range of conditions. However, it has been insufficient in modeling small-target acquisition with finger-touch based input on screens. We propose a dual-distribution hypothesis to interpret the distribution of the endpoints in finger touch input. We hypothesize the movement endpoint distribution as a sum of two independent normal distributions. One distribution reflects the relative precision governed by the speed-accuracy tradeoff rule in the human motor system, and the other captures the absolute precision of finger touch independent of the speed-accuracy tradeoff effect. Based on this hypothesis, we derived the FFitts model - an expansion of Fitts' law for finger touch input. We present three experiments in 1D target acquisition, 2D target acquisition and touchscreen keyboard typing tasks respectively. \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707761\n",
      "index                                        5591327d0cf232eb904fb336\n",
      "title               From the Deposit to the Exhibit Floor: An Expl...\n",
      "authors             Mark T. Marshall, Nick Dulake, Daniela Petrell...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390b68720f70186a0f1cb4c;5390972920f70186a0dfb...\n",
      "abstract            Museum objects have fascinating stories but ar...\n",
      "id                                                            1707761\n",
      "clustered_labels                                                    0\n",
      "Name: 1707761, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390baa120f70186a0f38c18   score: 0.64052343   abstract: When we go to the museum, we see many interesting objects that have fascinating stories related to them. However, visitors do not often share these experiences with people that have not visited the exhibition. Sharing is beneficial both because it can create interest and attract people to the museum, and because it can help people who cannot attend the visit (for example, is physically unable to do so) to still enjoy it. We were interested to understand the extent and motivation behind sharing (or not sharing) and test how to encourage visitors to do so. We conducted and are conducting various surveys and trials, for which we report the preliminary results in this paper. Initial findings show that i) people today rarely share their visits for lack of content to complement their storytelling, and ii) by providing visitors with a simple and easy-to-create virtual photobook with their deare\n",
      "\n",
      "2. id: 558b1265612c41e6b9d42193   score: 0.47860503   abstract: Museum professionals create exhibitions that tell stories about museum objects. The exhibits are usually arranged to reveal the relationships between them and to highlight the story being told. But sometimes objects are in fixed places and cannot be re-positioned. This paper presents a solution to the problem of how to tell conceptually coherent stories across a set of fixed artworks within the grounds of a museum and to reveal relationships between them. A study was conducted in which QR codes were used to provide access, through mobile devices, to online information about artworks. A notion of conceptual coherence and coverage of artworks was used to construct online story trails linking artworks to each other based on overlap of key story features such as setting, people and themes. Visitors were free at all times to follow their own path through the museum grounds and choose which ob\n",
      "\n",
      "3. id: 5390baa120f70186a0f38c97   score: 0.31415576   abstract: This paper discusses the ongoing development of an engaging system that will allow museum visitors to understand their emotional connections to artefacts. Through structured interviews with museum visitors and qualitative analysis, insight is gained as to how artefacts affect visitors which will provide the foundation for the design of an interactive system within the museum. The system, which will include a mobile device and contextual visualization, aims to encourage reflection and recognition of emotional responses to objects.\n",
      "\n",
      "4. id: 558bcfed0cf25dbdbb04daa1   score: 0.17895588   abstract: In this working paper we present the design and setup of a workshop environment that empowers users with tools to create and share digital narratives associated with physical artifacts. The approach focuses on two main design choices: (1) physical embodiment, by having people directly interact with the physical objects we want to augment and, (2) situated social interactions, by offering the possibility to share personal ideas and comment on others, to collaboratively produce stories about augmented objects. We also report about the main evaluation results we got in a workshop in which we studied users needs with respect to tangible interactive experiences in exhibitions of museums.\n",
      "\n",
      "5. id: 5390b86b20f70186a0f28e22   score: 0.17795369   abstract: Based on the problems of cultural heritage institutions, here historic museums, this article analysis the changes of communication between visitors and the museum when the presentation paradigm diverts from \"accessing inventory dat\" into an \"adaptive awareness and reflection\" paradigm based on the mapping between everyday discourse and related events within the cultural repository. Based on this analysis we suggest a number of challenges for socially-aware multimedia tools.\n",
      "\n",
      "6. id: 5390ad8920f70186a0ec0930   score: 0.17078441   abstract: This case study describes the collaboration between Xerox Research and The Henry Ford Museum to explore ways to enable museum staff and visitors to interact with artifacts and each other in online environments. Building on the premise that stories are an integral part of the museum experience, the team developed prototype technologies, initially for use by the museum staff, to create stories around collection elements and enable story sharing and collaboration in 3D immersive environments. Ongoing feedback and evaluations by the museum staff guided prototype revisions. Suggestions by the museum staff for use of the prototypes in museum and educational contexts are also discussed.\n",
      "\n",
      "7. id: 5390afc920f70186a0ed24e3   score: 0.1328517   abstract: We describe an interactive museum installation designed to extend visitor participation through personal reflection and contribution. The case study describes design approaches, which focused on multiple individual simultaneous use, which we describe as multi-user design. These approaches were deployed to support the visitor moving from viewer to contributor in a temporary museum exhibition. We present the anticipated use and early analysis of some of the data from actual use of the system. We outline our initial findings for the opportunities and limits in designing for personalised user-generated content through such approaches within museums and suggest areas of future work on qualities of participation and visitor contribution.\n",
      "\n",
      "8. id: 53909f8c20f70186a0e3faa7   score: 0.12885363   abstract: This paper investigates, through a case study, the interaction paradigms that can be adopted in a museum exhibition involving hybrid interactive artifacts, i.e., installations that support visitors manipulating and interacting with physical and digital exhibits [6], [1]. We discuss the design principles and solutions we adopted in a temporary exhibition titled \"The Fire and The Mountain\", where we integrated technological and physical artifacts within a multi-sensory exhibition space to foster enjoyment, engagement, and, ultimately, learning, and to promote a variety of social behaviors among visitors interacting together and with hybrid exhibits. We also discuss a field study we carried on to evaluate the user experience in \"The Fire and the Mountain\", and the lessons we learnt.\n",
      "\n",
      "9. id: 5390972920f70186a0dfbb5b   score: 0.12252322   abstract: Re-Tracing the Past: exploring objects, stories, mysteries, was an exhibition held at the Hunt Museum, in Limerick, Ireland from 9th--19th June 2003. We attempted to create an exhibition that would be an engaging experience for visitors, that would open avenues for exploration, allow for the collection of visitor opinions,and that would add to the understanding of material already in the Museum,rather than focus on \"gee-whiz\" technology. Thus our augmented environment completely hid the technology from view. A key objective was to be faithful to the ethos of the Museum, and to produce an exhibition that would stand up to scrutiny by Museum professionals. This design study paper gives a flavour of the exhibition by taking the reader on a tour of the whole design and development cycle-through site pictures, drawings, scenarios, pictures of the exhibition spaces, the interactive components,\n",
      "\n",
      "10. id: 558b0808612c41e6b9d40d62   score: 0.12085322   abstract: This paper proposes an approach for switching the level of abstraction for digital exhibition systems to provide an understanding of exhibit mechanisms. In conventional museum exhibitions, curators have tried to convey knowledge to visitors by displaying real exhibits. However, such conventional methods cannot effectively explain how a mechanism works. In contrast, digital media can be used to enhance delivery efficiency by providing interactivity to express the dynamic aspects of the exhibit. Based on this idea, we introduce the Digital Display Case system and interactive contents, whose level of abstraction can be changed at the Railway Museum (Japan). Our user study shows that switching the level of abstraction helps visitors understand the mechanisms of a rail car pendulum bogie.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1720591\n",
      "index                                        55914d790cf232eb904fbabe\n",
      "title               An integrated computer-aided cognitive task an...\n",
      "authors             Chen Zhong, John Yen, Peng Liu, Rob Erbacher, ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Symposium and Bootcamp...\n",
      "references          53908a7420f70186a0da4376;5390b00c20f70186a0ed4...\n",
      "abstract            As cyber-attacks become more sophisticated, cy...\n",
      "id                                                            1720591\n",
      "clustered_labels                                                    3\n",
      "Name: 1720591, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5592310d0cf2e74f816e39a5   score: 0.9159594   abstract: We quantitatively evaluate the role of knowledge when detecting cyber-attacks.Knowledge supports the identification of the relevant cues for classifying events.Knowledge facilitates integration of cues when detecting malicious network events.Knowledge makes a decision maker more aware of the type of cyber-attack (context).Situated knowledge is crucial to correctly integrate events and detect a cyber-attack. Ensuring cyber security is a complex task that relies on domain knowledge and requires cognitive abilities to determine possible threats from large amounts of network data. This study investigates how knowledge in network operations and information security influence the detection of intrusions in a simple network. We developed a simplified Intrusion Detection System (IDS), which allows us to examine how individuals with or without knowledge in cyber security detect malicious events a\n",
      "\n",
      "2. id: 558ad59d612c41e6b9d3b275   score: 0.8236123   abstract: The cognitive and organizational challenges associated with `Big Data' have not received much research attention. We have begun an interview study of analysts who work in the computer network (cyber) defense (CND) area and have experienced changes in data scale affecting their analytical work. Our goal is to understand any changes in analysts' mental models of their data and their domain. We used a qualitative inquiry method, starting with relatively open-ended questions. Our interview protocol also asked analysts to describe critical incidents related to data use, and probed for previously-identified cognitive biases that may affect analysis in this domain.\n",
      "\n",
      "3. id: 5390a88c20f70186a0e9a32f   score: 0.7358315   abstract: Uncertainty is an innate feature of intrusion analysis due to the limited views provided by system monitoring tools, intrusion detection systems (IDS), and various types of logs. Attackers are essentially invisible in cyber space and monitoring tools can only observe the symptoms or effects of malicious activities. When mingled with similar effects from normal or non-malicious activities they lead intrusion analysis to conclusions of varying confidence and high false positive/negative rates. This paper presents an empirical approach to the problem of uncertainty where the inferred security implications of low-level observations are captured in a simple logical language augmented with certainty tags. We have designed an automated reasoning process that enables us to combine multiple sources of system monitoring data and extract highly-confident attack traces from the numerous possible int\n",
      "\n",
      "4. id: 558bd0cc0cf25dbdbb04dbe4   score: 0.44996658   abstract: Understanding human dynamics of cyber security is a critical step for enhancing situation awareness of analysts. To this end, in this paper we focus on the requirements for building a comprehensive model of cyber analyst's decision making processes: we embrace an approach that leverages on cognitive aspects and knowledge representation to define the core elements of such model. In particular, we make the case for investigating the interplay between ontological underpinnings of cyber security and cognitive mechanisms of decision making in cyber operations. We claim that, by integrating ontologies and cognitive architectures in a hybrid-modeling framework, it's possible to rigorously characterize and simulate the core structures that govern the decisions of defenders and attackers and mediate interactions among them in the cyberspace.\n",
      "\n",
      "5. id: 5390b00c20f70186a0ed4c85   score: 0.41667795   abstract: The goal of our project is to create a set of next-generation cyber situational-awareness capabilities with applications to other domains in the long term. The objective is to improve the decision-making process to enable decision makers to choose better actions. To this end, we put extensive effort into making certain that we had feedback fromnetwork analysts and managers and understand what their genuine needs are. This article discusses the cognitive task-analysis methodology that we followed to acquire feedback from the analysts. This article also provides the details we acquired from the analysts on their processes, goals, concerns, the data and metadata that they analyze. Finally, we describe the generation of a novel task-flow diagram representing the activities of the target user base.\n",
      "\n",
      "6. id: 558bd3a40cf23f2dfc593d53   score: 0.3710227   abstract: While automated methods are the first line of defense for detecting attacks on webservers, a human agent is required to understand the attacker's intent and the attack process. The goal of this research is to understand the value of various log fields and the cognitive processes by which log information is grouped, searched, and correlated. Such knowledge will enable the development of human-focused log file investigation technologies. We performed controlled experiments with 65 subjects (IT professionals and novices) who investigated excerpts from six webserver log files. Quantitative and qualitative data were gathered to: 1) analyze subject accuracy in identifying malicious activity; 2) identify the most useful pieces of log file information; and 3) understand the techniques and strategies used by subjects to process the information. Statistically significant effects were observed in t\n",
      "\n",
      "7. id: 5390bf1320f70186a0f5052b   score: 0.36330795   abstract: As the demand for computational resources and connectivity increases and contemporary computer network systems become more complex, the management of cyber security is progressively becoming a serious issue. Cyber situation recognition is a challenging problem, particularly when the network size is large. The amount of data produced by existing intrusion detection tools and sensors usually significantly exceeds the cognition throughput of a human analyst. In attempting to align a huge amount of information and the limited human cognitive load, a critical disconnection between human cognition and cyber security tools has been identified. Although the problem of cyber intrusion detection has been studied from several perspectives using various approaches, the key component to bridging the gap between existing tools and human analysts' experiences is missing. A method to capture and leverag\n",
      "\n",
      "8. id: 5390ac5720f70186a0eb5dc0   score: 0.27639747   abstract: Analysts engaged in real-time monitoring of cybersecurity incidents must quickly and accurately respond to alerts generated by intrusion detection systems. We investigated two complementary approaches to improving analyst performance on this vigilance task: a graph-based visualization of correlated IDS output and defensible recommendations based on machine learning from historical analyst behavior. We tested our approach with 18 professional cybersecurity analysts using a prototype environment in which we compared the visualization with a conventional tabular display, and the defensible recommendations with limited or no recommendations. Quantitative results showed improved analyst accuracy with the visual display and the defensible recommendations. Additional qualitative data from a \"talk aloud\" protocol illustrated the role of displays and recommendations in analysts' decision-making p\n",
      "\n",
      "9. id: 539099b320f70186a0e1a0fd   score: 0.26246357   abstract: This paper describes elicitation by critiquing (EBC) as a cognitive task analysis (CTA) methodology. EBC takes advantage of the ability to analyze another’s task performance, a necessary skill for all domains. This technique can be used to help address some barriers to CTA methods such as domain access restrictions, frequency and predictability of observable and self-reported events, and difficulties in recruiting domain experts to participate. The technique enables controlled presentation of problem stimuli in order to obtain repeated measuring of the same task from multiple participants. To investigate this method, our team performed a CTA of inferential analysis using the EBC technique. Specifically, we observed six expert intelligence analysts critiquing a trainee analyzing the Ariane 501 launch failure. A second trainee was critiqued for reference. The method can be combined with ot\n",
      "\n",
      "10. id: 5390bb1d20f70186a0f3db3d   score: 0.22507577   abstract: In the Cyber Security domain, we have been collecting 'big data' for almost two decades. The volume and variety of our data is extremely large, but understanding and capturing the semantics of the data is even more of a challenge. Finding the needle in the proverbial haystack has been attempted from many different angles. In this talk we will have a look at what approaches have been explored, what has worked, and what has not. We will see that there is still a large amount of work to be done and data mining is going to play a central role. We'll try to motivate that in order to successfully find bad guys, we will have to embrace a solution that not only leverages clever data mining, but employs the right mix between human computer interfaces, data mining, and scalable data platforms. Traditionally, cyber security has been having its challenges with data mining. We are different. We will \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652312\n",
      "index                                        559171b80cf2e89307ca9d9d\n",
      "title               Probabilistic Termination: Soundness, Complete...\n",
      "authors                    Luis María Ferrer Fioriti, Holger Hermanns\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 42nd Annual ACM SIGPLAN-SIG...\n",
      "references          558b5400612c41e6b9d4959f;53909a9320f70186a0e22...\n",
      "abstract            We propose a framework to prove almost sure te...\n",
      "id                                                            1652312\n",
      "clustered_labels                                                    3\n",
      "Name: 1652312, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908b1820f70186a0db3960   score: 0.9020933   abstract: We present two algorithms to prove termination of programs by synthesizing linear ranking functions. The first uses an invariant generator based on iterative forward propagation with widening and extracts rankingf unctions from the generated invariants by manipulating polyhedral cones. It is capable of finding subtle ranking functions which are linear combinations of many program variables, but is limited to programs with few variables.The second, more heuristic, algorithm targets the class of structured programs with single-variable ranking functions. Its invariant generator uses a heuristic extrapolation operator to avoid iterative forward propagation over program loops. For the programs we have considered, this approach converges faster and the invariants it discovers are sufficiently strong to imply the existence of ranking functions.\n",
      "\n",
      "2. id: 5390b78a20f70186a0f22dbb   score: 0.8342719   abstract: Proving programs terminating is a fundamental computer science challenge. Recent research has produced powerful tools that can check a wide range of programs for termination. The analog for probabilistic programs, namely termination with probability one (\"almost-sure termination\"), is an equally important property for randomized algorithms and probabilistic protocols. We suggest a novel algorithm for proving almost-sure termination of probabilistic programs. Our algorithm exploits the power of state-of-the-art model checkers and termination provers for nonprobabilistic programs: it calls such tools within a refinement loop and thereby iteratively constructs a \"terminating pattern\", which is a set of terminating runs with probability one. We report on various case studies illustrating the effectiveness of our algorithm. As a further application, our algorithm can improve lower bounds on r\n",
      "\n",
      "3. id: 5390981d20f70186a0e056f9   score: 0.79198194   abstract: We summarise a verification method for probabilistic systems that is based on abstraction and refinement, and extends traditional assertional styles of verification.The approach makes extensive use of the expectation transformers of pGCL [17, 16, 13], a compact probabilistic programming language with an associated logic of real-valued functions. Analysis of large systems is made tractable by abstraction which, together with algebraic and logical reasoning, results in strong and general guarantees about probabilistic-system properties.Although our examples are specific (to pGCL), our overall goal in this note is to advocate the hierarchical development of probabilistic programs via levels of abstraction, connected by refinement, and to illustrate the proof obligations incurred by such an approach.\n",
      "\n",
      "4. id: 53908d6520f70186a0dd16fb   score: 0.78695047   abstract: Abstract: Program termination verification is a challenging research subject of significant practical importance. While there is already a rich body of literature on this subject, it is still undeniably a difficult task to design a termination checker for a realistic programming language that supports general recursion. In this paper, we present an approach to program termination verification that makes use of a form of dependent types developed in Dependent ML (DML), demonstrating a novel application of such dependent types to establishing a liveness property. We design a type system that enables the programmer to supply metrics for verifying program termination and prove that every well-typed program in this type system is terminating. We also provide realistic examples, which are all verified in a prototype implementation, to support the effectiveness of our approach to program termin\n",
      "\n",
      "5. id: 5390bda020f70186a0f4652b   score: 0.6917057   abstract: We present techniques for the analysis of infinite state probabilistic programs to synthesize probabilistic invariants and prove almost-sure termination. Our analysis is based on the notion of (super) martingales from probability theory. First, we define the concept of (super) martingales for loops in probabilistic programs. Next, we present the use of concentration of measure inequalities to bound the values of martingales with high probability. This directly allows us to infer probabilistic bounds on assertions involving the program variables. Next, we present the notion of a super martingale ranking function (SMRF) to prove almost sure termination of probabilistic programs. Finally, we extend constraint-based techniques to synthesize martingales and super-martingale ranking functions for probabilistic programs. We present some applications of our approach to reason about invariance an\n",
      "\n",
      "6. id: 554cedba0cf21c5c67b8fd7b   score: 0.61219376   abstract: The probabilistic guarded-command language pGCL Carroll Morgan, Annabelle McIver. pGCL: formal reasoning for random algorithms. South African Computer Journal (1999)] contains both demonic and probabilistic nondeterminism, which makes it suitable for reasoning about distributed random algorithms Carroll Morgan. Proof rules for probabilistic loops. In Proceedings of the BCS-FACS 7th Refinement Workshop. He Jifeng, John Cooke and Peter Wallis (eds). Springer Verlag Workshops in Computing, 1996]. Proofs are based on weakest precondition semantics, using an underlying logic of real- (rather than Boolean-) valued functions.We present a mechanization of the quantitative logic for pGCL Carroll Morgan, Annabelle McIver, and Karen Seidel, Probabilistic predicate transformers. ACM Transactions on Programming Languages and Systems, 18(3): 325--353, May 1996] using the HOL theorem prover M.J.C. Gord\n",
      "\n",
      "7. id: 5390a2e920f70186a0e683ab   score: 0.5963744   abstract: To prove that a program terminates, we can employ a ranking function argument, where program states are ranked so that every transition decreases the rank. Alternatively, we can use a set of ranking functions with the property that every cycle in the program’s flow-chart can be ranked with one of the functions. This “local” approach has gained interest recently on the grounds that local ranking functions would be simpler and easier to find. The current study is aimed at better understanding the tradeoffs involved, in a precise quantitative sense. We concentrate on a convenient setting, the Size-Change Termination framework (SCT). In SCT, programs are replaced by an abstraction whose termination is decidable. Moreover, sufficient classes of ranking functions (both global and local) are known. Our results show a tradeoff: either exponentially many local functions of certain simple forms, o\n",
      "\n",
      "8. id: 558fa7860cf2b66640467047   score: 0.5660185   abstract: Main issue is: The actual termination problem for finitely interpreted non-deterministic ALGOL-like programs without procedure selfapplication and without global variables is algorithmically solvable. This result offers a new and substantial application of a theorem of Lipton: The above mentioned programs, restricted to deterministic ones, have a sound and relatively complete Hoare logic. So we conjecture: ALGOL-like programs (even non-deterministic ones with formal sharing of variables) without procedure selfapplication and without global variables have a sound and relatively complete Hoare deduction system with axioms and inference rules which reflect the syntactical structure of programs.\n",
      "\n",
      "9. id: 53908af920f70186a0dafc00   score: 0.56517875   abstract: It is often useful to introduce probabilistic behavior in programs, either because of the use of internal random generators (probabilistic algorithms), either because of some external devices (networks, physical sensors) with known statistics of behavior. Previous works on probabilistic abstract interpretation have addressed safety properties, but somehow neglected probabilistic termination. In this paper, we propose a method to automatically prove the probabilistic termination of programs using exponential bounds on the tail of the distribution. We apply this method to an example and give some directions as to how to implement it. We also show that this method can also be applied to make unsound statistical methods on average running times sound.\n",
      "\n",
      "10. id: 5390880d20f70186a0d7af86   score: 0.55895907   abstract: We introduce a new method, combination of random testing and abstract interpretation, for the analysis of programs featuring both probabilistic and non-probabilistic nondeterminism. After introducing \"ordinary\" testing, we show how to combine testing and abstract interpretation and give formulas linking the precision of the results to the number of iterations. We then discuss complexity and optimization issues and end with some experimental results.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652381\n",
      "index                                        559172ff0cf2e89307ca9e29\n",
      "title                               Scratch: A Way to Logo and Python\n",
      "authors                                      Mark Dorling, Dave White\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 46th ACM Technical Symposiu...\n",
      "references          5390a28020f70186a0e62c8f;539098b820f70186a0e0c...\n",
      "abstract            There is concern amongst teachers about how to...\n",
      "id                                                            1652381\n",
      "clustered_labels                                                    0\n",
      "Name: 1652381, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a88c20f70186a0e99f14   score: 0.9730365   abstract: We present our experience from using Python to teach programming to high school students (aged 15). We selected Python as it is regarded to be a language suitable for teaching, while at the same time it has the advantage of being a production language, widely used around the world. Our experience shows that the success of a language in a professional setting does not predict success as a teaching tool. Based on our observations we offer some general conclusions on programming languages for teaching.\n",
      "\n",
      "2. id: 5390b4da20f70186a0effcc9   score: 0.9457011   abstract: Learning computer programming is known to be difficult for many students. In the context of a wider study, which aims to design a pedagogical strategy for introductory programming, we decided to use some less conventional activities. This strategy was applied in the last three academic years with some success. In this paper we will discuss a component that proved very relevant, the biweekly reflections we asked the students to write during the course. They were expected to reflect on the course, the different activities, their learning, the difficulties felt, and any other aspect they considered important. The analysis of the texts written gave the teacher several hints that lead to some successful individual interventions. From a research point of view this analysis gave also some important clues to the refinement of our pedagogical strategy.\n",
      "\n",
      "3. id: 5390881720f70186a0d80bee   score: 0.92944044   abstract: Preparing a new strategy for teaching introductory computer programming.\n",
      "\n",
      "4. id: 539087a620f70186a0d49f19   score: 0.9238675   abstract: We describe a first-year course sequence for computer science majors that covers most of the traditional first-year concepts, providing a balance between formal analysis and software synthesis, with examples and assignments in three high-level programming languages: Pascal, Scheme (a lexically scoped dialect of Lisp), and C. We argue that this balanced, tri-lingual approach promotes more effective pedagogy and provides students with a broader foundation than does an all-formal, all-programming, or single-language focus.\n",
      "\n",
      "5. id: 5390882420f70186a0d88058   score: 0.9003547   abstract: From the Publisher:A presentation of the computer programming language BASIC for students at all levels meeting the computer for the first time.\n",
      "\n",
      "6. id: 5390a17720f70186a0e53c96   score: 0.89947516   abstract: Educators have been working for many years to teach basic programming concepts to a younger audience. Many of these efforts involve alternatives to traditional programming courses; for example, some alternatives make use of languages with a simpler syntax or special-purpose languages and software packages tailored for younger learners.\n",
      "\n",
      "7. id: 53909a0220f70186a0e1fed0   score: 0.8718026   abstract: In this paper we describe our three course introductory sequence. The key features of this three course sequence are that CS1 and CS2 are taught using Python. We have found that after two semesters of Python students are more mature and make the transition to Java quite easily. Because we stay with Python for two semesters we can cover more data structures in depth. Because the students are already familiar with key concepts, they easily learn more advanced features of Java in CS3.\n",
      "\n",
      "8. id: 5591640d0cf2e89307ca9849   score: 0.84645087   abstract: Graphical blocks-based programming environments, such as Scratch and Snap!, are becoming increasingly popular tools for introducing learners to programming in formal educational settings. However, a growing body of research is finding that students struggle when transitioning from these tools to more conventional, text-based programming languages. To better understand students' difficulties and begin to explore potential solutions to facilitate this transition, a 10-week, quasi-experimental study was conducted with 80 students across three high-school introductory programming classes. Each class spent five weeks working with different version of a blocks-based programming tool, each of which integrated text-based programming in a different way. After working in the introductory environments, students transitioned to Java for the remainder of the study. The goal of this project is to unde\n",
      "\n",
      "9. id: 5390aca820f70186a0eb7c95   score: 0.83615357   abstract: After completing a pilot study using the Python programming language to transition to Java within our first-year introductory programming sequence, our department opted to make a more radical change. We assert that our students are better served in their first year of study by a focus on problems in computer science and their solutions, rather than programming. Our new introductory sequence emphasizes algorithm development and analysis, object-oriented design, and testing. As in our pilot, programming is first done in Python, switching to Java when object-oriented design and static typing become advantageous. Students reported liking the problem focus of the courses, while the distribution of grades remained similar to those in previous years. As a result, our department will be discontinuing our earlier introductory sequence, and offering the new problem-based one to all the groups of s\n",
      "\n",
      "10. id: 539089ab20f70186a0d950da   score: 0.82782817   abstract: This paper will consider issues that are important in the teaching and learning of programming to students in their first year of an undergraduate course in a computer science discipline. We will suggest that the current educational climate offers the opportunity to move the focus onto the learner and their experience, and that second language learning and teaching in the field of English as a Second, or Foreign, Language may be a fruitful area on which to draw. We will review a particular aspect of second language pedagogy-learner strategies-and discuss their applicability to students who are starting to learn how to program. We will consider ways in which these strategies might be useful to support learning programming at this level.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1663656\n",
      "index                                        55913ca60cf2127aa930c4ed\n",
      "title                      Delay-Bounded Routing for Shadow Registers\n",
      "authors             Eddie Hung, Joshua M. Levine, Edward Stott, Ge...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 ACM/SIGDA Internationa...\n",
      "references          5390b20120f70186a0ee4d5a;5390b04120f70186a0ed8...\n",
      "abstract            The on-chip timing behaviour of synchronous ci...\n",
      "id                                                            1663656\n",
      "clustered_labels                                                    3\n",
      "Name: 1663656, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539087d420f70186a0d5e3b9   score: 0.3915804   abstract: Abstract: Sequential place and route tools for FPGAs are inherently weak at addressing both wirability and timing optimizations. This is primarily due to the difficulty of accurately predicting wirability and delay during placement. A new performance-driven simultaneous placement/routing technique has been developed for island-style FPGA designs. On a set of industrial designs for Xilinx 4000-series FPGAs, our scheme produces 100% routed designs with 8%-15% improvement in delay when compared to the Xilinx XACT5.0 place and route system.\n",
      "\n",
      "2. id: 5390bed320f70186a0f4eb82   score: 0.19072403   abstract: In this paper, we describe the challenges that Place and Route tools face to implement the user designs on modern FPGAs while meeting the timing and power constraints.\n",
      "\n",
      "3. id: 53909fbd20f70186a0e42e83   score: 0.18787657   abstract: With constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. The FPGA community has only recently started focusing on the effects of variations. In this work we present a statistical analysis to compare the effects of variations on designs mapped to FPGAs and ASICs. We also present CAD and architecture techniques to mitigate the impact of variations. First we present a variation-aware router that optimizes statistical criticality. We then propose a modification to the clock network to deliver programmable skews to different flip-flops. Finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12% improvement in timing yield. When the desired timing yield is set to 99%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10% over a\n",
      "\n",
      "4. id: 5390a05a20f70186a0e4a2f4   score: 0.17709816   abstract: We present an efficient timing-driven placement algorithm for FPGAs. Our major contribution is a criticality history guided (CHG) approach that can simultaneously reduce the critical path delay and computation time. The proposed approach keeps track of the timing criticality history of each edge and utilizes this information to effectively guide the placer. We also present a cooling schedule that optimizes both timing and run time when combined with the CHG method. The proposed algorithm is applied to the 20 largest MCNC benchmark circuits. Experimental results show that compared with VPR, our algorithm yields an average of 21.7% reduction (maximum 45.8%) in the critical path delay and it runs 2.2X fasterthan VPR. In addition, our approach outperforms other algorithms discussed in the literature in both delay and run time.\n",
      "\n",
      "5. id: 5390980720f70186a0e02bc1   score: 0.13706978   abstract: In this paper, we propose a skew-programmable clock-routing architecture. The skews can be adjusted using programmable delay elements (PDEs) which we insert into the clock trees. We develop efficient, shortest-path-based algorithms for programming PDEs to optimize timing. Unlike previous methods for FPGA skew optimization which require large power and routing penalty, our method can achieve large timing improvement with small overhead. Typically, if timing requirements are tight, placers make efforts to satisfy them, often at a cost of compromising routability, total wire length, and power. In this work, we propose novel clock-skew-aware placement algorithms which allow us to relax the timing constraints during placement. Timing can be later optimized as a post process. Even though we demonstrate the efficiency of our approach using FPGAs, the new skew optimization method and the new pla\n",
      "\n",
      "6. id: 5390b4da20f70186a0f007af   score: 0.1161611   abstract: One of the goals of clock tree synthesis in ASIC design flow is skew minimization. There are several approaches used in traditional clock tree synthesis tools to achieve this goal. However, many of the approaches create a large number of clock-buffer levels while others result in congested clock routing. Increase in buffer level and routing congestion essentially triggers the problem of increase in buffer area and total power. Also the performance of the circuit is degraded due to on-chip variation in such situations. For certain fan-out number restricted designs, a few proposals with H-tree routed clock nets have been proposed to reduce the skew, but those proposals can hardly be used across various designs used in industry. Here we propose a method where skew minimization is mainly achieved by structured routing of clock nets. Finally, we show that with this proposal, for a few real de\n",
      "\n",
      "7. id: 539087fe20f70186a0d74a45   score: 0.112600096   abstract: In this paper we introduce a new Simulated Annealing-based timing-driven placement algorithm for FPGAs. This paper has three main contributions. First, our algorithm employs a novel method of determining source-sink connection delays during placement. Second, we introduce a new cost function that trades off between wire-use and critical path delay, resulting in significant reductions in critical path delay without significant increases in wire-use. Finally, we combine connection-based and path-based timing-analysis to obtain an algorithm that has the low time-complexity of connection-based timing-driven placement, while obtaining the quality of path-based timing-driven placement.A comparison of our new algorithm to a well known non-timing-driven placement algorithm demonstrates that our algorithm is able to increase the post-place-and-route speed (using a full path-based timing-driven ro\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3e8bf   score: 0.10539454   abstract: Embedded applications can often demand stringent latency requirements. While high degrees of parallelism within custom FPGA-based accelerators may help to some extent, it may also be necessary to limit the precision used in the data path to boost the operating frequency of the implementation. However, by reducing the precision, the engineer introduces quantization error into the design. In this paper, we demonstrate that for many applications it would be preferable to simply over clock the design and accept that timing violations may arise. Since the errors introduced by timing violations occur rarely, they will cause less noise than quantization errors. Through the use of analytical models and empirical results on a Xilinx Virtex-6 FPGA, we show that a geometric mean reduction of 67.9% to 98.8% in error expectation or a geometric mean improvement of 3.1% to 27.6% in operating frequency \n",
      "\n",
      "9. id: 5390975920f70186a0dfddc2   score: 0.10502681   abstract: Field-Programmable Gate Arrays (FPGAs) have been one of the most popular devices for system prototyping, logic emulation, and reconfigurable computing. Their user-programmable prefabricated logic modules and routing structures provide low manufacturing cost and fast time-to-market implementation solutions to the users. However, the routing delay due to their inherent routing structure has been one of the biggest bottlenecks of their speed performance. As the VLSI fabrication feature size is shrunk to deep submicron dimension in modern technology, the portion taken up by routing in both of area and timing grows even more significantly. In this dissertation, we address issues on routing algorithms to optimize area and timing of an FPGA system. We present a new timing-driven routing algorithm for FPGAs. The algorithm finds a routing solution with minimum critical path delay for a given plac\n",
      "\n",
      "10. id: 5390bfa220f70186a0f52d50   score: 0.10194652   abstract: Better-than-worst-case timing designs such as Razor introduce shadow flip-flops triggered by a delayed clock in parallel to the functional flip-flops for timing error detection through duplication and comparision. This arrangement suffers from the \"short path\" problem, whereby the activation of paths shorter than this timing skew can cause false errors to be flagged. The traditional solution is to add buffers to the short paths that are less than the clock skew between the duplicated error detection flip-flops. However, this approach adds considerable area and power overhead, particularly in the presence of significant process variations. This paper studies the use of latches to introduce extra delay on short paths. Holding short paths stable for the first phase of the clock allows the design to achieve a skew of half a clock period between the functional and shadow flip-flops without sh\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1703689\n",
      "index                                        55323b1e45cec66b6f9d95d2\n",
      "title               Greedy algorithms for actor redeployment in wi...\n",
      "authors             Bing-Hong Liu, Yao-Jen Tang, Chen-Wei Yu, Ming...\n",
      "year                                                           2015.0\n",
      "venue                                               Wireless Networks\n",
      "references          5390a5b020f70186a0e7d627;5390a1bc20f70186a0e54...\n",
      "abstract            In a wireless sensor---actor network, an actor...\n",
      "id                                                            1703689\n",
      "clustered_labels                                                    3\n",
      "Name: 1703689, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4ac58   score: 0.6271523   abstract: Research of the wireless sensor and actor networks has confronted with many problems, such as the coordination of sensors and actors, the model for network colony, and so on. This paper discusses the coordination frame of wireless sensor and actor networks. In order to solve the coordination problem of sensors and actors, we propose an algorithm based on an event driven clustering paradigm, which can find out data aggregation trees from all the sensors that discover the events to the appropriate actors. In each cluster, the decision-making algorithm based on contract nets is also proposed. Finally, the simulation experiments are carried out for the proposed algorithm.\n",
      "\n",
      "2. id: 5390a1bc20f70186a0e543f7   score: 0.5989575   abstract: Wireless sensor and actor networks (WSANs) have recently emerged with the idea of combining wireless sensor networks (WSNs) and mobile ad hoc networks (MANETs). In addition to resource constrained sensors, resource rich and mobile actor nodes are employed in WSANs. These actors can collect data from the sensors and perform appropriate actions as a result of processing such data. To perform the actions at all parts of the region in a timely manner, the actors should be deployed in such a way that they might be able to communicate with each other and cover the whole monitored area. This requires that the actors should be placed carefully prior to network operation in order to maximize the coverage and maintain the inter-actor connectivity. In this paper, we propose a distributed actor deployment algorithm that strives to maximize the coverage of actors without violating the connectivity re\n",
      "\n",
      "3. id: 553fbfae0cf2363314908b08   score: 0.51337874   abstract: Since sensor networks integrate virtual and physical worlds, spatial deployment of sensor nodes may have a significant impact on operation costs and performance. This paper investigates the problem of redeploying mobile sensor networks, in which the objective is to design an efficient algorithm which optimizes the global performance of networks. We take a novel approach in analyzing the problem from a macroscopic perspective, modeling the sensor deployment as a density distribution and the redeployment algorithm as an integral transform of that distribution. Despite lacking the details for individual node, we may still derive insightful results, such as number of moves, stability, and transitional behavior. We also derive the bound of total moving distance difference between an arbitrary one-move algorithm and the optimum redeployment algorithm, allowing for the estimation of the minimum\n",
      "\n",
      "4. id: 5390ada620f70186a0ec3ced   score: 0.4497249   abstract: Traditional wireless sensor networks (WSNs) act mostly as data collection and aggregation networks and do not possess the capability of interaction with ambient environment. But lots of application scenarios need WSNs interacting with ambient environment, such as fire prevention and control, etc.. According to most application scenarios, the response speed of actor nodes determines the effectiveness of WSANs applications. Thus, we utilize the moving capacity of actor nodes to relocate actor nodes during the network initialization, which can improve the real-time attribute of WSANs. The actors deployment problem in WSANs whether the amount of actors is redundant or not has been proved NP-Hard, but to the best of our knowledge, no effective distributed algorithms in previous research can solve the problem. Thus two actor deployment strategies are proposed to solve this problem approximatel\n",
      "\n",
      "5. id: 5390be6620f70186a0f4cb5d   score: 0.42847255   abstract: Wireless sensor and actor networks (WSANs) are more promising and most addressing research field in the area of wireless sensor networks in recent scenario. It composed of possibly a large number of tiny, autonomous sensor devices and resources rich actor nodes equipped with wireless communication and computation capabilities. Actors collect sensors' information and respond collaboratively to achieve an application specific mission. Since actors have to coordinate their operation, a strongly connected inter-actor network would be required at all the time in the network. Actor nodes may fail for many reasons (i.e. due of battery exhaustion or hardware failure due to hash environment etc.) and failures may convert connected network into disjoint networks. This can hinder sometimes not only the performance of network but also degrade the usefulness and effectiveness of the network. Thus, ha\n",
      "\n",
      "6. id: 5390a5b020f70186a0e7cc6b   score: 0.42119467   abstract: A sensor network consists of a collection of (possibly mobile) sensing devices that can coordinate their actions through wireless communication and aim at performing various tasks (e.g., surveillance, environmental monitoring) over a region sometimes referred to as the “mission space”. The performance of a sensor network is sensitive to the location of its nodes in the mission space. This leads to the basic “coverage control” problem of properly, and possibly optimally, deploying sensors in order to meet the overall system's objectives [1],[2],[3],[4],[5]. Clearly, to achieve such a goal, the nodes must share, at least partially, their state information. However, this may require a large amount of information exchange. Moreover, sensor nodes are frequently small, inexpensive devices with limited resources. Aside from energy required to move (if nodes are mobile), communication is known t\n",
      "\n",
      "7. id: 5390b29820f70186a0ee9ea8   score: 0.40580192   abstract: Wireless sensor and actor networks (WSANs) are composed of a large number of sensors and a small number of (mobile) resource-rich actors. Sensors gather information about the physical phenomenon, while actors take decisions and then perform appropriate actions upon the environment. Real time and network lifetime are important factors of WSANs. So in this paper, a single-actor selection problem for WSANs is addressed from real time and nodes' Relay Bound constraints first, and then a multi-objective programming is provided. After that, two approximate algorithms, Global Relay-Bounded and MIN-MAX Hops (GRBMMH) and Distributed Relay-Bounded and MIN-MAX Hops (DRBMMH), are put forward. In the performance evaluation, those algorithms are compared with MECT (Minimum Energy Cost Tree) and MPLCT (Minimum Path Length Cost Tree) algorithms.\n",
      "\n",
      "8. id: 5390a30b20f70186a0e6ae3d   score: 0.3994602   abstract: In this paper, we consider the connected target coverage (CTC) problem with the objective of maximizing the network lifetime by scheduling sensors into multiple sets, each of which can maintain both target coverage and connectivity among all the active sensors and the sink.We model the CTC problem as a maximum cover tree (MCT) problem and prove that the MCT problem is NP-Complete. We determine an upper bound on the network lifetime for the MCT problem and then develop a (1+w) H (M) approximation algorithm to solve it, where is an arbitrarily small number, H(M) = Σ1≤i≤M (1/i) and M is the maximum number of targets in the sensing area of any sensor. As the protocol cost of the approximation algorithm may be high in practice, we develop a faster heuristic algorithm based on the approximation algorithm called Communication Weighted Greedy Cover (CWGC) algorithm and present a distributed impl\n",
      "\n",
      "9. id: 5390b13020f70186a0edc93e   score: 0.39641863   abstract: We consider a wireless sensor network's lifetime maximization problem as an integer linear programming problem when the redundant set of covers is given, and it is necessary to choose a lifetime of each cover subject to the limited sensor's resources. Irrespective of the function of any sensor in a cover, we suppose that the sensor may be active during some time, and the number of time rounds when it is active we call the sensor's resource. We proved that the considered problem is strong NP-hard; proposed the ways to reduce the problem; shown that the problem is not approximable within the 0.997; found special cases when it is in the class APX; proposed several heuristics and performed a simulation.\n",
      "\n",
      "10. id: 5390995d20f70186a0e1501b   score: 0.39338502   abstract: In a wireless sensor and actor network (WSAN), a group of sensors and actors are geographically distributed and linked by wireless networks. Sensors gather information sensed for an event in the physical world and send them to actors. Actors perform appropriate actions on actuation devices by making a decision on receipt of sensed information from sensors. Sensors are low cost, low powered devices with limited energy, computation, and wireless communication capabilities. Sensors may not only stop by fault but also suffer from arbitrary faults. Furthermore, wireless communication is less reliable due to noise and shortage of power of sensors. Reliable real time communication among sensors, actors, and actuation devices, is required in WSAN applications. We newly propose a multi-actor/multi-sensor (MAMS) model. In addition, multiple actors may perform actions on receipt of sensed informati\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652281\n",
      "index                                        5592573e0cf28b1a968ffccf\n",
      "title               A Module-based Approach to Adopting the 2013 A...\n",
      "authors             Martin Burtscher, Wuxu Peng, Apan Qasem, Hongc...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 46th ACM Technical Symposiu...\n",
      "references          5390a06e20f70186a0e4dfd2;5390879d20f70186a0d43...\n",
      "abstract            The widespread deployment of multicore systems...\n",
      "id                                                            1652281\n",
      "clustered_labels                                                    0\n",
      "Name: 1652281, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558bd0940cf25dbdbb04db8c   score: 0.97622   abstract: Parallel computing is a new knowledge area in the 2013 ACM-IEEE curricular recommendations. This paper describes our experiences adding a brief module on parallel Haskell to the undergraduate programming languages courses at two colleges. We offer suggestions for others wishing to introduce parallelism in such courses.\n",
      "\n",
      "2. id: 5390ba3820f70186a0f36e34   score: 0.92905515   abstract: For several years, an ACM/IEEE Task Force has been hard at work developing and refining CS 2013, the new curricular recommendations for undergraduate computer science. As part of this effort, the Task Force has decided to identify existing courses as sample exemplars of how various topics (general Knowledge Areas and detailed Knowledge Units) might fit together in courses. This approach provides a different perspective from the past, in which curricular recommendations included hypothetical courses rather than specific ones. The first part of this talk will discuss this general approach of course exemplars within CS 2013.\n",
      "\n",
      "3. id: 5390ba0a20f70186a0f34276   score: 0.9175984   abstract: The computing landscape has shifted towards multicore architectures. To learn about software development, it is increasingly important for students to gain hands-on parallel programming experience in multicore environments. This experience will be significantly different from programming for uniprocessors, because it involves a profound understanding of how to write software that is (1) free of concurrency bugs and (2) able to effectively utilize the underlying parallel hardware architecture. We present our work at Yonsei University and The University of Sydney to teach parallel programming to first and second-year undergraduate students. Our objective is to introduce parallelism early on in the curriculum, to instill it as a first principle of computation. We introduce a series of five parallel programming course modules suitable for a one semester introductory programming course. Each \n",
      "\n",
      "4. id: 5390b0ca20f70186a0ed9a7a   score: 0.81257004   abstract: The multi-core revolution has brought forth the pressing issue of deciding where in the curriculum to introduce parallel programming. At least 3 different strategies can be found, including the current one used, where concurrent programming is taught in Operating Systems. It is the position of this paper that we can embrace the 3 strategies and present parallelism as a fundamental notion in computing throughout the curriculum. We analyse curriculum directions, present parameters to consider to introduce parallelism, as well as a spiral approach to the subject starting in CS2.\n",
      "\n",
      "5. id: 5390a8b220f70186a0e9cc78   score: 0.80654806   abstract: Recent hardware improvements are derived by adding additional processing cores to the microprocessor package. For software developers, this change raises many challenges. No longer will programmers be able to \"ride the wave\" of increasing performance without explicitly taking advantage of -parallelism. For computer science educators, this change will require radical shifts in the way computer science is taught. Parallelism will need to be introduced early in the curriculum, preferably in many different classes in the sequence. This workshop will summarize the OpenMP approach to multi-threading, and illustrate how it can be used to introduce parallelism into the undergraduate curriculum to novice and intermediate C/C++ programmers.\n",
      "\n",
      "6. id: 5390bded20f70186a0f48aa7   score: 0.73659015   abstract: The ACM-IEEE CS2013 curricular recommendations include a dramatic growth in parallel and distributed computing (PDC) topics, in response to the necessary industry shift toward multicore computing, and to emerging technologies such as cloud computing. How can your institution integrate those recommendations into your undergraduate CS curriculum? In this special session, leaders in PDC education will succinctly present their curricular strategies in relation to CS2013 recommendations, while attendees carry out a workshop-style activity to identify opportunities and assemble resources for blending more PDC content into their own local CS curricula.\n",
      "\n",
      "7. id: 5390bd1520f70186a0f42d8e   score: 0.6713628   abstract: This paper summarizes our experiences and findings in teaching the concepts of parallel computing in two undergraduate programming courses and an undergraduate hardware design course. The first is a junior-senior level elective course Object-Oriented Programming using C++ and Java. The second is a sophomore-level required course on Advanced C Programming. The third course, Introduction to Digital System Design, is also a sophomore-level required course. We will describe how parallel concepts have been integrated in the courses, the assessments, and the results.\n",
      "\n",
      "8. id: 558b0d1a612c41e6b9d418d1   score: 0.5553749   abstract: Two recent curriculum studies, the ACM/IEEE Curricula 2013 Report and the NSF/IEEE-TCPP Curriculum Initiative on Parallel and Distributed Computing, argue that every undergraduate computer science program should include topics in parallel and distributed computing (PDC). Although not within the scope of these reports, there is also a need for students in computing related general education courses to be aware of the role that parallel and distributed computing technologies play in the computing landscape. One approach to integrating these topics into existing curricula is to spread them across several courses. However, this approach requires development of multiple instructional modules targeted to introduce PDC concepts at specific points in the curriculum. Such modules need to mesh with the goals of the courses for which they are designed in such a way that minimal material has to be r\n",
      "\n",
      "9. id: 5390b3da20f70186a0ef72e5   score: 0.5517549   abstract: Over three decades of parallel computing, new computational requirements and systems have steadily evolved, yet parallel software remains notably more difficult relative to its sequential counterpart, especially for fine-grained parallel applications. We discuss the role of education to address challenges posed by applications such as informatics, scientific modeling, enterprise processing, and numerical computation. We outline new curricula both in computational science and in computer science. There appear to be new directions in which graduate education in parallel computing could be directed toward fulfilling needs in science and industry.\n",
      "\n",
      "10. id: 5390b7fe20f70186a0f26a46   score: 0.52859426   abstract: If our students are to make full use of multi-core processors, parallel programming must be taught to all students in required courses rather than only in the specialized electives where it has historically resided. We believe that widespread parallel programming will rely on high-level parallel languages and tools. This workshop presents one such language, Habanero Java (HJ), an extension of Java that adds keywords to create and manage tasks. HJ is powerful enough to be used by researchers, but also particularly well-suited for students because it builds on their knowledge of Java and its conciseness means that parallel algorithms can be illustrated with little code. Workshop attendees with learn about HJ, write HJ programs, and hear about how we have used it with students, including a brief unit in CS 2 that introduced parallelism.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1695002\n",
      "index                                        559249730cf28b1a968ff6e9\n",
      "title               Spatio-temporal features for the automatic con...\n",
      "authors                                 Belhassen Akrout, Walid Mahdi\n",
      "year                                                           2015.0\n",
      "venue                                 Machine Vision and Applications\n",
      "references          539098b820f70186a0e0a90f;5390881820f70186a0d81...\n",
      "abstract            Driver fatigue is one of the leading causes of...\n",
      "id                                                            1695002\n",
      "clustered_labels                                                    2\n",
      "Name: 1695002, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a96f20f70186a0ea431e   score: 0.7879312   abstract: International statistics shows that a large number of road accidents are caused by driver fatigue. Therefore, a system that can detect oncoming driver fatigue and issue timely warning could help prevent many accidents, and consequently save money and reduce personal suffering. The authors have made an attempt to design a system that uses security camera that points directly towards the driver's face and monitors the driver's eyes in order to detect fatigue. If the fatigue is detected a warning signal is issued to alert the driver. The authors have used the skin color based algorithm to detect the face of the driver. Once the face area is found, the eyes are found by computing the horizontal averages in the area. Once the eyes are located, measuring the distances between the intensity changes in the eye area determine whether the eyes are open or closed. A large distance corresponds to ey\n",
      "\n",
      "2. id: 5390b04120f70186a0ed7081   score: 0.7252596   abstract: Lack of concentration in a driver due to fatigue is a major cause of road accidents. This paper investigates approaches that can be used to develop a video-based system to automatically detect driver fatigue and warn the driver, in order to prevent accidents. Ocular cues such as percentage eye closure (PERCLOS) are considered strong fatigue indicators; thus, accurately locating and tracking the driver’s eyes is vital. Tests were carried out based on two approaches to track the eyes and estimate PERCLOS: (1) classification approach and (2) optical flow approach. In the first approach, the eyes are tracked by finding local regions, the state (open or closed) of the eyes in each image frame is estimated using a classifier, and thereby the PERCLOS is calculated. In the second approach, the movement of the upper eyelid is tracked using a newly proposed simple eye model, which captures image v\n",
      "\n",
      "3. id: 5390aefc20f70186a0ece53d   score: 0.7122322   abstract: The aim of this paper is to describe a system whose final goal is to detect if a driver is drowsy, in order to prevent potentially danger situations. The system is based on the processing of the driver’s face image, acquired by a web cam installed on the dashboard of the car. After a brief introduction explaining the connection of the present work to the European project REFLECT, the relashonship between drowsiness condition and fatal car crashes is dicussed. Then, an overview of the most used techiques for face and eye detection is given, and the developed algorithm is described in detail. Finally, preliminary results of in-laboratory and in-car tests are presented and commented.\n",
      "\n",
      "4. id: 5390bed320f70186a0f4f8d2   score: 0.6887825   abstract: Many studies show that driver drowsiness is one of the main reasons for road accidents. To prevent such car crashes, systems are needed to monitor and characterize the driver based on the driving information. In order to have highly reliable assistant systems, reference drowsiness measurements are required. Among different physiological measures, previous studies have introduced driver eye movements, particularly blinking, as a measure with high correlation to drowsiness. Hence, in this study, eye movements of 14 drivers have been observed using electrooculography (EOG) at the moving-base driving simulator of Mercedes Benz to assess driver drowsiness. Based on the measured signals, an adaptive detection approach is introduced to simultaneously detect not only eye blinks, but also other driving-relevant eye movements such as saccades and micro sleep events. Moreover, in spite of the fact \n",
      "\n",
      "5. id: 5390a6d920f70186a0e86d22   score: 0.6861598   abstract: In order to prevent fatigue driving, through many studies of the driver's eyes, a Matlab-based fatigue driving detection system is designed and simulated based on image processing technology. The system samples sequences of images with an camera and its built-in light source, and then locates the eye-point in the image and tracks it, calculates the area of the eyes, finally makes judgment on whether the driver is driving with fatigue or not and warns him according to the judgment. The simulation test result shows that the recognition ratio of shallow fatigued drivers is 81.5%, and the recognition ratio of moderate fatigued drivers and deep fatigued drivers is 100%.\n",
      "\n",
      "6. id: 5390bfa120f70186a0f52b7c   score: 0.64846635   abstract: In recent years, driver drowsiness and distraction have been important factors in a large number of accidents because they reduce driver perception level and decision making capability, which negatively affect the ability to control the vehicle. One way to reduce these kinds of accidents would be through monitoring driver and driving behavior and alerting the driver when they are drowsy or in a distracted state. In addition, if it were possible to predict unsafe driving behavior in advance, this would also contribute to safe driving. In this paper, we will discuss various monitoring methods for driver and driving behavior as well as for predicting unsafe driving behaviors. In respect to measurement methods of driver drowsiness, we discussed visual and non-visual features of driver behavior, as well as driving performance behaviors related to vehicle-based features. Visual feature measure\n",
      "\n",
      "7. id: 5390a1bc20f70186a0e55f4a   score: 0.6434414   abstract: A video-based driver fatigue detection system is presented. The system automatically locates the face in the first frame, and then tracks the eyes in subsequent frames. Four cues which characterises fatigue are used to determine the fatigue level. We used Support Vector Machines to estimate the percentage eye closure, which is the strongest cue. Improved results were achieved by using Support Vector Machines in comparison to Naive Bayes classifier. The performance was further improved by fusing all four cues using fuzzy rules.\n",
      "\n",
      "8. id: 53909a0220f70186a0e1e771   score: 0.63770795   abstract: Driver fatigue problem is one of the important factors that cause traffic accidents. Therefore the vision-based driver fatigue detection is the most prospective commercial applications of HCI. However, it is a challenging issue due to a variety of factors such as head and eyes moving fast, external illuminations interference and realistic lighting conditions, etc. This tends to significantly limit its scope of application. In this paper, we present an intelligent vehicle control based on driver fatigue detection. Firstly, the face is located using Haar algorithm and eye location is found with projection technique. After finding eye templates, we propose a new real time eye tracking method based on Unscented Kalman Filter. Thirdly, driver fatigue can be detected whether the eyes are closed over 5 consecutive frames using vertical projection matching. Finally, if driver fatigue is confirme\n",
      "\n",
      "9. id: 558b0163612c41e6b9d40280   score: 0.63329685   abstract: Drowsiness is a transition state between being awake and asleep and can have serious consequences when occurring in tasks that require sustained attention such as driving. During the state of drowsiness, reaction time is slower, vigilance is reduced, and information processing is less efficient, which may cause accidents. The proposed Driver Fatigue Detection System (called FDS) aims to monitor the alertness of drivers to prevent them from falling asleep at the wheel. The system monitors the driver's face using Haar feature classifiers with an increased training set to detect changes in the face of the driver quickly. A correlation matching algorithm is used to accurately provide the target's position and track the target's eyes according to the intensity, shape, and size of the pupils. FDS uses an IR illuminator to produce the desired bright pupil effect when the driver is wearing sungl\n",
      "\n",
      "10. id: 5390a1f720f70186a0e5b988   score: 0.60981464   abstract: The International statistics shows that a large number of road accidents are caused by driver fatigue. Therefore, a system that can detect oncoming driver fatigue and issue timely warning could help in preventing many accidents, and consequently save money and reduce personal suffering. The authors have made an attempt to design a system that uses video camera that points directly towards the driver’s face in order to detect fatigue. If the fatigue is detected a warning signal is issued to alert the driver. The authors have worked on the video files recorded by the camera. Video file is converted into frames. Once the eyes are located from each frame, by measuring the distances between the intensity changes in the eye area one can determine whether the eyes are open or closed. If the eyes are found closed for 5 consecutive frames, the system draws the conclusion that the driver is fallin\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1700363\n",
      "index                                        55922f9b0cf2c3a0875c9f2a\n",
      "title               A new approach to query segmentation for relev...\n",
      "authors                  Haocheng Wu, Yunhua Hu, Hang Li, Enhong Chen\n",
      "year                                                           2015.0\n",
      "venue                                           Information Retrieval\n",
      "references          5390a54720f70186a0e78980;5390a88c20f70186a0e99...\n",
      "abstract            In this paper, we try to determine how best to...\n",
      "id                                                            1700363\n",
      "clustered_labels                                                    0\n",
      "Name: 1700363, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b86b20f70186a0f2a64d   score: 0.967225   abstract: Query segmentation is the problem of identifying those keywords in a query, which together form compound concepts or phrases like \"new york times\". Such segments can help a search engine to better interpret a user's intents and to tailor the search results more appropriately. Our contributions to this problem are threefold. (1) We conduct the first large-scale study of human segmentation behavior based on more than 500000 segmentations. (2) We show that the traditionally applied segmentation accuracy measures are not appropriate for such large-scale corpora and introduce new, more robust measures. (3) We develop a new query segmentation approach with the basic idea that, in cases of doubt, it is often better to (partially) leave queries without any segmentation. This new in-doubt-without approach chooses different segmentation strategies depending on query types. A large-scale evaluation\n",
      "\n",
      "2. id: 5390b04120f70186a0ed869d   score: 0.9513234   abstract: Query segmentation is an important task toward understanding queries accurately, which is essential for improving search results. Existing segmentation models either use labeled data to predict the segmentation boundaries, for which the training data is expensive to collect, or employ unsupervised strategy based on a large text corpus, which might be inaccurate because of the lack of relevant information. In this paper, we propose a probabilistic model to exploit clickthrough data for query segmentation, where the model parameters are estimated via an efficient EM algorithm. We further study how to properly interpret the segmentation results and utilize them to improve retrieval accuracy. Specifically, we propose an integrated language model based on the standard bigram language model to exploit the probabilistic structure obtained through query segmentation. Experiment results on two da\n",
      "\n",
      "3. id: 53909f2d20f70186a0e38be8   score: 0.91920847   abstract: We introduce a new relevance scoring technique that enhances existing relevance scoring schemes with term position information. This technique uses chronological term rank (CTR) which captures the positions of terms as they occur in the sequence of words in a document. CTR is both conceptually and computationally simple when compared to other approaches that use document structure information, such as term proximity, term order and document features. CTR works well when paired with Okapi BM25. We evaluate the performance of various combinations of CTR with Okapi BM25 in order to identify the most effective formula. We then compare the performance of the selected approach against the performance of existing methods such as Okapi BM25, pivoted length normalization and language models. Significant improvements are seen consistently across a variety of TREC data and topic sets, measured by t\n",
      "\n",
      "4. id: 5390b0ca20f70186a0edb4f9   score: 0.8934954   abstract: Traditional ranking models for information retrieval lack the ability to make a clear distinction between relevant and nonrelevant documents at top ranks if both have similar bag-of-words representations with regard to a user query. We aim to go beyond the bag-of-words approach to document ranking in a new perspective, by representing each document as a sequence of sentences. We begin with an assumption that relevant documents are distinguishable from nonrelevant ones by sequential patterns of relevance degrees of sentences to a query. We introduce the notion of relevance flow, which refers to a stream of sentence-query relevance within a document. We then present a framework to learn a function for ranking documents effectively based on various features extracted from their relevance flows and leverage the output to enhance existing retrieval models. We validate the effectiveness of our\n",
      "\n",
      "5. id: 5390aa0f20f70186a0ea8a9f   score: 0.89312315   abstract: This paper extends the state-of-the-art probabilistic model BM25 to utilize term proximity from a new perspective. Most previous work only consider dependencies between pairs of terms, and regard phrases as additional independent evidence. It is difficult to estimate the importance of a phrase and its extra contribution to a relevance score, as the phrase actually overlaps with the component terms. This paper proposes a new approach. First, query terms are grouped locally into non-overlapping phrases that may contain one or more query terms. Second, these phrases are not scored independently but are instead treated as providing a context for the component query terms. The relevance contribution of a term occurrence is measured by how many query terms occur in the context phrase and how compact they are. Third, we replace term frequency by the accumulated relevance contribution. Consequen\n",
      "\n",
      "6. id: 5390985e20f70186a0e08e65   score: 0.89274967   abstract: We propose a method for document ranking that combines a simple document-centric view of text, and fast evaluation strategies that have been developed in connection with the vector space model. The new method defines the importance of a term within a document qualitatively rather than quantitatively, and in doing so reduces the need for tuning parameters. In addition, the method supports very fast query processing, with most of the computation carried out on small integers, and dynamic pruning an effective option. Experiments on a wide range of TREC data show that the new method provides retrieval effectiveness as good as or better than the Okapi BM25 formulation, and variants of language models.\n",
      "\n",
      "7. id: 5390a06e20f70186a0e4c975   score: 0.8872046   abstract: We demonstrate effective new methods of document ranking based on lexical cohesive relationships between query terms. The proposed methods rely solely on the lexical relationships between original query terms, and do not involve query expansion or relevance feedback. Two types of lexical cohesive relationship information between query terms are used in document ranking: short-distance collocation relationship between query terms, and long-distance relationship, determined by the collocation of query terms with other words. The methods are evaluated on TREC corpora, and show improvements over baseline systems.\n",
      "\n",
      "8. id: 5390b36120f70186a0ef1274   score: 0.87914675   abstract: In web search, relevance ranking of popular pages is relatively easy, because of the inclusion of strong signals such as anchor text and search log data. In contrast, with less popular pages, relevance ranking becomes very challenging due to a lack of information. In this paper the former is referred to as head pages, and the latter tail pages. We address the challenge by learning a model that can extract search-focused key n-grams from web pages, and using the key n-grams for searches of the pages, particularly, the tail pages. To the best of our knowledge, this problem has not been previously studied. Our approach has four characteristics. First, key n-grams are search-focused in the sense that they are defined as those which can compose \"good queries\" for searching the page. Second, key n-grams are learned in a relative sense using learning to rank techniques. Third, key n-grams are l\n",
      "\n",
      "9. id: 5390a88c20f70186a0e993b3   score: 0.8609547   abstract: Modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. Most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. In this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. We demonstrate that the weighted dependence model can be trained using existing learning-to-rank techniques, even with a relatively small number of training queries. Our study compares the effectiveness of both endogenous (collection-based) and exogenous (based on external sources) features for determining concept importance. To test the weighted dependence model, we pe\n",
      "\n",
      "10. id: 5390a88c20f70186a0e993be   score: 0.8600169   abstract: Queries describe the users' search intent and therefore they play an essential role in the context of ranking for information retrieval and Web search. However, most of existing approaches for ranking do not explicitly take into consideration the fact that queries vary significantly along several dimensions and entail different treatments regarding the ranking models. In this paper, we propose to incorporate query difference into ranking by introducing query-dependent loss functions. In the context of Web search, query difference is usually represented as different query categories; and, queries are usually classified according to search intent such as navigational, informational and transactional queries. Based on the observation that such kind of query categorization has high correlation with the user's different expectation on the result accuracy on different rank positions, we develo\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675442\n",
      "index                                        559123500cf232eb904faf00\n",
      "title                 The Effects of Sequence and Delay on Crowd Work\n",
      "authors             Walter S. Lasecki, Jeffrey M. Rzeszotarski, Ad...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          539096cb20f70186a0df7118;5390aca920f70186a0eb9...\n",
      "abstract            A common approach in crowdsourcing is to break...\n",
      "id                                                            1675442\n",
      "clustered_labels                                                    0\n",
      "Name: 1675442, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bf1320f70186a0f51563   score: 0.87831426   abstract: Large tasks can be overwhelming. For example, many people have thousands of digital photographs that languish in unorganized archives because it is difficult and time consuming to gather them into meaningful collections. Such tasks are hard to start because they seem to require long uninterrupted periods of effort to make meaningful progress. We propose the idea of selfsourcing as a way to help people to perform large personal information tasks by breaking them into manageable microtasks. Using ideas from crowdsourcing and task management, selfsourcing can help people take advantage of existing gaps in time and recover quickly from interruptions. We present several achievable selfsourcing scenarios and explore how they can facilitate information work in interruption-driven environments.\n",
      "\n",
      "2. id: 5390b7fe20f70186a0f26d0c   score: 0.8435088   abstract: We measure crowdsourcing performance based on a standard IQ questionnaire, and examine Amazon's Mechanical Turk (AMT) performance under different conditions. These include variations of the payment amount offered, the way incorrect responses affect workers' reputations, threshold reputation scores of participating AMT workers, and the number of workers per task. We show that crowds composed of workers of high reputation achieve higher performance than low reputation crowds, and the effect of the amount of payment is non-monotone---both paying too much and too little affects performance. Furthermore, higher performance is achieved when the task is designed such that incorrect responses can decrease workers' reputation scores. Using majority vote to aggregate multiple responses to the same task can significantly improve performance, which can be further boosted by dynamically allocating wo\n",
      "\n",
      "3. id: 559137710cf232eb904fb4c0   score: 0.7816641   abstract: A large, seemingly overwhelming task can sometimes be transformed into a set of smaller, more manageable microtasks that can each be accomplished independently. For example, it may be hard to subjectively rank a large set of photographs, but easy to sort them in spare moments by making many pairwise comparisons. In crowdsourcing systems, microtasking enables unskilled workers with limited commitment to work together to complete tasks they would not be able to do individually. We explore the costs and benefits of decomposing macrotasks into microtasks for three task categories: arithmetic, sorting, and transcription. We find that breaking these tasks into microtasks results in longer overall task completion times, but higher quality outcomes and a better experience that may be more resilient to interruptions. These results suggest that microtasks can help people complete high quality work\n",
      "\n",
      "4. id: 5390be6620f70186a0f4c56e   score: 0.7690802   abstract: Recent research in human computation has focused on improving the quality of work done by crowd workers on crowdsourcing platforms. Multiple approaches have been adopted like filtering crowd workers through qualification tasks, and aggregating responses from multiple crowd workers to obtain consensus. We investigate here how improving the presentation of the task itself by using cognitively inspired features affects the performance of crowd workers. We illustrate this with a case-study for the task of extracting text from scanned images. We generated six task-presentation designs by modifying two parameters - visual saliency of the target fields and working memory requirements - and conducted experiments on Amazon Mechanical Turk (AMT) and with an eye-tracker in the lab setting. Our results identify which task-design parameters (e.g. highlighting target fields) result in improved perform\n",
      "\n",
      "5. id: 5390b19020f70186a0ee05c8   score: 0.7411123   abstract: Crowdsourcing has been shown to be an effective approach for solving difficult problems, but current crowdsourcing systems suffer two main limitations: (i) tasks must be repackaged for proper display to crowd workers, which generally requires substantial one-off programming effort and support infrastructure, and (ii) crowd workers generally lack a tight feedback loop with their task. In this paper, we introduce Legion, a system that allows end users to easily capture existing GUIs and outsource them for collaborative, real-time control by the crowd. We present mediation strategies for integrating the input of multiple crowd workers in real-time, evaluate these mediation strategies across several applications, and further validate Legion by exploring the space of novel applications that it enables.\n",
      "\n",
      "6. id: 558f8f920cf2cb5aa767449d   score: 0.7153245   abstract: Crowdsourcing has become a popular and indispensable component of many problem-solving pipelines in the research literature, with crowd workers often treated as computational resources that can reliably solve problems that computers have trouble with, such as image labeling/classification, natural language processing, or document writing. Yet, obviously crowd workers are human, and long sequences of the same monotonous tasks might intuitively reduce the amount of good quality work done by the workers. Here we propose an investigation into how we can use diversions containing small amounts of entertainment to improve crowd workers' experiences. We call these small period of entertainment ``micro-diversions\\\", which we hypothesize to provide timely relief to workers during long sequences of micro-tasks. We hope to improve productivity by retaining workers to work on our tasks longer and to\n",
      "\n",
      "7. id: 558aedfd612c41e6b9d3d9d3   score: 0.6940954   abstract: With the rapid growth of mobile smartphone users, several commercial mobile companies have exploited crowd sourcing as an effective approach to collect and analyze data, to improve their services. In a crowd sourcing system, \\\"human workers\\\" are enlisted to perform small tasks, that are difficult to be automated, in return for some monetary compensation. This paper presents our crowd sourcing system that seeks to address the challenge of determining the most efficient allocation of tasks to the human crowd. The goal of our algorithm is to efficiently determine the most appropriate set of workers to assign to each incoming task, so that the real-time demands are met and high quality results are returned. We empirically evaluate our approach and show that our system effectively meets the requested demands, has low overhead and can improve the number of tasks processed under the defined co\n",
      "\n",
      "8. id: 5390ba0a20f70186a0f33a72   score: 0.6104535   abstract: We study quality control mechanisms for a crowdsourcing system where workers perform object comparison tasks. We study error masking techniques (e.g., voting) and detection of bad workers. For the latter, we consider using gold-standard questions, as well as disagreement with the plurality answer. We perform experiments on Mechanical Turk that yield insights as to the role of task difficulty in quality control, and the effectiveness of the schemes.\n",
      "\n",
      "9. id: 558b4a65612c41e6b9d484c0   score: 0.59837085   abstract: Microtask crowdsourcing organizes complex work into workflows, decomposing large tasks into small, relatively independent microtasks. Applied to software development, this model might increase participation in open source software development by lowering the barriers to contribu-tion and dramatically decrease time to market by increasing the parallelism in development work. To explore this idea, we have developed an approach to decomposing programming work into microtasks. Work is coordinated through tracking changes to a graph of artifacts, generating appropriate microtasks and propagating change notifications to artifacts with dependencies. We have implemented our approach in CrowdCode, a cloud IDE for crowd development. To evaluate the feasibility of microtask programming, we performed a small study and found that a small crowd of 12 workers was able to successfully write 480 lines of\n",
      "\n",
      "10. id: 5390b0ca20f70186a0edad72   score: 0.5855752   abstract: Crowdsourcing is a new kind of organizational structure, one that is conducive to large amounts of short parallel work: thousands of individuals may work for several minutes on tasks, their outputs aggregated into a useful product or service. The dimensions of this new organizational form are described. Areas for future research are identified, focusing on open-ended tasks and the coordination structures that might foster collective creativity.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1741871\n",
      "index                                        554261830cf259acb4589839\n",
      "title               A ciphertext-policy attribute-based proxy re-e...\n",
      "authors             Kaitai Liang, Liming Fang, Duncan S. Wong, Wil...\n",
      "year                                                           2015.0\n",
      "venue               Concurrency and Computation: Practice & Experi...\n",
      "references          558ce7f10cf2a2c70f68c0d2;5590a5270cf2baaad971533b\n",
      "abstract            Ciphertext-policy attribute-based proxy re-enc...\n",
      "id                                                            1741871\n",
      "clustered_labels                                                    3\n",
      "Name: 1741871, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558af4d6612c41e6b9d3e89a   score: 0.9944666   abstract: A Ciphertext-Policy Attribute-Based Proxy Re-Encryption (CP-ABPRE) employs the PRE technology in the attribute-based encryption cryptographic setting, in which the proxy is allowed to convert an encryption under an access policy to another encryption under a new access policy. CP-ABPRE is applicable to many real world applications, such as network data sharing. The existing CP-ABPRE systems, however, leave how to achieve adaptive CCA security as an interesting open problem. This paper, for the first time, proposes a new CP-ABPRE to tackle the problem by integrating the dual system encryption technology with selective proof technique. The new scheme supports any monotonic access structures. Although our scheme is built in the composite order bilinear group, it is proven adaptively CCA secure in the standard model without jeopardizing the expressiveness of access policy.\n",
      "\n",
      "2. id: 5390be6620f70186a0f4b434   score: 0.99253935   abstract: Cipher text-Policy Attribute-Based Proxy Re-Encryption (CP-ABPRE) extends the traditional Proxy Re-Encryption (PRE) by allowing a semi-trusted proxy to transform a cipher text under an access policy to the one with the same plaintext under another access policy (i.e. attribute-based re-encryption). The proxy, however, learns nothing about the underlying plaintext. CP-ABPRE has many real world applications, such as fine-grained access control in cloud storage systems and medical records sharing among different hospitals. Previous CP-ABPRE schemes leave how to be secure against Chosen-Cipher text Attacks (CCA) as an open problem. This paper, for the first time, proposes a new CP-ABPRE to tackle the problem. The new scheme supports attribute-based re-encryption with any monotonic access structures. Despite our scheme is constructed in the random oracle model, it can be proved CCA secure und\n",
      "\n",
      "3. id: 5390a40520f70186a0e6f9d0   score: 0.9800149   abstract: Attribute based proxy re-encryption scheme (ABPRE) is a new cryptographic primitive which extends the traditional proxy re-encryption (public key or identity based cryptosystem) to the attribute based counterpart, and thus empower users with delegating capability in the access control environment. Users, identified by attributes, could freely designate a proxy who can re-encrypt a ciphertext related with a certain access policy to another one with a different access policy. The proposed scheme is proved selective-structure chosen plaintext secure and master key secure without random oracles. Besides, we develop another kind of key delegating capability in our scheme and also discuss some related issues including a stronger security model and applications.\n",
      "\n",
      "4. id: 5390a2be20f70186a0e648d4   score: 0.97931457   abstract: In a proxy re-encryption system, a semi-trusted proxy can convert a ciphertext originally intended for Alice into a ciphertext intended for Bob, without learning the underlying plaintext. Proxy re-encryption has found many practical applications, such as encrypted email forwarding, secure distributed file systems, and outsourced filtering of encrypted spam. In ACM CCS'07, Canetti and Hohenberger presented a proxy re-encryption scheme with chosen-ciphertext security, and left an important open problem to construct a chosen-ciphertext secure proxy re-encryption scheme without pairings. In this paper, we solve this open problem by proposing a new proxy re-encryption scheme without resort to bilinear pairings. Based on the computational Diffie-Hellman (CDH) problem, the chosen-ciphertext security of the proposed scheme is proved in the random oracle model.\n",
      "\n",
      "5. id: 5390b5df20f70186a0f0a097   score: 0.97409195   abstract: Proxy re-encryption (PRE) is a public key encryption that allows a semi-trusted proxy with some information (a.k.a., re-encryption key) to transform a ciphertext under one public key into another ciphertext under another public key. Because of this special property, PRE has many applications, such as the distributed file system. Some of these applications demand that the underlying PRE scheme is anonymous under chosen-ciphertext attacks (CCAs); that is, the adversary cannot identify the recipient of the original/transformed ciphertext, even if it knows the PRE key and can launch the CCA. However, to the best of our knowledge, none of the existing PRE schemes satisfy this requirement. In this work, we propose the first anonymous PRE with CCA security and collusion resistance. Our proposal is proved in the random oracle model based on the DDH assumption. Copyright © 2011 John Wiley & Sons,\n",
      "\n",
      "6. id: 5390aeba20f70186a0ecb34e   score: 0.9740426   abstract: We present a novel ciphertext policy attribute-based proxy re-encryption (CP-AB-PRE) scheme. The ciphertext policy realized in our scheme is AND-gates policy supporting multi-value attributes, negative attributes and wildcards. Our scheme satisfies the properties of PRE, such as unidirectionality, non-interactivity and multi-use. Moreover, the proposed scheme has master key security, allows the encryptor to decide whether the ciphertext can be re-encrypted and allows the proxy to add access policy when re-encrypting ciphertext. Furthermore, our scheme can be modified to have constant ciphertext size in original encryption.\n",
      "\n",
      "7. id: 53909f8c20f70186a0e3fd8d   score: 0.97339284   abstract: In a proxy re-encryption (PRE) scheme, a proxy is given special information that allows it to translate a ciphertext under one key into a ciphertext of the same message under a different key. The proxy cannot, however, learn anything about the messages encrypted under either key. PRE schemes have many practical applications, including distributed storage, email, and DRM. Previously proposed re-encryption schemes achieved only semantic security; in contrast, applications often require security against chosen ciphertext attacks. We propose a definition of security against chosen ciphertext attacks for PRE schemes, and present a scheme that satisfies the definition. Our construction is efficient and based only on the Decisional Bilinear Diffie-Hellman assumption in the standard model. We also formally capture CCA security for PRE schemes via both a game-based definition and simulation-based\n",
      "\n",
      "8. id: 5390aca920f70186a0eb95cd   score: 0.97112906   abstract: Proxy re-encryption (PRE) is a useful primitive that allows a semi-trusted proxy to transform a ciphertext encrypted under one key into an encryption of the same plaintext under another key. A PRE scheme is bidirectional if the proxy is able to transform ciphertexts in both directions. In ACM CCS'07, Canetti and Hohenberger presented a bidirectional PRE scheme with chosen-ciphertext security, which captures the indistinguishability of ciphertexts even if the adversary has access to the standard decryption oracle as well as a re-encryption oracle and a re-encryption key oracle. They also left an important open problem to come up with a chosen-ciphertext secure PRE scheme without pairings. To resolve this problem, we propose a bidirectional PRE scheme without pairings, and prove its chosen-ciphertext security under the computational Diffie-Hellman assumption in the random oracle model. Bas\n",
      "\n",
      "9. id: 5390adfc20f70186a0ec44ef   score: 0.9688562   abstract: In a proxy re-encryption (PRE) scheme, a semi-trusted proxy is given special information that allows it to transform cipher texts encrypted under Alice’s public key into the different cipher texts that can be decrypted by Bob’s secret key. In 2006, Green and Ateniese proposed identity-based proxy re-encryption schemes, in which cipher texts are transformed from one identity to another. Based on this, we propose a new construction for certificate less proxy re-encryption scheme and the corresponding security model in this paper. The scheme is proved secure against chosen-plaintext attack in the random oracle model assuming that an underlying problem related to the Decisional Bilinear Diffie-Hellman problem is computationally hard.\n",
      "\n",
      "10. id: 558b39c8612c41e6b9d4734b   score: 0.95473194   abstract: Ciphertext-Policy Attribute-based Encryption (CP-ABE) is one of the most suitable technologies for data access control in cloud storage systems. This paper firstly presents the Attribute-based Encryption (ABE), secure deletion and secret-sharing schemes. Then we construct a CP-ABE model using secret-sharing methods to insure its security. At last, we propose an improved scheme on Data Access Control for Multi-Authority Cloud Storage Systems DAC-MACS to insure the security of the central authority (CA).\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717924\n",
      "index                                        559170e10cf2e89307ca9d2d\n",
      "title               Two New Schemes to Generate Automatic Variable...\n",
      "authors             Manash Pratim Dutta, Subhasish Banerjee, Chand...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 International Conferen...\n",
      "references          53909f8220f70186a0e3d2bf;53908b2a20f70186a0db9...\n",
      "abstract            The confidentiality of shared information is a...\n",
      "id                                                            1717924\n",
      "clustered_labels                                                    3\n",
      "Name: 1717924, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bb1d20f70186a0f3ed71   score: 0.22388572   abstract: Realizing the time variant key or variable key from session to session is the key challenge in achieving perfect security [1-2]. Automatic Variable Key (AVK) is a novel approach to realize time variant key. In AVK, key is generated for use between the sender and the receiver in using a simple computation technique as proposed by Bhunia [3-5]. The AVK was widely experimented by other researchers that established the superiority of AVK over a single / fixed key [6-10]. In this paper we propose two new techniques for generating time variant key. Our techniques ensure better randomness among the successive two keys so generated.\n",
      "\n",
      "2. id: 5390a2be20f70186a0e656e0   score: 0.13184245   abstract: In SOK key distribution scheme, the shared key for safe channel can be generated without any interaction between two entities. But the shared key will be never update through life period of Key Generate Center (KGC) that cause more attacked risks. In this paper, we propose improved key distribution scheme and key update scheme to overcome the shortage of SOK scheme which are also non-interactive. Last, security analysis on the improved schemes above is given.\n",
      "\n",
      "3. id: 5390a80e20f70186a0e95788   score: 0.12368282   abstract: Notwithstanding the adoption of strong cryptographic mechanisms, the anticipated degree of security in the protection and management of privacy sensitive data can only be achieved, if secret keys can be shielded adequately. In practice, most implementations are based on software tokens that shall guard the keys against eavesdropping. This fact alleviates the hardness of circumvention of used cryptographic protocols and with this the disclosure of secret keys. In this paper we propose a key management architecture which - based on the capabilities of Trusted Computing Technology - will provide a higher level of security.\n",
      "\n",
      "4. id: 5390a2e920f70186a0e67dd1   score: 0.10302442   abstract: This paper deals with new problems which arise in the application of cryptography to computer communication systems with large numbers of users. Foremost among these is the key distribution problem. We suggest two techniques for dealing with this problem. The first employs current technology and requires subversion of several separate key distribution nodes to compromise the system's security. Its disadvantage is a high overhead for single message connections. The second technique is still in the conceptual phase, but promises to eliminate completely the need for a secure key distribution channel, by making the sender's keying information public. It is also shown how such a public key cryptosystem would allow the development of an authentication system which generates an unforgeable, message dependent digital signature.\n",
      "\n",
      "5. id: 539098b820f70186a0e0baa2   score: 0.08225395   abstract: Sharing and maintaining long, random keys is one of the central problems in cryptography. This thesis provides about ensuring the security of a cryptographic key when partial information about it has been, or must be, leaked to an adversary. We consider two basic approaches: 1. Extracting a new, shorter, secret key from one that has been partially compromised. Specifically, we study the use of noisy data, such as biometrics and personal information, as cryptographic keys. Such data can vary drastically from one measurement to the next. We would like to store enough information to handle these variations, without having to rely on any secure storage—in particular, without storing the key itself in the clear. We solve the problem by casting it in terms of key extraction. We give a precise definition of what “security” should mean in this setting, and design practical, general solutions wit\n",
      "\n",
      "6. id: 5390a74f20f70186a0e8cb30   score: 0.08093671   abstract: Data being transmitted through a communications network can be protected by cryptography. In a data processing environment, cryptography is implemented by an algorithm which utilizes a secret key, or sequence of bits. Any key-controlled cryptographic algorithm, such as the Data Encryption Standard, requires a protocol for the management of its cryptographic keys. The complexity of the key management protocol ultimately depends on the level of functional capability provided by the cryptographic system. This paper discusses a possible key management scheme that provides the support necessary to protect communications between individual end users (end-to-end encryption) and that also can be used to protect data stored or transported on removable media.\n",
      "\n",
      "7. id: 5390ba0a20f70186a0f32fbf   score: 0.08021325   abstract: The requirement of data security is an important parameter for all organizations for their survival in the world. Cryptography is the best method to avoid unauthorized access to data. It involves an encryption algorithm and the keys that are being used by the users. Multiple keys provide a more secure cryptographic model with a minimum number of overheads. There are various factors that affect the security pattern such as the number of keys and their length, encryption algorithm, latency, key shifting time, and users. In this paper, a new approach is proposed for generating keys from the available data. The analysis of various times, such as encryption, decryption, key setup, processing, and key shifting times, has been done. The model takes minimum time to replace the faulty keys with the fresh keys. In this paper, we consider all the above-mentioned factors and suggest an optimized way\n",
      "\n",
      "8. id: 53908bad20f70186a0dc29f7   score: 0.07850098   abstract: The problem of computer security can be considered to consist of four distinct components: secrecy (ensuring that information is only disclosed to authorized users), authentication (ensuring that information is not forged), integrity (ensuring that information is not destroyed), and availability (ensuring that access to information can not be maliciously interrupted). The paper describes a new protection mechanism called cryptographic sealing that provides primitives for secrecy and authentication. The mechanism is enforced with a synthesis of classical cryptography, public-key cryptography, and a threshold scheme.\n",
      "\n",
      "9. id: 5390995d20f70186a0e15224   score: 0.07821887   abstract: This work presents a scheme to build data communication system that can effectively protect data integrity and confidentiality. Firstly, This work briefly introduces the situation of integrity and confidentiality protection. Then, This work brings forward a new cipher, which uses a keystream generator to produce infinite number of frame secret keys basing on an infinite root key space, and use a unique one-off frame secret key for each data encryption/decryption. Basing on this cipher, we construct a data communication system. This work illustrates how to build such a system and analyze its protections of data integrity and confidentiality. With the character of cipher, it offers high resistance against cryptanalysis to prevent data disclosure, and it gives little opportunities to those intractable attacks that can compromise data integrity.\n",
      "\n",
      "10. id: 539087cb20f70186a0d596ca   score: 0.07654563   abstract: A new public-key (two-key) cipher scheme is proposed in this paper. In our scheme, keys can be easily generated. In addition, both encryption and decryption procedures are simple. To encrypt a message, the sender needs to conduct a vector product of the message being sent and the enciphering key. On the other hand, the receiver can easily decrypt it by conducting several multiplication operations and modulus operations. For security analysis, we also examine some possible attacks on the presented scheme\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698612\n",
      "index                                        558ad6b9612c41e6b9d3b379\n",
      "title               On the particular solution of constant coeffic...\n",
      "authors                                          Manuel D. Ortigueira\n",
      "year                                                           2015.0\n",
      "venue                             Applied Mathematics and Computation\n",
      "references          539087f320f70186a0d6e9b7;5390c04b20f70186a0f58...\n",
      "abstract            The paper presents an approach that comes dire...\n",
      "id                                                            1698612\n",
      "clustered_labels                                                    2\n",
      "Name: 1698612, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908a7420f70186a0da3bad   score: 0.9651191   abstract: In this paper we propose an algorithm for the numerical solution of arbitrary differential equations of fractional order. The algorithm is obtained by using the following decomposition of the differential equation into a system of differential equation of integer order connected with inverse forms of Abel-integral equations.The algorithm is used for solution of the linear and non-linear equations.\n",
      "\n",
      "2. id: 5390ae2e20f70186a0ec74dd   score: 0.921499   abstract: Abstract: The main focus of this paper is the solution of some partial differential equations of fractional order. Promising methods based on matrix functions are taken in consideration. The features of different approaches are discussed and compared with results provided by classical convolution quadrature rules. By means of numerical experiments accuracy and performance are examined.\n",
      "\n",
      "3. id: 5390b24420f70186a0ee796a   score: 0.91730267   abstract: The operator method for solving of differential equations and their systems is presented in the paper. Practical applicability of the method – methodology, parallelization of the computational algorithm and the complex solution of concrete differential equations – is described.\n",
      "\n",
      "4. id: 5390bd1520f70186a0f435ff   score: 0.8996516   abstract: The objective of this paper is to demonstrate that the Infinite State Approach, used for fractional order system modeling, initialization and transient prediction is the generalization of integer order system theory. The main feature of this classical theory is the integer order integrator, according to Lord Kelvin's principle. So, fractional order system theory has to be based on the fractional order integrator, characterized by an infinite dimension frequency distributed state. As a consequence Fractional Differential Systems or Equations generalize Ordinary Differential Equation properties with an infinite dimension state vector. Moreover, Caputo and Riemann-Liouville fractional derivatives, analyzed through their associated fractional integrators, are no longer convenient tools for the analysis of fractional systems.\n",
      "\n",
      "5. id: 559230a90cf2e74f816e3963   score: 0.88263035   abstract: The paper describes different approaches to generalize the trapezoidal method to fractional differential equations. We analyze the main theoretical properties and we discuss computational aspects to implement efficient algorithms. Numerical experiments are provided to illustrate potential and limitations of the different methods under investigation.\n",
      "\n",
      "6. id: 558b26c6612c41e6b9d44f04   score: 0.86083776   abstract: We use the expansion formula for the fractional derivatives to reduce the problem of solving non-linear fractional order differential equations arising in mechanics to the problem of solving a system of integer order differential equations. We prove the convergence of the solutions to the reduced integer order systems to the solutions of the original problem. The procedure is illustrated by two specific examples.\n",
      "\n",
      "7. id: 5390a7f520f70186a0e931fe   score: 0.85415626   abstract: An algorithm for solving systems of differential equations based on Laplace transform method is presented. There are considered ordinary linear differential equations with constant coefficients, nonzero initial conditions and right-hand sides as composite functions reducible to sums of exponents with polynomial coefficients. An algorithm to compute an error of calculations sufficient to obtain a preassigned accuracy of solution of linear differential equations system is included.\n",
      "\n",
      "8. id: 5390a01420f70186a0e46ea6   score: 0.8349459   abstract: This paper presents approximate analytical solutions for systems of fractional differential equations using the differential transform method. The fractional derivatives are described in the Caputo sense. The application of differential transform method, developed for differential equations of integer order, is extended to derive approximate analytical solutions of systems of fractional differential equations. The solutions of our model equations are calculated in the form of convergent series with easily computable components. Some examples are solved as illustrations, using symbolic computation. The numerical results show that the approach is easy to implement and accurate when applied to systems of fractional differential equations. The method introduces a promising tool for solving many linear and nonlinear fractional differential equations.\n",
      "\n",
      "9. id: 5390ac1820f70186a0eb444d   score: 0.83169055   abstract: In this work, we introduced the differential equations of non integer order. Then, we generalized the Taylor series method to special class of equation of order (1, q) by applying the Taylor series method to linear differential equations of order (1, 2), known as semi-differential equations To established this approach, we compared this result against results obtained by Adomian decomposition method. This study shows that the Taylor series method can be generalized for the differential equations of order (1, q), which, encourages us to study the same approach for other types of linear and non-linear fractional and extra-ordinary differential equations.\n",
      "\n",
      "10. id: 5390a40520f70186a0e6f3e2   score: 0.82782817   abstract: The use of explicit methods in the numerical treatment of differential equations of fractional order is an area not yet widely investigated. In this paper stability properties of some multistep methods of explicit type are investigated and new methods with larger intervals of stability are proposed. Some numerical experiments are presented in order to validate theoretical results.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710868\n",
      "index                                        55323be745cec66b6f9dad68\n",
      "title               Delay-dependent non-fragile H∞ control for lin...\n",
      "authors                      Jun-Jun Hui, He-Xin Zhang, Xiang-Yu Kong\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Automation and Computing\n",
      "references          53909e8a20f70186a0e2d2f5;5390a74f20f70186a0e8b...\n",
      "abstract            This paper considers the problem of delay-depe...\n",
      "id                                                            1710868\n",
      "clustered_labels                                                    1\n",
      "Name: 1710868, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55323c4b45cec66b6f9dbb5e   score: 0.9639839   abstract: A new delay-dependent robust H ¿ filtering design for uncertain linear systems with time-varying delay is investigated. Two kinds of time-varying delays are considered. One is differentiable uniformly bounded with a bounded delay derivative; the other is continuous uniformly bounded. A full-order filter is designed which ensures the asymptotic stability of the filtering error system and a prescribed level of H ¿ performance for all possible parameters which reside in a given polytope. By constructing a new Lyapunov functional which contains a triple integral term, new delay-dependent conditions for the existence of the H ¿ filter are derived which are less conservative than the existing ones. The filter gain can be obtained by solving a set of linear matrix inequalities (LMIs). Finally, two numerical examples are given to show the effectiveness and the advantages of the proposed method.\n",
      "\n",
      "2. id: 5390ba3820f70186a0f36f3c   score: 0.9572778   abstract: A general class of linear systems with multiple successive delay components is considered in this article. The delays are assumed to vary in intervals, and delay-dependent exponential stability conditions are derived in terms of linear matrix inequalities. To reduce conservativeness, a new Lyapunov–Krasovskii functional is designed to contain more complete state information, so that a derivation procedure with time-varying delays treated as uncertain parameters can be adopted. Usage of slack variables and inequalities are refrained as much as possible when bounds on the Lyapunov derivative are sought. The stability criteria are tested by two popular numerical examples, with less conservative results obtained in all the checked cases. Besides, a practical application of the derived conditions is illustrated.\n",
      "\n",
      "3. id: 5390bb7b20f70186a0f40998   score: 0.9516839   abstract: This article addresses the H \u001e control problem of delayed neural networks, where the state input and observation output contain interval non-differentiable time-varying delays. Based on constructing a new set of Lyapunov---Krasovskii functionals, new delay-dependent sufficient criteria for H \u001e control are established in terms of linear matrix inequalities. The Lyapunov---Krasovskii functional is mainly based on the information of the lower and upper delay bounds, which allows us to avoid using additional free-weighting matrices and any assumption on the differentiability of the delay function. The obtained condition is less conservative because of the technique of designing state feedback controller. The H \u001e controller to be designed must satisfy some exponential stability constraints on the closed-loop poles. A numerical example is given to illustrate the effectiveness of our results.\n",
      "\n",
      "4. id: 5390b78a20f70186a0f24e88   score: 0.9510514   abstract: This paper investigates the stability analysis problem for linear systems with time-varying delay. The state delay is assumed to be varying within a given interval. By constructing a novel Lyapunov-Krasovskii functional and developing new techniques, an improved delay-interval-dependent criterion is derived to ensue the stability of the systems. The stability condition is presented in terms of linear matrix inequalities, which can be easily solved by means of standard numerical software. A numerical example is given to illustrate the effectiveness of the proposed method.\n",
      "\n",
      "5. id: 5390b5df20f70186a0f0a2e5   score: 0.94728357   abstract: A robust H\"~ control for uncertain linear systems with a state-delay is described. Systems with norm-bounded parameter uncertainties are considered and linear memoryless state feedback controllers are obtained. Firstly, a delay-dependent bounded real lemma for systems with a state-delay is presented in terms of linear matrix inequalities (LMIs). By taking a new Lyapunov-Krasovsii functional, neither model transformation nor bounding for cross terms is required to obtain delay-dependent results. Secondly, based on the bounded real lemma obtained, delay-dependent condition for the existence of robust H\"~ control is presented in terms of nonlinear matrix inequalities. In order to solve these nonlinear matrix inequalities, an iterative algorithm involving convex optimization is proposed. Numerical examples show that the proposed methods are much less conservative than existing results.\n",
      "\n",
      "6. id: 5390a88c20f70186a0e98a0a   score: 0.94427973   abstract: Addressing time-delay systems, this paper proposes an innovative approach to significantly improve the stability performance while reducing the computing demand considerably. The key features of the approach include the removal of the redundant information from the augmented matrix and the use of a tighter bounding technology. With the proposed approach, a new delay-dependent bounded real lemma for uncertain systems with interval time-varying delay are derived. Numerical examples are given to demonstrate the effectiveness of the proposed approach.\n",
      "\n",
      "7. id: 5390a1f820f70186a0e5dfb6   score: 0.9422947   abstract: In this paper, the delay-dependent H∞ controlproblem is considered for Linear Parameter-Varying (LPV) systemswith time-varying delays. By using a descriptor modeltransformation of the system and by applying Park's inequality forbounding cross terms, some sufficient conditions onH∞ performance analysis are given in terms ofLinear Matrix Inequalities (LMIs). Based on these conditions thestate-feedback controller is designed. A numerical example isincluded to illustrate the proposed method.\n",
      "\n",
      "8. id: 53909fca20f70186a0e44156   score: 0.94165415   abstract: A delay-dependent robust H control problem for a class of discrete-time linear state-delayed systems with norm-bounded uncertainties is considered. The delay- dependent bound real lemma for systems with a state- delay is first presented in terms of linear matrix inequalities (LMIs). Then, based on the bounded real lemma obtained, delay-dependent condition for the existence of robust H controller is presented. A numerical example is employed to illustrate the feasibility and advantage of the proposed design.\n",
      "\n",
      "9. id: 5390c04b20f70186a0f5853a   score: 0.9305845   abstract: In this paper, the problem of H\"~ performance and stability analysis for linear systems with interval time-varying delays is considered. First, by constructing a newly augmented Lyapunov-Krasovskii functional which has not been proposed yet, an improved H\"~ performance criterion with the framework of linear matrix inequalities (LMIs) is introduced. Next, the result and method are extended to the problem of a delay-dependent stability criterion. Finally, numerical examples are given to show the superiority of proposed method.\n",
      "\n",
      "10. id: 558bd0eb0cf23f2dfc59399e   score: 0.9159594   abstract: In this paper, the problem of robust H∞ control for uncertain linear systems with interval time-varying delay is investigated. Based on Lyapunov stability theory and reciprocally convex method, delay-dependent robust H∞ stability criteria are obtained, which are in terms of linear matrix inequalities(LMIs). Numerical examples and simulation results are given to illustrate the effectiveness and less conservativeness of the proposed method.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672895\n",
      "index                                        559170da0cf2e89307ca9d29\n",
      "title                                     Data-Driven Color Manifolds\n",
      "authors             Chuong H. Nguyen, Tobias Ritschel, Hans-Peter ...\n",
      "year                                                           2015.0\n",
      "venue                              ACM Transactions on Graphics (TOG)\n",
      "references          5390baa120f70186a0f37aa9;539099a220f70186a0e17...\n",
      "abstract            Color selection is required in many computer g...\n",
      "id                                                            1672895\n",
      "clustered_labels                                                    3\n",
      "Name: 1672895, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b4c320f70186a0efd415   score: 0.8768455   abstract: We survey tools for colour selection, reviewing their shortcomings and strengths, and suggest a way of integrating their best features into a single tool.\n",
      "\n",
      "2. id: 539087f820f70186a0d7253b   score: 0.86748546   abstract: User interfaces for color selection consist of a visible screen representation, an input method, and the underlying conceptual organization of the color model. We report a two-way factorial, between-subjects variable experiment that tested the effect of high and low visual feedback interfaces on speed and accuracy of color matching for RGB and HSV color models. The only significant effect was improved accuracy due to increased visual feedback. Using color groups as a within-subjects variable, we found differences in performance of both speed and accuracy. We recommend that experimental tests adopt a color test set that does not show bias toward a particular model, but is based instead on a range of colors that would be most likely matched in practice by people using color selection software. We recomment the Macbeth Color Checker naturals, primaries, and grays. As a follow-up study, a qu\n",
      "\n",
      "3. id: 5390a1e620f70186a0e59878   score: 0.81888163   abstract: Color is fundamental in computer graphics imagery. But despite the importance of good color selection and the difficulty most people have making those color selections, current graphics applications provide only minimal tools for mixing and organizing colors; exploring color combinations; and soliciting historical, theoretical, or expert sources.This article describes a new tool set that addresses these needs. The tools are based on artists' methods, perceptual science, and a task analysis survey conducted by the authors. With these tools, users can quickly experiment with color within the context of their compositions to arrive at deliberate, confident selections. The article describes the tools' rationale, inner workings, and interfaces, and discusses important implementation issues.\n",
      "\n",
      "4. id: 5390bf1320f70186a0f516d4   score: 0.717309   abstract: With the introduction of low-cost color graphics systems comes a host of problems specifically concerned with the color aspect of the system. This paper discusses two of these problems: the selection and manipulation of colors by (possibly) inexperienced users, and the automatic selection of colors by the system to achieve high contrast effects on the screen. A new color space based on color opponency theory is described. This space is useful both in the user interface and in the automatic selection of high contrast colors.\n",
      "\n",
      "5. id: 53908b9320f70186a0dc0cc7   score: 0.6737286   abstract: Color specification is a time-consuming and challenging task in computer graphics applications. In order to investigate how certain fundamental attributes of any color selection system may affect its usability, we systematically varied both color space and navigational device and measured simultaneously several usability parameters during a simple color matching task. A new analysis method, called the Standardized Performance Trajectory (SPT), was used to measure the rate of convergence during color specification as an indication of a system's ease of understanding. The SPT analysis revealed significant results that the traditional usability measures did not. The purpose of this paper is to describe the SPT, and to discuss its merits.\n",
      "\n",
      "6. id: 5390878e20f70186a0d39ef3   score: 0.6722241   abstract: Color is used in computer graphics to code information, to call attention to items, to signal a user, and to enhance display aesthetics, but using color effectively and tastefully is often beyond the abilities of application programmers because the study of color crosses many disciplines, and many aspects, such as human color vision, are not completely understood. We compiled a comprehensive set of guidelines for the proper use of color, but even these guidelines cannot provide all of the aesthetic and human factors knowledge necessary for making good color selections. Furthermore, programmers may misinterpret or ignore the guidelines. To alleviate some of these problems, we have implemented ACE, A Color Expert system which embodies the color rules and applies them to user interface design. The goal of the implementation was to test whether an automated mechanism would be a viable soluti\n",
      "\n",
      "7. id: 5390b7ff20f70186a0f278cb   score: 0.43842453   abstract: This paper presents a method to automate rendering parameter selection, simplifying tedious user interaction and improving the usability of visualization systems. Our approach acquires regions-of-interest for a dataset with an eye tracker and simple user interaction. Based on this importance information, we then automatically compute reasonable rendering parameters using a set of heuristic rules adapted from visualization experience and psychophysics experiments. While the parameter selections for a specific visualization task are subjective, our approach provides good starting results that can be refined by the user. Our system improves the interactivity of a visualization system by significantly reducing the necessary parameter selection and providing good initial rendering parameters for newly acquired datasets of similar types.\n",
      "\n",
      "8. id: 53908bcc20f70186a0dc6cd6   score: 0.40904373   abstract: The paper presents an interactive approach for guiding the user's select of colormaps in visualization. PRAVDAColor, implemented as a module in the IBM Visualization Data Explorer, provides the user a selection of appropriate colormaps given the data type and spatial frequency, the user's task, and properties of the human perceptual system.\n",
      "\n",
      "9. id: 539089bb20f70186a0d98d19   score: 0.3706809   abstract: The advantages of using perceptually uniform color spaces in data displays are described. It is shown how one-, two-, and three-dimensional representations of color gamuts can provide an understanding, at various visualization levels, of the colors that can be produced on display devices, of how they restrict color displays in practice, and of how they form an essential part of a user interface in the design of color displays.\n",
      "\n",
      "10. id: 5390893e20f70186a0d92b6f   score: 0.3211406   abstract: From the Publisher:Using a jargon-free style, it offers accessible and practical advice on how to use colour effectively for presentation--both on the computer screen and for output to paper. Contains numerous depictions of pitfalls to avoid, 32 pages of colour illustrations, a slew of practical examples, look-up charts and tables.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1694843\n",
      "index                                        5590fb9e0cf2baaad9717c35\n",
      "title               A double projection method for solving variati...\n",
      "authors                                           Minglu Ye, Yiran He\n",
      "year                                                           2015.0\n",
      "venue                     Computational Optimization and Applications\n",
      "references          5390879920f70186a0d41988;53908b4920f70186a0dbb...\n",
      "abstract            We present a double projection algorithm for s...\n",
      "id                                                            1694843\n",
      "clustered_labels                                                    2\n",
      "Name: 1694843, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a2be20f70186a0e639a6   score: 0.9985392   abstract: In this paper, we propose two methods for solving variational inequalities. In the first method, we modified the extragradient method by using a new step size while the second method can be viewed as an extension of the first one by performing an additional projection step at each iteration and another optimal step length is employed to reach substantial progress in each iteration. Under certain conditions, the global convergence of two methods is proved. Preliminary numerical experiments are included to illustrate the efficiency of the proposed methods.\n",
      "\n",
      "2. id: 558d70490cf2e6675805024e   score: 0.9875683   abstract: In this paper, we propose new methods for solving variational inequalities. The proposed methods can be viewed as a refinement and improvement of the method of He et al. [B.S. He, X.M. Yuan, J.J. Zhang, Comparison of two kinds of prediction-correction methods for monotone variational inequalities, Comp. Opt. Appl. 27 (2004) 247-267] by performing an additional projection step at each iteration and another optimal step length is employed to reach substantial progress in each iteration. Under certain conditions, the global convergence of the both methods is proved. Preliminary numerical experiments are included to illustrate the efficiency of the proposed methods.\n",
      "\n",
      "3. id: 5590f7680cf237666fc299c5   score: 0.985663   abstract: In this paper, we propose a new projection method for solving variational inequality problems, which can be viewed as an improvement of the method of Li et al. [M. Li, L.Z. Liao, X.M. Yuan, A modified projection method for co-coercive variational inequality, European Journal of Operational Research 189 (2008) 310-323], by adopting a new direction. Under the same assumptions as those in Li et al. (2008), we establish the global convergence of the proposed algorithm. Some preliminary computational results are reported, which illustrated that the new method is more efficient than the method of Li et al. (2008).\n",
      "\n",
      "4. id: 5390a88c20f70186a0e98ee2   score: 0.9843364   abstract: We present a modification of a double projection algorithm proposed by Solodov and Svaiter for solving variational inequalities. The main modification is to use a different Armijo-type linesearch to obtain a hyperplane strictly separating current iterate from the solutions of the variational inequalities. Our method is proven to be globally convergent under very mild assumptions. If in addition a certain error bound holds, we analyze the convergence rate of the iterative sequence. We use numerical experiments to compare our method with that proposed by Solodov and Svaiter.\n",
      "\n",
      "5. id: 539099a220f70186a0e170ee   score: 0.9843364   abstract: We present a modification of a double projection algorithm proposed by Solodov and Svaiter for solving variational inequalities. The main modification is to use a different Armijo-type linesearch to obtain a hyperplane strictly separating current iterate from the solutions of the variational inequalities. Our method is proven to be globally convergent under very mild assumptions. If in addition a certain error bound holds, we analyze the convergence rate of the iterative sequence. We use numerical experiments to compare our method with that proposed by Solodov and Svaiter.\n",
      "\n",
      "6. id: 5390b19020f70186a0edfd02   score: 0.9757161   abstract: We study two projection algorithms for solving the variational inequality problem in Hilbert space. One algorithm is a modified subgradient extragradient method in which an additional projection onto the intersection of two half-spaces is employed. Another algorithm is based on the shrinking projection method. We establish strong convergence theorems for both algorithms.\n",
      "\n",
      "7. id: 5390b19020f70186a0ee06b8   score: 0.9757161   abstract: We study two projection algorithms for solving the variational inequality problem in Hilbert space. One algorithm is a modified subgradient extragradient method in which an additional projection onto the intersection of two half-spaces is employed. Another algorithm is based on the shrinking projection method. We establish strong convergence theorems for both algorithms.\n",
      "\n",
      "8. id: 53909fbd20f70186a0e42b8c   score: 0.9685601   abstract: In this paper, we propose a new projection method for solving variational inequality problems, which can be viewed as an improvement of the method of Han and Lo [D.R. Han, Hong K. Lo, Two new self-adaptive projection methods for variational inequality problems, Computers & Mathematics with Applications 43 (2002) 1529-1537], by adopting a new step-size rule. The method is as simple as Han and Lo's methods [D.R. Han, Hong K. Lo, Two new self-adaptive projection methods for variational inequality problems, Computers & Mathematics with Applications 43 (2002) 1529-1537] and other extra-gradient-type methods, which uses only function evolutions and projections onto the feasible set. We prove that under the condition that the underlying function is co-coercive, the sequence generated by the method converges to a solution of the variational inequality problem globally. Some preliminary computati\n",
      "\n",
      "9. id: 539099b320f70186a0e1a12d   score: 0.9664738   abstract: In this paper, we proposed a modified Logarithmic-Quadratic Proximal (LQP) method [Auslender et al.: Comput. Optim. Appl. 12, 31---40 (1999)] for solving variational inequalities problems. We solved the problem approximately, with constructive accuracy criterion. We show that the method is globally convergence under that the operator is pseudomonotone which is weaker than the monotonicity and the solution set is nonempty. Some preliminary computational results are given.\n",
      "\n",
      "10. id: 5590df940cf2ce4b6f3a0a9a   score: 0.9653158   abstract: General variational inequalities provide us with a unified, natural, novel and simple framework to study a wide class of equilibrium problems arising in pure and applied sciences. In this paper, we present a number of new and known numerical techniques for solving general variational inequalities using various techniques including projection, Wiener-Hopf equations, updating the solution, auxiliary principle, inertial proximal, penalty function, dynamical system and well-posedness. We also consider the local and global uniqueness of the solution and sensitivity analysis of the general variational inequalities as well as the finite convergence of the projection-type algorithms. Our proofs of convergence are very simple as compared with other methods. Our results present a significant improvement of previously known methods for solving variational inequalities and related optimization probl\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1608964\n",
      "index                                        55913dd40cf232eb904fb6ab\n",
      "title               Exact Safety Verification of Hybrid Systems Ba...\n",
      "authors                              Zhengfeng Yang, Wang Lin, Min Wu\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Embedded Computing Systems...\n",
      "references          5390995d20f70186a0e15ac3;539087e120f70186a0d66...\n",
      "abstract            In this article, we address the problem of saf...\n",
      "id                                                            1608964\n",
      "clustered_labels                                                    3\n",
      "Name: 1608964, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908cde20f70186a0dcf635   score: 0.96538115   abstract: A method of verifying hybrid systems is given. Such systems involve state components whose values are changed by continuous (physical) processes. The verification method is based on proving that only those executions that satisfy constraints imposed by an environment also satisfy the property of interest. A suitably expressive logic then allows the environment to model state components that are changed by physical processes.\n",
      "\n",
      "2. id: 53909e7c20f70186a0e2a7ae   score: 0.9646559   abstract: This paper deals with the problem of safety verification of nonlinear hybrid systems. We start from a classical method that uses interval arithmetic to check whether trajectories can move over the boundaries in a rectangular grid. We put this method into an abstraction refinement framework and improve it by developing an additional refinement step that employs interval-constraint propagation to add information to the abstraction without introducing new grid elements. Moreover, the resulting method allows switching conditions, initial states, and unsafe states to be described by complex constraints, instead of sets that correspond to grid elements. Nevertheless, the method can be easily implemented, since it is based on a well-defined set of constraints, on which one can run any constraint propagation-based solver. Tests of such an implementation are promising.\n",
      "\n",
      "3. id: 5390b3da20f70186a0ef75f2   score: 0.95814794   abstract: This paper deals with the problem of safety verification of non-linear hybrid systems. We start from a classical method that uses interval arithmetic to check whether trajectories can move over the boundaries in a rectangular grid. We put this method into an abstraction refinement framework and improve it by developing an additional refinement step that employs constraint propagation to add information to the abstraction without introducing new grid elements. Moreover, the resulting method allows switching conditions, initial states and unsafe states to be described by complex constraints instead of sets that correspond to grid elements. Nevertheless, the method can be easily implemented since it is based on a well-defined set of constraints, on which one can run any constraint propagation based solver. First tests of such an implementation are promising.\n",
      "\n",
      "4. id: 5390b68720f70186a0f1c23c   score: 0.9579909   abstract: In this paper we discuss how to generate inequality invariants for continuous dynamical systems involved in hybrid systems. A hybrid symbolic-numeric algorithm is presented to compute inequality invariants of the given systems, by transforming this problem into a parameterized polynomial optimization problem. A numerical inequality invariant of the given system can be obtained by applying polynomial Sum-of-Squares (SOS) relaxation via Semidefinite Programming (SDP). And a method based on Gauss-Newton refinement is deployed to obtain candidates of polynomials with rational coefficients, and finally we certify that this polynomial exactly satisfies the conditions of invariants, by use of SOS representation of polynomials with rational coefficients. Several examples are given to show that our algorithm can successfully yield inequality invariants with rational coefficients.\n",
      "\n",
      "5. id: 5390a1d420f70186a0e57d92   score: 0.91920847   abstract: We introduce a fixedpoint algorithm for verifying safety properties of hybrid systems with differential equations whose right-hand sides are polynomials in the state variables. In order to verify nontrivial systems without solving their differential equations and without numerical errors, we use a continuous generalization of induction, for which our algorithm computes the required differential invariants. As a means for combining local differential invariants into global system invariants in a sound way, our fixedpoint algorithm works with a compositional verification logic for hybrid systems. To improve the verification power, we further introduce a saturation procedurethat refines the system dynamics successively with differential invariants until safety becomes provable. By complementing our symbolic verification algorithm with a robust version of numerical falsification, we obtain a\n",
      "\n",
      "6. id: 5390a1f820f70186a0e5c986   score: 0.9032942   abstract: Aimed at verifying safety properties and improving simulation coverage for hybrid systems models of embedded control software, we propose a technique that combines numerical simulation and symbolic methods for computing state-sets. We consider systems with linear dynamics described in the commercial modeling tool Simulink/Stateflow. Given an initial state x, and a discrete-time simulation trajectory, our method computes a set of initial states that are guaranteed to be equivalent to x, where two initial states are considered to be equivalent if the resulting simulation trajectories contain the same discrete components at each step of the simulation. We illustrate the benefits of our method on two case studies. One case study is a benchmark proposed in the literature for hybrid systems verification and another is a Simulink demo model from Mathworks.\n",
      "\n",
      "7. id: 5390a63c20f70186a0e81fcf   score: 0.8984106   abstract: We introduce a fixedpoint algorithm for verifying safety properties of hybrid systems with differential equations whose right-hand sides are polynomials in the state variables. In order to verify nontrivial systems without solving their differential equations and without numerical errors, we use a continuous generalization of induction, for which our algorithm computes the required differential invariants. As a means for combining local differential invariants into global system invariants in a sound way, our fixedpoint algorithm works with a compositional verification logic for hybrid systems. With this compositional approach we exploit locality in system designs. To improve the verification power, we further introduce a saturation procedure that refines the system dynamics successively with differential invariants until safety becomes provable. By complementing our symbolic verificatio\n",
      "\n",
      "8. id: 5390b20120f70186a0ee54ce   score: 0.8595459   abstract: We present a hybrid symbolic-numeric algorithm for certifying a polynomial or rational function with rational coefficients to be non-negative for all real values of the variables by computing a representation for it as a fraction of two polynomial sum-of-squares (SOS) with rational coefficients. Our new approach turns the earlier methods by Peyrl and Parrilo at SNC'07 and ours at ISSAC'08 both based on polynomial SOS, which do not always exist, into a universal algorithm for all inputs via Artin's theorem. Furthermore, we scrutinize the all-important process of converting the numerical SOS numerators and denominators produced by block semidefinite programming into an exact rational identity. We improve on our own Newton iteration-based high precision refinement algorithm by compressing the initial Gram matrices and by deploying rational vector recovery aside from orthogonal projection. W\n",
      "\n",
      "9. id: 5390b4c320f70186a0efe293   score: 0.8159669   abstract: We present the first verification methods that automatically generate bases of invariants expressed by multivariate formal power series and transcendental functions. We discuss the convergence of solutions generated over hybrid systems that exhibit non-linear models augmented with parameters. We reduce the invariant generation problem to linear algebraic matrix systems, from which one can provide effective methods for solving the original problem. We obtain very general sufficient conditions for the existence and the computation of formal power series invariants over multivariate polynomial continuous differential systems. The formal power series invariants generated are often composed by the expansion of some well-known transcendental functions like log or exp and have an analysable closed-form. This facilitates their use to verify safety properties. Moreover, we generate inequality and\n",
      "\n",
      "10. id: 5390b4c320f70186a0efe295   score: 0.8044059   abstract: Although a growing number of dynamical systems studied in various fields are hybrid in nature, the verification of properties, such as stability, safety, etc., is still a challenging problem. Reachability analysis is one of the promising methods for hybrid system verification, which together with all other verification techniques faces the challenge of making the analysis scale with respect to the number of continuous state variables. The bottleneck of many reachability analysis techniques for hybrid systems is the geometrically computed intersection with guard sets. In this work, we replace the intersection operation by a nonlinear mapping onto the guard, which is not only numerically stable, but also scalable, making it possible to verify systems which were previously out of reach. The approach can be applied to the fairly common class of hybrid systems with piecewise continuous soluti\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674002\n",
      "index                                        5591726f0cf2e89307ca9df0\n",
      "title               The interactive technological devices: play or...\n",
      "authors                                              Myzan Binti Noor\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 9th International Conferenc...\n",
      "references          5390972920f70186a0dfa912;53909ce520f70186a0e28842\n",
      "abstract            This paper discusses the works related to the ...\n",
      "id                                                            1674002\n",
      "clustered_labels                                                    3\n",
      "Name: 1674002, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390882c20f70186a0d8ccd4   score: 0.96544635   abstract: From the Publisher:Television will be interactive, computers will provide feature-length motion pictures and TV programs, and information will be delivered like never before. Access to digital technologies is rapidly changing how children experience media, changing how technologies will impact children's development, and making media an increasingly active gateway for experiencing and learning about the world.This volume considers how children use media today, and how new media is emerging and merging with existing technologies. The distinctive features of both older and newer media are examined, along with why these technologies are attractive to children and adolescents.An interdisciplinary group of scholars from the fields of psychology, communication, sociology, and linguistics examine the effect of media experiences on children's social, cognitive, familial, and consumerist experien\n",
      "\n",
      "2. id: 5390b3da20f70186a0ef6e49   score: 0.94154674   abstract: We produced case studies of fourteen families based on nine rounds of data collection during the period from June 2008 to October 2009. We focused on fourteen children who were three years old when our visits started and used an ecocultural approach to examine their experiences of learning and playing with technologies at home. The study describes i) which technologies children encounter at home, ii) how family practices influence children's encounters with technology, and iii) what children are learning through their interactions with technology. We present a framework of four areas of learning that could be supported by technology: acquiring operational skills, extending knowledge and understanding of the world, developing dispositions to learn, and understanding the role of technology in everyday life.\n",
      "\n",
      "3. id: 5390a1f820f70186a0e5d7cf   score: 0.938124   abstract: Children are increasingly using computer technologies as reflected inï戮 reports of computer use in schools in the United States. Given theï戮 greater exposure of children to these technologies, it is imperative that they be designed taking into account children's abilities, interests, and developmental needs. This survey aims to contribute toward this goal through a review of research on children's cognitive and motor development, safety issues related to technologies and design methodologies and principles. It also provides and overview of current research trends in the field of interaction design and children and identifies challenges for future research.\n",
      "\n",
      "4. id: 5390975920f70186a0dfd2a2   score: 0.9284088   abstract: Researchers have investigated the possibilities for supporting language learning through a range of technologies, most recently mobile phones and interactive television (iTV). Drawing on a focus group study, we present a scenario demonstrating an approach that blends the features of these two technologies. Three areas are identified for further exploration: pedagogy, technical feasibility and interaction design issues.\n",
      "\n",
      "5. id: 5390b29820f70186a0ee9efc   score: 0.92455155   abstract: There is a natural alliance between learning and personal mobile technology, making it feasible to equip learners with powerful tools to support learning in many contexts, emphasising the skills and knowledge needed for a rapidly changing society. eMapps.com, under IST EC FP6 in the New Member States, is demonstrating how games and mobile technologies can be combined to provide enriching experiences for children in the school curriculum and beyond, using Advanced Reality Games, played ‘live' in the individual territory using Internet, GPRS/3G, SMS and MMS technologies.\n",
      "\n",
      "6. id: 53909f8220f70186a0e3c787   score: 0.9181873   abstract: Social interaction and collaboration are essential to the emotional and cognitive development of young children [40]. Constructionism [32] is a learning theory where children learn as they build or construct a public artifact. Creative activities that promote collaboration, especially those based on principles of constructionism, provide enhanced learning opportunities for young children. Mobile devices can support the learning experience as children can create artifacts in various contexts. The proposed research incorporates collaboration, constructionism, children, stories and mobile technologies; specifically investigating developmentally appropriate interfaces to support mobile collaboration for young children.\n",
      "\n",
      "7. id: 5390882c20f70186a0d8d3ce   score: 0.9174507   abstract: From the Publisher:After taking a hard look at the computer scene in a three year investigation and drawing the not surprising conclusion that most children use computers to play games, this text proposes directions to facilitate the educational use of computers and other promising technology.\n",
      "\n",
      "8. id: 539089ab20f70186a0d95050   score: 0.9158089   abstract: This paper will discuss the various issues surrounding personal technologies for children. Three critical questions will be discussed: Why can technology be important for children? What activities can technology support? And what changes should be considered for the future?\n",
      "\n",
      "9. id: 5390979920f70186a0e0131c   score: 0.9084556   abstract: Today's technologies are shaping the way children live, learn, and play. Technologies can provide a wealth of meaningful new experiences and support children's exploration of their neighborhoods, other cultures, and even the universe. Technologies are changing what it means for children to read a book, play a game, or listen to music. Innovative tools can also foster communication, collaboration, storytelling, and creativity among children.\n",
      "\n",
      "10. id: 5390b13020f70186a0edcce8   score: 0.90632194   abstract: The domain of learning context for people with special needs is a big challenge for digital media in education. Usage of mobile technology is growing, and it affects other technologies by bringing in new innovation and methods. The reason for this growth is not only ease of use and mobility, but also improvements in interaction and functionality in different contexts. Meanwhile, the difference between cell phones and handheld computers is becoming less and less evident. Such convergence offers the opportunity of ubiquitous learning \"anytime, anywhere\", so that the learners do not have to wait for a fixed time and place for learning to take place. Mobile learning can be seen as a bridge between higher level of abstracted knowledge and practical experiences, which supports the personalization in a learning process.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1700474\n",
      "index                                        559257b30cf28b1a968ffcf0\n",
      "title               An overview of kernel alignment and its applic...\n",
      "authors                    Tinghua Wang, Dongyan Zhao, Shengfeng Tian\n",
      "year                                                           2015.0\n",
      "venue                                  Artificial Intelligence Review\n",
      "references          5390ab8820f70186a0eb0091;5390994d20f70186a0e12...\n",
      "abstract            The success of kernel methods is very much dep...\n",
      "id                                                            1700474\n",
      "clustered_labels                                                    2\n",
      "Name: 1700474, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bb7b20f70186a0f40a52   score: 0.91565824   abstract: The success of kernel-based learning methods depends on the choice of kernel. Recently, kernel learning methods have been proposed that use data to select the most appropriate kernel, usually by combining a set of base kernels. We introduce a new algorithm for kernel learning that combines a continuous set of base kernels, without the common step of discretizing the space of base kernels. We demonstrate that our new method achieves state-of-the-art performance across a variety of real-world datasets. Furthermore, we explicitly demonstrate the importance of combining the right dictionary of kernels, which is problematic for methods that combine a finite set of base kernels chosen a priori. Our method is not the first approach to work with continuously parameterized kernels. We adopt a two-stage kernel learning approach. We also show that our method requires substantially less computation \n",
      "\n",
      "2. id: 5390b52620f70186a0f0305d   score: 0.7858021   abstract: The main advantage of kernel methods stems from the implicit transformation of patterns to a high-dimensional feature space, thus a choice of a kernel function and proper setting of its parameters is of crucial importance. Learning a kernel from the data requires evaluation measures to assess the quality of the kernel. In this paper current state-of-the-art kernel evaluation measures are examined and their application to the kernel optimization is verified, showing limitations of these methods. As a result, alternative evaluation measures are proposed that strive to overcome these disadvantages. Results of experiments are provided to demonstrate that the application of the optimization process that leverages introduced measures results in kernels that correspond to the classifiers that achieve significantly lower error rate.\n",
      "\n",
      "3. id: 5390aefb20f70186a0ecd10a   score: 0.7799929   abstract: Kernel methods have been successfully used in many practical machine learning problems. Choosing a suitable kernel is left to the practitioner. A common way to an automatic selection of optimal kernels is to learn a linear combination of element kernels. In this paper, a novel framework of multiple kernel learning is proposed based on conditional entropy minimization criterion. For the proposed framework, three multiple kernel learning algorithms are derived. The algorithms are experimentally shown to be comparable to or outperform kernel Fisher discriminant analysis and other multiple kernel learning algorithms on benchmark data sets.\n",
      "\n",
      "4. id: 5390a77d20f70186a0e8df6c   score: 0.76171577   abstract: Appropriate choice of kernels is the most important task when using kernel-based learning methods such as support vector machines. The current widely used kernels (such as polynomial kernel, Gaussian kernel, two-layer perceptron kernel, and so on) are all functional kernels for general purposes. Currently, there is no kernel proposed in a data-driven way. This paper proposes a new kernel generating method dependent on classifying related properties of the data structure itself. The new kernel concentrates on the similarity of paired data in classes, where the calculation of similarity is based on fuzzy theories. The experimental results with four medical data sets show that the proposed kernel has superior classification performance than polynomial and Gaussian kernels.\n",
      "\n",
      "5. id: 53909a9320f70186a0e22c3e   score: 0.7541916   abstract: Choosing an appropriate kernel is one of the key problems in kernel-based methods. Most existing kernel selection methods require that the class labels of the training examples are known. In this paper, we propose an adaptive kernel selection method for kernel principal component analysis, which can effectively learn the kernels when the class labels of the training examples are not available. By iteratively optimizing a novel criterion, the proposed method can achieve nonlinear feature extraction and unsupervised kernel learning simultaneously. Moreover, a noniterative approximate algorithm is developed. The effectiveness of the proposed algorithms are validated on UCI datasets and the COIL-20 object recognition database.\n",
      "\n",
      "6. id: 5390af8920f70186a0ed04d2   score: 0.6788595   abstract: Kernel-based algorithms have been used with great success in a variety of machine learning applications. These include algorithms such as support vector machines for classification, kernel ridge regression, ranking algorithms, clustering algorithms, and virtually all popular dimensionality reduction algorithms. But, the choice of the kernel, which is crucial to the success of these algorithms, has been traditionally left entirely to the user. Rather than requesting the user to commit to a specific kernel, multiple kernel algorithms require the user only to specify a family of kernels. This family of kernels can be used by a learning algorithm to form a combined kernel and derive an accurate predictor. This is a problem that has attracted a lot of attention recently, both from the theoretical point of view and from the algorithmic, optimization, and application point of view. This thesis \n",
      "\n",
      "7. id: 5390aeba20f70186a0ecb2b8   score: 0.67779404   abstract: This talk will review recent advances in the kernel methods focusing on support vector machines (SVM) for pattern recognition. Topics discussed include the kernel design issue through the multi kernel approach and the optimization issue with emphasis on scalability and non convex cost functions.\n",
      "\n",
      "8. id: 53908b4920f70186a0dbabfe   score: 0.6657367   abstract: Kernel methods, a new generation of learning algorithms, utilize techniques from optimization, statistics, and functional analysis to achieve maximal generality, flexibility, and performance. These algorithms are different from earlier techniques used in machine learning in many respects: For example, they are explicitly based on a theoretical model of learning rather than on loose analogies with natural learning systems or other heuristics. They come with theoretical guarantees about their performance and have a modular design that makes it possible to separately implement and analyze their components. They are not affected by the problem of local minima because their training amounts to convex optimization. In the last decade, a sizable community of theoreticians and practitioners has formed around these methods, and a number of practical applications have been realized. Although the r\n",
      "\n",
      "9. id: 5390b1d220f70186a0ee16a2   score: 0.66225064   abstract: The performance of a kernel method often depends mainly on the appropriate choice of a kernel function. In this study, we present a data-dependent method for scaling the kernel function so as to optimize the classification performance of kernel methods. Instead of finding the support vectors in feature space, we first find the region around the separating boundary in input space, and subsequently scale the kernel function correspondingly. It is worth noting that the proposed method does not require a training step to enable a specified classification algorithm to find the boundary and can be applied to various classification methods. Experimental results using both artificial and real-world data are provided to demonstrate the robustness and validity of the proposed method.\n",
      "\n",
      "10. id: 5390bed320f70186a0f4d9cc   score: 0.530723   abstract: Kernel learning is becoming an important research topic in the area of machine learning, and it has wide applications in pattern recognition, computer vision, image and signal processing. Kernel learning provides a promising solution to nonlinear problems, including nonlinear feature extraction, classification and clustering. However, in kernel-based systems, the problem of the kernel function and its parameters remains to be solved. Methods of choosing parameters from a discrete set of values have been presented in previous studies, but these methods do not change the data distribution structure in the kernel-based mapping space. Accordingly, performance is not improved because the current kernel optimization does not change the data distribution. Based on this problem, this paper presents a uniform framework for kernel self-optimization with the ability to adjust the data structure. Th\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675059\n",
      "index                                        5591592e0cf232eb904fbdae\n",
      "title                       Making Software Tutorial Video Responsive\n",
      "authors                                        Cuong Nguyen, Feng Liu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390b7fe20f70186a0f26b92;5390be6620f70186a0f4c...\n",
      "abstract            Tutorial videos are widely available to help p...\n",
      "id                                                            1675059\n",
      "clustered_labels                                                    0\n",
      "Name: 1675059, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ad0720f70186a0ebbb0c   score: 0.9725715   abstract: We conducted a user study with 4 video clips and 37 viewing sessions on how users interact with a web-based zoomable video system, where users can zoom and pan within the video to view selected regions-of-interest with more detail. The study shows that frequency of interaction is very high and the period during which users watch the video without interacting is comparable to the period of interaction. Users spend most of their time viewing a magnified version of the video. We also observe that their behavior is not easily predictable. Users, however, tend to be interested in common regions of the video.\n",
      "\n",
      "2. id: 5390b48420f70186a0efbcc5   score: 0.96672606   abstract: Interactive navigation through a video is a simple way for a user to get a quick overview of its content and to find interesting scenes. Although common video players provide only poor navigation facilities – in comparison to real video search applications – they are often employed by users due to their simplicity. We present a tool using a similarly simple interaction method but enabling much more efficient navigation.\n",
      "\n",
      "3. id: 53909fca20f70186a0e44965   score: 0.9364014   abstract: Almost all video players use interaction features similar to those invented for traditional, sequential VCRs. In our current world where video is intensively used in many areas and in many different ways, existing features are not suffi- cient anymore. Although video abstraction is a means to ease the handling of videos, it lacks rich interactivity. We propose a video browsing tool that provides more flexibil- ity and interactivity to the end-user. It should enable a user to get knowledge quickly about the content of a video and to easily find relevant parts in a video. We will evaluate our current implementation with user studies performed in a usability-lab.\n",
      "\n",
      "4. id: 5390a01420f70186a0e482a9   score: 0.9235924   abstract: We present a method for browsing videos by directly dragging their content. This method brings the benefits of direct manipulation to an activity typically mediated by widgets. We support this new type of interactivity by: 1) automatically extracting motion data from videos; and 2) a new technique called relative flow dragging that lets users control video playback by moving objects of interest along their visual trajectory. We show that this method can outperform the traditional seeker bar in video browsing tasks that focus on visual content rather than time.\n",
      "\n",
      "5. id: 5390ada620f70186a0ec32e2   score: 0.8916225   abstract: This work presents a web lecture systems that allows its users to visualize and re-use aggregated actions taken from user’s past interaction for enhanced content navigation and multimedia retrieval.\n",
      "\n",
      "6. id: 5390a01420f70186a0e47c09   score: 0.8838389   abstract: Existing approaches to interaction with digital video are complex, and some operations lack the immediacy of interactive feedback. In this thesis, I present a framework for video annotation, visualization, and interaction that harnesses computer vision to aid users in understanding and commu nicating with digital video. I first review the literature concerning visual representations, navigation, and manipulation of video data, and explore the literature of professional film editing, summarizing some of the techniques applied by and operations performed by film editors. I describe a new approach for computing the motion of points and objects in a video clip, and I present my interactive system that utilizes this data to visually annotate independently moving objects in the video, including speech and thought balloons, video graffiti, hyperlinks, and path arrows. I also demonstrate an appl\n",
      "\n",
      "7. id: 53909f6a20f70186a0e3b5c3   score: 0.87037706   abstract: Today, videos can be replayed on modern handheld devices, such as multimedia cellphones and personal digital assistants (PDAs), due to significant improvements in their processing power. However, screen size remains a limiting resource making it hard, if not impossible to adapt common approaches for video browsing to such mobile devices. In this paper we propose a new interface for the pen-based navigation of videos on PDAs and multimedia cellphones. Our solution - called the MobileZoomSlider - enables users to intuitively skim a video along the timeline on different granularity levels. In addition, it allows for continuous manipulation of replay speed for browsing purposes. Both interaction concepts are seamlessly integrated into the overall interface, thus taking optimum advantage of the limited screen space. Our claims are verified with a first evaluation which proves the suitability \n",
      "\n",
      "8. id: 5390ba3820f70186a0f35469   score: 0.86476797   abstract: We describe direct video manipulation interactions applied to screen-based tutorials. In addition to using the video timeline, users of our system can quickly navigate into the video by mouse-wheel, double click over a rectangular region to zoom in and out, or drag a box over the video canvas to select text and scrub the video until the end of a text line even if not shown in the current frame. We describe the video processing techniques developed to implement these direct video manipulation techniques, and show how they are implemented to run in most modern web browsers using HTML5's CANVAS and Javascript.\n",
      "\n",
      "9. id: 539099ec20f70186a0e1d0c9   score: 0.8244619   abstract: The continuous growth of media databases necessitates development of novel visualization and interaction techniques to support management of these collections. We present Videotater, an experimental tool for a Tablet PC that supports the efficient and intuitive navigation, selection, segmentation, and tagging of video. Our veridical representation immediately signals to the user where appropriate segment boundaries should be placed and allows for rapid review and refinement of manually or automatically generated segments. Finally, we explore a distribution of modalities in the interface by using multiple timeline representations, pressure sensing, and a tag painting/erasing metaphor with the pen.\n",
      "\n",
      "10. id: 5390a01420f70186a0e482aa   score: 0.79565805   abstract: We present DRAGON, a direct manipulation interaction technique for frame-accurate navigation in video scenes. This technique benefits tasks such as professional and amateur video editing, review of sports footage, and forensic analysis of video scenes. By directly dragging objects in the scene along their movement trajectory, DRAGON enables users to quickly and precisely navigate to a specific point in the video timeline where an object of interest is in a desired location. Examples include the specific frame where a sprinter crosses the finish line, or where a car passes a traffic light. Through a user study, we show that DRAGON significantly reduces task completion time for in-scene navigation tasks by an average of 19-42% compared to a standard timeline slider. Qualitative feedback from users is also positive, with multiple users indicating that the DRAGON interaction felt more natura\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674212\n",
      "index                                        55912b220cf232eb904fb13a\n",
      "title               Getting the Message?: A Study of Explanation I...\n",
      "authors             James Schaffer, Prasanna Giridhar, Debra Jones...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 20th International Conferen...\n",
      "references          558af50b612c41e6b9d3e92d;5390bb7b20f70186a0f40...\n",
      "abstract            In many of today's online applications that fa...\n",
      "id                                                            1674212\n",
      "clustered_labels                                                    0\n",
      "Name: 1674212, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558af50b612c41e6b9d3e92d   score: 0.97057647   abstract: As an interactive intelligent system, recommender systems are developed to give predictions that match users preferences. Since the emergence of recommender systems, a large majority of research focuses on objective accuracy criteria and less attention has been paid to how users interact with the system and the efficacy of interface designs from the end-user perspective. The field has reached a point where it is ready to look beyond algorithms, into users interactions, decision making processes and overall experience. Accordingly, the goals of this workshop (IntRS@RecSys) are to explore the human aspects of recommender systems, with a particular focus on the impact of interfaces and interaction design on decision-making and user experiences with recommender systems, and to explore methodologies to evaluate these human aspects of the recommendation process that go beyond traditional autom\n",
      "\n",
      "2. id: 5390b78a20f70186a0f239b9   score: 0.96672606   abstract: There is an increasing consensus in the field of recommender systems that we should move beyond the offline evaluation of algorithms towards a more user-centric approach. This tutorial teaches the essential skills involved in conducting user experiments, the scientific approach to user-centric evaluation. Such experiments are essential in uncovering how and why the user experience of recommender systems comes about.\n",
      "\n",
      "3. id: 5390b78a20f70186a0f22f3a   score: 0.91565824   abstract: In the quest to develop better and more useful search systems, many novel search user interface features have been developed, such as relevance feedback, clusters, tag clouds, facets, and so on. Yet all of these novel 'interactions' have required novel forms of 'information', or metadata, to make them work. Consequently, we do not know whether users have been benefiting from better interaction or simply richer forms of metadata, or both. In this research, we aimed to show that better interaction can be provided, regardless of whether we have access to, or the ability to generate, richer forms of metadata. Using only search engine query suggestions as a consistent form of metadata, we built interface conditions for three common interaction models for search: query suggestions (our baseline), hierarchical browsing, and faceted filtering. Our results showed that, despite interacting with th\n",
      "\n",
      "4. id: 5390b19020f70186a0edfb4e   score: 0.8056322   abstract: Recommender systems provide users with products or content intended to satisfy their information needs. The primary evaluation measures for recommender systems emphasize either the perceived relevance of the recommendations or the actions driven by those recommendations (e.g., purchases on ecommerce sites or clicks on news and social networking sites). Unfortunately, this transactional emphasis neglects the inherently interactive nature of the user experience. This tutorial explores recommendations as part of a conversation between users and systems. A conversational approach should provide transparency, control, and guidance. Transparency means that users understand why systems offer particular recommendations. Control means that users can explicitly manipulate the behavior of recommender systems based on personal needs and preferences. Guidance means that systems offers plausible and p\n",
      "\n",
      "5. id: 53909ed120f70186a0e30f0f   score: 0.73354733   abstract: Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit.allOur findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or\n",
      "\n",
      "6. id: 539089ab20f70186a0d96fac   score: 0.713132   abstract: Current recommender systems, based on collaborative filtering, implement a rather limited model of interaction. These systems intelligently elicit information from a user only during the initial registration phase. Furthermore, users tend to collaborate only indirectly. We believe there are several unexplored opportunities in which information can be effectively elicited from users by making the underlying interaction model more conversational and collaborative. In this paper, we propose a set of techniques to intelligently select what information to elicit from the user in situations in which the user may be particularly motivated to provide such information. We argue that the resulting interaction improves the user experience. We conclude by reporting results of an offline experiment in which we compare the influence of different elicitation techniques on both the accuracy of the syste\n",
      "\n",
      "7. id: 53909f2d20f70186a0e38bb1   score: 0.68867785   abstract: We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity.\n",
      "\n",
      "8. id: 5390b71120f70186a0f1fd55   score: 0.6872103   abstract: Recommender Systems (RS) attempt to discover users' preferences, and to learn about them in order to anticipate their needs. The main task normally associated with a RS is to offer suggestions for items. However, for most users, RSs are black boxes, computerized oracles that give advice, but cannot be questioned. In order to improve the quality of predictions and the satisfaction of the users, explanations facilities are needed. We present a novel methodology to explain recommendations: showing predictions over a set of observed items. Our proposal has been validated by means of user studies and lab experiments using MovieLens dataset.\n",
      "\n",
      "9. id: 5390a37f20f70186a0e6dafb   score: 0.6454552   abstract: For reasons ranging from obligation to curiosity, users have a strong inclination to seek information from others during the search process. Search systems using statistical analytics over traces left behind by others can help support the search experience.\n",
      "\n",
      "10. id: 5390b20120f70186a0ee5910   score: 0.6378207   abstract: This paper explores the distinctions between searching and exploring when looking for information. We propose that, while traditional search engines work well in supporting search behaviour, they are more limited in assisting those who are looking to explore new information, especially when the exploration task is ill-defined. We ran a pilot study using two systems: one based on a traditional database search engine, and the other -- a highly innovative, engaging and playful system called iFISH -- that we designed specifically to support exploration through the use of user preferences. We looked for evidence to support the concept that exploration requires a different kind of interaction. The initial results report a positive response to our exploration system and indicate the differences in preferences amongst users for systems that match their searching or exploring behaviours.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672594\n",
      "index                                        559257a70cf205530abc97fa\n",
      "title                            Dynamic feature-adaptive subdivision\n",
      "authors             H. Schäfer, J. Raab, B. Keinert, M. Meyer, M. ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 19th Symposium on Interacti...\n",
      "references          5390995d20f70186a0e155a4;5390881720f70186a0d80...\n",
      "abstract            Feature-adaptive subdivision (FAS) is one of t...\n",
      "id                                                            1672594\n",
      "clustered_labels                                                    0\n",
      "Name: 1672594, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a77d20f70186a0e8dca8   score: 0.8834373   abstract: Subdivision surfaces provide a compact representation for smooth surfaces that facilitate modeling and animation. They have widespread application in the movie industry, and there's a natural desire to use them also in real-time applications. This course presents theoretical results, implementations, applications, and future research directions. Topics include an introduction to subdivision surfaces, an overview of the surface-evaluation algorithms that are suitable for real-time rendering, implementation of those algorithms on current and next-generation GPUs, and other practical considerations. The course concludes with a section on practical application of these theoretical schemes and GPU implementations to Valve's source game engine and ILM's film production.\n",
      "\n",
      "2. id: 5390ada620f70186a0ec3c99   score: 0.8191712   abstract: We propose a new GPU method for synthesizing subdivision meshes with exact adaptive geometry in real time. Our GPU kernel builds upon precomputed tables of basis functions for subdivision surfaces and is therefore supporting all subdivision schemes, either interpolating or approximating, for triangle or quad meshes. We designed our kernel so that it can be integrated seamlessly within a standard tessellation pipeline, exploiting software or hardware (adaptive) tessellation methods. We make use of the tessellator unit as an adaptive mesher for maximum subdivision level, exploiting the linear nature of subdivision surfaces to enable arbitrary level of detail adaptivity and %extend the idea of Subdivision Shading to control the visual smoothness using the same tables as control the visual smoothness using Subdivision Shading by applying the same tables as for geometry. We evaluate our kerne\n",
      "\n",
      "3. id: 5390b24320f70186a0ee6d0e   score: 0.7360213   abstract: We present a novel method for high-performance GPU-based rendering of Catmull-Clark subdivision surfaces. Unlike previous methods, our algorithm computes the true limit surface up to machine precision, and is capable of rendering surfaces that conform to the full RenderMan specification for Catmull-Clark surfaces. Specifically, our algorithm can accommodate base meshes consisting of arbitrary valence vertices and faces, and the surface can contain any number and arrangement of semisharp creases and hierarchically defined detail. We also present a variant of the algorithm which guarantees watertight positions and normals, meaning that even displaced surfaces can be rendered in a crack-free manner. Finally, we describe a view-dependent level-of-detail scheme which adapts to both the depth of subdivision and the patch tessellation density. Though considerably more general, the performance o\n",
      "\n",
      "4. id: 5390b36120f70186a0ef0859   score: 0.7229184   abstract: We present a GPU-based beam-casting method for rendering implicit surfaces in real time with antialiasing. We use interval arithmetic to model the beams and to detect their intersections with the surface. We show how beams can be used to quickly discard large empty regions in the image, thus leading to a fast adaptive subdivision method.\n",
      "\n",
      "5. id: 5390985d20f70186a0e085cd   score: 0.71472764   abstract: By organizing the control mesh of subdivision in texture memory so that irregularities occur strictly inside independently refinable fragment meshes, all major features of subdivision algorithms can be realized in the framework of highly parallel stream processing. Our implementation of Catmull-Clark subdivision as a GPU kernel in programmable graphics hardware can model features like semi-smooth creases and global boundaries; and a simplified version achieves near-realtime depth-five re-evaluation of moderate-sized subdivision meshes. The approach is easily adapted to other refinement patterns, such as Loop, Doo-Sabin or √3 and it allows for postprocessing with additional shaders.\n",
      "\n",
      "6. id: 5390ad8920f70186a0ec093d   score: 0.68710536   abstract: As real time graphics aspires to movie-quality rendering, higher order, smooth surface representations take center stage. Catmull-Clark subdivision surfaces are the dominant higher-order surface type used in feature films as they can model surfaces of arbitrary topological type and provide a compact representation for smooth surfaces that facilitate modeling and animation. Although Catmull-Clark surfaces are popular in Digital Content Creation packages (DCC) and feature films, their use has been hindered in real time applications because the exact evaluation of such surfaces on modern GPUs is neither memory nor performance efficient. Developments in hardware and recent theoretical results in efficient substitutes for subdivision surfaces bring us to the possibility to see real-time cinematic rendering in the near future.\n",
      "\n",
      "7. id: 5390a30b20f70186a0e69784   score: 0.64646024   abstract: We propose a view-dependent adaptive subdivision algorithm for rendering parametric surfaces on parallel hardware. Our framework allows us to bound the screen space error of a piecewise linear approximation. We naturally assign more primitives to curved areas while keeping quads large for flatter parts of the model and avoid cracks resulting from the polygonal approximation of non-uniform patch subdivision. The overall algorithm is simple, fits current GPUs extremely well, and is surprisingly fast while producing little to no artifacts.\n",
      "\n",
      "8. id: 5390994d20f70186a0e137de   score: 0.6056824   abstract: One problem in subdivision surfaces is the number of facets grows exponentially with the level of subdivision. Subdivision schemes are cost intensive at higher levels of subdivision. In this paper, we propose an adaptive subdivision scheme for subdivision surfaces based on triangular meshes and exploit the local smoothness information of a surface for adaptive refinement of a model. With this approach, we can avoid unnecessary subdivision in relative smooth areas and represent surfaces with lower cost when compared with those obtained by uniform subdivision schemes. We compare our methods for various 3D graphic meshes and present our results.\n",
      "\n",
      "9. id: 5390b7fe20f70186a0f27504   score: 0.5924308   abstract: We present a practical importance-driven method for GPU-based final gathering. We take as input a point cloud representing directly illuminated scene geometry; we then project and splat the points to microbuffers, which store each shading pixel's occluded radiance field. We select points for projection based on importance, defined as each point's estimated contribution to a shading pixel. For each selected point, we calculate its splat size adaptively based on its importance value. The main advantage of our method is that it's simple and fast, and provides the capability to incorporate additional importance factors such as glossy reflection paths. We also introduce an image-space adaptive sampling method, which combines adaptive image subdivision with joint bilateral upsampling to robustly preserve fine details. We have implemented our algorithm on the GPU, providing high-quality renderi\n",
      "\n",
      "10. id: 5390a54720f70186a0e78688   score: 0.5520568   abstract: Increased realism in interactive graphics and gaming requires complex smooth surfaces to be rendered at ever higher frame rates. In particular, representations used to model surfaces offline, such as spline and subdivision surfaces, have to be modified or reorganized to allow for efficient usage of the graphics processing unit and its SIMD (Single Instruction, Multiple Data) parallelism. This dissertation presents a novel algorithm for converting quad meshes on the GPU to smooth, water-tight surfaces at the highest speed documented so far. The conversion reproduces bi-cubic splines wherever possible and closely mimics the shape of the Catmull-Clark subdivision surface by c-patches where a vertex has a valence different from 4. The smooth surface is piecewise polynomial and has well-defined normals everywhere.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1704153\n",
      "index                                        55323b2c45cec66b6f9d9797\n",
      "title               Complementary Split Ring Resonator Based Compa...\n",
      "authors             Dinesh Kumar Singh, Binod Kumar Kanaujia, Sant...\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references                                   5390bf1320f70186a0f51a88\n",
      "abstract            In this paper, the design of compact, wideband...\n",
      "id                                                            1704153\n",
      "clustered_labels                                                    1\n",
      "Name: 1704153, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55323ada45cec66b6f9d8e8d   score: 0.9777564   abstract: A printed planar antenna having a compact dimension of 27×23.5 mm2, fed by a microstrip line is proposed in this article. The proposed compact antenna consists of a circular disc radiator and partial ground plane with a rectangular slot on its' top edge. Experimental results show that the antenna has an ¿10 dB impedance bandwidth ranges from 3.3 GHz to 20 GHz (193%) which almost covers the entire UWB band. A symmetric and stable radiation pattern with an average realized gain of 5.11 dBi makes the proposed antenna suitable for using in different UWB applications. Details of the proposed antenna design are presented and discussed.\n",
      "\n",
      "2. id: 5390a2e820f70186a0e66aba   score: 0.9718326   abstract: A wideband stacked microstrip patch antenna is presented. The proposed structure combines the merits of the stacked patch antennas and L-shaped feed for broadband operation. This paper presents extended E and H shaped patch stacked with rectangular patch (53mm 脳 35mm). The composite effect of integrating inverting techniques and by introducing the proposed patch, offer a low profile, high gain, and compact antenna element. The simulated impedance bandwidth of the proposed antenna is 680 MHz (33%). It is shown that the wide bandwidth is caused by a two-frequency resonance. The study showed maximum achievable gain of about 9.58 dBi and gain variation of 0.76 dBi between the frequency ranges of 1.72GHz to 2.4GHz. Details of the proposed compact and broadband design is presented and discussed in this paper.\n",
      "\n",
      "3. id: 5390baa120f70186a0f39970   score: 0.9514138   abstract: A very compact, high efficiency and simple micro strip patch antenna for wideband applications is presented in this communication. The design employs a slotted rectangular patch fed by a 50 ? micro strip line. The bandwidth is increased by cutting some rectangular slits on the ground plane. The antenna is designed on a small size ground plane (25 mm x 30 mm) and is simulated by a full wave electromagnetic simulator IE3D, a software package of Zeland. The impedance bandwidth at-10 dB return loss for the proposed antenna is about 35% (3.8 to 5.4 GHz) with center frequency at 4.54 GHz.\n",
      "\n",
      "4. id: 5390c04520f70186a0f55ebd   score: 0.9443824   abstract: In this paper the antenna designing is shown for a compact nature and co-axial fed for Ultra Wide Band (UWB) applications. The antenna has been designed by introducing some slots on the radiating patch and some slots on the ground plane. FR4 substrate has been used here with a dielectric constant of 3.4. The ultra wide band of the antenna ranges from 2.70 GHz to 6.40 GHz (Bandwidth of 3.70 GHz). The minimum return loss observed here is -37.05 dB in the resonating frequency of 3.30 GHz. So the percentage bandwidth is 112 %. The maximum gain achieved here is 4.0 dBi at 5.80 GHz.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f51a88   score: 0.9353465   abstract: A tunable L-strip fed circular microstrip antenna on thick substrate with CSRR in the ground plane has been analysed and investigated. The antenna is analysed using cavity model and circuit theoretic approach for initial design and then simulated on IE3D simulation software. The antenna is made tunable with PIN diode which makes it to work in different configurations. Two diodes were used to implement a double annular slot, one annular slot and one split slot and CSRR in the ground plane. While other configurations of diodes provide bandwidth and radiation pattern diversity, CSRR provides size reduction of upto 13.31 % along with high gain directivity and radiation efficiency. A maximum gain of 8 dBi, directivity 8.3 dBi has been achieved in the respective band of operations. The antenna exhibits wideband along with multiband characteristic.\n",
      "\n",
      "6. id: 5390b20120f70186a0ee3db8   score: 0.91655886   abstract: A new compact printed monopole antenna with dual-wideband characteristics is presented for simultaneously satisfying wireless local area network and worldwide interoperability for microwave access applications. The antenna structure consists of a circular monopole with a microstrip feed-line for excitation and a hexagon conductor-backed parasitic plane. The antenna has a small size of 13 mm × 26 mm × 1 mm. © 2011 Wiley Periodicals, Inc. Int J RF and Microwave CAE, 2011. © 2011 Wiley Periodicals, Inc.\n",
      "\n",
      "7. id: 5390a06e20f70186a0e4bf11   score: 0.89912134   abstract: Nowadays, in modern mobile and wireless communication systems, there is an increased demand for compact, wide bandwidth and low-cost antennas with dual band capability. In this paper, two shapes for a low cost reduced size PIFA with wide bandwidth are investigated. The antenna substrate is from a cheap foam material with εr=1.07. The antenna is fed by single coaxial feed. Size reduction is achieved by cutting U-shaped slot on the radiating surface of the PIF A structure. The antenna size becomes (0.22 λ × 0.18 λ). Wide bandwidth is reached by making the U-shaped slot with two unequal arms. Moreover, dual operating frequencies are achieved by cutting either L-shaped or U-shaped slits on the coupled feeding part of the radiating surface. The antenna resonates in Bluetooth ISM band at 2.4 GHz as well as in WLAN band at 5.2GHz. The bandwidths are about 31% and 7.5% for the lower and upper ba\n",
      "\n",
      "8. id: 5390b29820f70186a0ee91b9   score: 0.89823216   abstract: Theoretical investigations of a U-slot loaded half disk patch antenna are presented using equivalent circuit concept. It is found that the antenna shows dual band characteristics with resonant frequency at 4.76 and 6.79 GHz. The dual nature of the antenna is realized by loading shorting pin with U-slot loaded patch. The lower and upper frequency bands are achieved as 443 and 287 MHz respectively. It is noted that the antenna shows frequency ratio of 1.4.\n",
      "\n",
      "9. id: 5390b1d220f70186a0ee28ec   score: 0.8889517   abstract: In this paper, we proposed an asymmetry structure of printed dipole antenna which acquires wideband impedance match delivered by a double-side and center-feed-design and an open slot in the small radiator to expand the high resonant frequency band and harvest an antenna that can reach dual and wide band requirements as we proposed here. The frequency bands of this proposed antenna cover 2.31-3.83 GHz and 4.88-6.0 GHz, which can be applied to WLAN and WiMax operations and the fractional bandwidths reach 49.51% and 20.59%. We minimize the antenna size to 20聞e47 mm2 by using the feed lines as design parameters to achieve the dual-band and wideband design. This proposed antenna structure can be included in a WLAN/WiMax array antenna as a structural element. In this paper, we will present simulation and implemented results of antenna structure, frequency bands, gain value and the radiation pa\n",
      "\n",
      "10. id: 5390bb7b20f70186a0f408c8   score: 0.8879841   abstract: Designing a compact wideband microstrip patch antenna which is composed of a folded-patch feed, a symmetric E-shaped edge and shorting pins is presented in this paper. One pin is applied in order to expand the impedance bandwidth. Two other pins are utilized to miniaturize the size of patch as well. The measured impedance bandwidth ( $$\\text{ VSWR}\\le 2$$ ) of the fabricated antenna is more than 90 % in the frequency range 3.92---10.67 GHz for ultra-wideband (UWB) applications. The antenna size is $$0.438\\lambda _{0}\\times 0.365\\lambda _{0}\\times 0.170\\lambda _{0}$$ at its center operating frequency. Also, radiation patterns with acceptable stability within the bandwidth are obtained. In addition, the effects of some key parameters are investigated to describe the performance of the proposed design.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1730338\n",
      "index                                        55323d8445cec66b6f9de929\n",
      "title               Streaming Algorithms for Extent Problems in Hi...\n",
      "authors                            Pankaj K. Agarwal, R. Sharathkumar\n",
      "year                                                           2015.0\n",
      "venue                                                    Algorithmica\n",
      "references                                   5590fb130cf2ce4b6f3a0efb\n",
      "abstract            We present (single-pass) streaming algorithms ...\n",
      "id                                                            1730338\n",
      "clustered_labels                                                    2\n",
      "Name: 1730338, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ad0720f70186a0ebb07d   score: 0.99528164   abstract: We develop (single-pass) streaming algorithms for maintaining extent measures of a stream S of n points in Rd. We focus on designing streaming algorithms whose working space is polynomial in d (poly(d)) and sublinear in n. For the problems of computing diameter, width and minimum enclosing ball of S, we obtain lower bounds on the worst-case approximation ratio of any streaming algorithm that uses poly(d) space. On the positive side, we introduce the notion of blurred ball cover and use it for answering approximate farthest-point queries and maintaining approximate minimum enclosing ball and diameter of S. We describe a streaming algorithm for maintaining a blurred ball cover whose working space is linear in d and independent of n.\n",
      "\n",
      "2. id: 5390bded20f70186a0f49477   score: 0.8304567   abstract: At SODA@?10, Agarwal and Sharathkumar presented a streaming algorithm for approximating the minimum enclosing ball of a set of points in d-dimensional Euclidean space. Their algorithm requires one pass, uses O(d) space, and was shown to have approximation factor at most (1+3)/2+@e~1.3661. We prove that the same algorithm has approximation factor less than 1.22, which brings us much closer to a (1+2)/2~1.207 lower bound given by Agarwal and Sharathkumar. We also apply this technique to the dynamic version of the minimum enclosing ball problem (in the non-streaming setting). We give an O(dn)-space data structure that can maintain a 1.22-approximate minimum enclosing ball in O(dlogn) expected amortized time per insertion/deletion.\n",
      "\n",
      "3. id: 5390b13020f70186a0edd5da   score: 0.77814394   abstract: At SODA'10, Agarwal and Sharathkumar presented a streaming algorithm for approximating the minimum enclosing ball of a set of points in d-dimensional Euclidean space. Their algorithm requires one pass, uses O(d) space, and was shown to have approximation factor at most (1 +√3)/2 + ε ≈ 1.3661. We prove that the same algorithm has approximation factor less than 1.22, which brings us much closer to a (1 +√2)/2 ≈ 1.207 lower bound given by Agarwal and Sharathkumar. We also apply this technique to the dynamic version of the minimum enclosing ball problem (in the non-streaming setting). We give an O(dn)- space data structure that can maintain a 1.22-approximate minimum enclosing ball in O(d log n) expected amortized time per insertion/deletion.\n",
      "\n",
      "4. id: 5390bd1520f70186a0f447c3   score: 0.7606507   abstract: Large scale geometric data is ubiquitous. In this dissertation, we design algorithms and data structures to process large scale geometric data efficiently. We design algorithms for some fundamental geometric optimization problems that arise in motion planning, machine learning and computer vision.For a stream S of n points in d-dimensional space, we develop (single-pass) streaming algorithms for maintaining extent measures such as the minimum enclosing ball and diameter. Our streaming algorithms have a work space that is polynomial in d and sub-linear in n. For problems of computing diameter, width and minimum enclosing ball of S, we obtain lower bounds on the worst-case approximation ratio of any streaming algorithm that uses polynomial in d space. On the positive side, we design a summary called the blurred ball cover and use it for answering approximate farthest-point queries and main\n",
      "\n",
      "5. id: 539099a220f70186a0e18833   score: 0.6531269   abstract: Streaming is an important paradigm for handling data sets that are too large to fit in main memory. In the streaming computational model, algorithms are restricted to using much less space than they would need to store the input. The massive data set is accessed in a sequential fashion and, therefore, can be viewed as a stream of data elements. The order of the data elements in the stream is not controlled by the algorithm. There are three important resources considered in the streaming model: the size of the workspace, the number of passes that the algorithm makes over the stream, and the time to process each data element in the stream. In this thesis, we study computational-geometry problems and graph problems in the streaming model. We design algorithms for computing diameter in the streaming and the sliding-window models and prove some corresponding lower bounds. The sliding-window m\n",
      "\n",
      "6. id: 5390b29820f70186a0eea9a8   score: 0.5324859   abstract: We show how to compute the width of a dynamic set of low-dimensional points in the streaming model. In particular, we assume the stream contains both insertions of points and deletions of points to a set S, and the goal is to compute the width of the set S, namely the minimal distance between two parallel lines sandwiching the pointset S. Our algorithm 1 + ε approximates the width of the set S using space polylogarithmic in the size of S and the aspect ratio of S. This is the first such algorithm that supports both insertions and deletions of points to the set S: previous algorithms for approximating the width of a pointset only supported additions [AHPV04, Cha06], or a sliding window [CS06]. This solves an open question from the \"2009 Kanpur list\" of Open Problems in Data Streams, Property Testing, and Related Topics [IMNO11].\n",
      "\n",
      "7. id: 5390bda020f70186a0f47e7d   score: 0.51148784   abstract: In this paper we design a simple streaming algorithm for maintaining two smallest balls (of equal radius) in d-dimension to cover a set of points in an on-line fashion. Different from most of the traditional streaming models, at any step we use the minimum amount of space by only storing the locations and the (common) radius of the balls. Previously, such a geometric algorithm is only investigated for covering with one ball (one-center) by Zarrabi-Zadeh and Chan (2006) [16]. We give an analysis of our algorithm, which is significantly different from the one-center algorithm due to the obvious possibility of grouping points wrongly under this streaming model. We show that our algorithm has an approximation ratio 2 for d=1 and at most 5.611 for any fixed d1. We also present lower bounds of 1.5 and 1.604 for the problem in the d=1 and d1 cases respectively.\n",
      "\n",
      "8. id: 5390bf1320f70186a0f51877   score: 0.50784236   abstract: This paper deals with computing the smallest enclosing ball of a set of points subject to probabilistic data. In our setting, any of the n points may not or may occur at one of finitely many locations, following its own discrete probability distribution. The objective is therefore considered to be a random variable and we aim at finding a center minimizing the expected maximum distance to the points according to their distributions. Our main contribution presented in this paper is the first polynomial time (1 + &epsis;)-approximation algorithm for the probabilistic smallest enclosing ball problem with extensions to the streaming setting.\n",
      "\n",
      "9. id: 5390882420f70186a0d894b0   score: 0.49540725   abstract: We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop the concept of list-efficient streaming algorithms that are essential to the design of efficient streaming algorithms through reductions.Our results include a suite of list-efficient streaming algorithms for basic statistical primitives. Using the reduction paradigm along with these tools, we design streaming algorithms for approximately counting the number of triangles in a graph presented as a stream.A specific highlight of our work is the first algorithm for the number of distinct elements in a data stream that achieves arbitrary approximation factors. (Independently, Trevisan [Tre01] has solved this problem via a different approach; our algorithm has the advantage of being list-efficient.)\n",
      "\n",
      "10. id: 55924d9a0cf26384af04a354   score: 0.45725787   abstract: Given an n -point set in a d -dimensional space and an integer k , consider the problem of finding a smallest ball enclosing at least k of the points. In the case of a fixed dimension, the problem is polynomial-time solvable but in general, when d is not fixed, the complexity status is not known. We prove the strong NP -hardness in the case of Euclidean space and the ( 2 - ε ) -inapproximability in the case of L ∞ metric. We also present a simple 2-approximation algorithm for any metric, and PTAS for L ∞ -space of dimension O ( log n ) .\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675604\n",
      "index                                        5591318a0cf2127aa930c159\n",
      "title               Challenges in Developing a Collaborative Robot...\n",
      "authors                        Vaibhav Vasant Unhelkar, Julie A. Shah\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Tenth Annual ACM/IEEE Inter...\n",
      "references          5390a77d20f70186a0e8e776;53909e8b20f70186a0e2e...\n",
      "abstract            Industrial robots are on the verge of emerging...\n",
      "id                                                            1675604\n",
      "clustered_labels                                                    2\n",
      "Name: 1675604, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a8b220f70186a0e9c7c7   score: 0.9712385   abstract: It is believed that from this point forward, there will be a need for industrial robots that work alongside and cooperatively with humans. However, the current development of existing robotics technology is inadequate to ensure the safety of such new industrial robots. Our research proposes a safety-planning technology called Coexistence Hazard Avoidance Technology, designed for use with a low-powered human-collaborative industrial robot. We load this technology into a risk-management simulator and verify that by using it, a dynamic planning method for the safe operation of robots can be reasonably undertaken in terms of both theory and calculation.\n",
      "\n",
      "2. id: 5390b7fe20f70186a0f263ea   score: 0.9458012   abstract: Traditional automotive manufacturing tasks that are automated to date by industrial robots utilize a human robot exclusion strategy for normal production. Although new generations of robots are envisioned to emulate humans' capabilities to collaborate with people, we propose a near term and a practical strategy for robots to complement humans' capabilities instead in order to achieve a shared goal in a shared context of manufacturing task execution. We describe two types, transitional and partnership, of human and robot collaborations in the automotive manufacturing environment. We illustrate both types of human robot collaboration with detailed manufacturing tasks. We explore potential solutions to the human and robot collaboration problem in manufacturing: how to interact optimally and fail-safely for a given task collaboration between a human and a robot.\n",
      "\n",
      "3. id: 558fce5a612c29c89cd7b366   score: 0.9436599   abstract: While human-robot collaboration has been studied intensively in the literature, little attention has been given to understanding the role of collaborative endeavours on enhancing the companionship between humans and robots. In this position paper, we explore the possibilities of building the human-robot companionship through collaborative activities. The design guideline of a companion robot Nancy developed at SRL is introduced, and preliminary studies on human-robot collaboration conducted at SRL and I2R are elaborated. Critical issues and technical challenges in human-robot collaboration systems are discussed. Through these discussions, we aim to draw the attention of the social robotics community to the importance of human-robot collaboration in companionship building, and stimulate more research effort in this emerging area.\n",
      "\n",
      "4. id: 5390ab8820f70186a0eb1bd3   score: 0.93045825   abstract: Efficient cooperation of humans and industrial robots is based on a common understanding of the task as well as the perception and understanding of the partner's action in the next step. In this article, a hybrid assembly station is presented, in which an industrial robot can learn new tasks from worker instructions. The learned task is performed by both the robot and the human worker together in a shared workspace. This workspace is monitored using multi-sensory perception for detecting persons as well as objects. The environmental data are processed within the collision avoidance module to provide safety for persons and equipment. The real-time capable software architecture and the orchestration of the involved modules using a knowledge-based system controller is presented. Finally, the functionality is demonstrated within an experimental cell in a real-world production scenario.\n",
      "\n",
      "5. id: 5390a8b220f70186a0e9c755   score: 0.9260366   abstract: Direct physical human-robot interaction has become a central part in the research field of robotics today. To use the advantages of the potential for humans and robots to work together as a team in industrial settings, the most important issues are safety for the human and an easy way to describe tasks for the robot. In this work, we present an approach of a hierarchical structured control of industrial robots for joint-action scenarios. Multiple atomic tasks including dynamic collision avoidance, operational position, and posture can be combined in an arbitrary order respecting constraints of higher priority tasks. The controller flow is based on the theory of orthogonal projection using nullspaces and constraint least-square optimization. To proof the approach, we present three collaboration scenarios between a human and an industrial robot.\n",
      "\n",
      "6. id: 5390a5dc20f70186a0e7f384   score: 0.91550726   abstract: In this paper, we present a novel approach for multimodal interactions between humans and industrial robots. The application scenario is situated in a factory, where a human worker is supported by a robot to accomplish a given hybrid assembly scenario, that covers manual and automated assembly steps. The robot is acting as an assistant as well as a fully autonomous assembly unit. For interacting with the presented system, the human is able to give his commands via three different input modalities (speech, gaze and the so-called soft-buttons).\n",
      "\n",
      "7. id: 5390ba0a20f70186a0f349b8   score: 0.9147487   abstract: In order to develop intelligent robots that are able to cooperate well with human in a collaborative work, this work aims to integrate work sequence and embodied interaction capabilities into an integrated intelligence system for collaborative robot development. A task modeling approach is proposed to build a hierarchical task model of the entire work sequence to generate state transition table, and grounding with the actual condition (state) of the objects and the action (transition) of work in the embodied dimension. The system is implemented in a simulation environment with human interaction to materialize human-robot collaboration by robot work support in work sequence (what, when and how), assist on parallel task, and error correction.\n",
      "\n",
      "8. id: 5390990f20f70186a0e103a2   score: 0.85776806   abstract: Human-robot interaction requires explicit reasoning on the human environment and on the robot capacities to achieve its tasks in a collaborative way with a human partner.This paper focuses on organization of the robot decisional abilities and more particularly on the management of human interaction as an integral part of the robot control architecture. Such an architecture should be the framework that will allow the robot to accomplish its tasks but also produce behaviors that support its engagement vis-a-vis its human partner and interpret similar behaviors from him.Together and in coherence with this framework, we intend to develop and experiment various task planners and interaction schemes, that will allow the robot to select and perform its tasks while taking into account explicitly the constraints imposed by the presence of humans, their needs and preferences.We have considered a s\n",
      "\n",
      "9. id: 558b5de1612caa4dd659f073   score: 0.84273374   abstract: We participated DARPA Robotics Challenges (DRC) Trial in Homestead, Florida, USA in December 2013. In this paper, we describe human robot collaboration (HRC) extending typical human machine interaction (HMI) for human and robot to work together to achieve better performance where either one cannot achieve alone. We use interactive model fitting as an example to illustrate how human collaborates with robot to leverage strength of each other to accomplish the fitting.\n",
      "\n",
      "10. id: 5390a79f20f70186a0e90931   score: 0.8003801   abstract: We present a new framework for controlling a robot collaborating with a human to accomplish a common mission. Knowing that we are interested in collaboration domains where there is no shared plan between the human and the robot, the constraints on the decision process are more challenging. We study the decision process of a robot agent for a specific shared mission with a human considering the effect of the human presence, the planning flexibility according to human comfortability and achieving mission. We choose to formalize this problem with Partially Observable Markov Decision Process, then we describe a new domain example that represent human-robot collaboration with no shared plan and we show some preliminary results of solving the POMDP model with standard optimal algorithms as a base work to compare with state-of-the-art and future-work approximate algorithms.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1683488\n",
      "index                                        55923d2b612c4fa28ff7a356\n",
      "title               Rough set over dual-universes in intuitionisti...\n",
      "authors                         Zhi-Lian Guo, Hai-Long Yang, Jue Wang\n",
      "year                                                           2015.0\n",
      "venue               Journal of Intelligent & Fuzzy Systems: Applic...\n",
      "references          5390a80f20f70186a0e96dbd;539087d420f70186a0d5e...\n",
      "abstract            In this paper we generalize the rough set mode...\n",
      "id                                                            1683488\n",
      "clustered_labels                                                    1\n",
      "Name: 1683488, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390aa0e20f70186a0ea7d18   score: 0.99187535   abstract: In this paper, rough approximations of intuitionistic fuzzy sets with respect to an intuitionistic fuzzy approximation space are first introduced. Basic properties of intuitionistic fuzzy rough sets are then examined. Finally, roughness measures of intuitionistic fuzzy sets are defined and their properties are explored.\n",
      "\n",
      "2. id: 5390b72d20f70186a0f20433   score: 0.989791   abstract: Intuitionistic fuzzy rough sets are investigated in a general framework which includes generalizations of many related results in early literatures. A new definition of intuitionistic fuzzy rough sets is given with the analysis of its basic properties based on the notion of two universes, general binary relations, and a pair (T, I) of intuitionistic fuzzy t-norm T and intuitionistic fuzzy implicator I. For t-representable intuitionistic fuzzy t-norms and its residual implicators, intuitionistic fuzzy rough approximation operators are defined by axioms, and its connections with special intuitionistic fuzzy relations are investigated.\n",
      "\n",
      "3. id: 5390a55520f70186a0e79bfe   score: 0.9890131   abstract: A fuzzy rough set is a fuzzy generalization of rough set. There are already several definitions for it, and most of them are given with reference to a t-norm *, a fuzzy (*-)similarity relation and the duality principle. In this paper, a generalization of fuzzy rough sets is investigated regarding a general fuzzy relation and a lower semi-continuous fuzzy conjunction logical operator in its second argument. The generalized fuzzy rough approximation operators are established by using the adjunction between the fuzzy conjunction operator and a fuzzy implication operator. Algebraic properties of the generalized fuzzy rough approximation operators are discussed. It has been shown that information with much more necessity measure and with less probability measure for a fuzzy set can be mined in comparison with existing methods of fuzzy rough sets.\n",
      "\n",
      "4. id: 5390a55520f70186a0e79688   score: 0.9862046   abstract: The intuitionistic fuzzy rough set is introduced in an attempt to solve certain problems in incomplete information systems. Some properties of the corresponding intuitionistic fuzzy set are studied and application of the intuitionistic fuzzy set is explored by illustrations.\n",
      "\n",
      "5. id: 5390a28020f70186a0e61420   score: 0.9743861   abstract: The objective of this paper is to provide a new generalizations of rough fuzzy sets in generalized approximation space. The equivalance relations are viewed as a special type of an binary relations of a universe. Then, the rough fuzzy sets in generalized approximation space is defined. Furthermore, the relationships between generalizations rough fuzzy sets and the rough fuzzy sets, which is proposed by Dubois and Prade first time, Pawlak rough set , etc are investigated. Some propseties of the generalizations rough fuzzy sets are discussed. Meanwhile, the roughness and precision of the generalizations rough fuzzy sets are illuminated, too. At last, the definitions of the rough fuzzy sets in generalized approximation space based on the others binary relations are built.\n",
      "\n",
      "6. id: 5390bae520f70186a0f3a34e   score: 0.9732915   abstract: Based on the analysis of rough set model on a tolerance relation and the theory of fuzzy rough set, a new extended rough set model is constructed, which is the optimistic multi-granulation fuzzy rough sets based on tolerance relations. It follows the research on the properties of the lower and upper approximations of the new multi-granulation fuzzy rough set models based on tolerance relations. From which it can be found that the new extended model is a generalized rough set model based on tolerance relations from the perspective of granulation.\n",
      "\n",
      "7. id: 53909fbd20f70186a0e43783   score: 0.9730877   abstract: The notion of a rough set was originally proposed by Pawlak (1982). Later on, there is a fast growing interest in this theory. In this paper we present a more general approach to the generalization of rough sets. Specifically, generalized formulation has been studied by using a binary relation on two universes without any restriction on the car- dinality. The algebraic properties of generalized rough sets are given, and the extension principle for crisp sets is ex- plained as the upper approximations of rough sets. The re- lationship between the extension principle for fuzzy sets and the upper approximations of fuzzy rough sets is investigated.\n",
      "\n",
      "8. id: 5390ad8920f70186a0ec0f9f   score: 0.9657707   abstract: The primitive notions in rough set theory are lower and upper approximation operators defined by a fixed binary relation and satisfy many interesting properties. Many types of generalized rough set models have been developed in the literature. This paper investigates intuitionistic fuzzy rough sets resulted from approximations of intuitionistic fuzzy sets with respect to an arbitrary intuitionistic fuzzy approximation space. By employing two intuitionistic fuzzy implicators, a pair of lower and upper intuitionistic fuzzy rough approximation operators are first defined. Properties of intuitionistic fuzzy rough approximation operators are then presented.\n",
      "\n",
      "9. id: 5390b36120f70186a0ef2829   score: 0.96073127   abstract: Rough set theory is developed based on the notion of equivalence relation, but the property of equivalence has limited its application fields, which may not provide a realistic description of real-world relationships between elements. The paper presents a transition from the equivalence relation to the compatibility relation, called Compatibility Rough Set Theory or, in short, CRST. A specific type of fuzzy compatibility relations, called conditional probability relations, is discussed. All basic concepts or rough set theory are extended. Generalized rough set approximations are defined by using coverings of the universe induced by a fuzzy compatibility relation. Generalized rough membership functions are defined and their properties are examined.\n",
      "\n",
      "10. id: 5390ac5720f70186a0eb50d6   score: 0.9517737   abstract: In this paper, in the framework of many-valued logic, the crisp lower and upper approximation operators of rough set theory are generalized to fuzzy environment, and the basic properties of that two operators are studied. Also, it is proved that our upper approximation operator is a generalization of $T$-upper fuzzy approximation operator defined by Mi [cf. J.S. Mi, Y. Leung, H.Y. Zhao, T. Feng, Generalized fuzzy rough sets determined by a triangular norm, Information Sciences 178(2008) 3203-3213], but the lower approximation is quite different from the corresponding $T$-lower fuzzy approximation operator.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1655972\n",
      "index                                        559145430cf232eb904fb8b0\n",
      "title               A Hybrid Task Mapping Algorithm for Heterogene...\n",
      "authors                                    Wei Quan, Andy D. Pimentel\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Embedded Computing Systems...\n",
      "references          5390a0b720f70186a0e4f81d;5390a17720f70186a0e52...\n",
      "abstract            The application workloads in modern MPSoC-base...\n",
      "id                                                            1655972\n",
      "clustered_labels                                                    3\n",
      "Name: 1655972, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bb1d20f70186a0f3d4e5   score: 0.99903023   abstract: The application workloads in modern MPSoC-based embedded systems are becoming increasingly dynamic. Different applications concurrently execute and contend for resources in such systems which could cause serious changes in the intensity and nature of the workload demands over time. To cope with the dynamism of application workloads at run time and improve the efficiency of the underlying system architecture, this paper presents a novel scenario-based run-time task mapping algorithm. This algorithm combines a static mapping strategy based on workload scenarios and a dynamic mapping strategy to achieve an overall improvement of system efficiency. We evaluated our algorithm using a homogeneous MPSoC system with three real applications. From the results, we found that our algorithm achieves an 11.3% performance improvement and a 13.9% energy saving compared to running the applications withou\n",
      "\n",
      "2. id: 5390a8db20f70186a0e9d27f   score: 0.99097973   abstract: Task mapping is an important issue in MPSoC design. Most recent mapping algorithms perform them at design time, an approach known as static mapping. Nonetheless, applications running in MPSoCs may execute a varying number of simultaneous tasks. In some cases, applications may be defined only after system design, enforcing a scenario that requires the use of dynamic task mapping. Static mappings have as main advantage the global view of the system, while dynamic mappings normally provide a local view, which considers only the neighborhood of the mapping task. This work aims to evaluate the pros and cons of static and dynamic mapping solutions. Due to the global system view, it is expected that static mapping algorithms achieve superior performance (w.r.t. latency, congestion, energy consumption). As dynamic scenarios are a trend in present MPSoC designs, the cost of dynamic mapping algori\n",
      "\n",
      "3. id: 5390ad8920f70186a0ec0748   score: 0.9861514   abstract: Multi-Processor Systems-on-Chip (MPSoC) are an increasingly important design paradigm not only for mobile embedded systems but also for industrial applications such as automotive and avionic systems. Such systems typically execute multiple concurrent applications, with different execution modes. Modes define differences in functionality and computational resource demands and are assigned with an execution probability. We propose a dynamic mapping approach to maintain low power consumption over the system lifetime. Mapping templates for different application modes and execution probabilities are computed offline and stored on the system. At runtime a manager monitors the system and chooses an appropriate pre-computed template. Experiments show that our approach outperforms global static mapping approaches up to 45%.\n",
      "\n",
      "4. id: 5390b19020f70186a0edea74   score: 0.9853843   abstract: Modern embedded systems are based on Multiprocessor-Systems-on-Chip (MPSoCs) to meet the strict timing deadlines of multiple applications. MPSoC resources must be utilized efficiently by mapping the applications in throughput-aware manner in order to meet throughput constraints for each of them. A design-time methodology is applicable only to predefined set of applications with static behavior, which is incapable of handling dynamism in applications. On the other hand, a run-time approach can cater to the dynamism but cannot provide timing guarantees for all the applications due to large computation requirements at run-time. This paper presents a hybrid flow which performs compute intensive analysis at design-time to derive multiple resource-throughput trade-off points and selects one of these at run-time subject to available resources and desired throughput. Experimental results show th\n",
      "\n",
      "5. id: 5390b0ca20f70186a0edaaa0   score: 0.9846347   abstract: Task mapping defines the best placement of a given task in the MPSoC, according to some criteria, as energy or Manhattan distance minimization. The ITRS roadmap forecast in a near future MPSoCs with hundreds of processing elements (PEs). Therefore, dynamic mapping heuristics are required. An important gap is observed in the mapping literature: the lack of proposals targeting multi-task dynamic mapping. In this context, the present work proposes an energy-aware dynamic task mapping heuristic, allowing multiple tasks allocation per PE. Experimental results are executed in an actual MPSoC running distributed applications. Comparing a single-task to the multi-task mapping, the energy spent in the NoC is reduced in average by 51% (best case: 72%), with an average execution time overhead of 18%. Besides the communication energy reduction, the multi-task mapping enables a greater number of appl\n",
      "\n",
      "6. id: 5390ad0620f70186a0eba752   score: 0.983343   abstract: Design-time application mapping is limited to a predefined set of applications and a static platform. Resource management at run-time is required to handle future changes in the application set, and to provide some degree of fault tolerance, due to imperfect production processes and wear of materials. This paper concerns resource allocation at run-time, allowing multiple real-time applications to run simultaneously on a heterogeneous MPSoC. Low-complexity algorithms are required, in order to respond fast enough to unpredictable execution requests. We present a decomposition of this problem into four phases. The allocation of tasks to specific locations in the platform is the main contribution of this work. Experiments on a real platform show the feasibility of this approach, with execution times in tens of milliseconds for a single allocation attempt.\n",
      "\n",
      "7. id: 5390ba0a20f70186a0f338d8   score: 0.98201376   abstract: The mapping of tasks to processing elements of an MPSoC has critical impact on system performance and energy consumption. To cope with complex dynamic behavior of applications, it is common to perform task mapping during runtime so that the utilization of processors and interconnect can be taken into account when deciding the allocation of each task. This paper has two major contributions, one of them targeting the general problem of evaluating dynamic mapping heuristics in NoC-based MPSoCs, and another focusing on the specific problem of finding a task mapping that optimizes energy consumption in those architectures.\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3d463   score: 0.97118384   abstract: The reliance on multi/many-core systems to satisfy the high performance requirement of complex embedded software applications is increasing. This necessitates the need to realize efficient mapping methodologies for such complex computing platforms. This paper provides an extensive survey and categorization of state-of-the-art mapping methodologies and highlights the emerging trends for multi/many-core systems. The methodologies aim at optimizing system's resource usage, performance, power consumption, temperature distribution and reliability for varying application models. The methodologies perform design-time and run-time optimization for static and dynamic workload scenarios, respectively. These optimizations are necessary to fulfill the end-user demands. Comparison of the methodologies based on their optimization aim has been provided. The trend followed by the methodologies and open \n",
      "\n",
      "9. id: 5390b1d220f70186a0ee2d85   score: 0.9687381   abstract: Design-time strategies are suited only for mapping predefined set of applications and thus cannot predict dynamic behavior. This dynamism demands run-time mapping of application tasks to maintain a critical balance between performance and resource optimization. This paper proposes a run-time heuristic that intelligently distributes the application tasks among multiple processors taking communication overhead, computation load and resource utilization in consideration.\n",
      "\n",
      "10. id: 53908bfb20f70186a0dca60c   score: 0.9650533   abstract: A mapping algorithm for heterogeneous computing systemsis proposed in this paper. This algorithm utilizes a new indicator - the relative cost - to obtain optimal mapping.The existing Min-min algorithm can be well explainedunder synergy of this new indicator. It is found that the Min-min algorithm leaves room for improvement because of itshaste to reduce completion time by overlooking the impactof load balance. Our new algorithm retains the advantagesof the Min-min algorithm and balances the load very well.It demonstrates the ability to generate good mapping in various heterogeneous environments.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672725\n",
      "index                                        559257ab0cf205530abc97fc\n",
      "title               Interactive continuous collision detection for...\n",
      "authors             Liang He, Ricardo Ortiz, Andinet Enquobahrie, ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 19th Symposium on Interacti...\n",
      "references          558b7d04612c6b62e5e89c73;558b8e6b612c6b62e5e8b...\n",
      "abstract            We present a fast algorithm for continuous col...\n",
      "id                                                            1672725\n",
      "clustered_labels                                                    0\n",
      "Name: 1672725, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a5dc20f70186a0e7ed0c   score: 0.9190633   abstract: Collision detection between deformable models is one of fundamental tools of various applications including games. Collision detection can be classified into two categories: discrete and continuous collision detection methods. Discrete collision detection (DCD) has been demonstrated to show the interactive performance by using bounding volume hierarchies (BVHs). However, some colliding primitives may be missed since DCD methods find intersecting primitives only at discrete time steps. This issue can be a very serious problem in physical based simulation, CAD/CAM applications and etc. On the other hand, continuous collision detection (CCD) identifies the first time of contact of colliding primitives during a time interval between two discrete time steps.\n",
      "\n",
      "2. id: 5390893e20f70186a0d9328d   score: 0.9180404   abstract: We present a novel bounding volume hierarchy that allows for extremely small data structure sizes while still performing collision detection as fast as other classical hierarchical algorithms in most cases. The hierarchical data structure is a variation of axis-aligned bounding box trees. In addition to being very memory efficient, it can be constructed efficiently and very fast.We also propose a criterion to be used during the construction of the BV hierarchies is more formally established than previous heuristics. The idea of the argument is general and can be applied to other bounding volume hierarchies as well. Furthermore, we describe a general optimization technique that can be applied to most hierarchical collision detection algorithms.Finally, we describe several box overlap tests that exploit the special features of our new BV hierarchy. These are compared experimentally among e\n",
      "\n",
      "3. id: 5390a93b20f70186a0ea013a   score: 0.8989441   abstract: We present a novel parallel algorithm for fast continuous collision detection (CCD) between deformable models using multi-core processors. We use a hierarchical representation to accelerate these queries and present an incremental algorithm that exploits temporal coherence between successive frames. Our formulation distributes the computation among multiple cores by using fine-grained front-based decomposition. We also present efficient techniques to reduce the number of elementary tests and analyze the scalability of our approach. We have implemented the parallel algorithm on eight core and 16 core PCs, and observe up to 7x and 13x speedups respectively, on complex benchmarks.\n",
      "\n",
      "4. id: 5390893e20f70186a0d9328c   score: 0.896614   abstract: Many collision detection methods have been proposed. Most of them can only be applied to rigid objects. In general, these methods precompute some geometric information of each object, such as bounding boxes, to be used for run-time collision detection. However, if the object deforms, the precomputed information may not be valid anymore and hence needs to be recomputed in every frame while the object is deforming. In this paper, we presents an efficient collision detection framework for deformable objects, which considers both inter-collisions and self-collisions of deformable objects modeled by NURBS surfaces. Towards the end of the paper, we show some experimental results to demonstrate the performance of the new method.\n",
      "\n",
      "5. id: 539089bb20f70186a0d9884c   score: 0.85536855   abstract: Collision detection is of paramount importance for many applications in computer graphics and visualization. Typically, the input to a collision detection algorithm is a large number of geometric objects comprising an environment, together with a set of objects moving within the environment. In addition to determining accurately the contacts that occur between pairs of objects, one needs also to do so at real-time rates. Applications such as haptic force-feedback can require over 1,000 collision queries per second.In this paper, we develop and analyze a method, based on bounding-volume hierarchies, for efficient collision detection for objects moving within highly complex environments. Our choice of bounding volume is to use a \"discrete orientation polytope\" (\"k-dop\"), a convex polytope whose facets are determined by halfspaces whose outward normals come from a small fixed set of k orien\n",
      "\n",
      "6. id: 5390a77d20f70186a0e8fd2f   score: 0.84972197   abstract: In many interactive computer graphics applications, users are represented as virtual characters called avatars.Collision detection is a pre-requisite in interactive computer graphics application so that appropriate response and realistic behaviour can be generated.However, it will not be easy to anticipate collisions since objects’ motion are dependant on user interaction and avatar manipulation.Ideally, collision detection needs to be done efficiently, with accurate result in the shortest time possible. However, due to its computationally intensive nature, collision detection could cause a bottleneck to the system. A variety of techniques, data structures and algorithms were proposed to accelerate the collision detection process.Acceleration data structures are widely used not only to promote faster collision detection, but also in the field of ray tracing and deformable objects simulat\n",
      "\n",
      "7. id: 5390a80e20f70186a0e95d2b   score: 0.8468312   abstract: We present a simple and fast algorithm to perform continuous collision detection between polygonal models undergoing rigid motion for interactive applications. Our approach can handle all triangulated models and makes no assumption about the underlying geometry and topology. The algorithm uses the notion of conservative advancement (CA), originally developed for convex polytopes [1], [2]. We extend this formulation to general models using swept sphere volume hierarchy and present a compact formulation to compute the motion bounds along with a novel controlling scheme. We have implemented the algorithm and highlight its performance on various benchmarks. In practice, our algorithm can perform continuous collision queries in few milli-seconds on models composed of tens of thousands of triangles.\n",
      "\n",
      "8. id: 53908bcc20f70186a0dc5571   score: 0.83775276   abstract: In the animation of deformable objects, collision detection and response are crucial for the performance. Furthermore, a physically correct cloth simulation requires robust collision avoidance, since any overlapping is visible and often results in expensive correction procedures. Much progress has been achieved in improving the numerical solution, and therefore most animations employ large time steps for fast simulations. This even more demands for accurate collision detection and response.In this work we show how collision detection for deformable meshes can be extended to detect proximities in advance. Several heuristics are introduced to save computation time, and constraints ensure an accurate collision response.\n",
      "\n",
      "9. id: 53909fca20f70186a0e45246   score: 0.8200375   abstract: We present a new approach to accelerate collision detection for deformable models. Our formulation applies to all triangulated models and significantly reduces the number of elementary tests between features of the mesh, i.e., vertices, edges and faces. We introduce the notion of Representative-Triangles, standard geometric triangles augmented with mesh feature information and use this representation to achieve better collision query performance. The resulting approach can be combined with bounding volume hierarchies and works well for both inter-object and self-collision detection. We demonstrate the benefit of Representative-Triangles on continuous collision detection for cloth simulation and N-body collision scenarios. We observe up to a one-order of magnitude reduction in feature-pair tests and up to a 5X improvement in query time.\n",
      "\n",
      "10. id: 5390b3da20f70186a0ef72b4   score: 0.81946033   abstract: A fast collision detection and resolution scheme is one of the key components for interactive simulation of deformable objects. It is particularly challenging to reduce the computational cost in collision detection and to achieve the robust treatment at the same time. Since the shape and topology of a deformable object changes continuously unlike the rigid body, an efficient and effective collision detection and resolution is a major challenge. We present a fast and robust collision detection and resolution scheme for deformable objects using a new enhanced spherical implicit surface hierarchy. The penetration depth and separating distance criteria can be adjusted depending on the application specific error tolerance. Our comparative experiments show that the proposed method performs substantially faster than existing algorithms for deformable object simulation with massive element-level\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710778\n",
      "index                                        55323be645cec66b6f9dad13\n",
      "title               Automatic Lung Segmentation Using Control Feed...\n",
      "authors             Norliza M. Noor, Joel C. Than, Omar M. Rijal, ...\n",
      "year                                                           2015.0\n",
      "venue                                      Journal of Medical Systems\n",
      "references          558b8450612c6b62e5e8a7f1;559256ca0cf2aff368683...\n",
      "abstract            Interstitial Lung Disease (ILD) encompasses a ...\n",
      "id                                                            1710778\n",
      "clustered_labels                                                    1\n",
      "Name: 1710778, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b4c420f70186a0efe5aa   score: 0.98427594   abstract: Interstitial lung diseases (ILDs) are regrouping over 150 heterogeneous disorders of the lung parenchyma. High---Resolution Computed Tomography (HRCT) plays an important role in diagnosis, as standard chest x---rays are often non---specific for ILDs. Assessment of ILDs is considerd hard for clinicians because the diseases are rare, patterns often look visually similar and various clinical data need to be integrated. An image retrieval system to support interpretation of HRCT images by retrieving similar images is presented in this paper. The system uses a wavelet transform based on Difference of Gaussians (DoG) in order to extract texture descriptors from a set of 90 image series containing 1679 manually annotated regions corresponding to various ILDs. Visual words are used for feature aggregation and to describe tissue patterns. The optimal scale---progression scheme, number of visual w\n",
      "\n",
      "2. id: 5390b48420f70186a0efbc30   score: 0.9635748   abstract: In this paper, a computer–aided diagnosis (CAD) system that retrieves similar cases affected with an interstitial lung disease (ILDs) to assist the radiologist in the diagnosis workup is presented and evaluated. The multimodal inter–case distance measure is based on a set of clinical parameters as well as automatically segmented 3–dimensional regions of lung tissue in high–resolution computed tomography (HRCT) of the chest. A global accuracy of 75.1% of correct matching among five classes of lung tissues as well as a mean average retrieval precision at rank 1 of 71% show that automated lung tissue categorization in HRCT data is complementary to case–based retrieval both from the user's viewpoint and also on the algorithmic side.\n",
      "\n",
      "3. id: 558fcacb612c29c89cd7b1f4   score: 0.9305845   abstract: In diagnosing interstitial lung disease (ILD) using HRCT Thorax images, the radiologists required to view large volume of images (30 slices scanned at 10 mm interval or 300 slices scanned at 1 mm interval). However, in the development of scoring index to assess the severity of the disease, viewing 3 to 5 slices at the predetermined levels of the lung is suffice for the radiologist. To develop an algorithm to determine the severity of the ILD, it is important for the computer aided system to capture the main anatomy of the chest, namely the lung and heart at these 5 predetermined levels. In this paper, an automatic segmentation algorithm is proposed to obtain the shape of the heart and lung. In determine the quality of the segmentation, ground truth or manual tracing of the lung and heart boundary done by senior radiologist was compared with the result from the proposed automatic segmenta\n",
      "\n",
      "4. id: 53909f8c20f70186a0e40528   score: 0.87191176   abstract: The preprocessing step of most Computer-Aided Diagnosis (CAD) systems for identifying the lung diseases is lung segmentation. We present a novel lung segmentation technique based on anisotropic diffusion and morphological operation which is performed fast and accurately. The proposed method consists of three steps. At first step, gray image is produced by the input image. And then anisotropic diffusion is preformed to blur the gray image. The second step is that morphological operation is performed to remove the airway and mediastinum and get the right and left lung area. At the third step, the binary image of the right and left lung area obtained in the second step is generated and is matched to the original to segment the lung part. The proposed method eliminates the tasks of finding an optimal threshold and separating the attached left and right lungs .We have applied our new approach\n",
      "\n",
      "5. id: 5390b2d620f70186a0eeb922   score: 0.8564525   abstract: We propose an automatic segmentation method for accurately identifying lung surfaces, airways, and pulmonary vessels in chest CT images. Our method consists of four steps. First, lungs and airways are extracted by inverse seeded region growing and connected component labeling. Second, pulmonary vessels are extracted from the result of first step by gray-level thresholding. Third, trachea and large airways are delineated from the lungs by three-dimensional region growing based on partitioning. Finally, accurate lung regions are obtained by subtracting the result of third step from the result of first step. The proposed method has been applied to 10 patient datasets with lung cancer or pulmonary embolism. Experimental results show that our segmentation method extracts lung surfaces, airways, and pulmonary vessels automatically and accurately.\n",
      "\n",
      "6. id: 55323c5f45cec66b6f9dbe8f   score: 0.8493474   abstract: Computer-aided detection and diagnosis (CAD) has been widely investigated to improve radiologists¿ diagnostic accuracy in detecting and characterizing lung disease, as well as to assist with the processing of increasingly sizable volumes of imaging. Lung segmentation is a requisite preprocessing step for most CAD schemes. This paper proposes a parameter-free lung segmentation algorithm with the aim of improving lung nodule detection accuracy, focusing on juxtapleural nodules. A bidirectional chain coding method combined with a support vector machine (SVM) classifier is used to selectively smooth the lung border while minimizing the over-segmentation of adjacent regions. This automated method was tested on 233 computed tomography (CT) studies from the lung imaging database consortium (LIDC), representing 403 juxtapleural nodules. The approach obtained a 92.6% re-inclusion rate. Segmentati\n",
      "\n",
      "7. id: 5390ac5720f70186a0eb6f44   score: 0.8472108   abstract: Estimation of the volume of the lungs and the viable lung tissue is an important step in the management of patients with severe pulmonary disease. The presence of gross pathology makes it impossible to perform lung segmentation automatically and reliably in CT scans of such patients. An interactive system for lung segmentation is presented, based on precomputed compact regions with homogeneous texture for which general texture feature have been computed. A statistical classifier trained on prior data has classified these regions beforehand and the user corrects any errors until the segmentation of an entire slice is correct. The system proceeds to subsequent slices, which were preclassified using a combined classification strategy that uses both the prior data and the previously approved slices from the test scan. The resulting lung segmentations show a large overlap and a small average \n",
      "\n",
      "8. id: 5390b19020f70186a0edfe7d   score: 0.80115914   abstract: Although radiologists can employ CAD systems to characterize malignancies, pulmonary fibrosis and other chronic diseases; the design of imaging techniques to quantify infectious diseases continue to lag behind. There exists a need to create more CAD systems capable of detecting and quantifying characteristic patterns often seen in respiratory tract infections such as influenza, bacterial pneumonia, or tuborculosis. One of such patterns is Tree-in-bud (TIB) which presents thickened bronchial structures surrounding by clusters of micro-nodules. Automatic detection of TIB patterns is a challenging task because of their weak boundary, noisy appearance, and small lesion size. In this paper, we present two novel methods for automatically detecting TIB patterns: (1) a fast localization of candidate patterns using information from local scale of the images, and (2) a Möbius invariant feature ext\n",
      "\n",
      "9. id: 5390b8d720f70186a0f2b216   score: 0.7512834   abstract: A single click ensemble segmentation (SCES) approach based on an existing ''Click & Grow'' algorithm is presented. The SCES approach requires only one operator selected seed point as compared with multiple operator inputs, which are typically needed. This facilitates processing large numbers of cases. Evaluation on a set of 129 CT lung tumor images using a similarity index (SI) was done. The average SI is above 93% using 20 different start seeds, showing stability. The average SI for 2 different readers was 79.53%. We then compared the SCES algorithm with the two readers, the level set algorithm and the skeleton graph cut algorithm obtaining an average SI of 78.29%, 77.72%, 63.77% and 63.76%, respectively. We can conclude that the newly developed automatic lung lesion segmentation algorithm is stable, accurate and automated.\n",
      "\n",
      "10. id: 5390990f20f70186a0e0fe8e   score: 0.75110084   abstract: In general, segmentation is difficult because surrounding soft tissues and organs have similar CT values and sometimes contact with each other. We propose a new technique for automatic segmentation of lung regions and its classification for ground-glass opacity from the segmented lung regions by computer based on a set of the thorax CT images. In this paper, we segment the lung region for extraction of the region of interest employing binarization and labeling process from the inputted each slices images. The region having the largest area is regarded as the tentative lung regions. Furthermore, the ground-glass opacity is classified by correlation distribution on the slice to slice from the extracted lung region with respect to the thorax CT images. Experiment is performed employing twenty six thorax CT image sets and 96% of recognition rates were achieved. Obtained results are shown alo\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675700\n",
      "index                                        559138fd0cf232eb904fb53d\n",
      "title                                                         Art.CHI\n",
      "authors             David England, Linda Candy, Celine Latulipe, T...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390ac1820f70186a0eb3a05;53909a0220f70186a0e1f...\n",
      "abstract            At CHI2014 our two day workshop \\\"Curating the...\n",
      "id                                                            1675700\n",
      "clustered_labels                                                    0\n",
      "Name: 1675700, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4cc28   score: 0.19759925   abstract: Over the last three years the Digital Arts community of CHI has established itself and is a Spotlight for CHI2014. The focus for CHI2014 is the development of a Catalog for the Digital Arts that we hope will lead on to an Art Gallery as a future CHI conference event. This SIG will be preceded by a workshop \"Curating the Digital\" that will have as its outcome a research-informed design of the Catalog. The purpose of the SIG will be to open the Catalog design to wider audience participation and discussion, and invite the CHI community to support the development of the Catalog.\n",
      "\n",
      "2. id: 5390be6620f70186a0f4cc33   score: 0.06656919   abstract: This workshop intends to use the key strength of the CHI Community; research linked to practice, to design an Art Catalog for CHI. The workshop will start with an examination of current research in curating interactive art. The outcomes of the first phase of the workshop will then feed into Design Charrette exercises that will involve prototyping an Art Catalog and developing ideas for presenting a future Art Gallery event as part of the CHI conference. The results from the workshop will then form the basis of an agenda of a Spotlight SIG meeting where we will discuss the nature of the CHI Art Catalog. Workshop outcomes will also be disseminated to a wider audience.\n",
      "\n",
      "3. id: 5390b52620f70186a0f044e7   score: 0.030559862   abstract: This paper builds on the recent CHI2011 SIG on Digital Arts and the work of the author to examine the nature of collaboration between HCI researchers and new media (or digital) artists. We look at three particular collaborative projects spread over a number of years. We examine the lessons for future collaboration so that productive CHI Arts collaborations can flourish to sustain the community. The chief lessons are that such partnerships need; to early and ongoing collaboration between the parties in order to develop mutually agreeable goals, and that practices and techniques on both sides need to develop to support further understanding.\n",
      "\n",
      "4. id: 559143570cf232eb904fb81b   score: 0.0075187176   abstract: This proposal introduces research which is trying to bridge the gap between Computer Human Interaction (CHI) research and Exhibition Design studies through an investigation of how to design enjoyable exhibition events for audiences in China.\n",
      "\n",
      "5. id: 558b1077612c41e6b9d41f5e   score: 0.003429604   abstract: Interactive digital art can create innovative ways to stimulate and engage audience, what could benefit the space where the installation is done. Aiming at promoting the adoption of non-used places through pleasant experiences, we considered an art project that promotes people engagement to make them become the community's wishes expression. We focus on understanding the process to translate the essence of an artistic expression using Information and Communications Technology (ICT). We translated this artistic expression into digital art installation within a socially 'abandoned' space at a workplace. The biggest challenge is to understand how people interact with the dynamic art-system, that, potentially, it leads audience to experience a highly intimate relationship with the installation and the space. Preliminary results reveal a similar behavior in the audience at both installations,\n",
      "\n",
      "6. id: 5590a58d0cf237666fc2747c   score: 0.0031603526   abstract: Demo Hour highlights new prototypes and projects that exemplify innovation and novel forms of interaction.Leah Maestri, EditorThe Art Explorations track at TEI 2012 featured interactive artworks that explore the intersections between materiality, sensory interaction, and computational expression. New works were reviewed by a jury based on relevance, technological quality, aesthetics, and creative inventiveness. These pieces were selected for interactions by Thecla Schiphorst, a media artist and associate professor in the School of Interactive Arts + Technology at Simon Fraser University, and Alissa Antle, also an associate professor in the School of Interactive Arts + Technology at SFU.\n",
      "\n",
      "7. id: 53909a9320f70186a0e21944   score: 0.0028783479   abstract: Interact with digital experiences that move beyond digital tradition, blur the boundaries between art and science, and transform social assumptions. Emerging Technologies presents work from many sub-disciplines of interactive techniques, with a special emphasis on projects that explore science, high-resolution digital-cinema technologies, and interactive art-science narrative.\n",
      "\n",
      "8. id: 5390afc920f70186a0ed239d   score: 0.0022343625   abstract: This SIG proposal, sponsored by the CHI Design Community, looks at the intersection and cross-fertilization between HCI, and Digital and Performance Arts. We consider how the exploration of engaging and meaningful artistic experience can further push the boundaries of HCI research and practice and how tool use and models of evaluation can be explored to assist the development of creative enterprises. We consider how artists' early experiments with technology can inform mainstream design thinking, and how theories and practice in aesthetics can feed into User Experience.\n",
      "\n",
      "9. id: 5390990f20f70186a0e10e07   score: 0.0022083889   abstract: The authors present highlights from the Interactive Art Exhibition at ACM Multimedia's 2005 conference.\n",
      "\n",
      "10. id: 558b3763612c41e6b9d46d93   score: 0.0020911063   abstract: The theme of the workshop is to examine the nature of creative processes as part of designing interactive systems. In particular, the workshop will focus on how to organize design sessions to improve creative processes, for instance by setting up environments or spaces that support creative activities. In order to provide insights for how such environments can be set up, the workshop will explore a number of themes related to creative design processes, including: Individual and social activities, Creativity constraints, Transformation of design ideas, Generative design materials, and Creativity methods.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1697928\n",
      "index                                        558e414b0cf2c779a6477d94\n",
      "title               Increasing the accessibility of learning objec...\n",
      "authors                                                 Katja Niemann\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Fifth International Confere...\n",
      "references          5390b19020f70186a0ee0080;5390b95520f70186a0f2f3ec\n",
      "abstract            Data sets coming from the educational domain o...\n",
      "id                                                            1697928\n",
      "clustered_labels                                                    0\n",
      "Name: 1697928, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b1d220f70186a0ee1122   score: 0.86647177   abstract: In this article, we explore the task of automatically identifying educational materials by classifying documents with respect to their educational value. Through experiments carried out on a dataset of manually annotated documents, we show that the generally accepted notion of a learning object's “educational value” is indeed a property that can be reliably assigned through automatic classification. Moreover, an analysis of cross-topic and cross-domain portability shows that the automatic classifier can be ported to other topics and domains, with minimal performance loss.\n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3d93d   score: 0.84376645   abstract: We propose, in this paper, a model that extracts automatically learning objects as response to a user request. To do this, we proceed by automatically annotating texts with semantic metadata. These metadata will allow us to index and extract learning objects from texts. Thus, our model is composed of two principal parts: the first part consists of a semantic annotation of learning objects according to their semantic categories definition, example, exercise, etc.. The second part uses automatic semantic annotation which is generated by the first part to create a semantic inverted index able to find relevant learning objects for queries associated with semantic categories. We add a secondary part to our model which sorts the results offered to the user according to their relevance. We have implemented a system called SRIDoP, on the basis of the proposed model and we have verified its effec\n",
      "\n",
      "3. id: 5390b1d220f70186a0ee2bd4   score: 0.8264319   abstract: Learning objects have arisen in response to the need of high-quality and reusable instructional materials. The repositories that hold learning objects allow educators to create and share their instructional contents in an organized infrastructure and where information should be easily searchable. Therefore, a key point of these repositories is the way learning objects are categorized. In this paper, we present an approach for classifying semantically learning objects whose metadata are described in Dublin Core. Specifically, our objective is to annotate automatically the subject of the learning object with instances of the DBpedia ontology, that is, to annotate the learning objects repository with linked open data. The syntactic and semantic analysis of the learning object will drive the selection of the most appropriate categories in the linked data repository.\n",
      "\n",
      "4. id: 539099b320f70186a0e1ad04   score: 0.8109285   abstract: In previous work, we suggested a model driven approach allowing for tracking and management of both learning objects' usage and content-related activities of users involved in a wide-distributed Webbased Learning Environment. Here we present an extension of our previous model in order to provide end-users with a personalized search tool. This application recommends learning resources according to their preferences and does not require users to fill in an electronic form; the tool also offers them the opportunity to review learning content by adding tags, comments and ratings. Moreover, work is in progress to automate execution of actions that will facilitate users to be up-to-date with new learning resources matching with their preferences and created by the whole community of actors involved in the learning environment.\n",
      "\n",
      "5. id: 5390b19020f70186a0ee007e   score: 0.7779754   abstract: When users use tags they often have a rich semantic structure in mind, which can not be fully explicated using existing tagging systems. However, a tagging system needs to be simple in order to be successful, otherwise it will not be accepted by users. In our ELWMS.KOM system for the support of self-regulated Resource-Based Learning users can assign specific semantic types to the tags they use in order to manage their web-based learning resources. However studies have shown that most users would appreciate an automatic identification of tag types. In this paper we present a knowledge-based approach for the automatic identification of the tag types used in the ELWMS.KOM system. Evaluations conducted on different corpora show that the algorithm works with an overall accuracy of up to 84%.\n",
      "\n",
      "6. id: 5390b95420f70186a0f2dc0f   score: 0.7496374   abstract: Competence-annotations assist learners to retrieve and better understand the level of skills required to comprehend learning objects. However, the process of annotating learning objects with competence levels is a very time consuming task; ideally, this task should be performed by experts on the subjects of the educational resources. Due to this, most educational resources available online do not enclose competence information. In this paper, we present a method to tackle the problem of automatically assigning an educational resource with competence topics. To solve this problem, we exploit information extracted from external repositories available on the Web, which lead us to a domain independent approach. Results show that automatically assigned competences are coherent and may be applied to automatically enhance learning objects metadata.\n",
      "\n",
      "7. id: 5390975920f70186a0dfd2c0   score: 0.72076154   abstract: Learning objects are fundamental elements of a new conceptual model for content creation and course composition in Web-based education. Learning objectsý metadata (LOM) facilitate adaptive selection of learning objects on the Web as well as the instructional development of Web-based courses. Although there are a number of ongoing LOM standard initiatives a lot of problems with LOMs are reported: Reaching from more technical applications (e.g. data integrity deficiencies) to contextualization problems. In this paper we present an information model for learning objects, capable to solve many of these problems. We use proven object-oriented design patterns for modeling the different metadata elements of learning objects and their relationships.\n",
      "\n",
      "8. id: 5390a30b20f70186a0e68b6a   score: 0.7151257   abstract: The actual changes produced in the e-Learning field are being driven by two primary facts: the first one is the steady increase of information creation and the second is the new collaborative framework of the Web 2.0. While LO (Learning Objects) repositories are mare and more extensive, the automatic tools for search and location of proper learning content in these repositories, based on semantic tags are not yet effective. This article proposes a system architecture that combines automatic techniques of information retrieval with the user's collaborative annotation of the documents, for the semantic indexing of LO in a repository. Collaborative tagging provides real meaning derived from the learning experiences in real user's communities and improves LO reusability in new learning contexts.\n",
      "\n",
      "9. id: 5390a7f520f70186a0e93492   score: 0.7076068   abstract: Current educational systems use advanced mechanisms for adaptation by utilizing available knowledge about the domain. However, describing a domain area in sufficient detail to allow accurate personalization is a tedious and time-consuming task. Only few works are related to the support of teachers by discovering the knowledge from educational material. In this paper we present a method for automated metadata generation addressing the educational knowledge discovery problem. We employ several techniques of data mining with regards to the e-learning environment and evaluate the method on functional programming course.\n",
      "\n",
      "10. id: 5390b19020f70186a0ee0080   score: 0.6738359   abstract: An online presence is gradually becoming an essential part of every learning institute. As such, a large portion of learning material is becoming available online. Incongruently, it is still a challenge for authors and publishers to guarantee accessibility, support effective retrieval and the consumption of learning objects. One reason for this is that non-annotated learning objects pose a major problem with respect to their accessibility. Non-annotated objects not only prevent learners from finding new information; but also hinder a system's ability to recommend useful resources. To address this problem, commonly known as the cold-start problem, we automatically annotate specific learning resources using a state-of-the-art automatic tag annotation method: α-TaggingLDA, which is based on the Latent Dirichlet Allocation probabilistic topic model. We performed a user evaluation with 115 pa\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710318\n",
      "index                                        55323bda45cec66b6f9dab9d\n",
      "title               Maximum likelihood estimation in constrained p...\n",
      "authors                       Francesca Greselin, Salvatore Ingrassia\n",
      "year                                                           2015.0\n",
      "venue                                        Statistics and Computing\n",
      "references          5390b04120f70186a0ed7289;5390ab8820f70186a0eb1...\n",
      "abstract            Mixtures of factor analyzers are becoming more...\n",
      "id                                                            1710318\n",
      "clustered_labels                                                    2\n",
      "Name: 1710318, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909ed120f70186a0e31362   score: 0.98138225   abstract: The likelihood function for normal multivariate mixtures may present both local spurious maxima and also singularities and the latter may cause the failure of the optimization algorithms. Theoretical results assure that imposing some constraints on the eigenvalues of the covariance matrices of the multivariate normal components leads to a constrained parameter space with no singularities and at least a smaller number of local maxima of the likelihood function. Conditions assuring that an EM algorithm implementing such constraints maintains the monotonicity property of the usual EM algorithm are provided. Different approaches are presented and their performances are evaluated and compared using numerical experiments.\n",
      "\n",
      "2. id: 5390b04120f70186a0ed7289   score: 0.9394706   abstract: Model-based clustering typically involves the development of a family of mixture models and the imposition of these models upon data. The best member of the family is then chosen using some criterion and the associated parameter estimates lead to predicted group memberships, or clusterings. This paper describes the extension of the mixtures of multivariate t-factor analyzers model to include constraints on the degrees of freedom, the factor loadings, and the error variance matrices. The result is a family of six mixture models, including parsimonious models. Parameter estimates for this family of models are derived using an alternating expectation-conditional maximization algorithm and convergence is determined based on Aitken's acceleration. Model selection is carried out using the Bayesian information criterion (BIC) and the integrated completed likelihood (ICL). This novel family of m\n",
      "\n",
      "3. id: 5390a63c20f70186a0e813c9   score: 0.9352283   abstract: A model-based clustering approach which contextually performs dimension reduction and variable selection is presented. Dimension reduction is achieved by assuming that the data have been generated by a linear factor model with latent variables modeled as Gaussian mixtures. Variable selection is performed by shrinking the factor loadings though a penalized likelihood method with an L1 penalty. A maximum likelihood estimation procedure via the EM algorithm is developed and a modified BIC criterion to select the penalization parameter is illustrated. The effectiveness of the proposed model is explored in a Monte Carlo simulation study and in a real example.\n",
      "\n",
      "4. id: 5390afc920f70186a0ed35db   score: 0.92441523   abstract: Mixtures of factor analyzers have been receiving wide interest in statistics as a tool for performing clustering and dimension reduction simultaneously. In this model it is assumed that, within each component, the data are generated according to a factor model. Therefore, the number of parameters on which the covariance matrices depend is reduced. Several estimation methods have been proposed for this model, both in the classical and in the Bayesian framework. However, so far, a direct maximum likelihood procedure has not been developed. This direct estimation problem, which simultaneously allows one to derive the information matrix for the mixtures of factor analyzers, is solved. The effectiveness of the proposed procedure is shown on a simulation study and on a toy example.\n",
      "\n",
      "5. id: 5390975920f70186a0dfd4a9   score: 0.9158089   abstract: A mixture of factor analyzer is a semiparametric density estimator that performs clustering and dimensionality reduction in each cluster (component) simultaneously. It performs nonlinear dimensionality reduction by modeling the density as a mixture of local linear models. The approach can be used for classification by modeling each class-conditional density using a mixture model and the complete data is then a mixture of mixtures. We propose an incremental mixture of factor analysis algorithm where the number of components (local models) in the mixture and the number of factors in each component (local dimensionality) are determined adaptively. Our results on different pattern classification tasks prove the utility of our approach and indicate that our algorithms find a good trade-off between model complexity and accuracy.\n",
      "\n",
      "6. id: 53909fbc20f70186a0e41bd6   score: 0.8333246   abstract: Normal mixture models are widely used for statistical modeling of data, including cluster analysis.However maximum likelihood estimation (MLE) for normal mixtures using the EM algorithm may fail as the result of singularities or degeneracies. To avoid this, we propose replacing the MLE by a maximum a posteriori (MAP) estimator, also found by the EM algorithm. For choosing the number of components and the model parameterization, we propose a modified version of BIC, where the likelihood is evaluated at the MAP instead of the MLE. We use a highly dispersed proper conjugate prior, containing a small fraction of one observation's worth of information. The resulting method avoids degeneracies and singularities, but when these are not present it gives similar results to the standard method using MLE, EM and BIC.\n",
      "\n",
      "7. id: 5390ba3820f70186a0f36c36   score: 0.80639565   abstract: In model-based clustering and classification, the cluster-weighted model is a convenient approach when the random vector of interest is constituted by a response variable $$Y$$ and by a vector $${\\varvec{X}}$$ of $$p$$ covariates. However, its applicability may be limited when $$p$$ is high. To overcome this problem, this paper assumes a latent factor structure for $${\\varvec{X}}$$ in each mixture component, under Gaussian assumptions. This leads to the cluster-weighted factor analyzers (CWFA) model. By imposing constraints on the variance of $$Y$$ and the covariance matrix of $${\\varvec{X}}$$ , a novel family of sixteen CWFA models is introduced for model-based clustering and classification. The alternating expectation-conditional maximization algorithm, for maximum likelihood estimation of the parameters of all models in the family, is described; to initialize the algorithm, a 5-step h\n",
      "\n",
      "8. id: 5390b3ae20f70186a0ef40d3   score: 0.7988153   abstract: Mixture models, especially mixtures of Gaussian, have been widely used due to their great flexibility and power. Non-Gaussian clusters can be approximated by several Gaussian components, however, it can not always acquire appropriate results. By cancelling the nonnegative constraint to mixture coefficients and introducing a new concept of “negative components”, we extend the traditional mixture models and enhance their performance without increasing the complexity obviously. Moreover, we propose a parameter estimation algorithm based on an iteration mechanism, which can effectively discover patterns of “negative components”. Experiments on some synthetic data testified the reasonableness of the proposed novel model and the effectiveness of the parameter estimation algorithm.\n",
      "\n",
      "9. id: 53908a4020f70186a0d9d368   score: 0.7967673   abstract: We focus on mixtures of factor analyzers from the perspective of a method for model-based density estimation from high-dimensional data, and hence for the clustering of such data. This approach enables a normal mixture model to be fitted to a sample of n data points of dimension p, where p is large relative to n. The number of free parameters is controlled through the dimension of the latent factor space. By working in this reduced space, it allows a model for each component-covariance matrix with complexity lying between that of the isotropic and full covariance structure models. We shall illustrate the use of mixtures of factor analyzers in a practical example that considers the clustering of cell lines on the basis of gene expressions from microarray experiments.\n",
      "\n",
      "10. id: 5390a8dc20f70186a0e9e1cc   score: 0.7816641   abstract: Motivation: Model-based clustering has been widely used, e.g. in microarray data analysis. Since for high-dimensional data variable selection is necessary, several penalized model-based clustering methods have been proposed tørealize simultaneous variable selection and clustering. However, the existing methods all assume that the variables are independent with the use of diagonal covariance matrices. Results: To model non-independence of variables (e.g. correlated gene expressions) while alleviating the problem with the large number of unknown parameters associated with a general non-diagonal covariance matrix, we generalize the mixture of factor analyzers to that with penalization, which, among others, can effectively realize variable selection. We use simulated data and real microarray data to illustrate the utility and advantages of the proposed method over several existing ones. Cont\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1740257\n",
      "index                                        553fbfad0cf2363314908ace\n",
      "title               Service-oriented network architecture: signifi...\n",
      "authors                         Bhawana Rudra, A. P. Manu, O. P. Vyas\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Computational Science...\n",
      "references                                   558e29930cf2c779a64779b4\n",
      "abstract            The internet is not designed for any specific ...\n",
      "id                                                            1740257\n",
      "clustered_labels                                                    1\n",
      "Name: 1740257, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390985d20f70186a0e0885a   score: 0.92107415   abstract: The architecture of the Internet is based on a number of principles, including the self-describing datagram packet, the end-to-end arguments, diversity in technology and global addressing. As the Internet has moved from a research curiosity to a recognized component of mainstream society, new requirements have emerged that suggest new design principles, and perhaps suggest that we revisit some old ones. This paper explores one important reality that surrounds the Internet today: different stakeholders that are part of the Internet milieu have interests that may be adverse to each other, and these parties each vie to favor their particular interests. We call this process \"the tussle.\" Our position is that accommodating this tussle is crucial to the evolution of the network's technical architecture. We discuss some examples of tussle, and offer some technical design principles that take it\n",
      "\n",
      "2. id: 539089d320f70186a0d9b73f   score: 0.88263035   abstract: The architecture of the Internet is based on a number of principles, including the self-describing datagram packet, the end to end arguments, diversity in technology and global addressing. As the Internet has moved from a research curiosity to a recognized component of mainstream society, new requirements have emerged that suggest new design principles, and perhaps suggest that we revisit some old ones. This paper explores one important reality that surrounds the Internet today: different stakeholders that are part of the Internet milieu have interests that may be adverse to each other, and these parties each vie to favor their particular interests. We call this process \"the tussle\". Our position is that accommodating this tussle is crucial to the evolution of the network's technical architecture. We discuss some examples of tussle, and offer some technical design principles that take it\n",
      "\n",
      "3. id: 558e9c740cf2c779a647802b   score: 0.8784186   abstract: The Internet and its architecture have grown in evolutionary fashion from modest beginnings, rather than from a Grand Plan. While this process of evolution is one of the main reasons for the technology's success, it nevertheless seems useful to record a snapshot of the current principles of the Internet architecture. This is intended for general guidance and general interest, and is in no way intended to be a formal or invariant reference model.\n",
      "\n",
      "4. id: 5390b04120f70186a0ed6cd3   score: 0.75292206   abstract: We argue that the biggest problem with the current Internet architecture is not a particular functional deficiency, but its inability to accommodate innovation. To address this problem we propose a minimal architectural \"framework\" in which comprehensive architectures can reside. The proposed Framework for Internet Innovation (FII) --- which is derived from the simple observation that network interfaces should be extensible and abstract --- allows for a diversity of architectures to coexist, communicate, and evolve. We demonstrate FII's ability to accommodate diversity and evolution with a detailed examination of how information flows through the architecture and with a skeleton implementation of the relevant interfaces.\n",
      "\n",
      "5. id: 5390b24420f70186a0ee8665   score: 0.590425   abstract: The current Internet, which is based on TCP/IP architecture, exposed its inherent shortcomings increasingly in recent years, such as: network management is difficult to deploy, network security issues become more serious, \"best effort forwarding\" strategy cannot provide QoS that a user needs, and new applications are difficult to deploy, etc. All of these problems cannot be resolved satisfactorily by \"putting-patch\" way. It is time to redesign and reconstruct the architecture of Internet. Now some new design ideas about future Internet architecture have been put forward. In this paper, some representatives of these ideas are introduced, and their advantages and defects are analyzed. After these analyses, the possible direction for future Internet design in our opinion is pointed out.\n",
      "\n",
      "6. id: 53909f6920f70186a0e39b75   score: 0.50587434   abstract: The Internet has evolved greatly from its original incarnation. For instance, the vast majority of current Internet usage is data retrieval and service access, whereas the architecture was designed around host-to-host applications such as telnet and ftp. Moreover, the original Internet was a purely transparent carrier of packets, but now the various network stakeholders use middleboxes to improve security and accelerate applications. To adapt to these changes, we propose the Data-Oriented Network Architecture (DONA), which involves a clean-slate redesign of Internet naming and name resolution.\n",
      "\n",
      "7. id: 539098b820f70186a0e0a2ca   score: 0.48642302   abstract: There is widespread agreement on the need for architectural change in the Internet, but very few believe that current ISPs will ever effect such changes. In this paper we ask what makes an architecture evolvable, by which we mean capable of gradual change led by the incumbent providers. This involves both technical and economic issues, since ISPs have to be able, and incented, to offer new architectures. Our study suggests that, with very minor modifications, the current Internet architecture could be evolvable.\n",
      "\n",
      "8. id: 5390b2fc20f70186a0eef0b3   score: 0.43686226   abstract: Present Internet protocol was developed long way back with the intention to provide simple end-to-end network functionalities for network applications of 1980's. Since then many times network and its related applications were reformed over technological advancement. To support new applications more and more protocols were created as patches which resulted in IP bloat. As a result of this Internet community researcher worries about upcoming days of failure of current Internet. This paper emphasis on design and implementation issues of flexible network using with Service Oriented Architecture methodology. And also highlights a framework for Flexible Network Architecture which is one of the way to address current as well as future network application problem.\n",
      "\n",
      "9. id: 5390afc920f70186a0ed30f8   score: 0.34831256   abstract: In the near future, the high volume of content together with new emerging and mission critical applications is expected to stress the Internet to such a degree that it will possibly not be able to respond adequately to its new role. This challenge has motivated many groups and research initiatives worldwide to search for structural modifications to the Internet architecture in order to be able to face the new requirements. This paper is based on the results of the Future Internet Architecture (FIArch) group organized and coordinated by the European Commission (EC) and aims to capture the group's view on the Future Internet Architecture issue.\n",
      "\n",
      "10. id: 5390958920f70186a0dee9d9   score: 0.32594952   abstract: A system as complex as the Internet can only be designed effectively if it is based on a core set of design principles, or tenets, that identify points in the architecture where there must be common understanding and agreement. The tenets of the original Internet architecture [6] arose as a response to the technical, governmental, and societal environment of internetworking's earliest days, but have remained central to the Internet as it has evolved. In light of the increasing integration of the Internet into the social, economic, and political aspects of our lives, it is worth revisiting the underlying tenets of what is becoming a central element of the world's infrastructure.This paper examines three key tenets that we believe should guide the evolution of the Internet in its next generation and beyond. They are: design for change, controlled transparency, and the centrality of the tus\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710204\n",
      "index                                        55323bd845cec66b6f9dab43\n",
      "title               Querying with access patterns and integrity co...\n",
      "authors             Michael Benedikt, Julien Leblay, Efthymia Tsam...\n",
      "year                                                           2015.0\n",
      "venue                               Proceedings of the VLDB Endowment\n",
      "references          558b5659612c41e6b9d4994c;558b48f7612c41e6b9d48...\n",
      "abstract            Traditional query processing involves a search...\n",
      "id                                                            1710204\n",
      "clustered_labels                                                    0\n",
      "Name: 1710204, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539087f320f70186a0d6f875   score: 0.9740426   abstract: We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the different conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best-first search strategy in order to produce a first complete plan early in the search. We describe experiments to illustrate the performance of our \n",
      "\n",
      "2. id: 558b48f7612c41e6b9d482cb   score: 0.93257624   abstract: We look at generating plans that answer queries over restricted interfaces, making use of information about source integrity constraints, access restrictions, and access costs. Our method can exploit the integrity constraints to find low-cost access plans even when there is no direct access to relations appearing in the query. The key idea of our method is to move from a search for a plan to a search for a proof that a query is answerable, and then \\\\emph{generate a plan from a proof}. Discovery of one proof allows us to find a single plan that answers the query; exploration of several alternative proofs allows us to find low-cost plans. We start by overviewing a correspondence between proofs and restricted-interface plans in the context of arbitrary first-order constraints, based on interpolation. The correspondence clarifies the connection between preservation and interpolation theorem\n",
      "\n",
      "3. id: 53908a5720f70186a0da0842   score: 0.91655886   abstract: We consider a class of linear query programs and integrity constraints and develop methods for (i) computing the residues and (ii) pushing them inside the recursive programs, minimizing redundant computation and run-time overhead. We also discuss applications of our strategy to intelligent query answering.\n",
      "\n",
      "4. id: 5390880220f70186a0d75f05   score: 0.8112278   abstract: In a previous paper we proposed a novel method for generating alternative query plans that uses chasing (and back-chasing) with logical constraints. The method brings together use of indexes, use of materialized views, semantic optimization and join elimination (minimization). Each of these techniques is known separately to be beneficial to query optimization. The novelty of our approach is in allowing these techniques to interact systematically, eg. non-trivial use of indexes and materialized views may be enabled only by semantic constraints.We have implemented our method for a variety of schemas and queries. We examine how far we can push the method in term of complexity of both schemas and queries. We propose a technique for reducing the size of the search space by “stratifying” the sets of constraints used in the (back)chase. The experimental results demonstrate that our method is pr\n",
      "\n",
      "5. id: 539087be20f70186a0d51fe9   score: 0.8089744   abstract: The traditional focus of relational query optimization schemes has been on the choice of join methods and join orders. Restrictions have typically been handled in query optimizers by “predicate pushdown” rules, which apply restrictions in some random order before as many joins as possible. These rules work under the assumption that restriction is essentially a zero-time operation. However, today's extensible and object-oriented database systems allow users to define time-consuming functions, which may be used in a query's restriction and join predicates. Furthermore, SQL has long supported subquery predicates, which may be arbitrarily time-consuming to check. Thus restrictions should not be considered zero-time operations, and the model of query optimization must be enhanced.In this paper we develop a theory for moving expensive predicates in a query plan so that the total cost of the pl\n",
      "\n",
      "6. id: 5390ba0a20f70186a0f34bdd   score: 0.8039445   abstract: We consider which queries are answerable in the presence of access restrictions and integrity constraints, and which portions of the schema are accessible in the presence of access restrictions and constraints. Unlike prior work, we focus on integrity constraint languages that subsume inclusion dependencies. We also use a semantic definition of answerability: a query is answerable if the accessible information is sufficient to determine its truth value. We show that answerability is decidable for the class of guarded dependencies, which includes all inclusion dependencies, and also for constraints given in the guarded fragment of first-order logic. We also show that answerable queries have \"query plans\" in a restricted language. We give corresponding results for extractability of portions of the schema. Our results relate querying with limited access patterns, determinacy-vs-rewriting, a\n",
      "\n",
      "7. id: 5390baa120f70186a0f37b8c   score: 0.74539834   abstract: Although the declarative nature of SQL provides great utility to database users, its use in distributed database management systems can result in unintended consequences to user privacy over the course of query evaluation. By allowing users to merely say what data they are interested in accessing without providing guidance regarding how to retrieve it, query optimizers can generate plans that leak sensitive query intension. To address these types of issues, we have created a framework that empowers users with the ability to specify access controls on the intension of their queries through extensions to the SQL SELECT statement. In this demonstration, we present a version of PostgreSQL's query optimizer that we have modified to produce plans that respect these constraints while optimizing user-specified SQL queries in terms of performance.\n",
      "\n",
      "8. id: 539099b320f70186a0e1ab41   score: 0.7213508   abstract: Relational data may have access limitations, i.e., relations may require certain attributes to be selected when they are accessed; this happens, for instance, while querying web data sources (wrapped in relational form) or legacy databases. It is known that the evaluation of a conjunctive query under access limitations requires a recursive algorithm that is encoded into a Datalog program. In this paper we consider the problem of optimising query answering in this setting, where the query language is that of conjunctive queries. We review some optimisation techniques for this problem, that aim to reduce the number of accesses to the data in the query plan. Then we argue that checking query containment is necessary in this case for achieving effective query optimisation. Checking containment in the presence of access limitations would amount to check containment of recursive Datalog progra\n",
      "\n",
      "9. id: 5390881720f70186a0d803d4   score: 0.68605465   abstract: In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.\n",
      "\n",
      "10. id: 55323b9045cec66b6f9da29e   score: 0.6769403   abstract: The data needed to answer queries is often available through Web-based APIs. Indeed, for a given query there may be many Web-based sources which can be used to answer it, with the sources overlapping in their vocabularies, and differing in their access restrictions (required arguments) and cost. We introduce PDQ (Proof-Driven Query Answering), a system for determining a query plan in the presence of web-based sources. It is: (i) constraint-aware -- exploiting relationships between sources to rewrite an expensive query into a cheaper one, (ii) access-aware -- abiding by any access restrictions known in the sources, and (iii) cost-aware -- making use of any cost information that is available about services. PDQ takes the novel approach of generating query plans from proofs that a query is answerable. We demonstrate the use of PDQ and its effectiveness in generating low-cost plans.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1699117\n",
      "index                                        559122530cf232eb904faeae\n",
      "title               A heuristic approach to solve the elementary s...\n",
      "authors               Fabian Sobiech, Beate Eilermann, Andreas Rausch\n",
      "year                                                           2015.0\n",
      "venue                             ACM SIGAPP Applied Computing Review\n",
      "references          53909ee020f70186a0e33cef;5390a5dc20f70186a0e802e5\n",
      "abstract            One goal of Scrum is to deliver as much added ...\n",
      "id                                                            1699117\n",
      "clustered_labels                                                    3\n",
      "Name: 1699117, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558ae621612c41e6b9d3cc80   score: 0.9987503   abstract: One goal of Scrum is to deliver as much added value as possible for the customer at the end of an iteration but in an environment with multiple different customers and stakeholders the definition of value varies. Therefore we defined the concept of value in the context of an OEM in the automotive industry for management information systems. We identified seven relevant value dimensions in a first case study and in a second case study we investigated the relation between the seven different value dimensions. Afterwards we define an optimal iteration based on our value definition for multiple non-cross-functional teams and show that this problem is a NP-hard problem.\n",
      "\n",
      "2. id: 5390994d20f70186a0e1370c   score: 0.39957732   abstract: This paper provides results, and experiences from a longitudinal, 2 year industrial case study. The quantitative results indicate that after the introduction of a Scrum process into an existing software development organization the amount of overtime decreased, allowing the developers to work at a more sustainable pace while at the same time the qualitative results indicate that there was an increase in customer satisfaction.\n",
      "\n",
      "3. id: 5390a05a20f70186a0e4b036   score: 0.16211456   abstract: This paper provides an overview and position statement on the work undertaken as part of a project to explore the implementation of Scrum in the context of an interactive digital media software development company. The project is being undertaken in the Communication and Computing Research Centre at Sheffield Hallam University.\n",
      "\n",
      "4. id: 5390a2be20f70186a0e6592f   score: 0.15610489   abstract: When Scrum extends beyond software development into the organization, Agile techniques, notably Scrum, provide a built-in innovation process. There is more to innovation than coming up with good ideas. Innovating involves capitalizing on well implemented ideas. Implementing good ideas involves risk and consequently should be managed carefully.Scrum effectively implements Lean methods into organizations by providing simple yet effective tools and techniques. These tools and techniques extend beyond software. Unfortunately, because of Scrum's success and roots in software project management, the language of Scrum is software-centric. This paper describes the Scrum language from a business perspective so when Scrum extends into the organization, innovation in the form of value creation is a natural by-product.Two companies are in the process of implementing organizational Scrums, one a prof\n",
      "\n",
      "5. id: 5390b48420f70186a0efb660   score: 0.14633106   abstract: In an action research study, we describe the application of the scrum software development process in a small cross-organizational development project. The stakeholders in the project report many of the benefits we have found in previous studies, such as increased overview of the project, flexibility and motivation. In addition, we have found that estimation can be challenging in cross-organizational projects due to the customer-provider relationship between the participating organizations.\n",
      "\n",
      "6. id: 5390a8dc20f70186a0e9f4e7   score: 0.09687654   abstract: Different classes of information system stakeholders depend on different values to be successful. Understanding stakeholders' value dependencies is critical for developing software intensive systems. However, there is no universal one-size-fits-all stakeholder/ value metric that can be applied for a given system. This paper presents an analysis of major classes of stakeholders' value priorities using the win-win prioritization results from 16 real-client graduate software engineering course projects. Findings from this controlled experiment further verify and extend the hypotheses that \"different stakeholders have different value propositions\", bridge the value understanding gaps among different stakeholders, beneficial for further reasoning about stakeholders' utility functions and for providing process guidance for software projects involving various classes of stakeholders.\n",
      "\n",
      "7. id: 558b87ec612c6b62e5e8acd9   score: 0.09041373   abstract: We present a 15-month descriptive case study on a real-world Scrum process transformation from a single-site to a distributed development environment in a medium-sized software development organization in Austria. The study describes what effects the scaling to a distributed development had on several key process indicators in one of the organization's major projects. An action research approach has been selected to generate results from an in-depth and first-hand research setting. To increase objectivity and separation of concerns, a two-cycle approach, practitioner-oriented and research-oriented, has been established that aligns with sprint iterations. Many possible adaptations to the Scrum process have been tested over the course of the study. Key findings include that constant customer shipments after each sprint were a turning point in supporting the process of integrating the diffe\n",
      "\n",
      "8. id: 5390b78a20f70186a0f2500e   score: 0.08601588   abstract: We report the experience of scaling Scrum from one single team to seven feature teams. Our case study is the leading online payment system in Brazil. We introduce the background of the experiment and describe how we were able to scale Scrum in a team that had been previously unsuccessful in implementing it. Then we present ``The Mega Framework'' -- a consistent set of practices and meetings derived from our experience for coordinating the teams and the stakeholders. It improves communication, aligns expectations and ultimately enables scaling agile development. We also discuss the lessons we learned while creating this framework and how we continuously improve it.\n",
      "\n",
      "9. id: 53909fbd20f70186a0e43855   score: 0.0773781   abstract: Scrum describes a separation of roles; the productowner is accountable for achieving business objectivesand the team for technical execution. A pragmatic andcollegial relationship between a product owner and teamcan satisfy the definition of collaboration and honor roleswhile barely tapping or actually working against thepotential of a project and its participants. This papersurveys literature to describe different forms ofcollaboration, to establish that deep, unboundedcollaboration is at the heart of agile values, and thatpartnerships of high trust and shared risk lead to valueand innovation. Finally, this paper incorporates a real- world example of a product owner who, while remainingaccountable to the outcome, shared ownership overvision, priorities and execution with her Scrum/XPdevelopment team.\n",
      "\n",
      "10. id: 5390882c20f70186a0d8c88a   score: 0.067546695   abstract: From the Publisher:&#147Agile development methods are key to the future of flexible software systems. Scrum is one of the vangards of the new way to buy and manage software development when business conditions are changing. This book distills both the theory and practive and is essential reading for anyone who needs to cope with software in a volatile world.&#148 &#151Martin Fowler, industry consultant and CTO, ThoughtWorks &#147Most executives today are not happy with their organization's ability to deliver systems at reasonable cost and timeframes. Yet, if pressed, they will admit that they don't think their software developers are not competent. If it's not the engineers, then what is it that prevents fast development at reasonable cost? Scrum gives the answer to the question and the solution to the problem.&#148 &#151Alan Buffington, industry consultant, former Present, Fidelity Syst\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672316\n",
      "index                                        559258220cf205530abc9852\n",
      "title               Participatory Stoves: Designing Renewable Ener...\n",
      "authors                     Walter Ángel, Saiph Savage, Nataly Moreno\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference Compani...\n",
      "references          55916dad0cf2e89307ca9bc8;5390a0b720f70186a0e50...\n",
      "abstract            Wood represents a form of renewable energy tha...\n",
      "id                                                            1672316\n",
      "clustered_labels                                                    3\n",
      "Name: 1672316, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a01420f70186a0e487d4   score: 0.36105207   abstract: We describe our approach and initial results in the participatory design of technology relevant to local rural livelihoods. Our approach to design and usability proceeds from research in theory and practice of cross-cultural implementations, but the novelty is in beginning not with particular technologies but from community needs, and structuring technology in terms of activities. We describe our project aims and initial data collected, which show that while villagers have no clear mental models for using computers or the Internet, they show a desire to have and use them. We then describe our approach to interaction design, our expectations and next steps as the technology and activities are first introduced to the villages.\n",
      "\n",
      "2. id: 5390afc920f70186a0ed1ebc   score: 0.22970797   abstract: The article was to grasp trends of energy consumption of village in southern Shaanxi province. Selecting Huangjiagou village of Mian county in Hanzhong city as the investigation base 隆拢Respectively, in January 2009 and July2010, investigation was conducted in the area, tested indoor temperature and humidity of dwellings. Through analysis of the energy structure in the region, corresponding improvement measures was put forward to promote traditional energy sources conversing into clean energy as biogas and solar energy, and effective strategies for dwellings energy conservation from dwellings designing, cooking energy, home appliances were raised.\n",
      "\n",
      "3. id: 5390a8db20f70186a0e9d92a   score: 0.1559763   abstract: We reflect upon participation in design processes by people who emphasise 'primary orality', or direct, face-to-face, unmediated communication, due to their rural locations in places with low technology ambiance and cultural antecedents. We focus on issues and relationships between rural contexts and primary orality of relevance to our projects with Indigenous people in regional Australia and villagers in remote rural South Africa. We observe dilemmas as we apply methods, which are informed by ethnomethodology, ethnography and Participatory Design, in enabling local participation, such as intrusive recording practices, concerns about power structures and appropriate investment of time.\n",
      "\n",
      "4. id: 5390aefc20f70186a0ecdbd4   score: 0.10762492   abstract: The purpose of this study was to identify the knowledge for household energy based upon both a technology structure approach knowledge level approach. Energy research could put focus on either of source or consuming. Energy consumer is the one who really decide how to use the energy. It is important to understand user's before trying to promote safety, energy conservation, and carbon reduction.\n",
      "\n",
      "5. id: 5390a80f20f70186a0e971d4   score: 0.031439852   abstract: Appliances, not just in the kitchen anymore.\n",
      "\n",
      "6. id: 5390985d20f70186a0e073a8   score: 0.02784845   abstract: This article argues that because the home is so familiar, it is necessary to make it strange, or defamiliarize it, in order to open its design space. Critical approaches to technology design are of both practical and social importance in the home. Home appliances are loaded with cultural associations such as the gendered division of domestic labor that are easy to overlook. Further, homes are not the same everywhere---even within a country. Peoples' aspirations and desires differ greatly across and between cultures. The target of western domestic technology design is often not the user, but the consumer. Web refrigerators that create shopping lists, garbage cans that let advertisers know what is thrown away, cabinets that monitor their contents and order more when supplies are low are central to current images of the wireless, digital home of the future. Drawing from our research in the \n",
      "\n",
      "7. id: 5390994d20f70186a0e11a40   score: 0.015365342   abstract: We discuss homes as potential settings for the products of appliance design. We catalog the large international and regional differences. We look at differences in terms of infrastructure: heating, plumbing, electricity, and telephony. We examine differences in the home itself in terms of number of household members, and size of dwelling. We explore the implications of this variation for future ethnographies as well as product creation as we ask the question “appliances for whom?”\n",
      "\n",
      "8. id: 5390ad8920f70186a0ec1543   score: 0.014728614   abstract: The current condition of rural energy consumption in Zhejiang Province was investigated. According to the actual situation of new countryside construction, countermeasures were put forward on how to develop rural energy including enhancing the rural energy planning, popularizing suitable technology mode, increasing input, and accelerating technology supporting capacity through an analysis on the potency and restrictive factors. The results show that rural energy has developed swiftly, and its produce amount, variety and structure have also changed tremendously since the Reform and Opening. Yet the exploitation of rural energy, mainly traditional biomass energy, has had harmful influence on the regional ecology, economy and social sustainable development. It is necessary to develop new energy and renewable energy such as modern biomass energy.\n",
      "\n",
      "9. id: 5390bda020f70186a0f474d7   score: 0.012053779   abstract: An increasing number of researchers are using social engagement techniques such as neighborhood comparison and competition to encourage energy conservation, yet community reception and experience with such systems have not been well studied. We also find that researchers have not thoroughly investigated how different households use these systems and how their uses differ from one another. We explore these questions in a 4-10 month field deployment of a social-energy monitoring application across 15 households, in two distinct locations. We contribute results that describe conditions under which these techniques were effective and ineffective. Our results imply that understanding factors such as a building, or community's layout, context knowledge of community members, accountability and adherence to social norms, trust, and length of residence are key for future design of social-energy a\n",
      "\n",
      "10. id: 5390b95420f70186a0f2e38b   score: 0.010091316   abstract: This paper presents a theoretical review related to the contributions of technology development in the search for optimal solutions at the local level to face the renewable energies challenge. This, considering that those processes of promoting the development and implementation of new energy resources and environmental technologies in Colombia as well as in around the world, depends on the economic and political will in most cases. Also, the correct use of the available information allows the identification of potential threats and opportunities of implementing such technologies locally, as described in this paper. As part of the conclusions, the results of such processes can also be an important input for prospective, institutional and territorial planning in order to support adoption and technological adaptation processes to respond to local needs.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1747962\n",
      "index                                        554e3f090cf22ca2c80f9c80\n",
      "title               A context-aware approach for long-term behavio...\n",
      "authors             Abdur Rahim Mohammad Forkan, Ibrahim Khalil, Z...\n",
      "year                                                           2015.0\n",
      "venue                                             Pattern Recognition\n",
      "references                                   558e2ef20cf2c779a6477b4e\n",
      "abstract            This research aims to describe pattern recogni...\n",
      "id                                                            1747962\n",
      "clustered_labels                                                    2\n",
      "Name: 1747962, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a17720f70186a0e53c27   score: 0.9841546   abstract: In this work we propose a novel method to recognize daily routines as a probabilistic combination of activity patterns. The use of topic models enables the automatic discovery of such patterns in a user's daily routine. We report experimental results that show the ability of the approach to model and recognize daily routines without user annotation.\n",
      "\n",
      "2. id: 5390b78a20f70186a0f247a6   score: 0.9817707   abstract: We monitor activities of daily living of smart home residents to detect anomalies in their behavior. Unlike traditional anomaly detection systems, we aim to reduce false positives in anomaly detection with the help of semantic rules. Some of these rules are predefined based on expert knowledge and the rest are learned by the system with the help of resident/expert feedback. We also correlate trend of change in different activities to improve anomaly detection. In addition to monitor statistical deviation from regular behavior, we also detect deviation from healthy and social norms (defined by experts) as anomalies.\n",
      "\n",
      "3. id: 5390b63320f70186a0f17586   score: 0.9812749   abstract: Herein, we put forth a new similarity measure for anomaly detection and for comparing human behaviors based on the theories of learning automata, comparison of soft partitions, and temporal probabilistic order relations. In particular, focus is placed on monitoring individuals in a home setting for their own well-being. This work is a high-level investigation focused on the structure of human behavior. Examples demonstrate the utility of this approach for (1) understanding the similarity of pairs of behaviors for an individual (or alternatively between individuals) and (2) detecting significant change between changing behavior and a baseline model. In the context of eldercare, significant change in behavior can be a precursor to cognitive and/or functional health related problems. Simulated resident behavior is used to show different scenarios and the response of the proposed measure. © \n",
      "\n",
      "4. id: 53909fbd20f70186a0e4363e   score: 0.9774139   abstract: In this paper, we propose a method consists of two com- ponents, behavior patterns extraction and anomaly detec- tion algorithm in daily life. To begin with, sensor data are accumulated in a room environment and behavior de- scription labels are assigned for each data segment using HMM(Hidden Markov Model) and k-means method. An HMM is composed every day based on sensor data segments of the day. The behavior description label at each time seg- ment is determined by likelihood of the segment computed using the HMM. In anomaly detection step, typical behavior sequences are acquired using probabilistic density of behav- ior occurrence and behavior successive time. Each proba- bilistic density is composed based on accumulating labeled- data using Sequential Discounting Laplace Estimation and Sequential Discounting Expectation and Maximization al- gorithms. When a new datum comes, if typical \n",
      "\n",
      "5. id: 5390bb7b20f70186a0f41383   score: 0.9755304   abstract: The increasing aging population in the coming decades will result in many complications for society and in particular for the healthcare system due to the shortage of healthcare professionals and healthcare facilities. To remedy this problem, researchers have pursued developing remote monitoring systems and assisted living technologies by utilizing recent advances in sensor and networking technology, as well as in the data mining and machine learning fields. In this article, we report on our fully automated approach for discovering and monitoring patterns of daily activities. Discovering and tracking patterns of daily activities can provide unprecedented opportunities for health monitoring and assisted living applications, especially for older adults and individuals with mental disabilities. Previous approaches usually rely on preselected activities or labeled data to track and monitor d\n",
      "\n",
      "6. id: 55323ad445cec66b6f9d8e15   score: 0.97313875   abstract: In this paper, we present an automated behavior analysis system developed to assist the elderly and individuals with disabilities who live alone, by learning and predicting standard behaviors to improve the efficiency of their healthcare. Established behavioral patterns have been recorded using wireless sensor networks composed by several event-based sensors that captured raw measures of the actions of each user. Using these data, behavioral patterns of the residents were extracted using Bayesian statistics. The behavior was statistically estimated based on three probabilistic features we introduce, namely sensor activation likelihood, sensor sequence likelihood, and sensor event duration likelihood. Real data obtained from different home environments were used to verify the proposed method in the individual analysis. The results suggest that the monitoring system can be used to detect a\n",
      "\n",
      "7. id: 5390a6b120f70186a0e85e2e   score: 0.96814114   abstract: This paper presents a pattern recognition model for assessing behavioral rhythms in the framework of aging and technologies. The method, previously tried and tested using motion sensors installed in assisted living units, has permitted to establish motion-based behaviors of older-people based on their habits in term of displacements and activity-levels. The method is now expanded to measure more specific patterns of everyday life activity assuming an activity can be pre-identified on the long term using an activity recognition system. The study feasibility, carried out using semi-artificial data, includes an attempt to model disruptive patterns of living linked with dementia. The alert triggering method, part of the model is improved, and has been evaluated using a real-case study to detect behavioral changes with a higher sensibility. A customized software embedding the model shows the \n",
      "\n",
      "8. id: 5390a45620f70186a0e733c6   score: 0.96557647   abstract: We present a novel method for detecting unusual modes of behavior in video surveillance data, suitable for supporting home-based care of elderly patients. Our approach is based on detecting unusual patterns of inactivity. We first learn a spatial map of normal inactivity for an observed scene, expressed as a two-dimensional mixture of Gaussians. The map components are used to construct a Hidden Markov Model representing normal patterns of behavior. A threshold model is also inferred, and unusual behavior detected by comparing the model likelihoods. Our learning procedures are unsupervised, and yield a highly transparent model of scene activity. We present an evaluation of our approach, and show that it is effective in detecting unusual behavior across a range of parameter settings.\n",
      "\n",
      "9. id: 5390b78a20f70186a0f23c3c   score: 0.9614615   abstract: We present an example of unobtrusive, continuous monitoring in the home for the purpose of assessing early health changes. Sensors embedded in the environment capture activity patterns. Changes in the activity patterns are detected as potential signs of changing health. A simple alert algorithm has been implemented to generate health alerts to clinicians in a senior housing facility. Clinicians analyze each alert and provide a rating on the clinical relevance. These ratings are then used as ground truth in developing classifiers. Here, we present the methodology and results for two classification approaches using embedded sensor data and health alert ratings collected on 21 seniors over nine months. The results show similar performance for the two techniques, where one approach uses only domain knowledge and the second uses supervised learning for training.\n",
      "\n",
      "10. id: 5390a1d420f70186a0e57d79   score: 0.9591543   abstract: We propose software architecture to monitor elderly people in their own homes. We want to build patterns of monitored people dynamically from data about activity, movements and physiological information. To obtain this macroscopic view, we use a multi-agent method of classification: every agent has a simple skill of classification. They generate partial partitions and cooperate to obtain a set of patterns. The patterns are used at a personal level, for example to raise an alert, but also to evaluate global risks. These data are dynamic; the system has to maintain the built patterns and has to create new patterns. Therefore, the system is adaptive and can be spread on a large scale.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1724563\n",
      "index                                        55323d0e45cec66b6f9dd6d4\n",
      "title               Efficient biometric authenticated key agreemen...\n",
      "authors                   Der-Chyuan Lou, Tian-Fu Lee, Tsung-Hung Lin\n",
      "year                                                           2015.0\n",
      "venue                                      Journal of Medical Systems\n",
      "references          558b0b43612c41e6b9d41495;558b219b612c41e6b9d4456a\n",
      "abstract            Authenticated key agreements for telecare medi...\n",
      "id                                                            1724563\n",
      "clustered_labels                                                    1\n",
      "Name: 1724563, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55922c4e0cf2e74f816e3652   score: 0.9704086   abstract: We demonstrate the weaknesses of the protocols of Xiao et al. and Guo and Zhang.We present two secure password-based authenticated key agreement protocols using chaotic maps.The proposed protocols prevent the weaknesses in previous protocols and reduce transmissions.The proposed protocols do not require extra equipments for storing long-term secrets in the client. Password authenticated key agreement allows users to only keep a weak password and establish an authentication key shared with a server. However, the chaotic maps based authenticated key agreement protocol of Xiao et al. in which time-stamps are used to resist replaying attacks and the subsequent group key agreement protocol of Guo and Zhang in which chaotic hash is used as an alternative are insecure against off-line password guessing attacks and violate the session key security. Therefore, this work presents two secure passwo\n",
      "\n",
      "2. id: 5390b48420f70186a0efb4d8   score: 0.966347   abstract: The current paper presents an efficient and secure biometrics authenticated key agreement scheme based on one-way hash function. In the proposed scheme, after a user passes user authentication check of a remote system, they agree on a session key for protecting their subsequent communications.\n",
      "\n",
      "3. id: 558b2f56612c41e6b9d45ff5   score: 0.9628831   abstract: Telecare medicine information system (TMIS) is widely used for providing a convenient and efficient communicating platform between patients at home and physicians at medical centers or home health care (HHC) organizations. To ensure patient privacy, in 2013, Hao et al. proposed a chaotic map based authentication scheme with user anonymity for TMIS. Later, Lee showed that Hao et al.'s scheme is in no provision for providing fairness in session key establishment and gave an efficient user authentication and key agreement scheme using smart cards, in which only few hashing and Chebyshev chaotic map operations are required. In addition, Jiang et al. discussed that Hao et al.'s scheme can not resist stolen smart card attack and they further presented an improved scheme which attempts to repair the security pitfalls found in Hao et al.'s scheme. In this paper, we found that both Lee's and Jian\n",
      "\n",
      "4. id: 5541111b0cf2adbd8385c291   score: 0.96102494   abstract: The Telecare Medicine Information Systems (TMISs) provide an efficient communicating platform supporting the patients access health-care delivery services via internet or mobile networks. Authentication becomes an essential need when a remote patient logins into the telecare server. Recently, many extended chaotic maps based authentication schemes using smart cards for TMISs have been proposed. Li et al. proposed a secure smart cards based authentication scheme for TMISs using extended chaotic maps based on Lee's and Jiang et al.'s scheme. In this study, we show that Li et al.'s scheme has still some weaknesses such as violation the session key security, vulnerability to user impersonation attack and lack of local verification. To conquer these flaws, we propose a chaotic maps and smart cards based password authentication scheme by applying biometrics technique and hash function operatio\n",
      "\n",
      "5. id: 5390bf1320f70186a0f51ae1   score: 0.959001   abstract: In the field of the Telecare Medicine Information System, recent researches have focused on consummating more convenient and secure healthcare delivery services for patients. In order to protect the sensitive information, various attempts such as access control have been proposed to safeguard patients' privacy in this system. However, these schemes suffered from some certain security defects and had costly consumption, which were not suitable for the telecare medicine information system. In this paper, based on the elliptic curve cryptography, we propose a secure and efficient two-factor mutual authentication and key agreement scheme to reduce the computational cost. Such a scheme enables to provide the patient anonymity by employing the dynamic identity. Compared with other related protocols, the security analysis and performance evaluation show that our scheme overcomes some well-known\n",
      "\n",
      "6. id: 5390bed320f70186a0f4d8cf   score: 0.9352283   abstract: A smartcard-based authentication and key agreement scheme for telecare medicine information systems enables patients, doctors, nurses and health visitors to use smartcards for secure login to medical information systems. Authorized users can then efficiently access remote services provided by the medicine information systems through public networks. Guo and Chang recently improved the efficiency of a smartcard authentication and key agreement scheme by using chaotic maps. Later, Hao et al. reported that the scheme developed by Guo and Chang had two weaknesses: inability to provide anonymity and inefficient double secrets. Therefore, Hao et al. proposed an authentication scheme for telecare medicine information systems that solved these weaknesses and improved performance. However, a limitation in both schemes is their violation of the contributory property of key agreements. This investi\n",
      "\n",
      "7. id: 5390bf1320f70186a0f51b03   score: 0.93183565   abstract: To ensure only authorized access to medical services, several authentication schemes for telecare medicine information systems (TMIS) have been proposed in the literature. Due to its better performance than traditional cryptography, Hao et al. proposed an authentication scheme for TMIS using chaotic map based cryptography. They claimed that their scheme could resist various attacks, including the smart card stolen attack. However, we identify that their scheme is vulnerable to the stolen smart card attack. The reason causing the stolen smart card attack is that the scheme is designed based on the assumption that the scheme itself achieves user untraceability. Then, we propose a robust authentication and key agreement scheme. Compared with the previous schemes, our scheme not only enjoys more security features, but also has better efficiency. Our analysis indicates that designing a two-fa\n",
      "\n",
      "8. id: 5390ac1820f70186a0eb32a4   score: 0.92468774   abstract: Recently, Xiao et al. proposed an improved key agreement protocol based on chaotic maps, in which only a predetermined long-term key is utilized to ensure its security. This paper demonstrates that none of these schemes can satisfy the contributory nature of key agreement. To fill the gaps, we present a secure key agreement protocol based on chaotic Hash. The proposed scheme utilizes the chaotic Hash function to achieve the contributory nature and enhance its security. Cryptanalysis demonstrates that our chaotic Hash-based scheme can overcome all the current deficiencies.\n",
      "\n",
      "9. id: 5390be6620f70186a0f4cb88   score: 0.9019207   abstract: Recently Lee and Liu proposed an efficient password based authentication and key agreement scheme using smart card for the telecare medicine information system [J. Med. Syst. (2013) 37:9933]. In this paper, we show that though their scheme is efficient, their scheme still has two security weaknesses such as (1) it has design flaws in authentication phase and (2) it has design flaws in password change phase. In order to withstand these flaws found in Lee-Liu's scheme, we propose an improvement of their scheme. Our improved scheme keeps also the original merits of Lee-Liu's scheme. We show that our scheme is efficient as compared to Lee-Liu's scheme. Further, through the security analysis, we show that our scheme is secure against possible known attacks. In addition, we simulate our scheme for the formal security verification using the widely-accepted AVISPA (Automated Validation of Intern\n",
      "\n",
      "10. id: 558b3855612c41e6b9d47001   score: 0.89442116   abstract: Telecare medicine information systems (TMIS) present the platform to deliver clinical service door to door. The technological advances in mobile computing are enhancing the quality of healthcare and a user can access these services using its mobile device. However, user and Telecare system communicate via public channels in these online services which increase the security risk. Therefore, it is required to ensure that only authorized user is accessing the system and user is interacting with the correct system. The mutual authentication provides the way to achieve this. Although existing schemes are either vulnerable to attacks or they have higher computational cost while an scalable authentication scheme for mobile devices should be secure and efficient. Recently, Awasthi and Srivastava presented a biometric based authentication scheme for TMIS with nonce. Their scheme only requires the\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1608929\n",
      "index                                        559031f40cf2e9668dc4db30\n",
      "title               Accurate and Robust Moving-Object Segmentation...\n",
      "authors               Meiyu Huang, Yiqiang Chen, Wen Ji, Chunyan Miao\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Intelligent Systems and Te...\n",
      "references          5390b4da20f70186a0f0071c;55323bf445cec66b6f9da...\n",
      "abstract            Moving-object segmentation is the key issue of...\n",
      "id                                                            1608929\n",
      "clustered_labels                                                    2\n",
      "Name: 1608929, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a7f520f70186a0e94352   score: 0.91006696   abstract: We describe our approach to segmenting moving objects from the color video data supplied by a nominally stationary camera. There are two main contributions in our work. The first contribution augments Zivkovic and Heijden's recursively updated Gaussian mixture model approach, with a multi-dimensional Gaussian kernel spatio-temporal smoothing transform. We show that this improves the segmentation performance of the original approach, particularly in adverse imaging conditions, such as when there is camera vibration. Our second contribution is to present a comprehensive comparative evaluation of shadow and highlight detection appoaches, which is an essential component of background subtraction in unconstrained outdoor scenes. A comparative evelaution of these approaches over different color-spaces is currently lacking in the literature. We show that both segmentation and shadow removal per\n",
      "\n",
      "2. id: 5390bda020f70186a0f46ac6   score: 0.7444706   abstract: A novel layered stereoscopic moving-object segmentation method is proposed in this paper by exploiting both motion information and depth information to extract moving objects for each depth layer with high accuracy on their shape boundary. By taking a higher-order statistics on two frame-difference fields across three adjacent frames, the computed motion information are used to conduct change detection and generate one motion mask that consists of all the moving objects from all the depth layers involved at each view. It would be highly desirable, and challenging, to further differentiate them according to their residing depth layer to achieve layered segmentation. For that, multiple depth-layer masks are generated using our proposed disparity estimation method, one for each depth layer. By intersecting the motion mask and one depth-layer mask at any given layer-of-interest, the moving o\n",
      "\n",
      "3. id: 5390a63c20f70186a0e81d59   score: 0.6773673   abstract: We present a novel approach for adaptive foreground/background segmentation in non-static environments using multiview silhouette fusion. Our focus is on coping with moving objects in the background and influences of lighting conditions. It is shown, that by integrating 3d scene information, background motion can be compensated to achieve a better segmentation and a less error prone 3d reconstruction of the foreground. The proposed algorithm is based on a closed loop idea of segmentation and 3d reconstruction in form of a low level vision feedback system. The functionality of our approach is evaluated on two different data sets in this paper and the benefits of our algorithm are finally shown based on a quantitative error analysis.\n",
      "\n",
      "4. id: 53909f8220f70186a0e3d703   score: 0.655336   abstract: segmentation of moving regions in outdoor environment under a moving camera is a fundamental step in many vision systems including automated visual surveillance, human-machine interface, tracking etc. It is also a chal- lenging task due to camera motion, object motion, and out- door scene challenges i.e. periodic motions of swaying of trees, gradual illumination changes, etc. In this paper, a wide area scene modeling approach for object segmenta- tion under a moving camera is proposed. This approach suffers due to parallax effect, misallignment errors etc and needs their concurrent removal for its success. we explic- itly model the dense correspondence between input image and panoramic background model. Foreground segmenta- tion and correspondence estimation are expressed as a uni- fied labeling problem, and are solved efficiently via tree dy- namic programming (TDP). Lucas-Kanade method\n",
      "\n",
      "5. id: 5390aaf920f70186a0eae52c   score: 0.6380463   abstract: Identifying moving objects from a video sequence is a fundamental and critical task in many computer vision applications. We develop an efficient adaptive segmentation algorithm for color video surveillance sequence in real time with non-stationary background; background is modeled using partial correlation coefficient using pixel-level based approach. At runtime, segmentation is performed by checking color intensity values at corresponding pixels using temporal differencing. The segmentation starts from a seed in the form of 3×3 image blocks to avoid the noise. Usually, temporal differencing generates holes in motion objects. After subtraction, holes are filled using image fusion, which uses spatial clustering as criteria to link motion objects. The emphasis of this approach is on the robust detection of moving objects even under noise or environmental changes (indoor as well as outdoor\n",
      "\n",
      "6. id: 5390bed320f70186a0f4f2e8   score: 0.61254144   abstract: Image segmentation is one of the most important topics in the field of computer vision. So lots of approaches for image segmentation have been proposed, and interactive methods based on energy minimization such as Grab Cut, etc have shown successful results. It, however, is not easy to automate the full process for segmentation because almost all of interactive methods require considerable user interaction. So if additional information is provided to users in order to guide them effectively, we can reduce interaction with them. In this paper we propose an efficient foreground extraction algorithm, which makes use of depth information from RGB-D sensors like Microsoft Kinect and offers users guidance for foreground extraction. Our approach can be applied as a pre-processing for interactive and energy-minimization-based segmentation approaches. Our proposed method is able to segment the fo\n",
      "\n",
      "7. id: 5390a6d920f70186a0e8674f   score: 0.59472775   abstract: Identifying moving objects from a video sequence is a fundamental and critical task in many computer vision applications. We develop an efficient adaptive segmentation algorithm for color video surveillance sequence in real time with non-stationary background; background is modeled using multiple correlation coefficient using pixel-level based approach. At runtime, segmentation is performed by checking color intensity values at corresponding pixels P(x,y) in three frames using temporal differencing (frame gap three). The segmentation starts from a seed in the form of 3脳3 image blocks to avoid the noise. Usually, temporal differencing generates holes in motion objects. After subtraction, holes are filled using image fusion, which uses spatial clustering as criteria to link motion objects. The emphasis of this approach is on the robust detection of moving objects even under noise or enviro\n",
      "\n",
      "8. id: 5390b8d720f70186a0f2b8ee   score: 0.5629575   abstract: We present an automatic approach to segment an object in calibrated images acquired from multiple viewpoints. Our system starts with a new piecewise planar layer-based stereo algorithm that estimates a dense depth map that consists of a set of 3D planar surfaces. The algorithm is formulated using an energy minimization framework that combines stereo and appearance cues, where for each surface, an appearance model is learnt using an unsupervised approach. By treating the planar surfaces as structural elements of the scene and reasoning about their visibility in multiple views, we segment the object in each image independently. Finally, these segmentations are refined by probabilistically fusing information across multiple views. We demonstrate that our approach can segment challenging objects with complex shapes and topologies, which may have thin structures and non-Lambertian surfaces. I\n",
      "\n",
      "9. id: 5390bfa220f70186a0f52e7f   score: 0.55425936   abstract: In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that\n",
      "\n",
      "10. id: 5390bed320f70186a0f4e4cf   score: 0.53509843   abstract: Low cost RGB-D cameras such as the Microsoft's Kinect or the Asus's Xtion Pro are completely changing the computer vision world, as they are being successfully used in several applications and research areas. Depth data are particularly attractive and suitable for applications based on moving objects detection through foreground/background segmentation approaches; the RGB-D applications proposed in literature employ, in general, state of the art foreground/background segmentation techniques based on the depth information without taking into account the color information. The novel approach that we propose is based on a combination of classifiers that allows improving background subtraction accuracy with respect to state of the art algorithms by jointly considering color and depth data. In particular, the combination of classifiers is based on a weighted average that allows to adaptively \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1730885\n",
      "index                                        5534d8ba45cedae85c3795a9\n",
      "title               Knowledge-intensive, causal reasoning for anal...\n",
      "authors                      Fanshu Jiao, Sergio Montano, Alex Doboli\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references                                   558be1500cf20e727d0f3588\n",
      "abstract            Analog circuit topology design has been diffic...\n",
      "id                                                            1730885\n",
      "clustered_labels                                                    0\n",
      "Name: 1730885, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390879920f70186a0d41e61   score: 0.9763557   abstract: A novel approach to the design automation of analog circuits is presented. The prototype implementation -OASE- has been realized as a set of cooperating expert systems with blackboard architectures. The circuit specific knowledge bases use hybrid representation schemes and are strictly separated from the execution engine. This alleviates the knowledge acquisition process as well as the extension and maintenance of existing knowledge. OASE has been developed as a design assistant, providing a hierarchical design style and fully embedded standard algorithmic tools.\n",
      "\n",
      "2. id: 559136610cf232eb904fb471   score: 0.97052056   abstract: Physical analog IC design has not been automated to the same degree as digital IC design. This shortfall is primarily rooted in the analog IC design problem itself, which is considerably more complex even for small problem sizes. Significant progress has been made in analog automation in several R&D target areas in recent years. Constraint engineering and generator-based module approaches are among the innovations that have emerged. Our paper will first present a brief review of the state of the art of analog layout automation. We will then introduce active and open research areas and present two visions -- a \\\"continuous layout design flow\\\" and a \\\"bottom-up meets top-down design flow\\\" -- which could significantly push analog design automation towards its goal of analog synthesis.\n",
      "\n",
      "3. id: 5390878720f70186a0d3480e   score: 0.96903247   abstract: An organization for a knowledge-based analog circuit synthesis tool is described. Analog circuit topologies are represented as a hierarchy of functional blocks; a planning mechanism is introduced to translate performance specifications between levels in this circuit hierarchy. A prototype implementation, OASYS, synthesizes sized transistor schematics for simple CMOS operational amplifiers from performance specifications and process parameters, and demonstrates the workability of the approach.\n",
      "\n",
      "4. id: 5390ad0720f70186a0ebb407   score: 0.96411926   abstract: This paper introduces a graph grammar based approach to automated topology synthesis of analog circuits. A grammar is developed to generate circuits through production rules, that are encoded in the form of a derivation tree. The synthesis has been sped up by using dynamically obtained design-suitable building blocks. Our technique has certain advantages when compared to other tree-based approaches like GP based structure generation. Experiments conducted on an opamp and a vco design show that unlike previous works, we are capable of generating both manual-like designs (bookish circuits) as well as novel designs (unfamiliar circuits) for multi-objective analog circuit design benchmarks.\n",
      "\n",
      "5. id: 5390c04520f70186a0f579e1   score: 0.96295285   abstract: This paper presents a reasoning-based approach to analog circuit synthesis using ordered node clustering representations (ONCR) to describe alternative circuit features and symbolic circuit comparison to characterize performance trade-offs of synthesized solutions. Case studies illustrate application of the proposed methods to topology selection and refinement.\n",
      "\n",
      "6. id: 5390994d20f70186a0e12ace   score: 0.950503   abstract: This paper presents a method of design automation for analog circuits, focusing on topology generation and quick performance evaluation. First we describe mechanisms to generate circuit topologies with hierarchical blocks. Those blocks are specialized by adding terminal information. The connection between blocks is in compliance with a set of synthesis rules, which are extracted from typical schematics in the literature. Symbolic analysis has been used to select an appropriate topology quickly and to help the designer gain a better understanding of a circuit's behavior. Finally, experimental results show the creativity and efficiency of our method.\n",
      "\n",
      "7. id: 5390958a20f70186a0def7e3   score: 0.94796216   abstract: A new design methodology for the design of arbitrary analogue functional blocks (op amps, comparators...) is presented. The method combines symbolic simulation, numerical optimization and knowledge-based techniques and covers the whole design path from analytic modeling over optimal circuit sizing down to layout. This path is repeatedly passed through on different hierarchical levels for higher-level blocks.The main advantage of the new CAD system is that it is not limited to a fixed set of circuit topologies. It can automatically design any new circuit by the introduction of artificial intelligence techniques: building block recognition, design equation manipulation, self-learning capabilities... These features make it an intelligent and flexible analogue design system.\n",
      "\n",
      "8. id: 5390b56a20f70186a0f06541   score: 0.9370958   abstract: We present a method for automated synthesis of analog circuits using evolutionary search and a set of circuit design rules based on topological reuse. The system requires only moderate expert knowledge on part of the user. It allows circuit size, circuit topology, and device values to evolve. The circuit representation scheme employs a topological reuse-based approach-it uses commonly used subcircuits for analog design as inputs and utilizes these to create the final circuit. The connectivity between these blocks is governed by a well-defined set of rules and the scheme is capable of representing most standard analog circuit topologies. The system operation consists of two phases-in the first phase, the circuit size and topology are evolved. A limited amount of device sizing also occurs in this phase. The second phase consists entirely of device value optimization. The design of the eval\n",
      "\n",
      "9. id: 5390a37f20f70186a0e6b762   score: 0.92879725   abstract: The design of analog circuits has historically been a time consuming, manual task. The stringent constraints that must be considered simultaneously make the task particularly difficult, and are a major reason analog design has often not been automated. We believe that constraint-driven design is a prerequisite to analog design automation as it enables expert knowledge to be included in the design flow. This paper provides an introduction to the concept of constraint-driven physical design. First, we identify the major challenges in analog physical design, which we show are mostly constrained-related. We then provide an overview of the essential components of a constraint-driven design methodology. Finally, we discuss the impact this approach has on the analog design flow and design algorithms.\n",
      "\n",
      "10. id: 5390a80f20f70186a0e97935   score: 0.9285385   abstract: This paper proposes an efficient automation platform that provides fast and reliable path to analog circuit design for desired specifications. Circuit heuristics and hierarchy are employed to aid efficient design flow. As a synthesis method, procedural planning of design equations is developed to improve accuracy. To cope with the low power requirements of recent analog design trend, automation flow of subthreshold analog circuit is developed based on weak inversion model. The proposed and developed tool is applied to several test cases. The results show that design time is reduced from weeks to seconds, and at the same time, a significantly accurate circuit behavior for the desired performance specification is obtained.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717198\n",
      "index                                        55323c6e45cec66b6f9dc0ee\n",
      "title               H∞ State Control for 2D Fuzzy FM Systems with ...\n",
      "authors             Bensalem Boukili, Abdelaziz Hmamed, Abdellah B...\n",
      "year                                                           2015.0\n",
      "venue                        Circuits, Systems, and Signal Processing\n",
      "references          558bd4460cf23f2dfc593e08;558b9430612c6b62e5e8b...\n",
      "abstract            This paper investigates the $$H_{\\\\infty }$$H¿...\n",
      "id                                                            1717198\n",
      "clustered_labels                                                    2\n",
      "Name: 1717198, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba0a20f70186a0f339e9   score: 0.94901186   abstract: This paper addresses the state feedback control of nonlinear continuous-time, Markovian-jump systems. The nonlinearity is represented by Takagi-Sugeno fuzzy models and the transition probability matrix is assumed to be partly known: some elements in the matrix are known, some are unknown but with known lower and upper bounds, and some are completely unknown. By making full use of the continuous property of the transition probability matrix, new sufficient conditions for the stochastic stability of the system are obtained in terms of linear matrix inequalities. We show that the conditions given are less conservative than or at least the same as those for existing results. Moreover, using the conditions obtained, we establish a method for design of a H\"~ state feedback controller. Numerical examples illustrate the effectiveness of the proposed method.\n",
      "\n",
      "2. id: 5390aca820f70186a0eb7abf   score: 0.92762595   abstract: In this article, we first investigate the variable sampling control problem of Takagi-Sugeno (T-S) fuzzy systems without uncertainties. By utilising the input delay approach and the descriptor model transformation, an equivalent continuous-time generalised T-S fuzzy system with a state delay is obtained. Some sufficient conditions for the existence of a state feedback controller are derived in terms of linear matrix inequalities (LMIs), which guarantee the asymptotic stability of the whole closed-loop system. Moreover, some results are also expanded to T-S fuzzy systems with norm-bounded parameter uncertainties. In order to relax the conservatism of the obtained conditions, a feasible algorithm is provided to remove a slightly restricted condition in LMIs. An illustrative example is given to show the validity of this control strategy.\n",
      "\n",
      "3. id: 5390aca920f70186a0eb9ab3   score: 0.92762595   abstract: In this article, we first investigate the variable sampling control problem of Takagi-Sugeno (T-S) fuzzy systems without uncertainties. By utilising the input delay approach and the descriptor model transformation, an equivalent continuous-time generalised T-S fuzzy system with a state delay is obtained. Some sufficient conditions for the existence of a state feedback controller are derived in terms of linear matrix inequalities (LMIs), which guarantee the asymptotic stability of the whole closed-loop system. Moreover, some results are also expanded to T-S fuzzy systems with norm-bounded parameter uncertainties. In order to relax the conservatism of the obtained conditions, a feasible algorithm is provided to remove a slightly restricted condition in LMIs. An illustrative example is given to show the validity of this control strategy.\n",
      "\n",
      "4. id: 5390b5c720f70186a0f0931c   score: 0.90105337   abstract: In this paper, the problems of quadratic stability conditions and H∞ control designs for Takagi-Sugeno (T-S) fuzzy systems have been studied. First, a new quadratic stability condition, which is more simple than that in a previous paper, has been proposed. Second, two new sufficient conditions in the terms of linear matrix inequalities (LMIs) which guarantee the existence of the state feedback H∞ control for the T-S fuzzy systems have been proposed. The conditions are not only simple but also consider the interactions among the fuzzy subsystems. Finally, based on the LMIs, the H∞ controller designing methods for the T-S fuzzy systems have been given.\n",
      "\n",
      "5. id: 558bd4460cf23f2dfc593e08   score: 0.89912134   abstract: The H ¿ filtering problem for two-dimensional Takagi---Sugeno fuzzy systems described by the Fornasini---Marchesini (FM) model is studied. Attention is focused on the design of an H ¿ fuzzy filter such that the filter error system is asymptotically stable and preserves a guaranteed H ¿ performance. By using basis-dependent Lyapunov functions and adding slack matrix variables, the coupling between the Lyapunov matrix and the system matrices is eliminated. Then, a linear matrix inequality (LMI)-based approach is developed for designing the H ¿ fuzzy filter. Finally, an illustrative example is provided to show the effectiveness of the proposed approach and less conservatism.\n",
      "\n",
      "6. id: 5390b5c720f70186a0f0941b   score: 0.89010334   abstract: Fuzzy models of the Takagi-Sugeno type enable representation of a wide class of nonlinear models. Conditions about their stabilization can be derived systematically. Usually these conditions are written as linear matrix inequality (LMI) problems. In this paper, we use a collection of properties concerning matrices to extend the area of solutions involving the feedback control laws. We will work out with continuous fuzzy models, and supply results about the stabilization, the regulator problem, and also Hinfin control. Solutions based on a reduced number of decision variables, implying a reduced number of LMI conditions, are also proposed. We show that a good tradeoff can been obtained.\n",
      "\n",
      "7. id: 5390b5c720f70186a0f09254   score: 0.8866169   abstract: Quadratic stability has enabled, mainly via the linear matrix inequality framework, the analysis and design of a nonlinear control system from the local matrices of the system's Takagi-Sugeno (T-S) fuzzy model. It is well known, however, that there exist stable differential inclusions, hence T-S fuzzy models whose stability is unprovable by a globally quadratic Lyapunov function. At present, literature in the broader area of stability analysis suggests piecewise-quadratic stability as a means to avoid such conservatism. This paper generalizes the idea and proposes a framework that supports less conservative sufficient conditions for the stability of the T-S model by using piecewise-quadratic generalized Lyapunov functions. The advocated approach results in the formulation of the controller synthesis, which, herein, aims for robust stabilization, as a problem of bilinear rather than linea\n",
      "\n",
      "8. id: 5390b0ca20f70186a0edb43a   score: 0.87125593   abstract: This article further studies the observer-based H∞-control problem for discrete-time Takagi-Sugeno (T-S) fuzzy systems. By using fuzzy Lyapunov functions and introducing slack variables, a sufficient condition, which can guarantee observer-based H∞-control performance for T-S fuzzy systems, is proposed in terms of a set of bilinear matrix inequalities. Moreover, in the so-called two-step procedure, results of the first step are allowed to select in order to reduce the conservatism of previous approaches. In comparison with the existing literature, the proposed approach not only provides more relaxed H∞-control conditions but also ensures better H∞-control performance. Finally, the validity and applicability of the proposed approach are successfully demonstrated through two numerical examples.\n",
      "\n",
      "9. id: 5390a5b020f70186a0e7cc3c   score: 0.8510886   abstract: This paper deals with sufficient conditions of asymptotic stability for non linear discrete-time 2D systems represented by a Takagi-Sugeno fuzzy model of Roesser type with state feedback control. This work is based on common and multiple Lyapunov functions. The results are presented in LMI's form.\n",
      "\n",
      "10. id: 5390b78a20f70186a0f246d0   score: 0.84530497   abstract: This paper investigates H\"~ control for general 2D nonlinear systems based on a 2D Takagi-Sugeno (T-S) fuzzy model. The system under consideration is an extension of the general 2D linear system to a nonlinear case. Taking the spatial and structural features into consideration, a 2D T-S fuzzy model is first established. In designing the H\"~ controller, the inputs are regarded as variables independent from the states and then basis-dependent free matrices are introduced. It has been shown that separation of the input and state variables does not lead to any conservativeness compared with the commonly used method. Thus, a fuzzy H\"~ controller is designed using a common free matrix, resulting in only r linear matrix inequalities. The computational advantage is obvious for fuzzy systems with a large number of fuzzy rules. We also demonstrate that the proposed method is less computationally e\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707618\n",
      "index                                        55913a8a0cf232eb904fb5b5\n",
      "title               Using Accelerometer Data to Estimate Surface I...\n",
      "authors             Ilyas Uyanik, Ashik Khatri, Dinesh Majeti, Muh...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390a93b20f70186a0ea0c17;5390a6b120f70186a0e83...\n",
      "abstract            Walking is a fundamental human activity and it...\n",
      "id                                                            1707618\n",
      "clustered_labels                                                    0\n",
      "Name: 1707618, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a6d920f70186a0e87b33   score: 0.8193158   abstract: This paper presents a novel human activity recognizer used to estimate a user's activities with sensors on a widespread consumer mobile device. Our recognizer can estimate a user's means of migration by using a combination of an acceleration sensor and a GPS. We evaluate the accuracy rate of the estimation with cellular phones carried freely and the result is 90.6 percent even in the case of intermittent working mode for lower power consumption.\n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3ea08   score: 0.7360213   abstract: Smart phones equipped with a rich set of sensors are explored as alternative platforms for human activity recognition in the ubiquitous computing domain. However, there exist challenges that should be tackled before the successful acceptance of such systems by the masses. In this paper, we particularly focus on the challenges arising from the differences in user behavior and in the hardware. To investigate the impact of these factors on the recognition accuracy, we performed tests with 20 different users focusing on the recognition of basic locomotion activities using the accelerometer, gyroscope and magnetic field sensors. We investigated the effect of feature types, to represent the raw data, and the use of linear acceleration for user, device and orientation-independent activity recognition.\n",
      "\n",
      "3. id: 558af670612c41e6b9d3ebc8   score: 0.6148566   abstract: We present an algorithm which analyzes walking cadence (momentary step frequency) via frequency-domain analysis of accelerometer signals available in common smartphones, and report its accuracy relative to the published state-of-the-art algorithms based on the data gathered in a controlled user study. We show that our algorithm (RRACE) is more accurate in all conditions, and is also robust to speed change and largely insensitive to orientation, location on person, and user differences. RRACE's performance is suitable for interactive mobile applications: it runs in realtime (~2 s latency), requires no tuning or a priori information, uses an extensible architecture, and can be optimized for the intended application. In addition, we provide an implementation that can be easily deployed on common smartphone platforms. Power consumption is measured and compared to that of the current commerci\n",
      "\n",
      "4. id: 5390b4da20f70186a0f00006   score: 0.60940796   abstract: In this paper, we propose a new system for estimating walking paths, and people's indoor location (e.g., a floor or a room where user is staying) for life-logging. Our proposed system utilizes an acceleration sensor and an orientation sensor of an Android device, and the map matching with the preplotted vector map. Particularly emphasized new techniques in this paper are methods to estimate locations where walking directions are changed, to specify whether an elevator is used to move to another floor, and to estimate the walking path with the vector map matching. Through the experiment and evaluation, we have clarified that the proposed method has successfully estimated a walking path with accuracy of more than 80[%] in all routes, and 92[%] if an elevator is not included.\n",
      "\n",
      "5. id: 5390a1d420f70186a0e57473   score: 0.49673656   abstract: This paper develops an algorithm for robust human activity recognition in the face of imprecise sensor placement. It is motivated by the emerging body sensor networksthat monitor human activities (as opposed to environmental phenomena) for medical, entertainment, health-and-wellness, training, assisted-living, or entertainment reasons. Activities such as sitting, writing, and walking have been successfully inferred from data provided by body-worn accelerometers. A common concern with previous approaches is their sensitivity with respect to sensor placement. This paper makes two contributions. First, we explicitly address robustness of human activity recognition with respect to changes in accelerometer orientation. We develop a novel set of features based on relative activity-specific body-energy allocation and successfully apply them to recognize human activities in the presence of impre\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3d632   score: 0.44782248   abstract: Calibration of accelerometers can be reduced to 3D-ellipsoid fitting problems. Changing extrinsic factors like temperature, pressure or humidity, as well as intrinsic factors like the battery status, demand to calibrate the measurements permanently. Thus, there is a need for fast calibration algorithms, e.g. for online analyses. The primary aim of this paper is to propose a non-iterative calibration algorithm for accelerometers with the focus on minimal execution time and low memory consumption. The secondary aim is to benchmark existing calibration algorithms based on 3D-ellipsoid fitting methods. We compared the algorithms regarding the calibration quality and the execution time as well as the number of quasi-static measurements needed for a stable calibration. As evaluation criterion for the calibration, both the norm of calibrated real-life measurements during inactivity and simulati\n",
      "\n",
      "7. id: 5390bb1d20f70186a0f3e1f6   score: 0.40922078   abstract: Aiming to realize the application which supports users to enjoy walking with an appropriate physical load, we propose a method to estimate physical load and its variation during walking only with available functions of a smartphone. Since physical load has a linear relationship with heart rate, our purpose is to estimate heart rate with a smartphone. To this end, we build heart rate prediction models which predict heart rate variation from walking data including acceleration and walking speed by machine learning. In order to track unexpected change of physical load, we focus attention on oxygen uptake which has a similar property to heart rate and devise a novel technique to estimate the oxygen uptake from acceleration and GPS data so that it is used as an input of the model. Moreover, to adapt to difference of heart rate variation among individuals, we devise techniques to optimize para\n",
      "\n",
      "8. id: 558be9ab0cf20e727d0f3e6a   score: 0.2858701   abstract: This paper presents the results of applying gait and activity recognition on a commercially available mobile smartphone, where both data collection and real-time analysis was done on the phone. The collected data was also transferred to a computer for further analysis and comparison of various distance metrics and machine learning techniques. In our experiment 5 users created each 3 templates on the phone, where the templates were related to different walking speeds. The system was tested for correct identification of the user or the walking activity with 20 new users and with the 5 enrolled users. The activities are recognised correctly with an accuracy of over 99%. For gait recognition the phone learned the individual features of the 5 enrolled participants at the various walk speeds, enabling the phone to afterwards identify the current user. The new Cross Dynamic Time Warping (DTW) M\n",
      "\n",
      "9. id: 539098dc20f70186a0e0d414   score: 0.28081354   abstract: This paper describes a method to evaluate daily physical activity by means of a portable device that determines the type of physical activity based on accelerometers and a barometer. Energy consumption of a given type of physical activity was calculated according to relative metabolic ratio (RMR) of each physical activity type that reflects exercise intensity of activities. Special attention was paid to classification algorithms for activity typing that identify detailed ambulatory movements considering vertical movements, such as stair/slope climbing or use of elevators. A portable measurement device with accelerometers and a barometer, and a Kalman filter was designed to detect the features of vertical movements. Furthermore, walking speed was calculated by an equation which estimates the walking speed as a function of signal energy of vertical body acceleration during walking. To conf\n",
      "\n",
      "10. id: 5390b78a20f70186a0f2474f   score: 0.2786492   abstract: We describe and evaluate two methods for device pose classification and walking speed estimation that generalize well to new users, compared to previous work. These machine learning based methods are designed for the general case of a person holding a mobile device in an unknown location and require only a single low-cost, low-power sensor: a triaxial accelerometer. We evaluate our methods in straight-path indoor walking experiments as well as in natural indoor walking settings. Experiments with 14 human participants to test user generalization show that our pose classifier correctly selects among four device poses with 94% accuracy compared to 82% for previous work, and our walking speed estimates are within 12-15% (straight/indoor walk) of ground truth compared to 17-22% for previous work. Implementation on a mobile phone demonstrates that both methods can run efficiently online.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1715851\n",
      "index                                        55323c5645cec66b6f9dbda9\n",
      "title               GaiusT: supporting the extraction of rights an...\n",
      "authors             Nicola Zeni, Nadzeya Kiyavitskaya, Luisa Mich,...\n",
      "year                                                           2015.0\n",
      "venue                                        Requirements Engineering\n",
      "references          559000be0cf2e9668dc4d6b6;558e10870cf222bc17bc0351\n",
      "abstract            Ensuring compliance of software systems with g...\n",
      "id                                                            1715851\n",
      "clustered_labels                                                    0\n",
      "Name: 1715851, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a28020f70186a0e62ed2   score: 0.9942251   abstract: Government regulations are increasingly affecting the security, privacy and governance of information systems in the United States, Europe and elsewhere. Consequently, companies and software developers are required to ensure that their software systems comply with relevant regulations, either through design or re-engineering. We previously proposed a methodology for extracting stakeholder requirements, called rights and obligations, from regulations. In this paper, we examine the challenges to developing tool support for this methodology using the Cerno framework for textual semantic annotation. We present the results from two empirical evaluations of a tool called \"Gaius T.\" that is implemented using the Cerno framework and that extracts a conceptual model from regulatory texts. The evaluation, carried out on the U.S. HIPAA Privacy Rule and the Italian accessibility law, measures the qu\n",
      "\n",
      "2. id: 53909f8c20f70186a0e40dc2   score: 0.9828892   abstract: Security, privacy and governance are increasingly the focus of government regulations in the U.S., Europe and elsewhere. This trendhas created a \"regulation compliance problem\", whereby companiesand developers are required to ensure that their software complies with relevant regulations, either through design or reengineering. We previously proposed a methodology for extracting stakeholder requirements, called rights and obligations, from regulations. In this paper, we examine the challenges of developing tool support for this process. We apply the Cerno framework for textual semantic annotation to propose a tool for semi-automatic semantic annotation of concepts that constitute sources of requirements\n",
      "\n",
      "3. id: 558b16b5612c41e6b9d42c5a   score: 0.9561454   abstract: Every year, governments introduce new or revised regulations that are imposing new types of requirements on software development. Analyzing and modeling these legal requirements is time consuming, challenging and cumbersome for software and requirements engineers. Having regulation models can help understand regulations and converge toward better compliance levels for software and systems. This paper introduces a systematic method to extract legal requirements from regulations by mapping the latter to the Legal Profile for Goal-oriented Requirements Language (GRL) (Legal GRL). This profile provides a conceptual meta-model for the anatomy of regulations and maps its elements to standard GRL with specialized annotations and links, with analysis techniques that exploit this additional information. The paper also illustrates examples of Legal GRL models for The Privacy and Electronic Communi\n",
      "\n",
      "4. id: 5390a05a20f70186a0e4b5ed   score: 0.9540518   abstract: The increasing complexity of software systems and growing demand for regulations compliance require effective methods and tools to support requirements analysts activities. Internationalization of information systems due to both economics and Web based architectures call for the application of regulations written in different languages. Thus far existing approaches for extracting rights and obligations have concentrated on English documents. In this paper, we describe the results of the application of Cerno, a lightweight framework for semantic annotation, to legal documents written in Italian. In addition, we investigate critical issues for semantic annotation tools in a different cultural and environmental context. Results obtained, while preliminary, allow us to quantify the effort needed to port tools based on Cerno and give some insight on directions of future development ofa multil\n",
      "\n",
      "5. id: 5390b24320f70186a0ee6628   score: 0.8844391   abstract: A software system complies with a regulation if its operation is consistent with the regulation under all circumstances. The importance of regulatory compliance for software systems has been growing, as regulations are increasingly impacting both the functional and nonfunctional requirements of legacy and new systems. HIPAA and SOX are recent examples of laws with broad impact on software systems, as attested by the billions of dollars spent in the US alone on compliance. In this paper we propose a framework for establishing regulatory compliance for a given set of software requirements. The framework assumes as inputs models of the requirements (expressed in i*) and the regulations (expressed in Nòmos). In addition, we adopt and integrate with i* and Nòmos a modeling technique for capturing arguments and establishing their acceptability. Given these, the framework proposes a systematic \n",
      "\n",
      "6. id: 5390bda020f70186a0f47adc   score: 0.8701566   abstract: A software system complies with a regulation if its operation is consistent with the regulation under all circumstances. The importance of regulatory compliance for software systems has been growing, as regulations are increasingly impacting both the functional and non-functional requirements of legacy and new systems. HIPAA and SOX are recent examples of laws with broad impact on software systems, as attested by the billions of dollars spent in the US alone on compliance. In this paper we propose a framework for establishing regulatory compliance for a given set of software requirements. The framework assumes as inputs models of the requirements (expressed in i*) and the regulations (expressed in Nomos). In addition, we adopt and integrate with i* and Nomos a modeling technique for capturing arguments and establishing their acceptability. Given these, the framework proposes a systematic\n",
      "\n",
      "7. id: 5390a37f20f70186a0e6b992   score: 0.86431044   abstract: It is shown that the concepts of requirements and implementation exist in normative systems, in particular in law, and are similar to homologous concepts in software engineering. Concepts of compliance and conformance are also similar in the two areas. Further, it is shown how a logic analyzer such as Alloy can be used in order to verify legal compliance by checking consistency between legal and enterprise requirements. Examples are taken from privacy law and financial reporting law.\n",
      "\n",
      "8. id: 5390ad5620f70186a0ebcb7d   score: 0.8565725   abstract: Legislation is constantly affecting the way in which software developers can create software systems, and deliver them to their users. This raises the need for methods and tools that support developers in the creation and re-distribution of software systems with the ability of properly coping with legal constraints. We conjecture that legal constraints are another dimension software analysts, architects and developers have to consider, making them an important area of future research in software engineering.\n",
      "\n",
      "9. id: 5390aa7620f70186a0eaabd1   score: 0.85342485   abstract: To ensure legal compliance, requirements engineers need tools to determine existing software requirements' compliance with relevant law. We propose using a production rule model for requirements engineers to query as they check software requirements for legal compliance. In this paper, we perform a case study using our approach to evaluate the iTrust Medical Records System requirements for compliance with the u.s. Health Insurance Portability and Accountability Act (HIPAA). We identifY 12 new compliance requirements beyond the 63 functional requirements with which we began our analysis.\n",
      "\n",
      "10. id: 53909fca20f70186a0e44ef1   score: 0.852199   abstract: Information practices that use personal, financial and health-related information are governed by U.S. laws and regulations to prevent unauthorized use and disclosure. To ensure compliance under the law, the security and privacy requirements of relevant software systems must be properly aligned with these regulations. However, these regulations describe stakeholder rules, called rights and obligations, in complex and sometimes ambiguous legal language. These \"rules\" are often precursors to software requirements that must undergo considerable refinement and analysis before they are implementable. To support the software engineering effort to derive security requirements from regulations, we present a methodology to extract access rights and obligations directly from regulation texts. The methodology provides statement-level coverage for an entire regulatory document to consistently identi\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1695028\n",
      "index                                        559249650cf28b1a968ff6e3\n",
      "title               Nordhaus---Gaddum-type results for path coveri...\n",
      "authors             Damei Lü, Juan Du, Nianfeng Lin, Ke Zhang, Dan Yi\n",
      "year                                                           2015.0\n",
      "venue                           Journal of Combinatorial Optimization\n",
      "references          558d207de4b00b95a76702bb;5390b1d220f70186a0ee3...\n",
      "abstract            A Nordhaus---Gaddum-type result is a (tight) l...\n",
      "id                                                            1695028\n",
      "clustered_labels                                                    1\n",
      "Name: 1695028, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a37f20f70186a0e6c1ea   score: 0.9432431   abstract: We report properties, especially upper and lower bounds and the Nordhaus-Gaddum-type result for the reciprocal complementary Wiener number of a connected (molecular) graph.\n",
      "\n",
      "2. id: 5390b48420f70186a0efae2d   score: 0.88759494   abstract: A Nordhaus-Gaddum-type result is a (tight) lower or upper bound on the sum or product of a parameter of a graph and its complement. In this paper we examine the sum and product of @c\"t(G\"1),@c\"t(G\"2),...,@c\"t(G\"k) and the sum of @c(G\"1),@c(G\"2),...,@c(G\"k) where G\"1@?G\"2@?...@?G\"k=K\"n for positive integers n and k, @c(G) is the domination number and @c\"t(G) is total domination number of a graph G. We show that @?\"j\"=\"1^k@c(G\"j)@?(k-1)n+1 with equality if and only if G\"i=K\"n for some i@?{1,...,k}. For n=7, 3@?k@?n-2 and @d(G\"i)=1 for each i@?{1,2,...,k}, we show that @?\"j\"=\"1^k@c\"t(G\"j)@?(k-1)(n+1).\n",
      "\n",
      "3. id: 5390a25820f70186a0e5eb05   score: 0.8700462   abstract: Let G be a graph with minimum degree @d(G), edge-connectivity @l(G), vertex-connectivity @k(G), and let G@? be the complement of G. In this article we prove that either @l(G)=@d(G) or @l(G@?)=@d(G@?). In addition, we present the Nordhaus-Gaddum type result @k(G)+@k(G@?)=min{@d(G),@d(G@?)}+1. A family of examples will show that this inequality is best possible.\n",
      "\n",
      "4. id: 5390880d20f70186a0d7c13f   score: 0.8484707   abstract: We consider the problem of labeling the nodes of a graph in a way that will allow one to compute the distance between any two nodes directly from their labels (without using any additional information). Our main interest is in the minimal length of labels needed in different cases. We obtain upper bounds and (most importantly) lower bounds for several interesting families of graphs. In particular, our main results are the following:For general graphs, the length needed is &THgr;(n).For trees, the length needed is &THgr;(log2 n).For planar graphs, we show an upper bound of &Ogr;(√n log n) and a lower bound of &OHgr;(n1/3).For bounded degree graphs, we show a lower bound of &OHgr;(√n).The upper bounds for planar graphs and for trees follow by a more general upper bound for graphs with a r(n)- separator. The two lower bounds, however, are obtained by two different arguments that may be inte\n",
      "\n",
      "5. id: 559004970cf2351542718ff9   score: 0.82275945   abstract: For a graph G on n vertices with chromatic number @g(G), the Nordhaus-Gaddum inequalities state that @?2n@?@?@g(G)+@g(G@?)@?n+1, and n@?@g(G)@?@g(G@?)@?@?(n+12)^2@?. Much analysis has been done to derive similar inequalities for other graph parameters, all of which are integer-valued. We determine here the optimal Nordhaus-Gaddum inequalities for the circular chromatic number and the fractional chromatic number, the first examples of Nordhaus-Gaddum inequalities where the graph parameters are rational-valued.\n",
      "\n",
      "6. id: 55323c7245cec66b6f9dc1f9   score: 0.82147413   abstract: Suppose $$G$$ G is a graph. Let $$u$$ u be a vertex of $$G$$ G . A vertex $$v$$ v is called an $$i$$ i -neighbor of $$u$$ u if $$d_G(u,v)=i$$ d G ( u , v ) = i . A $$1$$ 1 -neighbor of $$u$$ u is simply called a neighbor of $$u$$ u . Let $$s$$ s and $$t$$ t be two nonnegative integers. Suppose $$f$$ f is an assignment of nonnegative integers to the vertices of $$G$$ G . If the following three conditions are satisfied, then $$f$$ f is called an $$(s,t)$$ ( s , t ) -relaxed $$L(2,1)$$ L ( 2 , 1 ) -labeling of $$G$$ G : (1) for any two adjacent vertices $$u$$ u and $$v$$ v of $$G,\\\\,f(u)\\\\not =f(v)$$ G , f ( u ) ¿ f ( v ) ; (2) for any vertex $$u$$ u of $$G$$ G , there are at most $$s$$ s neighbors of $$u$$ u receiving labels from $$\\\\{f(u)-1,f(u)+1\\\\}$$ { f ( u ) ¿ 1 , f ( u ) + 1 } ; (3) for any vertex $$u$$ u of $$G$$ G , the number of $$2$$ 2 -neighbors of $$u$$ u assigned the label $$f\n",
      "\n",
      "7. id: 5390994d20f70186a0e11bf0   score: 0.81888163   abstract: For given integers $j \\ge k \\ge 1$, an $L(j,k)$-labelling of a graph $\\Ga$ is an assignment of labels---nonnegative integers---to the vertices of $\\Ga$ such that adjacent vertices receive labels that differ by at least $j$, and vertices distance two apart receive labels that differ by at least $k$. The span of such a labelling is the difference between the largest and the smallest labels used, and the minimum span over all $L(j,k)$-labellings of $\\Ga$ is denoted by $\\l_{j,k}(\\Ga)$. The minimum number of labels needed in an $L(j,k)$-labelling of $\\Ga$ is independent of $j$ and $k$, and is denoted by $\\mu(\\Ga)$. In this paper we introduce a general approach to $L(j,k)$-labelling Cayley graphs $\\Ga$ over Abelian groups and deriving upper bounds for $\\l_{j,k}(\\Ga)$ and $\\mu(\\Ga)$. Using this approach we obtain upper bounds on $\\l_{j,k}(\\Ga)$ and $\\mu(\\Ga)$ for graphs $\\Ga$ admitting a vertex\n",
      "\n",
      "8. id: 5390adfd20f70186a0ec5a03   score: 0.8155265   abstract: For an integer k=1, the k-improper upper chromatic number@g@?\"k\"-\"i\"m\"p(G) of a graph G is introduced here as the maximum number of colors permitted to color the vertices of G such that, for any vertex v in G, at most k vertices in the neighborhood N(v) of v receive colors different from that of v. The exact value of @g@?\"k\"-\"i\"m\"p is determined for several types of graphs, and general estimates are given in terms of various graph invariants, e.g. minimum and maximum degree, vertex covering number, domination number and neighborhood number. Along with bounds on @g@?\"k\"-\"i\"m\"p for Cartesian products of graphs, exact results are found for hypercubes. Also, the analogue of the Nordhaus-Gaddum theorem is proved. Moreover, the algorithmic complexity of determining @g@?\"k\"-\"i\"m\"p is studied, and structural correspondence between k-improper C-colorings and certain kinds of edge cuts is shown.\n",
      "\n",
      "9. id: 5390b86b20f70186a0f2827e   score: 0.8118252   abstract: The induced path number 驴(G) of a graph G is defined as the minimum number of subsets into which the vertex set of G can be partitioned so that each subset induces a graph. A Nordhaus-Gaddum-type result is a (tight) lower or upper bound on the sum (or product) of a parameter of a graph and its complement. If G is a subgraph of H, then the graph H驴E(G) is the complement of G relative to H. In this paper, we consider Nordhaus-Gaddum-type results for the parameter 驴 when the relative complement is taken with respect to the complete bipartite graph K n,n .\n",
      "\n",
      "10. id: 558d02c50cf2a2c70f68c8f8   score: 0.81047887   abstract: For positive integers k,d\\\"1,d\\\"2, a k-L(d\\\"1,d\\\"2)-labeling of a graph G is a function f:V(G)-{0,1,2,...,k} such that |f(u)-f(v)|=d\\\"i whenever the distance between u and v is i in G, for i=1,2. The L(d\\\"1,d\\\"2)-number of G, @l\\\"d\\\"\\\"\\\"1\\\",\\\"d\\\"\\\"\\\"2(G), is the smallest k such that there exists a k-L(d\\\"1,d\\\"2)-labeling of G. This class of labelings is motivated by the code (or frequency) assignment problem in computer network. This article surveys the results on this labeling problem.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1697979\n",
      "index                                        55912b5d0cf232eb904fb146\n",
      "title               An Automated approach for Bug Categorization u...\n",
      "authors                                 Indu Chawla, Sandeep K. Singh\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 8th India Software Engineer...\n",
      "references          558b21c6612c41e6b9d445d2;5390a25820f70186a0e5f...\n",
      "abstract            Various automated techniques built to benefit ...\n",
      "id                                                            1697979\n",
      "clustered_labels                                                    3\n",
      "Name: 1697979, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b24c5612c41e6b9d44b20   score: 0.8665848   abstract: Context: Bug reports are the primary means by which users of a system are able to communicate a problem to the developers, and their contents are important - not only to support developers in maintaining the system, but also as the basis of automated tools to assist in the challenging tasks of finding and fixing bugs. Goal: This paper aims to investigate how users report bugs in systems: what information is provided, how frequently, and the consequences of this. Method: The study examined the quality and quantity of information provided in 1600 bugs reports drawn from four open-source projects (Eclipse, Firefox, Apache HTTP, and Facebook API), recorded what information users actually provide, how and when users provide the information, and how this affects the outcome of the bug. Results: Of the recorded sources of information, only observed behaviour and expected results appeared in mor\n",
      "\n",
      "2. id: 558b2adb612c41e6b9d456af   score: 0.83467656   abstract: Issue tracking systems are valuable resources during software maintenance activities. These systems contain different categories of issue reports such as bug, request for improvement (RFE), documentation, refactoring, task etc. While logging issue reports into a tracking system, reporters can indicate the category of the reports. Herzig et al. Recently reported that more than 40% of issue reports are given wrong categories in issue tracking systems. Among issue reports that are marked as bugs, more than 30% of them are not bug reports. The misclassification of issue reports can adversely affects developers as they then need to manually identify the categories of various issue reports. To address this problem, in this paper we propose an automated technique that reclassifies an issue report into an appropriate category. Our approach extracts various feature values from a bug report and pr\n",
      "\n",
      "3. id: 5390bb1d20f70186a0f3cf07   score: 0.79865825   abstract: Issue tracking systems play a central role in ongoing software development; they are used by developers to support collaborative bug fixing and the implementation of new features, but they are also used by other stakeholders including managers, QA, and end-users for tasks such as project management, communication and discussion, code reviews, and history tracking. Most such systems are designed around the central metaphor of the \"issue\" (bug, defect, ticket, feature, etc.), yet increasingly this model seems ill fitted to the practical needs of growing software projects; for example, our analysis of interviews with 20 Mozilla developers who use Bugzilla heavily revealed that developers face challenges maintaining a global understanding of the issues they are involved with, and that they desire improved support for situational awareness that is difficult to achieve with current issue manag\n",
      "\n",
      "4. id: 5390bb7b20f70186a0f3fec2   score: 0.73659015   abstract: While requirements for open source projects originate from a variety of sources like e.g. mailing lists or blogs, typically, they eventually end up as feature requests in an issue tracking system. When analyzing how these issue trackers are used for requirements evolution, we witnessed a high percentage of duplicates in a number of high-prole projects. Further investigation of six open source projects and their users led us to a number of important observations and a categorization of the root causes of these duplicates. Based on this, we propose a set of improvements for future issue tracking systems.\n",
      "\n",
      "5. id: 5554d68f0cf2edeeee33f4cb   score: 0.69461346   abstract: Improve automatic bug assignment (ABA) accuracy by using metadata in term weighting.Improve accuracy of common term-weighting technique, tf-idf, up to 14%.Recommend a light method for ABA based on the new term-weighting technique.Outperform the ML and IR methods by recommended method up to 55%. Bug assignment is one of the important activities in bug triaging that aims to assign bugs to the appropriate developers for fixing. Many recommended automatic bug assignment approaches are based on text analysis methods such as machine learning and information retrieval methods. Most of these approaches use term-weighting techniques, such as term frequency-inverse document frequency (tf-idf), to determine the value of terms. However, the existing term-weighting techniques only deal with frequency of terms without considering the metadata associated with the terms that exist in software repositori\n",
      "\n",
      "6. id: 5390b0ca20f70186a0eda914   score: 0.6670392   abstract: Background: Bug Tracking Repositories, such as Bugzilla, are designed to support fault reporting for developers, testers and users of the system. Allowing anyone to contribute finding and reporting faults has an immediate impact on software quality. However, this benefit comes with at least one side-effect. Users often file reports that describe the same fault. This increases the maintainer's triage time, but important information required to fix the fault is likely contributed by different reports. Aim: The objective of this paper is twofold. First, we want to understand the dynamics of bug report filing for a large, long duration open source project, Firefox. Second, we present a new approach that can reduce the number of duplicate reports. Method: The novel element in the proposed approach is the ability to concentrate the search for duplicates on specific portions of the bug reposito\n",
      "\n",
      "7. id: 558b8772612c6b62e5e8ac16   score: 0.6374823   abstract: Issue tracking systems are valuable resources during software maintenance activities and contain information about the issues faced during the development of a project as well as after its release. Many projects receive many reports of bugs and it is challenging for developers to manually debug and fix them. To mitigate this problem, past studies have proposed information retrieval (IR)-based bug localization techniques, which takes as input a textual description of a bug stored in an issue tracking system, and returns a list of potentially buggy source code files. These studies often evaluate their effectiveness on issue reports marked as bugs in issue tracking systems, using as ground truth the set of files that are modified in commits that fix each bug. However, there are a number of potential biases that can impact the validity of the results reported in these studies. First, issue r\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3e8f7   score: 0.62977463   abstract: Bug tracking systems play an important role in the software development process since they allow users to report bugs they have encountered. Unfortunately, the information provided in the bug reports may contain errors since the reporter is not always acquainted with the technical nature of the software project. It remains a manual process left to the bug triager to correct reports with erroneous information. Consequently, this results in a delay of the resolution time of a bug. In this study, we propose to use data-mining techniques to make early predictions of which particular reported bugs are assigned to the incorrect component. This way, bug triagers are assisted in their task of identifying reported bugs where the \"component\" field contains an incorrect value. By using open-source cases like Mozilla and Eclipse, we demonstrate the possibility of predicting which particular bug is l\n",
      "\n",
      "9. id: 5390bb1d20f70186a0f3e657   score: 0.62452257   abstract: The number of reported bugs in large open source projects is high and triaging these bugs is an important issue in software maintenance. As a step in the bug triaging process, assigning a new bug to the most appropriate developer to fix it, is not only a time-consuming and tedious task. The triager, the person who considers a bug and assigns it to a developer, also needs to be aware of developer activities at different parts of the project. It is clear that only a few developers have this ability to carry out this step of bug triaging. The main goal of this paper is to suggest a new approach to the process of performing automatic bug assignment. The information needed to select the best developers to fix a new bug report is extracted from the version control repository of the project. Unlike all the previous suggested approaches which used Machine Learning and Information Retrieval metho\n",
      "\n",
      "10. id: 5537da0b0cf23ee1cc7679f2   score: 0.5916643   abstract: Bug tracking systems play an important role in the development and maintenance of large-scale software systems. Having access to open source bug tracking systems has allowed researchers to take advantage of rich datasets and propose solutions to manage duplicate report classification, developer assignment and quality assessment. In spite of research advances, our understanding of the content of these repositories remains limited, primarily because of their size. In many cases, researchers analyze small portions of datasets thus limiting the understanding of the dynamics of problem reporting. The objective of this study is to explore the properties of two large-scale open source problem report repositories. The Eclipse dataset, at the time of download, consisted of 363; 770 reports spanning 11+ years, whereas Mozilla contained 699; 085 reports spanning 14+ years.Our research examines the \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1673062\n",
      "index                                        559129380cf232eb904fb0b2\n",
      "title               Reconfigurable Binding against FPGA Replay Att...\n",
      "authors                            Jiliang Zhang, Yaping Lin, Gang Qu\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Design Automation of Elect...\n",
      "references          558bcb6e612cf64242758570;5390a05920f70186a0e49...\n",
      "abstract            The FPGA replay attack, where an attacker down...\n",
      "id                                                            1673062\n",
      "clustered_labels                                                    3\n",
      "Name: 1673062, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a1d420f70186a0e56d0e   score: 0.9492946   abstract: FPGAs are widely used recently, and security on configuration bitstreams is of concern to both users and suppliers of configuration bitstreams (e.g., intellectual property vendors). In order to protect configuration bitstreams against the threats such as FPGA viruses, piracy and reverse engineering, configuration bitstreams need to be encrypted and authenticated before loaded into FPGAs. In this paper, we propose a new FPGA configuration scheme that can authenticate and/or decrypt a bitstream. The proposed scheme has flexibility in choosing authentication and/or decryption algorithms and causes only a small area overhead since it utilizes programmable logic blocks to implement authentication and/or decryption circuits.\n",
      "\n",
      "2. id: 558b8bd6612c6b62e5e8b39f   score: 0.9492005   abstract: This paper investigates and addresses the vulnerabilities of existing physical unclonable functions (PUFs). We first develop a PUF reverse engineering approach by conducting gate-level characterization (GLC). Based on the gate-level delay properties, we emulate the target PUF by designing a functionally equivalent PUF replication. Furthermore, in order to prevent such an attack, we develop a new sequential PUF architecture that is resilient to side channel-based reverse engineering. We obtain accurate results in emulating the timing behavior of the existing arbiter-based PUFs. Also, the randomness obtained from the sequential PUFs is significantly higher compared to the existing PUFs.\n",
      "\n",
      "3. id: 5390a40520f70186a0e6f6cc   score: 0.947186   abstract: We propose a number of techniques for securing finite state machines (FSMs) against fault injection attacks. The proposed security mechanisms are based on physically unclonable functions (PUFs), and they address different fault injection threats on various parts of the FSM. The first mechanism targets the protection of state-transitions in a specific class of FSMs. The second mechanism addresses the integrity of secret information. This is of particular interest in cryptographic FSMs which require a secret key. Finally, the last mechanism we propose introduces a new fault-resilient error detection network (EDN). Previous designs for EDNs always assume resilience to fault injection attacks without providing a particular construction. The PUF-based EDN design is suitable for a variety of applications, and is essential for most fault resilient state machines. Due to the usage of PUFs in the\n",
      "\n",
      "4. id: 53909f8220f70186a0e3d533   score: 0.92482364   abstract: This paper presents a solution to protect FSM implemented on FPGAs from SEU, exploiting the embedded memories available in modern FPGA devices and a Hamming code for error detection and correction. A fault tolerant FSM architecture is presented, along with a generator to automate the FSM implementation. Experimental results show that this solution is particularly suited especially when FSMs with a large number of outputs are present in the target design.\n",
      "\n",
      "5. id: 5390bb1d20f70186a0f3e8e0   score: 0.90000373   abstract: As reuse-based design methodology has prevailed in FPGA design field, the FPGA core industry is confronted with the increasing threat of cloning attacks. How to protect modular designed hardware IP (HW-IP) cores against non-authorized over-use and redistribution has become a serious problem. In this paper, we propose a new binding mechanism completely different from the traditional encryption-based binding methods, which restricts the hardware design's execution to the specific FPGA device and meanwhile provides a pay-per-use licensing manner to better meet the market needs. In this mechanism, the FPGA vendors provide each enrolled device with a Physical Unclonable Function (PUF) which can be deployed securely in the fabric of an FPGA. The core vendor embeds the added Finite State Machine (FSM) into the original FSM structure of HW-IP to react with the PUF response. Removing or tampering\n",
      "\n",
      "6. id: 5390b95520f70186a0f2e951   score: 0.8912444   abstract: A new Physically Unclonable Function (PUF) variant was developed on an FPGA, and its quality evaluated. It is conceptually similar to PUFs developed using standard SRAM cells, except it utilizes general FPGA reconfigurable fabric, which offers several advantages. Comparison between our approach and other PUF designs indicates that our design is competitive in terms of repeatability within a given instance, and uniqueness between instances. The design can also be tuned to achieve desired response characteristics which broadens the potential range of applications.\n",
      "\n",
      "7. id: 5390a2e920f70186a0e68712   score: 0.8806945   abstract: Physically unclonable functions (PUFs) provide a basis for many security and digital rights management protocols. PUF-based security approaches have numerous comparative strengths with respect to traditional cryptography-based techniques, including resilience against physical and side channel attacks and suitability for lightweight protocols. However, classical delay-based PUF structures have a number of drawbacks including susceptibility to guessing, reverse engineering, and emulation attacks, as well as sensitivity to operational and environmental variations. To address these limitations, we have developed a new set of techniques for FPGA-based PUF design and implementation. We demonstrate how reconfigurability can be exploited to eliminate the stated PUF limitations. We also show how FPGA-based PUFs can be used for privacy protection. Furthermore, reconfigurability enables the introdu\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3e8df   score: 0.85752964   abstract: Physically unclonable functions (PUFs) have been a hot research topic in hardware-oriented security for many years. Given a challenge as an input to the PUF, it generates a corresponding response, which can be treated as a unique fingerprint or signature for authentication purpose. In this paper, a delay-based PUF design involving multiplexers on FPGA is presented. Due to the intrinsic difference of the switching latencies of two chained multiplexers, a positive pulse may be produced at the output of the downstream multiplexer. This pulse can be used to set the output of a D flip-flop to '1'. Further, it is proposed to directly incorporate challenge bits into the primitive PUF design to bring another layer of randomness for the response. Evaluation results on various devices and under different operating temperatures demonstrate the applicability of the proposed PUF design.\n",
      "\n",
      "9. id: 5390bded20f70186a0f4a3a9   score: 0.8508408   abstract: In this work we consider the suitability of Phyiscaly Unclonable Functions (PUFs) for high-security applications. For PUFs to be considered secure in such scenarios they must be resilient to both semi-invasive and fully-invasive attacks. We introduce a new failure analysis technique for semi-invasive, single-trace, backside readout of logic states. We apply this technique to characterize the unique physical response of a memory-based PUF. With these results we identify several weakness in current PUF schemes. We extend current PUF definitions to be resilient against such attacks by requiring that PUFs be implemented in a serialized manner. Finally, we improve already existing PUF architectures to include these concepts.\n",
      "\n",
      "10. id: 55913b750cf232eb904fb5f8   score: 0.8456877   abstract: A new definition of \\\"Physical Unclonable Functions\\\" (PUFs), the first one that fully captures its intuitive idea among experts, is presented. A PUF is an information-storage system with a security mechanism that is 1. meant to impede the duplication of a precisely described storage-functionality in another, separate system and 2. remains effective against an attacker with temporary access to the whole original system. A novel classification scheme of the security objectives and mechanisms of PUFs is proposed and its usefulness to aid future research and security evaluation is demonstrated. One class of PUF security mechanisms that prevents an attacker to apply all addresses at which secrets are stored in the information-storage system, is shown to be closely analogous to cryptographic encryption. Its development marks the dawn of a new fundamental primitive of hardware-security enginee\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1705634\n",
      "index                                        55323b5e45cec66b6f9d9c93\n",
      "title               A Relay Based Routing Protocol for Wireless In...\n",
      "authors             Nadeem Javaid, Ashfaq Ahmad, Yahya Khan, Zahoo...\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references          5390b52620f70186a0f0327a;5390a88c20f70186a0e99...\n",
      "abstract            Efficient energy utilization of in-body sensor...\n",
      "id                                                            1705634\n",
      "clustered_labels                                                    1\n",
      "Name: 1705634, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908e0020f70186a0dd4325   score: 0.9084556   abstract: Abstract: Wireless sensor networks have become possible because of the on-going improvements in sensor technology and VLSI. One issue in smart sensor networks is achieving efficient operation because of the limited available power. For important classes of sensor networks, such as biomedical sensors, the locations of the sensing nodes are fixed and the placement can be pre-determined. In this paper, we consider the topology that best supports communication among these sensor nodes. We propose a power-aware routing protocol and simulate the performance, showing that our routing protocol adapts routes to the available power. This leads to a reduction in the total power used as well as more even power usage across nodes. We consider different routes and topologies, demonstrating the difference in performance and explaining the underlying causes.\n",
      "\n",
      "2. id: 5390a25820f70186a0e5ef39   score: 0.86916035   abstract: A body sensor network using human body as a communication medium is analyzed and designed to achieve both power-and energy-efficiency. An analysis of the body channel network on frequency, distance, transmitting power and received power is performed. The analysis reveals the star topology consumes less energy than the ad-hoc topology for body channel network. Based on the analysis results, the packet structure for body channel network, with variable payload size that minimizes energy consumption, is designed.\n",
      "\n",
      "3. id: 5390bed320f70186a0f4f28d   score: 0.8517063   abstract: In this work, we propose a reliable, power efficient and high throughput routing protocol for Wireless Body Area Networks (WBANs). We use multi-hop topology to achieve minimum energy consumption and longer network lifetime. We propose a cost function to select parent node or forwarder. Proposed cost function selects a parent node which has high residual energy and minimum distance to sink. Residual energy parameter balances the energy consumption among the sensor nodes while distance parameter ensures successful packet delivery to sink. Simulation results show that our proposed protocol maximize the network stability period and nodes stay alive for longer period. Longer stability period contributes high packet delivery to sink which is major interest for continuous patient monitoring.\n",
      "\n",
      "4. id: 558b2b4d612c41e6b9d457df   score: 0.7871142   abstract: Wireless Body Area sensor Networks (WBANs) enable innovative health care monitoring. Limited energy source of a sensor node limits WBANs for long time monitoring of health care. Efficient energy utilization is therefore one of the research challenges inWBANs. In this research work we analysed energy utilization of popular routing techniques. We formulate a mathematical framework to identify energy utilization in transmission, receive and over-hearing processes. Simulation results show that how distance, packet size, and over-hearing effect different routing techniques from energy consumption perspective. From the analysis, we produced useful results which are helpful in: identifying overloaded nodes in the network which may cause creation of energy holes, and in designing new routing techniques for specific WBANs application.\n",
      "\n",
      "5. id: 5390ad0720f70186a0ebb3c4   score: 0.7816641   abstract: Body sensor networks are emerging as a promising platform for healthcare monitoring. These systems are composed of battery-operated embedded devices which process physiological data. The reduction in the power consumption is an important factor to increase the lifetime for such systems and to enhance their wearability through reducing the size of the battery. In this paper, we develop an energy-efficient communication scheme that uses buffers to reduce the number of transmissions among the sensor nodes constrained to limited hardware resources. A direct acyclic graph is used to model the information flow. We define a communication optimization problem and solve it using convex optimization techniques. We present results that support the efficiency of the proposed technique.\n",
      "\n",
      "6. id: 5390bfa220f70186a0f53d4e   score: 0.7414869   abstract: Wireless body sensor networks are expected to extend human-centered applications in large-scale sensing and detecting environments. Energy savings has become one of the most important features of the sensor nodes to prolong their lifetime in such networks. To provide reasonable energy consumption and to improve the network lifetime of wireless body sensor network systems, new and efficient energy-saving schemes must be developed. An energy-saving routing architecture with a uniform clustering algorithm is proposed in this paper to reduce the energy consumption in wireless body sensor networks. We adopted centralized and cluster-based techniques to create a cluster-tree routing structure for the sensor nodes. The main goal of this scheme is to reduce the data transmission distances of the sensor nodes by using the uniform cluster structure concepts. To make an ideal cluster distribution, \n",
      "\n",
      "7. id: 5390be6620f70186a0f4c012   score: 0.7358315   abstract: Body area networks (BAN) require careful hardware and software design which will enable the battery powered sensor devices to operate effectively and reliably over long periods of time. On body sensor design is a challenging task due to the body being in the near-field of the radio and the complex interaction between the two. Off-the-shelf wireless sensors can be optimized to realize an efficient BAN by combining an enhanced shorted patch antenna design with a cross layer energy aware protocol. Here, we demonstrate a system design approach where the performance of WBANs is greatly enhanced.\n",
      "\n",
      "8. id: 5390b78a20f70186a0f25115   score: 0.69925445   abstract: This paper presents a survey of energy efficiency of Medium Access Control (MAC) protocols for Wireless Body Area Sensor Networks (WBASNs). We highlight the features of MAC protocols along with their advantages and limitations in context of WBASNs. Comparison of Low Power Listening (LPL), Scheduled Contention and Time Division Multiple Access (TDMA) is also elaborated. MAC protocols with respect to different approaches and techniques which are used for energy minimization, traffic control mechanisms for collision avoidance are discussed. We also present a survey of path loss models for In-body, On-body and Off-body communications in WBASNs and analytically discuss that path loss is maximum in In-body communication because of low energy levels to take care of tissues and organs located inside the body. Survey of Power model for WBANs of CSMA/CA and beacon mode is also presented.\n",
      "\n",
      "9. id: 5390a5dc20f70186a0e7f716   score: 0.68794453   abstract: We consider a body area network application of monitoring a patient continuously [1,2]. In this application, several sensors are used in measuring physiological and metabolic readings of a patient. In such a system, the sampling rates have to be coordinated to maximize the life time of this sensor network system. We formulate the relationship between energy consumption, sensor sampling rates, and utility of coordination as a Markov Decision Process and compute a globally optimal policy for sensor sampling rates. We then present an entropy-based mechanism for communication between nodes to execute this global policy. We show preliminary results on simulated data to demonstrate that this distributed control framework is feasible for a limited number of sensors.\n",
      "\n",
      "10. id: 5390aefb20f70186a0eccdf4   score: 0.6556668   abstract: Wireless sensor networks are composed of energy-constrained nodes. Therefore, it is crucial to design routing algorithms that optimize the energy usage of nodes. Aimed at maximizing the network lifetime, we introduced an optimal routing algorithm based on the max-min model. In this algorithm, the data transmission matrix is defined and the relaying node selection mechanism is designed to avoid possible routing loops. Based on the energy consumption for sending and receiving data and the available residual energy of nodes, the mathematical programming model is designed to find optimal routing. The routing paths and data volume are determined by nodes according to the optimization of parameters in the model. Simulation results show that the algorithm balances the energy consumption of nodes effectively and extends the network lifetime.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652382\n",
      "index                                        55916faa0cf2e89307ca9ca8\n",
      "title                    Computer Scientists at the Biology Lab Bench\n",
      "authors                               Andrea Tartaro, Renee J. Chosed\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 46th ACM Technical Symposiu...\n",
      "references          558aeb63612c41e6b9d3d761;5390bda020f70186a0f47...\n",
      "abstract            We present the development, implementation and...\n",
      "id                                                            1652382\n",
      "clustered_labels                                                    0\n",
      "Name: 1652382, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba0a20f70186a0f34267   score: 0.9740426   abstract: This paper describes our implementation and experience of incorporating computer science concepts into a team-taught, first-year interdisciplinary course for prospective science majors at the University of Richmond. The course integrates essential concepts from each of five STEM disciplines: biology, chemistry, computer science, mathematics, and physics. Including computer science in this course faces three primary challenges: few of the students have any CS background; the time devoted to CS instruction is reduced compared to a traditional introductory CS course; and the spirit of the course requires the CS material to be highly integrated with the other disciplines. Here we discuss our experience from three-plus years of offering the course and its impact on the major/minor pool of students in our own discipline.\n",
      "\n",
      "2. id: 539087cb20f70186a0d58a42   score: 0.9452985   abstract: This paper discusses the implementation and evolution over a five-year period of a breadth-first introductory computer science course which has both lectures and structured, closed laboratory sessions. This course significantly increased both the retention and passing rates for the next computer course (which emphasizes programming), and computer science graduation rates.\n",
      "\n",
      "3. id: 5390980720f70186a0e01e9b   score: 0.9443824   abstract: Many colleges and universities have recently begun offering courses and degree programs in bioinformatics. This paper describes an Introduction to Bioinformatics course that emphasizes the informatics issues, and can be taught to upper-level undergraduate computer science students and graduate students. The course can be adapted to fit the interests and experience of a faculty member in computer science who has only a moderate amount of familiarity with molecular biology.\n",
      "\n",
      "4. id: 5390980720f70186a0e01bdb   score: 0.9323302   abstract: Bioinformatics is a relatively new interdisciplinary field with computer science and biology as its core supporting disciplines. The rise of bioinformatics presents computer science faculty members with new opportunities and challenges both in education and research. This interdisciplinary panel, comprising two computer scientists and two biologists, will explore issues in teaching bioinformatics courses, developing bioinformatics curricula, and fostering student research projects in bioinformatics.\n",
      "\n",
      "5. id: 5390994d20f70186a0e134cb   score: 0.92643696   abstract: An interdisciplinary bioinformatics course has been taught at Wake Forest for three semesters. Undergraduate and graduate students from multiple academic specialties are brought together in a single classroom. In addition to focusing on traditional bioinformatics topics, this course concentrates on interdisciplinary collaboration in the in-class exercises and the research-based course project. A team of faculty from complementary disciplines teach the course. Productive communication is one key goal of this course.\n",
      "\n",
      "6. id: 5390a2be20f70186a0e63aaf   score: 0.92482364   abstract: This book is written by an experienced author team representing the many areas from which the new discipline of Bioinformatics is emerging. Their common sense approach and carefully detailed presentations in Bioinformatics: A Computing Perspective blends computing and biological sciences in an engaging and unique way. Bioinformatics: A Computing Approach helps students become conversant with key concepts in the biological sciences and knowledgeable about current programming tools and approaches. It successfully ties interesting computational challenges to relevant biological phenomenon in a way that will engage the next generation of scientists.\n",
      "\n",
      "7. id: 55323d3e45cec66b6f9dde23   score: 0.92455155   abstract: For a number of years we have been teaching a biology themed introductory computer science course at Harvey Mudd College.\n",
      "\n",
      "8. id: 53909e8b20f70186a0e2de98   score: 0.9199307   abstract: An interdisciplinary undergraduate research project in bioinformatics, jointly mentored by faculty in computer science and biology, has been developed and is being used to provide top-quality instruction to biology and computer science students. This paper explains the benefits of such a collaboration to computer science students and to the computer science discipline. Specific goals of the project include increased recruitment of students into computer science and increased retention within the discipline. The project is also intended to be particularly attractive to women students.\n",
      "\n",
      "9. id: 53909f6a20f70186a0e3b709   score: 0.9199307   abstract: This paper discusses the results of a survey conducted to evaluate the recently started bioinformatics curriculum at Fort Valley State University (FVSU). Bioinformatics curriculum implementation has been planned to take a three-pronged approach which includes a bioinformatics course, course-embedded modules that can be integrated into selected science courses, and training workshops for faculty and curriculum development. In particular, the bioinformatics course was evaluated for its pre-course preparation, outcomes, difficulty, workload requirement, and pace of covered material. The analysis of the data indicated more than satisfactory results in several sections. Overall 89.3% students evaluated the bioinformatics course as satisfactory. The goal is to seamlessly integrate biological and computer sciences to establish a bioinformatics curriculum that would appeal to both students and f\n",
      "\n",
      "10. id: 5390a4cc20f70186a0e750c2   score: 0.89460546   abstract: Combining biology, computer science, mathematics, and statistics, the field of bioinformatics has become a hot new discipline with profound impacts on all aspects of biology and industrial application. Now, Computational Intelligence in Bioinformatics offers an introduction to the topic, covering the most relevant and popular CI methods, while also encouraging the implementation of these methods to readers' research.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1686325\n",
      "index                                        55913ffe0cf232eb904fb759\n",
      "title               RaftLib: a C++ template library for high perfo...\n",
      "authors              Jonathan C. Beard, Peng Li, Roger D. Chamberlain\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Sixth International Worksho...\n",
      "references          558adac7612c41e6b9d3b9f6;558bf5600cf23f2dfc595...\n",
      "abstract            Stream processing or data-flow programming is ...\n",
      "id                                                            1686325\n",
      "clustered_labels                                                    3\n",
      "Name: 1686325, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b0b11612c41e6b9d413f9   score: 0.8778961   abstract: Current work on parallel programming models are trending towards the dataflow paradigm. Previous works on that topic have shown that dataflow programming is indeed a natural way to exploit parallelism in programs. However, there is still a gap in terms of ease of programming between high level languages adopted by the scientific community and the languages and tools available for dataflow programming. In this paper we present Sucuri: a minimalistic Python library that provides dataflow programming with reasonably simple syntax. To parallelize applications using our library, the programmer needs only to identify functions of his code that are good candidates for parallelization and instantiate a dataflow graph where each node is associated with one of such functions, and the edges between nodes describe data dependencies between functions. We then proceed to implement two benchmarks that \n",
      "\n",
      "2. id: 5390b5df20f70186a0f0b5b3   score: 0.84273374   abstract: Stream programming as a kind of important paradigm has been widely used in many applications. However, the diversity of multicore architecture makes the stream programs difficult to port between different platforms. X10 isolates the different multi-core architecture details and provides a unified parallel computation environment for the applications. In this paper, we propose StreamX10, a stream programming framework on X10. StreamX10 is composed of two parts: COStream programming language and the compiler which translates the COStream programs to the parallel X10 code. In the framework, we also propose a partitioning optimization phase to exploit the parallelisms for the stream programs based on the activity/place parallel execution model in X10. We implemented the StreamX10 stream programming framework and performed the experiments to evaluate the system.\n",
      "\n",
      "3. id: 5390b4da20f70186a0effb79   score: 0.8067004   abstract: Stream processing is a special form of the dataflow execution model that offers extensive opportunities for optimization and automatic parallelization. To take full advantage of the paradigm, however, typically requires programmers to learn a new language and re-implement their applications. This work shows that it is possible to exploit streaming as a safe and automatic optimization of a more general dataflow-based modelâ\"one in which computation kernels are written in standard, general-purpose languages and organizedas a coordination graph.We propose Streaming Concurrent Collections (SCnC), a streaming system that can efficiently run a subset of programs supported by Concurrent Collections (CnC). CnC is a general purpose parallel programming paradigm with a task-parallel look and feel but based on dataflow graph principles. Its expressiveness extends to any arbitrary task graph. Integ\n",
      "\n",
      "4. id: 539099ec20f70186a0e1d962   score: 0.7841538   abstract: As multicore architectures enter the mainstream, there is a pressing demand for high-level programming models that can effectively map to them. Stream programming offers an attractive way to expose coarse-grained parallelism, as streaming applications (image, video, DSP, etc.) are naturally represented by independent filters that communicate over explicit data channels.In this paper, we demonstrate an end-to-end stream compiler that attains robust multicore performance in the face of varying application characteristics. As benchmarks exhibit different amounts of task, data, and pipeline parallelism, we exploit all types of parallelism in a unified manner in order to achieve this generality. Our compiler, which maps from the StreamIt language to the 16-core Raw architecture, attains a 11.2x mean speedup over a single-core baseline, and a 1.84x speedup over our previous work.\n",
      "\n",
      "5. id: 5536867f0cf2dbb77a816b4c   score: 0.76153845   abstract: Embedded many-core systems offering thousands of cores should be available in the near future. Stream programming is a particular instance of data-flow programming where computations are expressed as the data-driven execution of repetitive \\\"filters\\\" on data streams. Stream programming fits these manycore systems' requirements in terms of parallelism, functional determinism, and local data reuse. Statically or semi-dynamically scheduled stream languages like e.g. StreamIt and ?C can generate very efficient parallel code, but have strict limitations with respect to the expression of dynamic computational tasks, context-dependent modes of operation, and dynamic memory management. This paper compares two state-of-the-art stream languages, StreamIt and ?C, with the aim of better understanding their strengths and weaknesses, and finding a way to improve them. We also propose an automatic con\n",
      "\n",
      "6. id: 5390aeba20f70186a0eca77e   score: 0.65379035   abstract: This paper introduces an extension to OpenMP3.0 enabling stream programming with minimal, incremental additions that seamlessly integrate into the current specification. The stream programming model decomposes programs into tasks and explicits the flow of data among them, thus exposing data, task and pipeline parallelism. It helps the programmers to express concurrency and data locality properties, avoiding non-portable low-level code and early optimizations. We survey the diverse motivations and constraints converging towards the design of our simple yet powerful language extension, and we present experimental results of a prototype implementation in a public branch of GCC 4.5.\n",
      "\n",
      "7. id: 5390b00c20f70186a0ed45ca   score: 0.624179   abstract: Parallel programming has become mandatory to fully exploit the potential of multi-core CPUs. The dataflow model provides a natural way to exploit parallelism. However, specifying dependences and control using fine-grained instructions in dataflow programs can be complex and present unwanted overheads. To address this issue, we have designed TALM: a coarse-grained dataflow execution model to be used on top of widespread architectures. We implemented TALM as the Trebuchet virtual machine for multi-cores. The programmer identifies code blocks that can run in parallel and connects them to form a dataflow graph, which allows one to have the benefits of parallel dataflow execution in a Von Neumann machine, with small programming effort. We parallelised a set of seven applications using our approach and compared with OpenMP implementations. Results show that Trebuchet can be competitive with st\n",
      "\n",
      "8. id: 5390a17720f70186a0e51ddb   score: 0.5601023   abstract: As multicore architectures gain widespread use, it becomes increasingly important to be able to harness their additional processing power to achieve higher performance. However, exploiting parallel cores to improve single-program performance is difficult from a programmer's perspective because most existing programming languages dictate a sequential method of execution. Stream programming, which organizes programs by independent filters communicating over explicit data channels, exposes useful types of parallelism that can be exploited. However, there is still the burden of mapping high-level stream programs to specific multicore architectures. The complexities of each architecture's underlying details makes it difficult to schedule the execution of a stream program with high performance. In this paper, we present the specifications for an intermediate layer between the stream program an\n",
      "\n",
      "9. id: 53908af920f70186a0daeaea   score: 0.53101176   abstract: The recent parallel language standard for shared memory multiprocessor (SMP) machines, OpenMP, promises a simple and portable interface for programmers who wish to exploit parallelism explicitly. In this paper, we present our effort to develop portable compilers for the OpenMP parallel directive language. Our compiler consists of two parts. Part one is an OpenMP parallelizer, which transforms sequential languages into OpenMP. Part two transforms programs written in OpenMP into thread-based form and links with our runtime library. Both compilers are built on the Polaris compiler infrastructure. We present performance measurements showing that our compiler yields results comparable to those of commercial OpenMP compilers. Our infrastructure is freely available with the intent to enable research projects on OpenMP-related language development and compiler techniques.\n",
      "\n",
      "10. id: 5390be6620f70186a0f4bb67   score: 0.53043413   abstract: Stream processing applications use online analytics to ingest high-rate data sources, process them on-the-fly, and generate live results in a timely manner. The data flow graph representation of these applications facilitates the specification of stream computing tasks with ease, and also lends itself to possible runtime exploitation of parallelization on multicore processors. While the data flow graphs naturally contain a rich set of parallelization opportunities, exploiting them is challenging due to the combinatorial number of possible configurations. Furthermore, the best configuration is dynamic in nature; it can differ across multiple runs of the application, and even during different phases of the same run. In this paper, we propose an autopipelining solution that can take advantage of multicore processors to improve throughput of streaming applications, in an effective and transp\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710966\n",
      "index                                        55323be845cec66b6f9dad6b\n",
      "title               Fast memory efficient evaluation of spherical ...\n",
      "authors                                Kamen Ivanov, Pencho Petrushev\n",
      "year                                                           2015.0\n",
      "venue                           Advances in Computational Mathematics\n",
      "references          558fae530cf2b6664046725a;539087c320f70186a0d55...\n",
      "abstract            A method for fast evaluation of band-limited f...\n",
      "id                                                            1710966\n",
      "clustered_labels                                                    2\n",
      "Name: 1710966, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390994d20f70186a0e11c10   score: 0.90665317   abstract: An algorithm is introduced for the rapid evaluation at appropriately chosen nodes on the two-dimensional sphere $S^2$ in ${\\mathbb R}^3$ of functions specified by their spherical harmonic expansions (known as the inverse spherical harmonic transform), and for the evaluation of the coefficients in spherical harmonic expansions of functions specified by their values at appropriately chosen points on $S^2$ (known as the forward spherical harmonic transform). The procedure is numerically stable and requires an amount of CPU time proportional to $N^2 (\\log N) \\log(1/\\epsilon)$, where $N^2$ is the number of nodes in the discretization of $S^2$, and $\\epsilon$ is the precision of computations. The performance of the algorithm is illustrated via several numerical examples.\n",
      "\n",
      "2. id: 53909ee020f70186a0e33f52   score: 0.8992984   abstract: A fast algorithm for the computation of spherical harmonic expansions of bandlimited functions on the 2-sphere is described. The algorithm also provides for an efficient inverse transform (synthesis) as well and consequently fast convolution on the 2-sphere is also possible. We discuss applications to image processing and medical imaging as well as aspects of our working implementation.\n",
      "\n",
      "3. id: 5390b60d20f70186a0f13025   score: 0.73469096   abstract: In this paper, we present a new and simple method for fitting a spherelike surface to a set of data points in 3-space. In comparison to the standard method that involves using spherical harmonics, this method is conceptually simpler and computationally less complex and intensive, especially when used to address problems in which data is sparse or nonuniform.\n",
      "\n",
      "4. id: 539087f320f70186a0d6ffc7   score: 0.63973606   abstract: Several algorithms for efficiently evaluating trigonometric polynomials at irregularly spaced points are presented and analyzed. The algorithms can be viewed as approximate generalizations of the fast Fourier transform (FFT), and they are compared with regard to their accuracy and their computational efficiency.\n",
      "\n",
      "5. id: 5390a01420f70186a0e46d9e   score: 0.5480084   abstract: We provide an efficient algorithm for calculating, at appropriately chosen points on the two-dimensional surface of the unit sphere in R^3, the values of functions that are specified by their spherical harmonic expansions (a procedure known as the inverse spherical harmonic transform). We also provide an efficient algorithm for calculating the coefficients in the spherical harmonic expansions of functions that are specified by their values at these appropriately chosen points (a procedure known as the forward spherical harmonic transform). The algorithms are numerically stable, and, if the number of points in our standard tensor-product discretization of the surface of the sphere is proportional to l^2, then the algorithms have costs proportional to l^2ln(l) at any fixed precision of computations. Several numerical examples illustrate the performance of the algorithms.\n",
      "\n",
      "6. id: 5390b00c20f70186a0ed5d4d   score: 0.41941014   abstract: Fast, accurate and memory-efficient method is proposed for computing orthogonal Fourier---Mellin moments. Since the basis polynomials are continuous orthogonal polynomials defined in polar coordinates over a unit disk, the proposed method is applied to polar coordinates where the unit disk is divided into a number of non-overlapping circular rings that are divided into circular sectors of the same area. Each sector is represented by one point in its center. The implementation of this method completely removes both approximation and geometrical errors produced by the conventional methods. Based on the symmetry property, a fast and memory-efficient algorithm is proposed to accelerate the moment's computations. A comparison to conventional methods is performed. Numerical experiments are performed to ensure the efficiency of the proposed method.\n",
      "\n",
      "7. id: 53909fca20f70186a0e44817   score: 0.39846498   abstract: The verification of the existence of certain spherical t- designs involves the evaluation of a degree-t polynomial Jt at a very large number of (interval) arguments. To make the overall verification process feasible computationally, this evaluation must be fast, and the enclosures for the function values must be affected with only modest over-estimation. We discuss several approaches for multi-argument interval evaluation of the polynomial Jt and show how they can be adapted to other polynomials p. One particularly effective new method is based on expanding the polynomial p around several points j and truncating each resulting expansion pj to a lower-degree polynomial.\n",
      "\n",
      "8. id: 5390a72220f70186a0e8afa8   score: 0.37983832   abstract: The purpose of this paper is to construct universal, auto-adaptive, localized, linear, polynomial (-valued) operators based on scattered data on the (hyper) sphere $\\mathbb{S}^q$ ($q\\ge2$). The approximation and localization properties of our operators are studied theoretically in deterministic as well as probabilistic settings. Numerical experiments are presented to demonstrate their superiority over traditional least squares and discrete Fourier projection polynomial approximations. An essential ingredient in our construction is the construction of quadrature formulas based on scattered data, exact for integrating spherical polynomials of (moderately) high degree. Our formulas are based on scattered sites; i.e., in contrast to such well-known formulas as Driscoll-Healy formulas, we need not choose the location of the sites in any particular manner. While the previous attempts to constr\n",
      "\n",
      "9. id: 5390a9a420f70186a0ea4ab0   score: 0.23493282   abstract: In this paper a new efficient algorithm for spherical interpolation of large scattered data sets is presented. The solution method is local and involves a modified spherical Shepard's interpolant, which uses zonal basis functions as local approximants. The associated algorithm is implemented and optimized by applying a nearest neighbour searching procedure on the sphere. Specifically, this technique is mainly based on the partition of the sphere in a suitable number of spherical zones, the construction of spherical caps as local neighbourhoods for each node, and finally the employment of a spherical zone searching procedure. Computational cost and storage requirements of the spherical algorithm are analyzed. Moreover, several numerical results show the good accuracy of the method and the high efficiency of the proposed algorithm.\n",
      "\n",
      "10. id: 539087f320f70186a0d6e9da   score: 0.23440666   abstract: This paper presents a new method for the fast evaluation of univariate radial basis functions of the form $s(x) = \\sum_{n=1}^N d_n \\phi ( | x -x_n | ) $ to within accuracy $\\epsilon$. The method can be viewed as a generalization of the fast multipole method in which calculations with far field expansions are replaced by calculations involving moments of the data. The method has the advantage of being adaptive to changes in $\\phi$. That is, with this method changing to a new $\\phi$ requires only coding a one- or two-line function for the (slow) evaluation of $\\phi$. In contrast, adapting the usual fast multipole method to a new $\\phi$ involves much mathematical analysis of appropriate series expansions and corresponding translation operators, followed by a substantial amount of work expressing this mathematics in code.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1686919\n",
      "index                                        558da19d0cf2af9ee80e9d8f\n",
      "title               Analysis and characterization of a video-on-de...\n",
      "authors             Ahmed Ali-Eldin, Maria Kihl, Johan Tordsson, E...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 6th ACM Multimedia Systems ...\n",
      "references          558b126b612c41e6b9d421a5;5390b60d20f70186a0f13...\n",
      "abstract            Video-on-Demand (VoD) and video sharing servic...\n",
      "id                                                            1686919\n",
      "clustered_labels                                                    3\n",
      "Name: 1686919, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908bde20f70186a0dc8ea6   score: 0.9444849   abstract: Several works have recently appeared in the literature on the design, performance and dimensioning of networks supporting video on demand and multimedia services. It has been shown that performance is strongly dependent on the model adopted to describe the user's behaviour in terms of requests for programs. In this paper we provide a detailed investigation of a non stationary model for the user's request distribution. Numerical results are provided for a network scenario proposed in the literature to investigate the error achieved when the non stationary distribution is approximated with a stationary exponential one.\n",
      "\n",
      "2. id: 5390979920f70186a0dff352   score: 0.93318766   abstract: In this paper, we study the live streaming workload from a large content delivery network. Our data, collected over a 3 month period, contains over 70 million requests for 5,000 distinct URLs from clients in over 200 countries. To our knowledge, this is the most extensive data of live streaming on the Internet that has been studied to date. Our contributions are two-fold. First, we present a macroscopic analysis of the workload, characterizing popularity, arrival process, session duration, and transport protocol use. Our results show that popularity follows a 2-mode Zipf distribution, session interarrivals within small time-windows are exponential, session durations are heavy-tailed, and that UDP is far from having universal reach on the Internet. Second, we cover two additional characteristics that are more specific to the nature of live streaming applications: the diversity of clients \n",
      "\n",
      "3. id: 5390a8b220f70186a0e9b829   score: 0.92696756   abstract: Recently, as the rapid growth in the number of VoD system user, people ask for more and more video resources, which cause overload on server. Therefore, how to use the capability of peers to alleviate the server loading and at the same time maintaining the streaming performance becomes a new challenge for p2p technology. In this paper, after analyzing a large amount of data, we present some valuable results on user behavior and the performance of system. Then we discuss the relationship between user behavior, server load and quality of video. Our paper provides valuable insights which can improve VoD system.\n",
      "\n",
      "4. id: 53909e7c20f70186a0e2c290   score: 0.8702668   abstract: Video-on-demand over IP (VOD) is one of the best-known examples of \"next-generation\" Internet applications cited as a goal by networking and multimedia researchers. Without empirical data, researchers have generally relied on simulated models to drive their design and developmental efforts. In this paper, we present one of the first measurement studies of a large VOD system, using data covering 219 days and more than 150,000 users in a VOD system deployed by China Telecom. Our study focuses on user behavior, content access patterns, and their implications on the design of multimedia streaming systems. Our results also show that when used to model the user-arrival rate, the traditional Poisson model is conservative and overestimates the probability of large arrival groups. We introduce a modified Poisson distribution that more accurately models our observations. We also observe a surprisi\n",
      "\n",
      "5. id: 5390b5c620f70186a0f08216   score: 0.8655654   abstract: Online social networks (OSNs) have become popular destinations for connecting friends and sharing information. Recent statistics suggest that OSN users regularly share contents from video sites, and a significant amount of requests of the video sites are indeed from them nowadays. These behaviors have substantially changed the workload of online video services. To better understand this paradigm shift, we conduct a long-term and extensive measurement of video sharing in RenRen, the largest Facebook-like OSN in China. In this paper, we focus on the video popularity distribution and evolution. In particular, we find that the video popularity distribution exhibits perfect power-law feature (while videos in YouTube exhibit a power-law waist with a long truncated tail). Moreover, we observe that the requests for the new published videos generally experience two or three days latency to reach \n",
      "\n",
      "6. id: 5390a93b20f70186a0e9ff3d   score: 0.7981868   abstract: Internet Video sharing sites, led by YouTube , have been gaining popularity in a dazzling speed, which also brings massive workload to their service data centers. In this paper we analyze Yahoo! Video, the 2nd largest U.S. video sharing site, to understand the nature of such unprecedented massive workload as well as its impact on online video data center design. We crawled the Yahoo! Video web site for 46days. The measurement data allows us to understand the workload characteristics at different time scales (minutes, hours, days, weeks), and we discover interesting statistical properties on both static and temporal dimensions of the workload including file duration and popularity distributions, arrival rate dynamics and predictability, and workload stationarity and burstiness. Complemented with queueing-theoretic techniques, we further extend our understanding on the measurement data wit\n",
      "\n",
      "7. id: 5390a55520f70186a0e7a234   score: 0.7937462   abstract: Despite the popularity of streaming applications over Internet, little is known about its practical properties which could be potentially exploited to guide the system design of next generation peer-to-peer (P2P) networks. In this paper, we hence investigate the problem of understanding and supporting the two types of dominant streaming services - live broadcasting and on-demand streaming - with high service availability and system scalability. We make a measurement study to traditional but popular client/server (C/S) systems to reveal many fundamental and interesting observations with the benefit of totally more than 30,000,000 real workload traces. The anatomy particularly focuses on request and popularity issues which play a vital role to the system performance and also characterize evolutional user behaviors in the service community. The inherent request characteristics in live broad\n",
      "\n",
      "8. id: 5390a4cc20f70186a0e7550e   score: 0.7844842   abstract: Accurate workload prediction in multimedia systems requires a description of both its probabilistic properties and the interdependence between the variables of the system. This paper analyzes the interactions of the users of a news and entertainment video-on-demand service (LNE TV, http://tv.lne.es) during six months of activity. The analysis was performed using information extracted from the server logs, analyzing more than 300,000 requests for almost 1500 videos. The type of content on the site and the structure of LNE TV, which is similar to most Internet news services supported by a traditional newspaper, make this work an interesting case study, and the results can be easily extrapolated to similar audio/video streaming services. This paper shows that client interactivity in multimedia systems has the dependence structure of a stochastic process. In previous work, this type of study\n",
      "\n",
      "9. id: 5390a28020f70186a0e622b1   score: 0.7595823   abstract: In this paper we measured and analyzed the workload on Yahoo! Video, the 2nd largest U.S. video sharing site, to understand its nature and the impact on online video data center design. We discovered interesting statistical properties on both static and temporal dimensions of the workload; they include file duration and popularity distributions, arrival rate dynamics and predictability, and workload stationarity and burstiness. Complemented with queueing-theoretic techniques, we extended our understanding on the measurement data with a virtual data center design assuming the same workload as measured, which reveals results regarding the impact of workload arrival distribution, Service Level Agreements (SLAs) and workload scheduling schemes on the design and operations of such large-scale video distribution systems.\n",
      "\n",
      "10. id: 53909f8220f70186a0e3ca14   score: 0.75437254   abstract: This paper presents a traffic characterization study of the popular video sharing service, YouTube. Over a three month period we observed almost 25 million transactions between users on an edge network and YouTube, including more than 600,000 video downloads. We also monitored the globally popular videos over this period of time. In the paper we examine usage patterns, file properties, popularity and referencing characteristics, and transfer behaviors of YouTube, and compare them to traditional Web and media streaming workload characteristics. We conclude the paper with a discussion of the implications of the observed characteristics. For example, we find that as with the traditional Web, caching could improve the end user experience, reduce network bandwidth consumption, and reduce the load on YouTube's core server infrastructure. Unlike traditional Web caching, Web 2.0 provides additio\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675333\n",
      "index                                        55913fc70cf232eb904fb74b\n",
      "title               Exploring Cyberbullying and Other Toxic Behavi...\n",
      "authors                 Haewoon Kwak, Jeremy Blackburn, Seungyeop Han\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390b4c420f70186a0efe9b6;5390bed320f70186a0f4e...\n",
      "abstract            In this work we explore cyberbullying and othe...\n",
      "id                                                            1675333\n",
      "clustered_labels                                                    0\n",
      "Name: 1675333, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bed320f70186a0f4eb44   score: 0.9874236   abstract: One problem facing players of competitive games is negative, or toxic, behavior. League of Legends, the largest eSport game, uses a crowdsourcing platform called the Tribunal to judge whether a reported toxic player should be punished or not. The Tribunal is a two stage system requiring reports from those players that directly observe toxic behavior, and human experts that review aggregated reports. While this system has successfully dealt with the vague nature of toxic behavior by majority rules based on many votes, it naturally requires tremendous cost, time, and human efforts. In this paper, we propose a supervised learning approach for predicting crowdsourced decisions on toxic behavior with large-scale labeled data collections; over 10 million user reports involved in 1.46 million toxic players and corresponding crowdsourced decisions. Our result shows good performance in detecting \n",
      "\n",
      "2. id: 5390bda020f70186a0f47538   score: 0.61641645   abstract: Deviant behavior in online social systems is a difficult problem to address. Consequences of deviance include driving off users and tarnishing the system's public image. We present an examination of these concepts in a popular online game, League of Legends. Using a large collection of game records and player-given feedback, we develop a metric, toxicity index, to identify deviant players. We then look at the effects of interacting with deviant players, including effects on retention. We find that toxic players have several significant predictive patterns, such as playing in more competitive game modes and playing with friends. We also show that toxic players drive away new players, but that experienced players are more resilient to deviant behavior. Based on our findings, we suggest methods to better identify and counteract the negative effects of deviance.\n",
      "\n",
      "3. id: 5390b72e20f70186a0f20b9b   score: 0.45565268   abstract: Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tr\n",
      "\n",
      "4. id: 5390b5ed20f70186a0f0d36f   score: 0.37250522   abstract: The rapid growth of social networking and gaming sites is associated with an increase of online bullying activities which, in the worst scenario, result in suicidal attempts by the victims. In this paper, we propose an effective technique to detect and rank the most influential persons (predators and victims). It simplifies the network communication problem through a proposed detection graph model. The experimental results indicate that this technique is highly accurate.\n",
      "\n",
      "5. id: 5390b1d220f70186a0ee1874   score: 0.29380956   abstract: Automated approaches to prevent the negative effects of cyber bullying mainly focus on affective agents that provide support for victims. The current paper takes a complementary approach, which attempts to minimise the amount of occurrences of cyber bullying in the first place. The approach consists of a system of normative agents, which are physically present in a virtual society. The agents, which reason based on a BDI-model, use a number of techniques to detect various norm violations, including insulting and following. By using rewards and punishments, they try to reinforce the desired behaviour of the users. The system has been implemented and tested within a virtual environment for children between 6 and 12 years old, called Club Time Machine. In a real world experiment, the behaviour of the users of the virtual environment has been logged and analysed by means of a logic-based che\n",
      "\n",
      "6. id: 558b299b612c41e6b9d453d2   score: 0.2590751   abstract: The issue of cyberbullying is a social concern that has arisen due to the prevalent use of computer technology today. In this paper, we present a multi-faceted solution to mitigate the effects of cyberbullying, one that uses computer technology in order to combat the problem. We propose to provide assistance for various groups affected by cyberbullying (the bullied and the bully, both). Our solution was developed through a series of group projects and includes i) technology to detect the occurrence of cyberbullying ii) technology to enable reporting of cyberbullying iii) proposals to integrate third-party assistance when cyberbullying is detected iv) facilities for those with authority to manage online social networks or to take actions against detected bullies. In all, we demonstrate how this important social problem which arises due to computer technology can also leverage computer tec\n",
      "\n",
      "7. id: 5390b5ed20f70186a0f0d8e3   score: 0.2166738   abstract: Bullying is a social phenomenon that is highly prevalent within the school population. To study this phenomenon, social scientists traditionally use questionnaires that are costly to administer and that cannot provide detailed information about children's interactions without causing a large amount of fatigue to the participants. An on-line computer game has been developed to aid social scientists in observing, in a non-intrusive way, children's behaviors and roles within their peer group. Participants solve a collaborative and an adversarial task, and are allowed to communicate only through a chat system. Observable data from the game, such as the amount of messages sent and received and points transactions, correlates well with questionnaire data while providing more detailed information about participants' interactions. The online game is a new tool that alleviates the cost of obtaini\n",
      "\n",
      "8. id: 558ae9cd612c41e6b9d3d3e8   score: 0.20561503   abstract: The Internet, the landmark invention of our lifetime, has brought us great benefit, but along with it, risk and antisocial behavior, including online bullying or \\\"cyberbullying\\\". Defined as the use of electronic technology to demonstrate behavior that teases, demeans, or harasses someone less powerful, the global pervasiveness of online bullying is supported by data including that in a 2012 Microsoft study of young people worldwide ages 8 to 17. Prevention lies in the promotion of \\\"digital citizenship\\\"-safer, responsible, and appropriate use of technology and services. And, while no singular entity can combat online bullying alone, Internet companies can play their part, as exemplified by the robust tools and resources offered by Microsoft and others. A collective focus, however, is needed to help raise awareness and change behavior and, that responsibility must be shared among paren\n",
      "\n",
      "9. id: 5390881820f70186a0d80f60   score: 0.18565173   abstract: 'Bad' behavior is a serious problem in many online social situations, such as chat rooms. One potential reason is that social norms for 'proper' interpersonal behavior are not invoked in these situations as they are in face-to-face interactions. We describe a game we developed to explore good and bad behavior in computer-mediated situations. We found that increasing the 'social' nature of the interaction through voice communication between game partners decreased aversive behavior, but having profile information about the other person had little impact.\n",
      "\n",
      "10. id: 5390b9d520f70186a0f3238a   score: 0.16721867   abstract: We present a first set of automatic information which is generated in the Web 2.0 with regard to people, whether it is individually or in groups, totally distort reality through the data that are associated to it. Besides, we analyze for the first time cyberbullyism in some European and American regions which violates the safety controls in the servers and the execution void in the application of the national and international laws in the custody of the rights of the internet users. Finally is presented an experimental heuristic technique for the first evaluation of the profile of the cyberbully, with the purpose of quantifying the human and economic damage in the credibility of on-line information.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1729983\n",
      "index                                        55323d8045cec66b6f9de891\n",
      "title               Coherent crosstalk noise analyses in ring-base...\n",
      "authors             Luan H. K. Duong, Mahdi Nikdast, Jiang Xu, Zhe...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references          558af186612c41e6b9d3e1df;558b0ae3612c41e6b9d41383\n",
      "abstract            Recently, optical interconnects have been prop...\n",
      "id                                                            1729983\n",
      "clustered_labels                                                    0\n",
      "Name: 1729983, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b52620f70186a0f0322a   score: 0.97767127   abstract: Optical network-on-chip (ONoC) architectures are emerging as promising contenders to solve bandwidth and latency issues in multiprocessor systems-on-chip (MPSoC). However, current on-chip integration technologies for optical interconnect allow interconnecting only dozens of IPs. Scaling with MPSoCs composed of hundreds of IPs thus, relies on unpredictable technological innovations. In this letter, we propose a method that combines multiple ONoCs. Each ONoC is small enough to rely on already existing and proven technologies. We evaluate the approach for various interconnect scenarios, showing that it scales well with the size of the MPSoC architectures.\n",
      "\n",
      "2. id: 5390ac1820f70186a0eb4a32   score: 0.96411926   abstract: Editor's note:On-chip optical links are an efficient means of designing the communication backbone for massive multicore chips. Using nanophotonic technology lets designers develop a low-power, low-latency interconnection infrastructure for many-core chips.—Partha Pande, Washington State University\n",
      "\n",
      "3. id: 5390a5b020f70186a0e7c762   score: 0.961171   abstract: Networks-on-chip (NoCs) can improve the communication bandwidth and power efficiency of multiprocessor systems-on-chip (MPSoC). However, traditional metallic interconnects consume significant amount of power to deliver even higher communication bandwidth required in the near future. Optical NoCs are based on optical interconnects and optical routers, and have significant bandwidth and power advantages. This paper proposed a high-performance low-power low-cost optical router, Cygnus, for optical NoCs. Cygnus is non-blocking and based on silicon microresonators. We compared Cygnus with other microresonator-based routers, and analyzed their power consumption, optical power insertion loss, and the number of microresonators used in detail. The results show that Cygnus has the lowest power consumption and losses, and requires the lowest number of microresonators. For example, Cygnus has 50% le\n",
      "\n",
      "4. id: 53909eef20f70186a0e36389   score: 0.95388025   abstract: In the near future, Multi-Processor Systems-on-Chip (MPSoC) will become the main thrust driving the evolution of integrated circuits. MPSoCs introduce new challenges, mainly due to growing communication through their interconnect structure. Current electrical interconnects will face hard challenges to overcome such data flows. Integrated optical interconnect is a potential technological improvement to reduce these problems. The main contributions of this paper are i) the optical network integration in a system-level MPSoC platform and ii) the quantitative evaluation of optical interconnect for MPSoC design using a multimedia application.\n",
      "\n",
      "5. id: 558ce3bc0cf2cffe760cdd82   score: 0.94994867   abstract: Crosstalk noise is an intrinsic characteristic as well as a potential issue of photonic devices. In large scale optical networks-on-chips (ONoCs), crosstalk noise could cause severe performance degradation and prevent ONoC from communicating properly. The novel contribution of this paper is the systematical modeling and analysis of the crosstalk noise and the signal-to-noise ratio (SNR) of optical routers and mesh-based ONoCs using a formal method. Formal analytical models for the worst-case crosstalk noise and minimum SNR in mesh-based ONoCs are presented. The crosstalk analysis is performed at device, router, and network levels. A general 5$\\\\,\\\\times\\\\,$5 optical router model is proposed for router level analysis. The minimum SNR optical link candidates, which constrain the scalability of mesh-based ONoCs, are identified. It is also shown that symmetric mesh-based ONoCs have the best \n",
      "\n",
      "6. id: 5390ab8820f70186a0eaf8a0   score: 0.9494823   abstract: Over the past years, there have been impressive advances in bandwidth, latency, and scalability of on-chip networks. However, unless the off-chip network bandwidth and latency are also improved, we might have unbalanced systems which will limit the improvements to overall system performance. In this paper, we show how dense wavelength-division multiplexing (DWDM) -based optical interconnects could be used to emulate multiple buses in a fully-buffered DIMM (FB-DIMM) -like memory system to improve both bandwidth and latency. We evaluate an optically connected memory using full-system simulations of an 8-core system running memory-intensive multithreaded workloads. We show that for the FFT benchmark, optically connected memory can reduce the average memory request latency by 29% compared to a single-channel DDR3 SDRAM system and provide an overall performance speedup of 1.20. We also show t\n",
      "\n",
      "7. id: 53909f2d20f70186a0e38e09   score: 0.92495924   abstract: Photonic interconnects have the long-term potential to reduce latency and crosstalk while increasing signalling bandwidth. This tutorial-style presentation for non-experts will provide a simple overview of photonics as it relates to the interconnect problem. We will touch upon the challenges faced by on-chip and chip-to-chip communications that motivate micro-photonic (optical) interconnects, and provide an introduction to the basics of optics. Topics such as the system impact of optical interconnects, impact on cache, chip-to-chip communication and global clock distribution will be examined. Practical feasibility issues and limitations related to power, speed, and technology hurdles will be evaluated. The talk will conclude with a look at some fascinating recent developments that bode well for the future of micro-photonic interconnects.\n",
      "\n",
      "8. id: 558b0ae3612c41e6b9d41383   score: 0.92262185   abstract: Photonic devices are widely used in optical networks-on-chip (ONoCs) and suffer from crosstalk noise. The accumulative crosstalk noise in large scale ONoCs diminishes the signal-to-noise ratio (SNR), causes severe performance degradation, and constrains the network scalability. For the first time, this paper systematically analyzes and models the worst-case crosstalk noise and SNR in folded-torus-based ONoCs. Formal analytical models for the worst-case crosstalk noise and SNR are presented. The crosstalk noise analysis is hierarchically performed at the basic photonic device level, then at the optical router level, and finally at the network level. We consider a general 5$\\\\,\\\\times\\\\,$5 optical router model to enable crosstalk noise and SNR analyses in folded-torus-based ONoCs using an arbitrary 5$\\\\,\\\\times\\\\,$ 5 optical router. Using the general optical router model, the worst-case SN\n",
      "\n",
      "9. id: 5390b00c20f70186a0ed5756   score: 0.90958625   abstract: The increasing number of cores in contemporary and future many-core processors will continue to demand high through-put, scalable, and energy efficient on-chip interconnection networks. To overcome the intrinsic inefficiency of electrical interconnects, researchers have leveraged recent developments in chip photonics to design novel optical network-on-chip (NoC). However, existing optical NoCs are mostly based on passively switched, channel-guided optical interconnect in which large amount of power is wasted in heating the micro-rings and maintaining the optical signal integrity. In this paper we present an optical NoC based on free-space optical interconnect in which optical signals emitted from the transmitter is propagated in the free space in the package. With lower attenuation and no coupling effects, free-space optical interconnects have less overheads to maintain the signal integr\n",
      "\n",
      "10. id: 53908bde20f70186a0dc7c89   score: 0.9079672   abstract: Optical communication, in particular, wavelength division multiplexing (WDM) technique, has become a promising networking choice to meet ever-increasing demands on bandwidth from emerging bandwidth-intensive computing/communication applications. As optics become a major networking media in all communications needs, optical interconnects will inevitably play an important role in interconnecting processors in parallel and distributed computing systems. In this paper, we consider cost-effective designs of WDM optical interconnects for current and future generation parallel and distributed computing and communication systems. We first classify WDM optical interconnects into two different connection models based on their target applications: the wavelength-based model and the fiber-link-based model. We then focus on the wavelength-based model and present a minimum cost design for WDM optical \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698617\n",
      "index                                        558b2c8a612c41e6b9d45b23\n",
      "title               Low frequency graphene resonators for acoustic...\n",
      "authors             Eldad Grady, Enrico Mastropaolo, Tao Chen, And...\n",
      "year                                                           2015.0\n",
      "venue                                     Microelectronic Engineering\n",
      "references                                   5390afc920f70186a0ed2dbe\n",
      "abstract            Monolayer graphene resonators have been fabric...\n",
      "id                                                            1698617\n",
      "clustered_labels                                                    0\n",
      "Name: 1698617, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390adfd20f70186a0ec5578   score: 0.99482024   abstract: This article talks about graphene and its properties.\n",
      "\n",
      "2. id: 558af760612c41e6b9d3ee78   score: 0.99433607   abstract: Monolayer graphene and graphite thin films were fabricated on SiO\\\"2/Si substrates by organic coating and post annealing. Pure nickel (Ni) was deposited on the substrate surface as the catalyst. Then the samples were dipped in the Orange II organic solution. Raman spectroscopy measurements suggested that the Orange II organic solution resolved on the Ni surface and formed graphene and graphite thin layers during the heating process.\n",
      "\n",
      "3. id: 5390bd1520f70186a0f4366f   score: 0.9938548   abstract: Monolayer graphene is obtained bymechanical exfoliation using scotch tapes. The effects of thermal annealing on the tape residues and edges of graphene are researched. Atomic force microscope images showed that almost all the residues could be removed in N2/H2 at 400°C but only agglomerated in vacuum. Raman spectra of the annealed graphene show both the 2D peak and G peak blueshift. The full width at halfmaximum (FWHM) of the 2D peak becomes larger and the intensity ratio of the 2D peak to G peak decreases. The edges of graphene are completely attached to the surface of the substrate after annealing.\n",
      "\n",
      "4. id: 5390b56a20f70186a0f04dda   score: 0.9912209   abstract: The coupling of the plasmon spectra of graphene and a nearby thick plasma is examined here in detail. The coupled modes include linear plasmons.\n",
      "\n",
      "5. id: 5390bda020f70186a0f47d02   score: 0.990732   abstract: While significant obstacles remain, researchers are optimistic about using DNA to guide graphene into complex circuit shapes on silicon.\n",
      "\n",
      "6. id: 5390bda020f70186a0f479ca   score: 0.98840266   abstract: We have investigated the possibility to use carbon-diamond layers exhibiting nano-sized grains for the fabrication of SAW resonators. One of the main purpose was to check the influence of the grain size on the resonator's spectral signature and to evaluate the fabrication of high frequency sources along this approach. We have investigated various material combinations on Nano-Carbon-Diamond (NCD) layers grown along different techniques (nano-seeding, BEN, followed by MPCVD). The best configuration was based on ZnO/NCD material combination provided the piezoelectric layer was deposited on the so-called nucleation surface of the NCD. Numerous resonators have been fabricated on such substrates, allowing to characterize and exploit the resonance near 3GHz. An oscillator then was built using such a configuration: its stability has been measured near 10^-^7 and its phase noise was found near -\n",
      "\n",
      "7. id: 558ae7b9612c41e6b9d3cf52   score: 0.9861514   abstract: Graphene is one of the carbon allotropes which is a single atom thin layer with sp2 hybridized and two-dimensional (2D) honeycomb structure of carbon. As an outstanding material exhibiting unique mechanical, electrical, and chemical characteristics including high strength, high conductivity, and high surface area, graphene has earned a remarkable position in today's experimental and theoretical studies as well as industrial applications. One such application incorporates the idea of using graphene to achieve accuracy and higher speed in detection devices utilized in cases where gas sensing is required. Although there are plenty of experimental studies in this field, the lack of analytical models is felt deeply. To start with modelling, the field effect transistor- (FET-) based structure has been chosen to serve as the platform and bilayer graphene density of state variation effect by NO2\n",
      "\n",
      "8. id: 55323cd345cec66b6f9dcf53   score: 0.9853279   abstract: HighlightsWe demonstrate the operation of double-layer graphene vertical tunnel FETs.We have used large-area CVD graphene, and atomic-layer deposited high-k dielectrics.We obtain\n",
      "\n",
      "9. id: 558cc3480cf221faa7370254   score: 0.98521465   abstract: A systematic study of the Cu-catalyzed chemical vapor deposition of graphene under extremely low partial pressure is carried out. A carbon precursor supply of just $\\\\it P$ $\\\\bf_{CH_4}$$\\\\sim$ 0.009 mbar during the deposition favors the formation of large-area uniform monolayer graphene verified by Raman spectra. A diluted $\\\\bf HNO_3$ solution is used to remove Cu before transferring graphene onto SiO$_2$/Si substrates or carbon grids. The graphene can be made suspended over a $\\\\sim$12 $\\\\mu$m distance, indicating its good mechanical properties. Electron transport measurements show the graphene sheet resistance of $\\\\sim$ 0.6 ${\\\\bf k\\\\Omega} /\\\\square$ at zero gate voltage. The mobilities of electrons and holes are $\\\\sim$1800 cm $^2$/Vs at 4.2 K and $\\\\sim$1200 cm $^2$/Vs at room temperature.\n",
      "\n",
      "10. id: 5390ae2e20f70186a0ec6ce2   score: 0.98445636   abstract: The graphene aggregates films were fabricated directly on Fe-Cr-Ni alloy substrates by microwave plasma chemical vapor deposition system (MPCVD). The source gas was a mixture of H2 and CH4 with flow rates of 100 sccm and 12 sccm, respectively. The micro- and nanostructures of the samples were characterized by Raman scattering spectroscopy, field emission scanning electron microscopy (SEM), and transparent electron microscopy (TEM). The field emission properties of the films weremeasured using a diode structure in a vacuum chamber. The turn-on field was about 1.0 V/µm. The current density of 2.1 mA/cm2 at electric field of 2.4 V/µm was obtained.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707932\n",
      "index                                        55323b9945cec66b6f9da397\n",
      "title               A fine-grained analysis of the support provide...\n",
      "authors             Gabriele Bavota, Carmine Gravino, Rocco Olivet...\n",
      "year                                                           2015.0\n",
      "venue                           Software and Systems Modeling (SoSyM)\n",
      "references          5390882820f70186a0d8b3e0;5390877920f70186a0d2e...\n",
      "abstract            This paper presents the results of an empirica...\n",
      "id                                                            1707932\n",
      "clustered_labels                                                    0\n",
      "Name: 1707932, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b1d220f70186a0ee131c   score: 0.9996402   abstract: In this paper we present an experiment and two replications aimed at comparing the support provided by ER and UML class diagrams during comprehension activities by focusing on the single building blocks of the two notations. This kind of analysis can be used to identify weakness in a notation and/or justify the need of preferring ER or UML for data modeling. The results reveal that UML class diagrams are generally more comprehensible than ER diagrams, even if the former has some weaknesses related to three building blocks, i.e., multivalue attribute, composite attribute, and weak entity. These findings suggest that a UML class diagram extension should be considered to overcome these weaknesses and improve the comprehensibility of the notation.\n",
      "\n",
      "2. id: 5390a45620f70186a0e72482   score: 0.9988305   abstract: We present the results of two controlled experiments carried out to compare the support given by the ER and UML class diagrams during the maintenance of data models. The experiments involved Master and Bachelor students performing maintenance tasks on data models represented by ER and UML class diagrams. The results reveal that the two notations give in general the same support. In particular, the correctness level achieved by a subject performing the task on data model represented by an ER diagram are comparable with the correctness level achieved by the same subject performing the task on a different data model represented by an UML class diagram. Moreover, by discriminating the levels of ability (high vs. low) and experience (graduate vs. undergraduate) of subjects we also provide some consideration about the influence of such factors on the correctness level achieved by subjects. In \n",
      "\n",
      "3. id: 5390ab8820f70186a0eb1995   score: 0.9983321   abstract: We present the results of three sets of controlled experiments aimed at analysing whether UML class diagrams are more comprehensible than ER diagrams during data models maintenance. In particular, we considered the support given by the two notations in the comprehension and interpretation of data models, comprehension of the change to perform to meet a change request, and detection of defects contained in a data model. The experiments involved university students with different levels of ability and experience. The results demonstrate that using UML class diagrams subjects achieved better comprehension levels. With regard to the support given by the two notations during maintenance activities the results demonstrate that the two notations give the same support, while in general UML class diagrams provide a better support with respect to ER diagrams during verification activities.\n",
      "\n",
      "4. id: 5390ba3820f70186a0f37459   score: 0.99788433   abstract: Context: UML has been the de facto standard notation for modeling object-oriented software systems since its appearance in 1997. UML diagrams are important for maintainers of a system, especially when the software was developed by a different team. These diagrams of the system are not always available, however, and are commonly recovered using Reverse Engineering (RE) techniques. When obtained through RE, UML diagrams have a high level of detail as compared to those developed in the forward design activity. Method: In this paper we report on a comparison of the attitude and performance of maintainers when using these two kinds of diagrams during the maintenance of source code. Our findings were obtained by carrying out a controlled experiment with 40 students of a Master's degree in Computer Science. Results: The results show a preference for forward design diagrams but do not display si\n",
      "\n",
      "5. id: 5390af8920f70186a0ecf9e6   score: 0.9969246   abstract: The Unified Modeling Language (UML) was created on the basis of expert opinion and has now become accepted as the ‘standard’ object-oriented modelling notation. Our objectives were to determine how widely the notations of the UML, and their usefulness, have been studied empirically, and to identify which aspects of it have been studied in most detail. We undertook a mapping study of the literature to identify relevant empirical studies and to classify them in terms of the aspects of the UML that they studied. We then conducted a systematic literature review, covering empirical studies published up to the end of 2008, based on the main categories identified. We identified 49 relevant publications, and report the aggregated results for those categories for which we had enough papers— metrics, comprehension, model quality, methods and tools and adoption. Despite indications that a number of\n",
      "\n",
      "6. id: 53909ed120f70186a0e2fe97   score: 0.9950177   abstract: Unified Modeling Language (UML) is the most comprehensive and widely accepted object-oriented modeling language due to its multi-paradigm modeling capabilities and easy to use graphical notations, with strong international organizational support and industrial production quality tool support. However, there is a lack of precise definition of the semantics of individual UML notations as well as the relationships among multiple UML models, which often introduces incomplete and inconsistent problems for software designs in UML, especially for complex systems. Furthermore, there is a lack of methodologies to ensure a correct implementation from a given UML design. The purpose of this investigation is to verify and validate software designs in UML, and to provide dependability assurance for the realization of a UML design.In my research, an approach is proposed to transform UML diagrams into \n",
      "\n",
      "7. id: 5390a6d920f70186a0e87803   score: 0.99262565   abstract: The Unified Modeling Language (UML) has been designed to be a full standard notation for Object Oriented Modeling. UML has nine types of diagrams [3, 6]: class, object, sequence, collaboration, use case, statechart, activity, deployment and, component diagram. All these diagrams together are used to describe one model. There are some basic rules should be followed when drawing a diagram. Also, one diagram could be correct but inconsistence with another. This paper proposed fifteen basic rules that help developers to build correct and consistent UML diagrams.\n",
      "\n",
      "8. id: 5390882c20f70186a0d8c72d   score: 0.989791   abstract: From the Publisher:The Unified Modeling Language (UML), the standard graphical notation for modeling business and software application needs, has emerged as an effective modeling tool for database design. When used as a common modeling language for the many facets of system development, the UML can serve as a unifying framework that facilitates the integration of database models with the rest of a system design. This pragmatic guide introduces you to the UML and leads you through the process of UML-based database modeling and design. The book presents the different types of UML diagrams, explaining how they apply to the database world, and shows how data modeling with the UML can be tied into the Rational Unified Process. UML for Database Design is structured around the database design process: business use case modeling, business object modeling, database requirements definition, analys\n",
      "\n",
      "9. id: 558b077c612c41e6b9d40c63   score: 0.98947024   abstract: Context: The Unified Modeling Language (UML), with its 14 different diagram types, is the de-facto standard modeling language for object-oriented modeling and documentation. Since the various UML diagrams describe different aspects of one, and only one, software under development, they are not independent but strongly depend on each other in many ways. In other words, the UML diagrams describing a software product must be consistent. Inconsistencies between these diagrams may be a source of faults in software systems. It is therefore paramount that these inconsistencies be detected, analyzed and hopefully fixed. Objective: The aim of this article is to deliver a comprehensive summary of UML consistency rules as they are described in the literature to date to obtain an extensive and detailed overview of the current research in this area. Method: We performed a Systematic Mapping Study by \n",
      "\n",
      "10. id: 53908b0320f70186a0db22df   score: 0.98555213   abstract: Conceptual models are formal descriptions of application domains that are used in early stages of system development to support requirements analysis.The Unified Modeling Language was formed by integrating several diagramming techniques for the purpose of software specification, design, construction and maintenance. It would be advantageous to use the same modeling method throughout the development process of an information system, namely, to extend the use of UML to conceptual modeling. This would require assigning well-defined, real-world meaning to UML constructs.In order to model the real-world, we need to specify what might exist in the world, namely, an ontology. We suggest that by mapping UML constructs to well-defined ontological concepts, we can form clear semantics for UML diagrams. Furthermore, based on the mapping we can suggest ontologically-based intra- and inter-diagram in\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691574\n",
      "index                                        559254300cf2aff368683b21\n",
      "title               Reconstruction of bipartite states via unambig...\n",
      "authors             Lian-Fang Han, Ming Yang, Shu-Dong Fang, Zhuo-...\n",
      "year                                                           2015.0\n",
      "venue                                  Quantum Information Processing\n",
      "references          5390a2e920f70186a0e67c00;5390b8d720f70186a0f2a...\n",
      "abstract            We propose a scheme for reconstructing an unkn...\n",
      "id                                                            1691574\n",
      "clustered_labels                                                    2\n",
      "Name: 1691574, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909e8b20f70186a0e2eab1   score: 0.8873999   abstract: We propose two experimental schemes for quantum state discrimination that achieve the optimal tradeoff between the probability of correct identification and the disturbance on the quantum state.\n",
      "\n",
      "2. id: 53909fbd20f70186a0e4333f   score: 0.6801355   abstract: A new quantum cryptography (quantum key distribution) scheme based on unambiguous measurements is presented, which is secure against any individual attack allowed by quantum physics. By carefully constructing the unambiguous measurements elements a higher efficiency than previous protocols is achieved. Furthermore, an equality for the probability of the participant's reliable measurement outcome and the polarization angle that he chooses when distinguishing between two non-orthogonal states is also derived in this paper.\n",
      "\n",
      "3. id: 5390b8d720f70186a0f2af8f   score: 0.5802926   abstract: We consider the unambiguous discrimination between two unknown qudit states in n-dimensional (n 驴 2) Hilbert space. By equivalence of unknown pure states to known mixed states and with the Jordan-basis method, we demonstrate that the optimal success probability of the discrimination between two unknown states is independent of the dimension n. We also give a scheme for a physical implementation of the programmable state discriminator that can unambiguously discriminate between two unknown states with optimal probability of success.\n",
      "\n",
      "4. id: 5390b9d520f70186a0f311a9   score: 0.48509657   abstract: Using a sufficient condition for POM which gives an optimal measurement to discriminate among given states, we obtain an optimum measurement maximizing the probability of correct detection of three equally-likely, symmetric, linearly-independent qutrit states. The maximum probability with which such states can be discriminated is also derived.\n",
      "\n",
      "5. id: 5390bae520f70186a0f3bcbd   score: 0.23739909   abstract: The discrimination between two unknown states can be performed by a universal programmable discriminator, where the copies of the two possible states are stored in two program systems respectively and the copies of data, which we want to confirm, are provided in the data system. In the present paper, we propose a group-theretic approach to the multi-copy programmable state discrimination problem. By equivalence of unknown pure states to known mixed states and with the representation theory of U(n) group, we construct the Jordan basis to derive the analytical results for both the optimal unambiguous discrimination and minimum-error discrimination. The POVM operators for unambiguous discrimination and orthogonal measurement operators for minimum-error discrimination are obtained. We find that the optimal failure probability and minimum-error probability for the discrimination between the m\n",
      "\n",
      "6. id: 558bd32e0cf2e30013db187a   score: 0.1821344   abstract: In unambiguous state discrimination, the measurement results consist of the error-free results and an inconclusive result, and an inconclusive result is conventionally regarded as a useless remainder from which no information about initial states is extracted. In this paper, we investigate the problem of extracting remaining information from an inconclusive result, provided that the optimal total success probability is determined. We present three simple examples. An inconclusive answer in the first two examples can be extracted partial information, while an inconclusive answer in the third one cannot be. The initial states in the third example are defined as the highly symmetric states.\n",
      "\n",
      "7. id: 558f3bac0cf2c779a6478ddc   score: 0.17023194   abstract: An explicit formula is obtained for the entropy defect and the (maximum) information for an ensemble of two pure quantum states; an optimal basis is found, that is, an optimal measurement procedure which enables one to obtain the maximum information. Some results are also presented for the case of two mixed states, described by second-order density matrices (for example, spin polarization matrices). It is shown that in the case of two states the optimal measurement is a direct von Neumann measurement performed in the subspace of the two states.\n",
      "\n",
      "8. id: 558b19e2612c41e6b9d4349c   score: 0.13939638   abstract: Two schemes via different entangled resources as the quantum channel are proposed to realize remote preparation of an arbitrary four-particle $$\\\\chi $$ ¿ -state with high success probabilities. To design these protocols, some useful and general measurement bases are constructed, which have no restrictions on the coefficients of the prepared states. It is shown that through a four-particle projective measurement and two-step three-particle projective measurement under the novel sets of mutually orthogonal basis vectors, the original state can be prepared with the probability 50 and 100 %, respectively. And for the first scheme, the special cases of the prepared state that the success probability reaches up to 100 % are discussed by the permutation group. Furthermore, the present schemes are extended to the non-maximally entangled quantum channel, and the classical communication costs are\n",
      "\n",
      "9. id: 5390b5fa20f70186a0f0fb80   score: 0.13477592   abstract: A certain class of mixed states artificially constructed that is used in quantum cryptography is crucial for representation of signals. This correspondence shows that the square-root measurement gives the optimal measurement for such a class of mixed states.\n",
      "\n",
      "10. id: 5390b04120f70186a0ed8b4c   score: 0.1322902   abstract: We find the necessary and sufficient condition under which two two-qubit mixed states can be purified into a pure maximally entangled state by local operations and classical communication. The optimal protocol for such transformation is obtained. This result leads to a necessary and sufficient condition for the exact purification of n copies of a two-qubit state.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1713909\n",
      "index                                        55323c3145cec66b6f9db75a\n",
      "title               Globalization of the social sciences in Easter...\n",
      "authors                                                   Dejan Pajić\n",
      "year                                                           2015.0\n",
      "venue                                                  Scientometrics\n",
      "references          558ae106612c41e6b9d3c2a6;558b0712612c41e6b9d40b68\n",
      "abstract            The introduction of new research evaluation po...\n",
      "id                                                            1713909\n",
      "clustered_labels                                                    2\n",
      "Name: 1713909, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b52620f70186a0f02e4e   score: 0.9868787   abstract: A well-designed and comprehensive citation index for the social sciences and humanities has many potential uses, but has yet to be realised. Significant parts of the scholarly production in these areas are not published in international journals, but in national scholarly journals, in book chapters or in monographs. The potential for covering these literatures more comprehensively can now be investigated empirically using a complete publication output data set from the higher education sector of an entire country (Norway). We find that while the international journals in the social sciences and humanities are rather small and more dispersed in specialties, representing a large but not unlimited number of outlets, the domestic journal publishing, as well as book publishing on both the international and domestic levels, show a concentration of many publications in few publication channels.\n",
      "\n",
      "2. id: 5390b04120f70186a0ed7402   score: 0.96989965   abstract: We applied a set of standard bibliometric indicators to monitor the scientific state-of-arte of 500 universities worldwide and constructed a ranking on the basis of these indicators (Leiden Ranking 2010). We find a dramatic and hitherto largely underestimated language effect in the bibliometric, citation-based measurements of research performance when comparing the ranking based on all Web of Science (WoS) covered publications and on only English WoS covered publications, particularly for Germany and France.\n",
      "\n",
      "3. id: 558b890b612c6b62e5e8aec3   score: 0.9540518   abstract: In this study we compare internationalization of academic journals in six fields of science. Internationalization was investigated through journals' concentration on publishing papers from particular countries, relationship between the geographical distributions of editors and authors, and relationship between language of publication and the geographical distribution of papers. Having analyzed more than 1,000 journals we can state that social sciences literature in the fields considered is still nationally and linguistically fragmented more than natural sciences literature, but in some cases the gap is not so big. One of the consequences concerning research output assessment is that usefulness of international databases having national disparity in coverage is still limited in social sciences.\n",
      "\n",
      "4. id: 5390985d20f70186a0e07144   score: 0.9114953   abstract: Most of the science indicators used in the literature for characterizing the research activity and performance of nations are based on journal articles and citations. As an alternative we have examined the use of journal gatekeepers as real and useful science indicators. As an example, gatekeepers of some analytical chemistry core journals were used. To reveal the changes in the contributions of different nations to the field studied, a comparison has been made between the 1970–1974 and the 2002 data on gatekeepers of the same journals. © 2005 Wiley Periodicals, Inc.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f51758   score: 0.9059898   abstract: The number of LA---C indexed journals in WoS has increased from 69 to 248 titles in just a period of four years (2006---2009). This unprecedented growth is related to a change in the editorial policy of WoS rather than to a change in the LA---C scientific community. We find that in the LA---C region, Brazil had the largest increase in its WoS production that also corresponded to a large increase in its production in its indexed local journals. As a consequence, Portuguese has been promoted to the second scientific language, only after English, in the LA---C production in WoS. However, while the Brazilian production in its local journals represents about one quarter of its whole WoS production, it shows a rather little effect on the respective number of citations. The rest of the LA---C countries represented in WoS still show very low levels in production and impact. Scopus has also enlar\n",
      "\n",
      "6. id: 5390b20120f70186a0ee579b   score: 0.8962514   abstract: The role of Kazakhstan and other countries of the Commonwealth of Independent States in the global scientific landscape in 1996---2006 is analyzed on the basis of statistical data obtained from the Scopus database. The analysis of bibliometric indicators, publication activity, and citation per country and per field of study is presented in absolute terms and per population of a given country.\n",
      "\n",
      "7. id: 559089dd612c8aa08ca8c4bc   score: 0.89199936   abstract: This paper provides a comprehensive comparative analysis of the South East European countries scientific output and impact by Frascati fields of science in the period of 2005---2010. The aim is to determine the volume of scientific output in the mentioned period, level of development of certain scientific fields in selected countries and quality of scientific publication production. SEE countries' scientific performance is examined on several indicators including total number of country publications per full time equivalent researcher, revealed publication advantage, the h index and top cited articles. Results of the study could be especially significant to the planners and policy-makers because they provide facts important for the long term S&T planning of the country.\n",
      "\n",
      "8. id: 5390b20120f70186a0ee5792   score: 0.86916035   abstract: The use of citation indexes, such as the impact factor of the Journal Citation Reports, the Scopus SJR (SCImago Journal Rank) and the SNIP (Source Normalized Impact per Paper) indicators, as well as the impact factor of the Russian Scientific Citation Index, is investigated in order to qualitatively assess the content of scientific information resources that are available at the Central Science Library of the National Academy of Sciences of Belarus.\n",
      "\n",
      "9. id: 5390a54620f70186a0e77da8   score: 0.83400166   abstract: The launching of Scopus and Google Scholar, and methodological developments in social-network analysis have made many more indicators for evaluating journals available than the traditional impact factor, cited half-life, and immediacy index of the ISI. In this study, these new indicators are compared with one another and with the older ones. Do the various indicators measure new dimensions of the citation networks, or are they highly correlated among themselves? Are they robust and relatively stable over time? Two main dimensions are distinguished—size and impact—which together shape influence. The h-index combines the two dimensions and can also be considered as an indicator of reach (like Indegree). PageRank is mainly an indicator of size, but has important interactions with centrality measures. The Scimago Journal Ranking (SJR) indicator provides an alternative to the journal impact f\n",
      "\n",
      "10. id: 558b09a3612c41e6b9d4104f   score: 0.80486643   abstract: This paper explores the changing role of world regions (North America, EU15, South EU, Central and Eastern Europe (CEE), Former-USSR, Latin America, Asia Pacific and the Middle East) in science from 1981 to 2011. We use bibliometric data extracted from Thomson Reuter's National Science Indicators (2011) for 21 broad disciplines, and aggregated the data into the four major science areas: life, fundamental, applied and social sciences. Comparing three sub-periods (1981---1989, 1990---2000 and 2001---2011), we investigate (i) over time changes in descriptive indicators such as publications, citations, and relative impact; (ii) static specialization measured by revealed comparative advantage (RCA) in citations and papers; and (iii) dynamic specialization measured by absolute growth in papers. Descriptive results show a global shift in science largely in quantity (papers) and much less in imp\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675487\n",
      "index                                        559255ec0cf28b1a968ffc7e\n",
      "title               Support Environment for Co-designing Micro Tas...\n",
      "authors                                                  Tomoyo Sasao\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390b44620f70186a0efa232;5390ad0620f70186a0eba066\n",
      "abstract            Designing usable tasks for coping with civic c...\n",
      "id                                                            1675487\n",
      "clustered_labels                                                    0\n",
      "Name: 1675487, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390aa0f20f70186a0ea9053   score: 0.7545535   abstract: This paper explores issues of participation in urban life, particularly new partnerships between city and citizens to co-design new services for their cities. We will share experiences from working on the design and development of a software infrastructure, Urban Mediator, and its related social practices. We conclude by pointing out the necessity of considering the software artifacts designed as being part of a toolkit for co-design that can enhance conversations between cities and citizens, and enable the envisioning of new practices related to city-citizen interactions.\n",
      "\n",
      "2. id: 5390ad8920f70186a0ec0900   score: 0.57367843   abstract: In this paper, we draw on material from a participatory design project that focused on the practices, infrastructures, and technologies used for creating and sharing information about the urban environment. The research strategy that we followed includes the collaborative design of a prototype environment and service called Urban Mediator (UM), as well as its subsequent deployment and appropriation in use through several cases. We examine some of the challenges and opportunities that exist in designing in-between infrastructures that can both address a more fluid and active notion of citizenship and understand it as practiced, rather than as a given role. Our research demonstrates that in-between infrastructures can have a role in encouraging a variety of stakeholders, including city officials and citizens, to experiment with and understand some of the complex aspects of participation. F\n",
      "\n",
      "3. id: 53908b4920f70186a0dbce61   score: 0.5629575   abstract: Designing and assessing systems to support neighborhood participation in design is difficult due to the challenges of involving real participants and the fragile nature of early instantiations of technologies aimed at supporting open-ended and ill-structured design tasks. We report on a scenario-based, semi-realistic field trial of two prototypes of the Envisionment and Discovery Collaboratory, an environment for supporting community involvement in design activities. By engaging subjects in playing participant roles, we have been able to gain some crucial insights into the facets of the design at multiple levels as part of an ongoing design process.\n",
      "\n",
      "4. id: 5390a17720f70186a0e53b53   score: 0.5263884   abstract: This paper is a proposal to organize a workshop about co-design during the during MobileHCI 2008 Conference. Main goals of the workshop are to discuss co-design practices and to articulate challenges and lessons learned.\n",
      "\n",
      "5. id: 5591489f0cf232eb904fb987   score: 0.45141718   abstract: This paper presents CityMockUp, our contribution to the CHI'15 student design competition. CityMocUup emphasizes and proposes a solution to the problem of involving citizens in the actual process of furnishing or shaping the urban environment that they inhabit. The design consists of digitally interconnected and tangible wooden modules that enable the citizens to construct their own desired urban architecture proposals. The product is rooted within and contributes to the emerging field of Urban Interaction Design (UIxD).\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3e9da   score: 0.39957732   abstract: How should citizens and communities interact with and in their city? Leveraging urban resources for civic purposes, such as citizen participation and community engagement, has been gaining interest in HCI. Essentially, citizens can be empowered to be heard and engage the city better through the use of modern technology. Examples of these technologies are mobile phones, public displays, sensor networks, digital art installations, or any other type of urban technology. This workshop seeks to investigate the progress in creating public human interfaces for interactive urban engagement. We wish to discuss issues such as citizen participation in public life and decision-making, informing citizens, and civic engagement in all its various forms.\n",
      "\n",
      "7. id: 5390a4cc20f70186a0e74e47   score: 0.39852345   abstract: Urban design today faces complex demands. It has become a necessity to negotiate between stakeholder objectives, the expectations of citizens, and the demands of planning. In this paper we describe how we use a set of participatory technologies in combination with methods for preparing and enabling a heterogeneous group of participants to create a vision of an urban project. Our observations show how space, materials, and different types of content affect participants' collaboration and their debate of the urban issues. We discuss how these participatory technologies and events may help build a community of practice around an urban project.\n",
      "\n",
      "8. id: 5390ae2e20f70186a0ec71f8   score: 0.37364724   abstract: User participation in social media design processes has similarities with civic participation for example in urban planning. Internet enables new virtual environments that can be planned collaboratively and be used for civic participation. In the Monimos case study we developed a social media website for immigrants and multicultural associations in a participatory design process together with the users. In this paper we present the critical issues in co-designing a social media service that aims at civic participation. We also give suggestions for how to cope with the challenges in a multicultural participation process. We claim that constant meta-level discussion about the goals and participation practices with the users is beneficial for the design process.\n",
      "\n",
      "9. id: 5390b2d720f70186a0eec6e0   score: 0.3539863   abstract: This paper reports on an exploratory participatory design process aimed at supporting citizen deliberation in municipal planning. It presents the main outcomes of this process in terms of selected prototypes and an approach to the use setting. We support and discuss different ways for citizens to act and reflect on proposed plans: in-situ, while physically close to the planning object, and ex-situ, when citizens are remote from this. The support of in-situ and ex-situ participation allows citizens to engage in continuous reflection-in and on-action as a collaborative activity with other citizens, hereby inspiring citizens to increase their democratic engagement.\n",
      "\n",
      "10. id: 5390bb1d20f70186a0f3e9e1   score: 0.33698532   abstract: Coping with ill-structured problems in a city involves continuous, opportunistic, and multi-perspective processes, which existing pervasive technologies for citizen participation cannot easily support. Based on two preliminary case studies, we propose Scene Memo, a mobile phone-based exploratory citizen-sensing environment that uses dynamically shared tags to provide social cues and scaffold participants.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691268\n",
      "index                                        559255050cf2aff368683b9c\n",
      "title               Infinite motion and 2-distinguishability of gr...\n",
      "authors             Wilfried Imrich, Simon M. Smith, Thomas W. Tuc...\n",
      "year                                                           2015.0\n",
      "venue               Journal of Algebraic Combinatorics: An Interna...\n",
      "references          5390a05920f70186a0e495e8;53909ed120f70186a0e3043f\n",
      "abstract            A group $$A$$ A acting faithfully on a set $$X...\n",
      "id                                                            1691268\n",
      "clustered_labels                                                    1\n",
      "Name: 1691268, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539099b320f70186a0e1a091   score: 0.56685793   abstract: Let G be a group acting faithfully on a set X. The distinguishing number of the action of G on X, denoted D G(X), is the smallest number of colors such that there exists a coloring of X where no nontrivial group element induces a color-preserving permutation of X. In this paper, we consider the distinguishing number of two important product actions, the wreath product and the direct product. Given groups G and H acting on sets X and Y respectively, we characterize the distinguishing number of the wreath product G 驴Y H in terms of the number of distinguishing colorings of X with respect to G and the distinguishing number of the action of H on Y. We also prove a recursive formula for the distinguishing number of the action of the Cartesian product of two symmetric groups S m 脳 S n on [m] 脳 [n].\n",
      "\n",
      "2. id: 5590f7430cf2ce4b6f3a0c79   score: 0.36761075   abstract: Let G be a graph and A an Abelian group. Denote by F(G, A) the set of all functions from E(G) to A. Denote by D an orientation of E(G). For f ¿ F(G,A), an (A,f)-coloring of G under the orientation D is a function c : V(G)¿A such that for every directed edge uv from u to v, c(u)¿c(v) ¿ f(uv). G is A-colorable under the orientation D if for any function f ¿ F(G, A), G has an (A, f)-coloring. It is known that A-colorability is independent of the choice of the orientation. The group chromatic number of a graph G is defined to be the least positive integer m for which G is A-colorable for any Abelian group A of order ¿m, and is denoted by ¿ g (G). In this note we will prove the following results. (1) Let H 1 and H 2 be two subgraphs of G such that V(H 1)¿V(H 2)=¿ and V(H 1)¿V(H 2)=V(G). Then ¿ g (G)≤min{max{¿ g (H 1), max v ¿ V(H 2) deg(v,G)+1},max{¿ g (H 2), max u ¿ V(H 1) deg (u, G) + 1}}. \n",
      "\n",
      "3. id: 5390994d20f70186a0e1323b   score: 0.3526475   abstract: In (Electron. J. Combin. 10 (2003); http://www.combinatorics.org/volume-10/Abstracts/v1oi1r28 html), the first author (Yuliya Gryshko) asked three questions. Is it true that every infinite group admitting a 2-coloring without infinite monochromatic symmetric subsets is either almost cyclic (i.e., have a finite index subgroup which is cyclic infinite) or countable locally finite? Does every infinite group G include a monochromatic symmetric subset of any cardinal G| for any finite coloring? Does every uncountable group G such that |B(G)| G| where B(G)={x ∈ G : x2 = 1}, admit a 2- coloring without monochromatic symmetric subsets of cardinality |G|? We answer the first question positively. Assuming the generalized continuum hypothesis (GCH), we give a positive answer to the second question in the abelian case. Finally, we build a counter-example for the third question and we give a necessar\n",
      "\n",
      "4. id: 5390b3ae20f70186a0ef4ea1   score: 0.34235165   abstract: Suppose Γ is a group acting on a set X, written as (Γ,X). An r-labeling f: X→{1,2, ..., r} of X is called distinguishing for (Γ,X) if for all σ∈Γ,σ≠1, there exists an element x∈X such that f(x)≠f(x σ ). The distinguishing number d(Γ,X) of (Γ,X) is the minimum r for which there is a distinguishing r-labeling for (Γ,X). If Γ is the automorphism group of a graph G, then d(Γ,V (G)) is denoted by d(G), and is called the distinguishing number of the graph G. The distinguishing set of Γ-actions is defined to be D*(Γ)={d(Γ,X): Γ acts on X}, and the distinguishing set of Γ-graphs is defined to be D(Γ)={d(G): Aut(G)≅Γ}. This paper determines the distinguishing set of Γ-actions and the distinguishing set of Γ-graphs for almost simple groups Γ.\n",
      "\n",
      "5. id: 5590b2960cf2ce4b6f39ef83   score: 0.2672169   abstract: Suppose @C is a group acting on a set X. An r-labeling f:X-{1,2,...,r} of X is distinguishing (with respect to @C) if the only label preserving permutation of X in @C is the identity. The distinguishing number, D\\\"@C(X), of the action of @C on X is the minimum r for which there is an r-labeling which is distinguishing. This paper investigates the relation between the cardinality of a set X and the distinguishing numbers of group actions on X. For a positive integer n, let D(n) be the set of distinguishing numbers of transitive group actions on a set X of cardinality n, i.e., D(n)={D\\\"@C(X):|X|=n and @C acts transitively on X}. We prove that |D(n)|=O(n). Then we consider the problem of an arbitrary fixed group @C acting on a large set. We prove that if for any action of @C on a set Y, for each proper normal subgroup H of @C, D\\\"H(Y)@?2, then there is an integer n such that for any set X w\n",
      "\n",
      "6. id: 5390bfa220f70186a0f54a36   score: 0.2487166   abstract: In this paper we study continuous actions of topological groups. We introduce a parametrized notion of periodicity - relative to a fixed class of compactifications of the acting group. This yields a natural generalization of Devaney's well-recognized concept of chaos. As our main result, we establish a geometric characterization of those classes of compactifications of a locally compact Hausdorff topological group for which the group admits a faithful chaotic continuous action on some (compact) Hausdorff space.\n",
      "\n",
      "7. id: 53908e0020f70186a0dd495f   score: 0.22884515   abstract: This paper is concerned with actions of finite hypergroups on sets. After introducing the definitions in the first section, we use the notion of ‘maximal actions’ to characterise those hypergroups which arise from association schemes, introduce the natural sub-class of *-actions of a hypergroup and introduce a geometric condition for the existence of *-actions of a Hermitian hypergroup. Following an insightful suggestion of Eiichi Bannai we obtain an example of the surprising phenomenon of a 3-element hypergroup with infinitely many pairwise inequivalent irreducible *-actions.\n",
      "\n",
      "8. id: 539087ef20f70186a0d6dab4   score: 0.21386933   abstract: Let G_1=(V_1, E_1) and G_2 = (V_2, E_2) be two edge-colored graphs (without multiple edges or loops). A homomorphism} is a mapping&phiv; : V_1 mapsto V_2 forwhich, for every pair of adjacent vertices u and vof G_1, &phiv;(u) and &phiv;(v) are adjacent inG_2 and the color of the edge &phiv;(u)&phiv;(v) isthe same as that of the edge uv.We prove a number of results asserting the existence of a graphG, edge-colored from a set C, into which everymember from a given class of graphs, also edge-colored from C,maps homomorphically.We apply one of these results to prove that every three-dimensional hyperbolic reflection group, having rotations of orders from the setM={m_1, m_2,..., m_k}, has a torsion-free subgroup ofindex not exceeding some bound, which depends only on the setM.\n",
      "\n",
      "9. id: 53909f2c20f70186a0e37304   score: 0.20753574   abstract: A base for a permutation group, G, is a sequence of elements of its permutation domain whose stabiliser in G is trivial. Using purely elementary and constructive methods, we obtain bounds on the minimum length of a base for the action of the symmetric group on partitions of a set into blocks of equal size. This upper bound is a constant when the size of each block is at most equal to the number of blocks and logarithmic in the size of a block otherwise. These bounds are asymptotically best possible.\n",
      "\n",
      "10. id: 539089ab20f70186a0d97661   score: 0.20040116   abstract: Let Γ be a G-symmetric graph admitting a nontrivial G-invariant partition B. For B ∈ B, let D(B)=(B,ΓB(B),I) be the 1-design in which αIC for α ∈ B and C ∈ ΓR(B) if and only if α is adjacent to at least one vertex of C, where ΓB(B) is the neighbourhood of B in the quotient graph ΓB of Γ relative to B. In a natural way the setwise stabilizer GB of B in G induces a group of automorphisms of D(B). In this paper, we study those graphs Γ such that the actions of GB on B and ΓB(B) are permutationally equivalent, that is, there exists a bijection ρ :B → ΓB(B) such that ρ(αx) = (ρ(α))x for α ∈ B and x ∈ GB. In this case the vertices of Γ can be labelled naturally by the arcs of B. By using this labelling technique we analyse ΓB, D(B) and the bipartite subgraph Γ[B, C] induced by adjacent blocks B, C of B, and study the influence of them on the structure of Γ. We prove that the class of such grap\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698174\n",
      "index                                        55912d770cf232eb904fb1e7\n",
      "title               SARATHI: Characterization Study on Regression ...\n",
      "authors                    Manisha Khattar, Yash Lamba, Ashish Sureka\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 8th India Software Engineer...\n",
      "references          55900270612c29c89cd7ce02;5390b60d20f70186a0f12...\n",
      "abstract            As a software system evolves, maintaining the ...\n",
      "id                                                            1698174\n",
      "clustered_labels                                                    3\n",
      "Name: 1698174, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b5c620f70186a0f07386   score: 0.9147487   abstract: Regression faults are inevitably introduced in software development. Identifying and fixing regression faults can be tedious and time-consuming. The goal of my doctoral research is to provide an automated practical technique to effectively and efficiently locating failure-inducing changes. In this work, the research problem and related work is discussed first. Then, our approach, research questions, completed work and future work is presented. Finally, the expected contributions of my thesis are listed.\n",
      "\n",
      "2. id: 5390a5b020f70186a0e7d27c   score: 0.9129032   abstract: Recent research has tried to identify changes in source code repositories that fix bugs by linking these changes to reports in issue tracking systems. These changes have been traced back to the point in time when they were previously modified as a way of identifying bug introducing changes. But we observe that not all changes linked to bug tracking systems are fixing bugs; some are enhancing the code. Furthermore, not all fixes are applied at the point in the code where the bug was originally introduced. We flesh out these observations with a manual review of several software projects, and use this opportunity to see how many defects are in the scope of static analysis tools.\n",
      "\n",
      "3. id: 5390b72e20f70186a0f206fb   score: 0.91117966   abstract: During software evolution, new released versions still contain many bugs. One common scenario is that end users encounter regression faults and submit them to bug tracking systems. Different from in-house regression testing, typically only one test input is available, which passes the old version and fails the modified new version. To address the issue, delta debugging has been proposed for failure-inducing changes identification between two versions. Despite promising results, there are two practical factors that thwart the application of delta debugging: a large number of tests and misleading false positives. In this work, we present a combination of coverage analysis and delta debugging that automatically isolates failure-inducing changes. Evaluations on twelve real regression faults in GNU software demonstrate both the speed gain and effectiveness improvements. Moreover, a case study\n",
      "\n",
      "4. id: 5390aa0e20f70186a0ea7b38   score: 0.89218736   abstract: A regression bug is a bug which causes a feature that worked correctly to stop working after a certain event (system upgrade, system patching, daylight saving time switch, etc.). Very often an encompassed bug fix included in a patch causes the regression bug. Regression bugs are an annoying and painful phenomena in the software development process, requiring a great deal of effort to find. Many tools have been developed in order to find the existence of such bugs. However, a great deal of manual work is still needed to find the exact source-code location that caused a regression bug. In this paper we present the CodePsychologist, a tool which assists the programmer to locate source code segments that caused a given regression bug. The CodePsychologist goes beyond current tools, that identify all the lines of code that changed since the feature in question worked properly (with the help o\n",
      "\n",
      "5. id: 53909eef20f70186a0e36c02   score: 0.876423   abstract: Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.\n",
      "\n",
      "6. id: 53909eef20f70186a0e35101   score: 0.876423   abstract: Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.\n",
      "\n",
      "7. id: 539098b820f70186a0e0ac46   score: 0.852199   abstract: As a software system evolves, programmers make changes that sometimes cause problems. We analyze CVS archives for fix-inducing changes---changes that lead to problems, indicated by fixes. We show how to automatically locate fix-inducing changes by linking a version archive (such as CVS) to a bug database (such as BUGZILLA). In a first investigation of the MOZILLA and ECLIPSE history, it turns out that fix-inducing changes show distinct patterns with respect to their size and the day of week they were applied.\n",
      "\n",
      "8. id: 5390a40520f70186a0e6edf1   score: 0.7874412   abstract: Working collaboratively on complex software systems often leads to situations where a developer enhances or extends system functionality, thereby however, introducing bugs. At best the unintentional changes are caught immediately by regression tests. Often however, the bugs are detected days or weeks later by other developers noticing strange system behavior while working on different parts of the system. Then it is a highly time-consuming task to trace back this behavior change to code changes in the past. In this paper we propose a technique for identifying the recently introduced change that is responsible for the unexpected behavior. The key idea is to combine dynamic, static, and code change information on the system to reduce the possibly great amount of code modifications to those that may affect the system while running its faulty behavior. After having applied this massive autom\n",
      "\n",
      "9. id: 5390b29820f70186a0ee8d3c   score: 0.7804952   abstract: There is a perception that when new features are added to a system that those added and modified parts of the source-code are more fault prone. Many have argued that new code and new features are defect prone due to immaturity, lack of testing, as well unstable requirements. Unfortunately most previous work does not investigate the link between a concrete requirement or new feature and the defects it causes, in particular the feature, the changed code and the subsequent defects are rarely investigated. In this paper we investigate the relationship between improvements, new features and defects recorded within an issue tracker. A manual case study is performed to validate the accuracy of these issue types. We combine defect issues and new feature issues with the code from version-control systems that introduces these features, we then explore the relationship of new features with the faul\n",
      "\n",
      "10. id: 5390bb1d20f70186a0f3cf05   score: 0.76629364   abstract: Debugging and isolating changes responsible for regression test failures are some of the most challenging aspects of modern software development. Automatic bug localization techniques reduce the manual effort developers spend examining code, for example, by focusing attention on the minimal subset of recent changes that results in the test failure, or on changes to components with most dependencies or highest churn. We observe that another subset of changes is worth the developers' attention: the complement of the maximal set of changes that does not produce the failure. While for simple, independent source-code changes, existing techniques localize the failure cause to a small subset of those changes, we find that when changes interact, the failure cause is often in our proposed subset and not in the subset existing techniques identify. In studying 45 regression failures in a large, ope\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1701276\n",
      "index                                        5591212b0cf232eb904fae48\n",
      "title               Monitoring HPC applications in the production ...\n",
      "authors                       Hadi Sharifi, Omar Aaziz, Jonathan Cook\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2nd Workshop on Parallel Pr...\n",
      "references          539098b820f70186a0e0a44e;5390a5b020f70186a0e7c...\n",
      "abstract            The advancement of HPC systems brings with it ...\n",
      "id                                                            1701276\n",
      "clustered_labels                                                    3\n",
      "Name: 1701276, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a6d920f70186a0e881f1   score: 0.8030194   abstract: This paper sets out to examine the future of performance monitoring on exascale HPC systems. In particular we put forth the idea that such machines will be sufficiently complex that performance monitoring of individual applications and the workload as a whole will change from being a beneficial option to being a necessity. This complexity arises from the number of components and concurrencies expected for such systems. We see the need for a shift from performance monitoring being a useful add-on toward it being a core requirement for basic operation and suggest some first steps toward meeting that need.\n",
      "\n",
      "2. id: 53908dff20f70186a0dd3ff7   score: 0.7595823   abstract: Abstract: Performance monitoring of distributed software executing on workstation cluster systems is a complex task because of the multitude of factors that can affect performance in these environments. For performance analysis to be done effectively, all runtime events which have an impact on performance have to be captured. We introduce an integrated monitoring system that is capable of recording performance data on the application level, the operating system level, and the network level. It is thus able to provide complete coverage of all subsystems relevant to application performance. One of the key features of the proposed monitoring system is its ability to correlate performance data with application source code, pointing the developer to the portions of his code that may be time-critical. The system thus supports a cyclic process of optimizing performance in multiple iterations of\n",
      "\n",
      "3. id: 5390a7f520f70186a0e93127   score: 0.68331385   abstract: Monitoring of software's execution is crucial in numerous software development tasks. Current monitoring efforts generally require extensive instrumentation of the software or dedicated hardware test rig designed to provide visibility into the software. To fully understand software's behavior, the production software must be studied in its production environment. To address this fundamental software engineering challenges, we propose a compiler and hardware supported framework for monitoring and observation of software-intensive systems.We place three fundamental requirements on our monitoring framework. The monitoring must be non-intrusive, low-overhead, and predictable so that the software is not unduly disturbed. The framework must also allow low-level monitoring and be highly flexible so we can accommodate a broad range of crucial monitoring activities.The general idea behind our wor\n",
      "\n",
      "4. id: 5390a30b20f70186a0e69de3   score: 0.6481324   abstract: Runtime monitoring, even the canonical \"logging\" example of AOP, has long been one of the domains into which AOP has effectively been deployed. Yet to date AOP has not supported the full breadth of needs across the scope of widely varied runtime monitoring applications. In this paper we present the directions in which we believe AOP needs to be extended in order to better support the breadth of runtime monitoring needs.\n",
      "\n",
      "5. id: 5390b71120f70186a0f1db64   score: 0.64646024   abstract: Runtime monitoring is a technique usable in all phases of the software development cycle, from initial testing, to debugging, to actually maintaining proper function in production code. Of particular importance are parametric monitoring systems, which allow the specification of properties that relate objects in a program, rather than only global properties. In the past decade, a number of parametric runtime monitoring systems have been developed. Here we give a demonstration of our system, JavaMOP. It is the only parametric monitoring system that allows multiple differing logical formalisms. It is also the most efficient in terms of runtime overhead, and very competitive with respect to memory usage.\n",
      "\n",
      "6. id: 5390c04520f70186a0f57e27   score: 0.59678566   abstract: This article describes a process that can manage an HPC operating system that includes critical application testing, integrating custom software, monitoring and configuration management.\n",
      "\n",
      "7. id: 5390a2e920f70186a0e673cb   score: 0.58776563   abstract: Creating instrumentation for runtime monitoring has typically required expertise in some low-level capabilities and how to integrate them into an application program. This thesis proposes that runtime monitoring would be much easier if it could be performed at a high level, and furthermore that aspect-oriented programming (AOP) is a good abstraction to raise runtime monitoring to a more abstract level. Our thesis will extend the ideas and implementations of AOP to show that AOP is the ideal high level abstraction for effectively and efficiently meeting the needs of runtime monitoring.\n",
      "\n",
      "8. id: 5390a1f820f70186a0e5d572   score: 0.46806112   abstract: We propose EndoScope, a software monitoring framework that allows users to pose declarative queries that monitor the state and performance of running programs. Unlike most existing monitoring tools, EndoScope is acquisitional, meaning that it only instruments the portions of the program that need to be monitored to answer queries. The use of a high level declarative language allows EndoScope to search for efficient physical instantiations of queries by applying a suite of optimizations, including control flow graph analysis, and traditional database query optimization techniques, such as predicate pushdown and join optimization, to minimize the number of program instrumentation points and overhead to the monitored program. Furthermore, a flexible, high level language and the ability to attach to running programs enable developers to build various program analysis and monitoring applicati\n",
      "\n",
      "9. id: 5390b5c620f70186a0f07389   score: 0.44752064   abstract: A runtime monitor is a tool that takes as input a model of some system, and observes in real time that the sequence of events produced by a run of that system follows the specification. While existing monitoring solutions generally use finite-state machines and temporal logic as their model language, the specification is ultimately tangled with hand-written, implementation-specific details which severely limit their range of application. We present a runtime monitoring platform that clearly separates the extraction of events in the running program from the specification and monitoring process. This separation allows one to cleanly monitor first-order properties involving arbitrarily complex native program objects, while still incurring reasonable overhead.\n",
      "\n",
      "10. id: 53909a0220f70186a0e1fcfb   score: 0.44673607   abstract: Runtime monitoring tools are invaluable for detecting various types of bugs, in both sequential and multi-threaded programs. However, these tools often slow down the monitored program by an order of magnitude or more [4], implying that the tools are ill-suited for always-on monitoring of deployed code. Fortunately, the emergence of chip multiprocessors as a dominant computing platform means that resources are available on-chip to assist in monitoring tasks. In this brief note, we advocate Log-Based Architectures (LBA) that exploit such on-chip resources in order to dramatically reduce the overhead of runtime program monitoring. Specifically, we propose adding hardware support for logging a main program's trace and delivering it to another (otherwise idle) processing core for inspection. A life-guard program running on this other core executes the desired monitoring task.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1699887\n",
      "index                                        55922ca30cf2c3a0875c9d09\n",
      "title               On methodologies to estimate optical-layer pow...\n",
      "authors               Partha Goswami, Soumya K. Ghosh, Debasish Datta\n",
      "year                                                           2015.0\n",
      "venue                                 Photonic Network Communications\n",
      "references          5390b63320f70186a0f17e2d;5390b61e20f70186a0f13...\n",
      "abstract            This paper deals with the methodologies to obt...\n",
      "id                                                            1699887\n",
      "clustered_labels                                                    3\n",
      "Name: 1699887, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b52620f70186a0f037b4   score: 0.9725715   abstract: We consider the power-efficient design of an Internet Protocol (IP)-over-Wavelength Division Multiplexing (WDM) network, tackling the problem of deciding which equipment needs to be installed in both the optical and IP layer. Our model explicitly targets the minimization of cost considered as either Capital Expenditures (CapEx) or power. In contrast to the models already presented in the literature, we take into account routing constraints and consider a comprehensive set of realistic scenarios defined by a network topology, traffic, cost and power values of network devices in both layers. Results indicate that the introduction of realistic constraints and parameters still allows power-efficient networks to be designed. The total power consumption in the considered network scenarios is at most 26.5% higher than when using previous models.\n",
      "\n",
      "2. id: 5390b60d20f70186a0f12d59   score: 0.9692078   abstract: We estimate potential energy savings in IP-over-WDM networks achieved by switching off router line cards in low-demand hours. We compare three approaches to react on dynamics in the IP traffic over time, Fufl, Dufl and Dudl. They provide different levels of freedom in adjusting the routing of lightpaths in the WDM layer and the routing of demands in the IP layer. Using MILP models based on three realistic network topologies as well as realistic demands, power, and cost values, we show that already a simple monitoring of the lightpath utilization in order to deactivate empty line cards (Fufl) may bring substantial benefits. The most significant savings, however, are achieved by rerouting traffic in the IP layer (Dufl). A sophisticated reoptimization of the virtual topology and the routing in the optical and electrical domains for every demand scenario (Dudl) yields nearly no additional pr\n",
      "\n",
      "3. id: 5390ab8820f70186a0eb1c3d   score: 0.96795994   abstract: We estimate potential energy savings in IP-over-WDM networks achieved by switching off router line cards in low-demand hours. We compare three approaches to react on dynamics in the IP traffic over time, FUFL, DUFL and DUDL. They provide different levels of freedom in adjusting the routing of lightpaths in the WDM layer and the routing of demands in the IP layer. Using MILP models based on realistic network topologies and node architectures as well as realistic demands, power, and cost values, we show that already a simple monitoring of the lightpath utilization in order to deactivate empty line cards (FUFL) brings substantial benefits. The most significant savings, however, are achieved by rerouting traffic in the IP layer (DUFL), which allows emptying and deactivating lightpaths together with the corresponding line cards. A sophisticated reoptimization of the virtual topologies and the\n",
      "\n",
      "4. id: 539088b920f70186a0d91741   score: 0.9588471   abstract: From the Publisher:The proposed book will be the first to focus on Internet Protocol (IP) over Wavelength Division Multiplexing (WDM) optical networks. It not only summarizes the fundamental mechanisms and the recent development and deployment of WDM optical networks but it also details both the network and the software architectures needed to implement WDM enabled optical networks designed to transport IP traffic.\n",
      "\n",
      "5. id: 5390b8d720f70186a0f2c279   score: 0.95388025   abstract: The wide interests in the power savings of IP over wavelength-division-multiplexing (WDM) optical networks have recently risen in both academic and industrial communities. In an effort to tackle this problem, the hybrid grooming (traffic grooming along with an optical bypass) approach has been presented to reduce the power consumed by the entire network infrastructure, including the transmission ports of routers and optical-electrical-optical (OEO) conversions. However, the related works pay little or no attention to the power consumed to ensure the resiliency of the overall network. Meanwhile, the power consumed by components used for establishing lightpaths is not simultaneously taken into account. One survivable network with the higher power efficiency thereby save more power with hybrid grooming, require the lower power consumption of establishing lightpaths and exhibit the shorter r\n",
      "\n",
      "6. id: 5390b63320f70186a0f17e27   score: 0.94355595   abstract: We study a class of all-optical networks using wavelength-division multiplexing (WDM) and wavelength routing, in which a connection between a pair of nodes in the network is assigned a path and a wavelength on that path. Moreover, on the links of that path no other connection can share the assigned wavelength. Using a generalized reduced load approximation scheme we calculate the blocking probabilities for the optical network model for two routing schemes: fixed routing and least loaded routing\n",
      "\n",
      "7. id: 53908bfb20f70186a0dcb5b6   score: 0.9425068   abstract: Studies a class of all-optical networks using wavelength division multiplexing and wavelength routing in which a connection between a pair of nodes in the network is assigned a path and a wavelength on that path. Moreover, on the links of that path no other connection can share the assigned wavelength. Using a generalized reduced load approximation scheme the authors calculate the blocking probabilities for the optical network model for two routing schemes: fixed routing and least loaded routing.\n",
      "\n",
      "8. id: 5390b60d20f70186a0f12d56   score: 0.93686515   abstract: Greening of the Internet has become one of the main challenges for the research community. Optical networks can provide an energy efficient solution, but it has become crucial to assess its power efficiency. In this context, dynamic operation of WDM networks is expected to provide significant power savings when compared to static operation; however, its benefits need to be evaluated to determine its actual impact and to analyze future trends. In this paper, a general framework for evaluating energy consumption in WDM networks is introduced. The proposed framework enables the analysis of different node architectures, link capacities and network topologies. In particular, the case of three different node architectures is discussed and compared. Results show that dynamic operation can significantly reduce power consumption when either the traffic load is below 0.4 or when short-reach transp\n",
      "\n",
      "9. id: 53908b9320f70186a0dc06a9   score: 0.93569994   abstract: We address efficient access to bandwidth in WDM (wavelength division multiplexing) optical networks. We consider tree topologies, ring topologies, as well as trees of rings. These are topologies of concrete practical relevance for which undirected underlying graph models have been studied before by P. Raghavan and E. Upfal (1993). As opposed to previous studies (A. Aggarwal et al., 1993; R. Pankaj, 1992; P. Raghavan and E. Upfal, 1993), we consider directed graph models. Directedness of fiber links is dictated by physical directedness of optical amplifiers. For trees, we give a polynomial time routing algorithm that satisfies requests of maximum load L/sub max/ per fiber link using no more than 15L/sub max//8/spl les/15OPT/8 optical wavelengths. This improves a 2L/sub max/ scheme that is implicit by P. Raghavan and E. Upfal by extending their undirected methods to our directed model. Alt\n",
      "\n",
      "10. id: 5390b64020f70186a0f18847   score: 0.9352283   abstract: We consider the problem of network design in transparent, or clear channel, optical networks associated with wavelength-division multiplexing (WDM). We focus on the class of traffic engineering models known as routing, wavelength, and capacity assignment problems. Here, in contrast to traditional networks, traffic flow paths must also be assigned an end-to-end wavelength. This additional requirement means that there can be an increased cost associated with optimal capacity allocations for such WDM-flows. In general, this can be arbitrarily worse than traditional network designs. We argue that in order to evaluate the benefit of different switch technologies, a good benchmark is to measure the increase in costs purely in terms of link capacity, we call this the cost of transparency. Experimental research shows that this cost is small in multifiber networks with modest switching functional\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1667928\n",
      "index                                        558f8f460cf2b66640466bb6\n",
      "title               The Speedup Theorem in a Primitive Recursive F...\n",
      "authors                                                Andrea Asperti\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Conference on Certifie...\n",
      "references          53909fbd20f70186a0e423a0;5390b13020f70186a0edd...\n",
      "abstract            Blum's speedup theorem is a major theorem in c...\n",
      "id                                                            1667928\n",
      "clustered_labels                                                    3\n",
      "Name: 1667928, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a8db20f70186a0e9dcd1   score: 0.95814794   abstract: A weakening of Blum's Axioms for abstract computational complexity is introduced in order to take into a better account measures that can be finite even when the computations diverge. How the new axioms affect the theory and how they can be used to get an insight in the theory of computations using a finite amount of resource, is shown.\n",
      "\n",
      "2. id: 5390a4cc20f70186a0e75b2d   score: 0.9069832   abstract: We show that, for any abstract complexity measure in the sense of Blum and for any computable function f (or computable operator F ), the class of problems which are f -speedable (or F -speedable) does not have effective measure 0. On the other hand, for sufficiently fast growing f (or F ), the class of the nonspeedable problems does not have effective measure 0 too. These results answer some questions raised by Calude and Zimand in [CZ96] and [Zim06]. We also give a short quantitative analysis of Borodin and Trakhtenbrot's Gap Theorem which corrects a claim in [CZ96] and [Zim06].\n",
      "\n",
      "3. id: 53908bad20f70186a0dc2574   score: 0.89218736   abstract: In a suitably general context, the following analogue of the Blum Speed-up Theorem is proven: There are some infinite sets which are so difficult to enumerate that, given any order for enumerating the set, there is some other order, and some one method of enumerating the set in this second order which is much faster than any method of enumerating the set in the first ordering. It may be possible to interpret this result as a statement about the relative merits of “hardware” vs. “programming” speed-ups. The proof itself is one of the first nontrivial applications of priority methods to questions of computational complexity. As such, it perhaps represents an advance in bringing the results and techniques of contemporary “pure” recursion theory to bear on questions of computational complexity. In this paper we shall prove, in a suitably general context, the following analogue of the Blum Sp\n",
      "\n",
      "4. id: 53908f5b20f70186a0dd97f7   score: 0.8513358   abstract: We give a complete proof of Theorem 3.1 in our paper in Theoretical Computer Science 158 (1996), 161-176. A pathological exception of Theorem 4.3 in the paper quoted is exhibited and a condition to remove it is mentioned.\n",
      "\n",
      "5. id: 5390ad8920f70186a0ebffa4   score: 0.8441522   abstract: We show that, for any abstract complexity measure in the sense of Blum and for any computable function f (or computable operator F), the class of problems that are f-speedable (or F-speedable) does not have effective measure 0. On the other hand, for sufficiently fast growing f (or F), the class of non-speedable computable problems does not have effective measure 0. These results answer some questions raised by Calude and Zimand. We also give a quantitative analysis of Borodin and Trakhtenbrot's Gap Theorem, which corrects a claim by Calude and Zimand.\n",
      "\n",
      "6. id: 5390a1e620f70186a0e5b321   score: 0.8265718   abstract: We study the effect of program structure on computational efficiency in a class of abstract languages which model actual high-level numerical programming languages (like ALGOL). The results have bearing on programming technique (the use of go to statements), and they yield interesting facts about Blum's speed-up theorem for subrecursive computational complexity.\n",
      "\n",
      "7. id: 5390a63c20f70186a0e81b5e   score: 0.82417905   abstract: A classic result known as the speed-up theorem in machine-independent complexity theory shows that there exist some computable functions that do not have best programs for them (Blum in J. ACM 14(2):322–336, 1967 and J. ACM 18(2):290–305, 1971). In this paper we lift this result into type-2 computations. Although the speed-up phenomenon is essentially inherited from type-1 computations, we observe that a direct application of the original proof to our type-2 speed-up theorem is problematic because the oracle queries can interfere with the speed of the programs and hence the cancellation strategy used in the original proof is no longer correct at type-2. We also argue that a type-2 analog of the operator speed-up theorem (Meyer and Fischer in J. Symb. Log. 37:55–68, 1972) does not hold, which suggests that this curious speed-up phenomenon disappears in higher-typed computations beyond typ\n",
      "\n",
      "8. id: 558f79da0cf2b66640466974   score: 0.8089744   abstract: In §6 we define an algebraic equivalent of the notion of computation, in §§7, 8 we show that for a large class of computational problems no speed-up may be obtained by the use of branching, except for a small set of inputs.\n",
      "\n",
      "9. id: 5390a1bc20f70186a0e56048   score: 0.79787195   abstract: A classic result known as the speed-up theorem in machine-independent complexity theory shows that there exist some computable functions that do not have best programs for them [2][3]. In this paper we lift this result into type-2 computation under the notion of our type-2 complexity theory depicted in [15][13][14]. While the speed-up phenomenon is essentially inherited from type-1 computation, we cannot directly apply the original proof to our type-2 speed-up theorem because the oracle queries can interfere the speed of the programs and hence the cancellation strategy used in the original proof is no longer correct at type-2. We also argue that a type-2 analog of the operator speed-up theorem [16] does not hold, which suggests that this curious phenomenon disappears in higher-typed computation beyond type-2.\n",
      "\n",
      "10. id: 5390a8db20f70186a0e9dc37   score: 0.7439129   abstract: A treatment is given of a class of program transformations @S with the property that for each program P, both P and @S(P) compute the same function. Many transformations encountered in the theory of computation can be shown to be in the class and, hence, formally proved to preserve equivalence. As an example, the theory is applied to the proof of a speed up theorem in computational complexity.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1704137\n",
      "index                                        55323b2c45cec66b6f9d977c\n",
      "title               Confidentiality and Integrity for Data Aggrega...\n",
      "authors             Soufiene Ben Othman, Abdullah Ali Bahattab, Ab...\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references          5390b9d520f70186a0f31a8e;5390b04120f70186a0ed6...\n",
      "abstract            Data aggregation is an important method to red...\n",
      "id                                                            1704137\n",
      "clustered_labels                                                    1\n",
      "Name: 1704137, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390afc920f70186a0ed2ebf   score: 0.99936634   abstract: In wireless sensor networks, performing data aggregation while preserving data confidentiality and integrity is challenging. Recently, privacy homomorphism-based secure data aggregation schemes have been proposed to seamlessly integrate confidentiality and data aggregation. However, these schemes do not provide data integrity or allow hierarchical data aggregation if more than one encryption key is used in the network. This paper presents a novel integrity protecting hierarchical concealed data aggregation protocol that allows the aggregation of data packets that are encrypted with different encryption keys. In addition, during the decryption of aggregated data, the base station is able to classify the encrypted and aggregated data based on the encryption keys. The proposed data aggregation scheme employs an elliptic curve cryptography-based homomorphic encryption algorithm to offer data\n",
      "\n",
      "2. id: 5390ad8920f70186a0ec0b39   score: 0.99776566   abstract: Since wireless sensor networks (WSNs) are resources- constrained, it is very essential to gather data efficiently so that the life of the networks can be prolonged. Data aggregation can conserve a significant amount of energy by minimizing the transmission costs in terms of the number of data packets. On the other hand, many applications require privacy of the sampled data protecting their integrity while they travel from the source sensor nodes to a data collecting device, say a query server. Thus, an efficient data aggregation scheme for preserving data privacy and integrity is required for WSNs. Although secure data aggregation in WSNs has been well studied in the recent past, there exists a little work, for instance iPDA (Integrity-Protecting Private Data Aggregation) scheme, which focuses on data aggregation, data privacy and integrity protection within a single application for WSNs\n",
      "\n",
      "3. id: 5390a30b20f70186a0e6a72f   score: 0.9967013   abstract: Wireless sensor networks (WSN) are used in many spheres, such as industry, commerce and military. WSN is composed of hundreds of thousands of sensor nodes with limited energy, computation and storage capacities. Thus the data aggregation technique becomes important because it can help to improving bandwidth usage and energy utilization. But the unprotected aggregation schemes are vulnerable because WSN often placed in untrust or even hostile environment, so the researches on secure aggregation become meaningful. Most prior works have focus on both data aggregation techniques and security techniques, but the researches on secure data aggregation were still few. In this paper, we focus on the integrality of data aggregation and present a resilient in-network data aggregation scheme for wireless sensor networks deploy in hostile environment.\n",
      "\n",
      "4. id: 5390a8b120f70186a0e9a816   score: 0.99666256   abstract: Many applications require the privacy of the sampled data while they travel from the source sensor nodes to data collecting device, say data sink. Providing an efficient data aggregation scheme with preserving data privacy is a challenging problem in the research of wireless sensor networks (WSNs). Although the secure data aggregation in WSNs has been well studied in the recent years, there exists a little work, for instance PDA (Privacy-preserving Data Aggregation), which focuses on protecting sensor data not only from adversaries but also from the participating trusted sensor nodes. However, PDA suffers from one main problem which is the high communication cost due to unnecessary traffics in the network during data transmissions. To resolve the problem, we, in this paper, propose a secure aggregation scheme of private data for WSNs. The proposed scheme applies the additive property of \n",
      "\n",
      "5. id: 5390be6620f70186a0f4bdce   score: 0.9966234   abstract: Recently, several data aggregation schemes based on privacy homomorphism encryption have been proposed and investigated on wireless sensor networks. These data aggregation schemes provide better security compared with traditional aggregation since cluster heads (aggregator) can directly aggregate the ciphertexts without decryption; consequently, transmission overhead is reduced. Based on our survey of existing research efforts for ensuring secure data aggregation, a novel approach that uses homomorphic encryption and Message Authentication Codes (MAC) to achieve confidentiality, authentication and integrity for secure data aggregation in wireless sensor networks is proposed. Our experiments show that our proposed secure aggregation method significantly reduces computation and communication overhead and can be practically implemented in on-the-shelf sensor platforms.\n",
      "\n",
      "6. id: 5390be6620f70186a0f4ba95   score: 0.9961456   abstract: With the exponential rise of pervasive computing applications, data privacy has become much more of an important issue than before. When data is aggregated at each hop in a sensor network, it becomes harder to protect its privacy. A number of privacy preserving data aggregation algorithms have recently appeared for wireless sensor networks (WSNs), very few of them however also address the issue of data integrity along with privacy. Data privacy and integrity are two contrasting objectives to achieve in general. In a privacy preserved data aggregation, it becomes easier for an attacker to inject false data hence, we suggest that both privacy and integrity of data should be treated together. In this paper, we present an energy efficient, privacy preserving data aggregation algorithm which also preserves data integrity in WSNs. We analyze the security of the algorithm and provide proofs for\n",
      "\n",
      "7. id: 5390a37f20f70186a0e6db8a   score: 0.99608517   abstract: Wireless sensor networks (WSNs) are composed of tiny devices with limited computation and battery capacities. For such resource-constrained devices, data transmission is a very energy-consuming operation. To maximize WSN lifetime, it is essential to minimize the number of bits sent and received by each device. One natural approach is to aggregate sensor data along the path from sensors to the sink. Aggregation is especially challenging if end-to-end privacy between sensors and the sink (or aggregate integrity) is required. In this article, we propose a simple and provably secure encryption scheme that allows efficient additive aggregation of encrypted data. Only one modular addition is necessary for ciphertext aggregation. The security of the scheme is based on the indistinguishability property of a pseudorandom function (PRF), a standard cryptographic primitive. We show that aggregation\n",
      "\n",
      "8. id: 5390ac5720f70186a0eb705c   score: 0.9952077   abstract: This work introduces a novel secure data aggregation framework for Wireless Sensor Networks. It ensures the accuracy of data aggregation value without neglecting energy efficiency, even if all aggregator nodes and some of sensors are compromised in the network. Compared to available solutions in which malicious data aggregation result is rejected by base station and then all steps in aggregation process are cancelled resulting in wasting valuable network resource, our framework relies on adaptive monitoring mechanism that can always provide base station by the correct aggregate result reducing thus total data rejection and providing much higher availability than other security protocols.\n",
      "\n",
      "9. id: 5390b00c20f70186a0ed4ea1   score: 0.9950177   abstract: Providing efficient data aggregation while preserving data privacy is a challenging problem in wireless sensor networks research. In this article, we present two privacy-preserving data aggregation schemes for additive aggregation functions, which can be extended to approximate MAX/MIN aggregation functions. The first scheme---Cluster-based Private Data Aggregation (CPDA)---leverages clustering protocol and algebraic properties of polynomials. It has the advantage of incurring less communication overhead. The second scheme---Slice-Mix-AggRegaTe (SMART)---builds on slicing techniques and the associative property of addition. It has the advantage of incurring less computation overhead. The goal of our work is to bridge the gap between collaborative data collection by wireless sensor networks and data privacy. We assess the two schemes by privacy-preservation efficacy, communication overhea\n",
      "\n",
      "10. id: 558da0390cf222bc17bbfdb7   score: 0.9945095   abstract: In wireless sensor networks, data aggregation protocols are used to prolong the network lifetime. However, the problem of how to perform data aggregation while preserving data privacy is challenging. This paper presents a polynomial regression-based data aggregation protocol that preserves the privacy of sensor data. In the proposed protocol, sensor nodes represent their data as polynomial functions to reduce the amount of data transmission. In order to protect data privacy, sensor nodes secretly send coefficients of the polynomial functions to data aggregators instead of their original data. Data aggregation is performed on the basis of the concealed polynomial coefficients, and the base station is able to extract a good approximation of the network data from the aggregation result. The security analysis and simulation results show that the proposed scheme is able to reduce the amount o\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1744893\n",
      "index                                        554a4a440cf2c69e96487274\n",
      "title               Indecomposable Coverings with Homothetic Polygons\n",
      "authors                                                 István Kovács\n",
      "year                                                           2015.0\n",
      "venue                               Discrete & Computational Geometry\n",
      "references                                   558b55cf612c41e6b9d49898\n",
      "abstract            We prove that for any convex polygon $$S$$S wi...\n",
      "id                                                            1744893\n",
      "clustered_labels                                                    2\n",
      "Name: 1744893, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac1820f70186a0eb3e45   score: 0.96909106   abstract: We show that for any concave polygon that has no parallel sides and for any k, there is a k-fold covering of some point set by the translates of this polygon that cannot be decomposed into two coverings. Moreover, we give a complete classification of open polygons with this property. We also construct for any polytope (having dimension at least three) and for any k, a k-fold covering of the space by its translates that cannot be decomposed into two coverings.\n",
      "\n",
      "2. id: 5390b44620f70186a0ef85a7   score: 0.80639565   abstract: We prove that octants are cover-decomposable; i.e., any 12-fold covering of any subset of the space with a finite number of translates of a given octant can be decomposed into two coverings. As a corollary, we obtain that any 12-fold covering of any subset of the plane with a finite number of homothetic copies of a given triangle can be decomposed into two coverings. We also show that any 12-fold covering of the whole plane with the translates of a given open triangle can be decomposed into two coverings. However, we exhibit an indecomposable 3-fold covering with translates of a given triangle.\n",
      "\n",
      "3. id: 5390a96e20f70186a0ea2b76   score: 0.78133065   abstract: We prove that for every k 1, there exist k-fold coverings of the plane (1) with strips, (2) with axis-parallel rectangles, and (3) with homothets of any fixed concave quadrilateral, that cannot be decomposed into two coverings. We also construct, for every k 1, a set of points P and a family of disks D in the plane, each containing at least k elements of P, such that no matter how we color the points of P with two colors, there exists a disk D ∈ D, all of whose points are of the same color.\n",
      "\n",
      "4. id: 5390bf1320f70186a0f51c13   score: 0.65279496   abstract: We prove that octants are cover-decomposable into multiple coverings, i.e., for any k there is an m(k) such that any m(k)-fold covering of any subset of the space with a finite number of translates of a given octant can be decomposed into k coverings. As a corollary, we obtain that any m(k)-fold covering of any subset of the plane with a finite number of homothetic copies of a given triangle can be decomposed into k coverings. Previously only some weaker bounds were known for related problems [20].\n",
      "\n",
      "5. id: 5390a1f820f70186a0e5ca58   score: 0.5157266   abstract: Let m(k) denote the smallest positive integer m such that any m-fold covering of the plane with axis-parallel unit squares splits into at least k coverings. J. Pach [J. Pach, Covering the plane with convex polygons, Discrete and Computational Geometry 1 (1986) 73-81] showed that m(k) exists and gave an exponential upper bound. We show that m(k)=O(k^2), and generalize this result to translates of any centrally symmetric convex polygon in the place of squares. From the other direction, we know only that m(k)=@?4k/3@?-1.\n",
      "\n",
      "6. id: 5390a5dc20f70186a0e7ff2e   score: 0.35947654   abstract: Let X be a simple region (e.g., a simple polygon), and let Q be a set of points in X . Let O be a convex object, such as a disk, a square, or an equilateral triangle. We present a scheme for computing a minimum cover of Q with respect to X , consisting of homothetic copies of O . In particular, a minimum disk cover of Q with respect to X , can be computed in polynomial time.\n",
      "\n",
      "7. id: 5390a8b220f70186a0e9c25c   score: 0.33036315   abstract: We show that for any open convex polygon P, there is a constant k(P) such that any k(P)-fold covering of the plane with translates of P can be decomposed into two coverings.\n",
      "\n",
      "8. id: 53908b1820f70186a0db5221   score: 0.31943974   abstract: The problem Minimum Convex Cover of covering a given polygon with a minimum number of (possibly overlapping) convex polygons is known to be NP-hard, even for polygons without holes [3]. We propose a polynomial-time approximation algorithm for this problem for polygons with or without holes that achieves an approximation ratio of O(log n), where n is the number of vertices in the input polygon. To obtain this result, we first show that an optimum solution of a restricted version of this problem, where the vertices of the convex polygons may only lie on a certain grid, contains at most three times as many convex polygons as the optimum solution of the unrestricted problem. As a second step, we use dynamic programming to obtain a convex polygon which is maximum with respect to the number of \"basic triangles\" that are not yet covered by another convex polygon.We obtain a solution that is at \n",
      "\n",
      "9. id: 53908f5b20f70186a0dd9e60   score: 0.30611205   abstract: We consider the problem of covering simple orthogonal polygons with convex orthogonal polygons. In the case of horizontally or vertically convex polygons we show that the polygon covering problem can be reduced to the problem of covering a permutation graph with minimum number of cliques. In general, orthogonal polygons can have concavities (dents) with four possible orientations. In the case where the polygon has three dent orientations, we show that the polygon covering problem can be reduced to the problem of covering a weakly triangulated graph with a minimum number of cliques. Since weakly triangulated graphs are perfect, we obtain the following duality relationship: the minimum number of orthogonally convex polygons needed to cover an orthogonal polygon P with at most three dent orientations is equal to the maximum number of points of P, no two of which can be contained together in\n",
      "\n",
      "10. id: 53909f8220f70186a0e3c6f5   score: 0.25590113   abstract: We propose a novel subdivision of the plane that consists of both convex polygons and pseudo-triangles. This pseudo-convex decomposition is significantly sparser than either convex decompositions or pseudo-triangulations for planar point sets and simple polygons. We also introduce pseudo-convex partitions and coverings. We establish some basic properties and give combinatorial bounds on their complexity. Our upper bounds depend on new Ramsey-type results concerning disjoint empty convex k-gons in point sets.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675074\n",
      "index                                        55915bfd0cf232eb904fbe84\n",
      "title               Curatorial Agents: How Systems Shape Our Under...\n",
      "authors             Rebecca Gulotta, Alex Sciuto, Aisling Kelliher...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558acf49612c41e6b9d3a93a;5390b0ca20f70186a0ed9...\n",
      "abstract            As people increasingly turn to digital channel...\n",
      "id                                                            1675074\n",
      "clustered_labels                                                    0\n",
      "Name: 1675074, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4c5e1   score: 0.73469096   abstract: We develop new understanding of how people engage in digital curation. We interview twenty users of Pinterest, a social curation platform. We find that through collecting, organizing, and sharing image bookmarks, users engage in processes of everyday ideation. That is, they use digital found objects as creative resources to develop ideas for shaping their lives. Curators assemble information into new contexts, forming and sharing ideas with practical and emotional value. We investigate cognitive and social aspects of creativity that affect the digital curation practices of everyday ideation. We derive implications for the design of curation environments that support information-based ideation.\n",
      "\n",
      "2. id: 558acf49612c41e6b9d3a93a   score: 0.5873515   abstract: The creation of a personal legacy is a process through which information, values, and memories are passed down to future generations. This process is inherently subjective, both as a curated collection of the elements of one's life, and as an evolving form of remembrance that is subject to the interpretations of those to whom it is left. Based on directed storytelling sessions with 14 adults from a large Midwestern city in the USA, we explore users' perceptions of how their use of digital systems and information will impact how their lives are interpreted and reflected upon by their families and by future generations. Our findings describe nuances regarding how shifting notions about technological systems and the long-term accessibility of digital information impact the ways in which we share, and subsequently manage, information online. This work, explored here in the context of legacy,\n",
      "\n",
      "3. id: 5390baa120f70186a0f38d3a   score: 0.45283836   abstract: Personal informatics systems that help people both collect and reflect on various kinds of personal information are growing rapidly. Despite the importance of journaling and the main role it has in tracking one's personal growth, a limited number of studies have examined journaling in the area of personal informatics in detail. In this paper, we critically examine the process of reflection on experiences, thoughts and evolving insights through a qualitative research study. We also present the design research process we conducted to develop the Wandering Mind as a support tool to help individuals record and reflect on their experiences.\n",
      "\n",
      "4. id: 5390b19020f70186a0edf5a9   score: 0.40239212   abstract: People are accumulating large amounts of personal digital content that play a role in reminiscing practices. But as these collections become larger, and older content is less frequently accessed, much of this content is simply forgotten. In response to this we explore the notions of randomness and serendipity in the presentation of content from people's digital collections. To do this we designed and deployed two devices - Meerkat and Tuba - that enable the serendipitous presentation of digital content from people's personal media collections. Each device emphasises different characteristics of serendipity that with a view to understanding whether people interpret and value these in different ways while reminiscing. In order explore the use of the devices in context, we deployed in real homes. We report on findings from the study and discuss their implications for design.\n",
      "\n",
      "5. id: 558b85d3612c6b62e5e8a9e5   score: 0.24889915   abstract: Digital technologies which now capture many aspects of everyday life increasingly act to mediate the process of remembering. This paper outlines a thesis that seeks to understand the experience of remembering as a socially situated activity, in the context of the design of personal informatics systems.\n",
      "\n",
      "6. id: 5390b3ae20f70186a0ef4d26   score: 0.1775255   abstract: As more and more information is exchanged digitally, and as the tools for sharing and collaborating become more pervasive, users are presented with new opportunities and challenges in how they manage their personal information. Despite the traditional emphasis on the individual in research related to personal information management (PIM), it is apparent that family, friends, co-workers and other collaborators can strongly influence one's PIM behaviors. In this workshop, we will explore: 1) the role that other people play in an individual's PIM in a variety of collaborative and sharing contexts, and 2) the effects that web and cloud-based services are having on PIM practices. We focus on the challenges users face in an evolving communication ecology when sharing and exposing personal information in a variety of situations.\n",
      "\n",
      "7. id: 55912ec60cf232eb904fb23a   score: 0.14965561   abstract: This paper explores how people generate cues for capturing personal meaningful daily events, which can be used for later recall. Such understanding can be explored to inform the design and development of personal informatics systems, aimed to support reflection and increased self-awareness. We describe a diary study with six participants and discuss initial findings showing the qualities of daily meaningful events, the value of different types of cues and their distinct contents for supporting episodic recall.\n",
      "\n",
      "8. id: 5390a17720f70186a0e53b0a   score: 0.14330755   abstract: Digital media content can contain items that are very personal and valuable for their owner. Such items can form life memories, such as media collages from happy events or recordings of the first steps of one's children. Memories can be evoked and created \"anytime, anyplace\", and thus mobility is a key factor in managing them. Even though related systems for sharing photographs exist, users' needs for managing personal content have not been investigated specifically from the viewpoint of life memories. This paper describes our empirical research on users' needs for sharing the digital representations of their life memories. As the main contribution, we present design guidelines for services for sharing digital life memories. Furthermore, we present a mobile service prototype which was designed based on the guidelines. Our research shows that the creation, sharing, managing and viewing of\n",
      "\n",
      "9. id: 5390a01420f70186a0e48291   score: 0.14092636   abstract: Current technology makes it possible to capture huge amounts of information related to everyday experiences. Despite this, we know little about the processes by which people identify and manage mementos - objects which are directly meaningful to their memories. Among the millions of objects people encounter in a lifetime, few become such reminders of people, places or events. We report fieldwork where participants gave us a tour of their homes describing how and why particular objects become mementos. Our findings extend the existing digital memory literature; first our participants didn't view their activities as experiential 'capture', nor were mementos limited to pictorial representations of people and events; instead they included everyday objects. Furthermore, mementos were not only displayed and shared, but also integrated into everyday activities. Finally there were complex relati\n",
      "\n",
      "10. id: 53908b4920f70186a0dbad49   score: 0.1261379   abstract: In this paper we describe a user study designed to understand current practices for recording and utilizing information in everyday life. We describe a subset of the results that suggest that improving on current practices will require physical and digital artifacts, flexibility, multi-modality, and ubiquity.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672284\n",
      "index                                        559161e40cf2e89307ca9789\n",
      "title               Live-Feedback Supported Collaborative Environm...\n",
      "authors             Shah Rukh Humayoun, Artem Avtandilov, Syed Ati...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference Compani...\n",
      "references          558b8fcf612c6b62e5e8b991;558adeb9612c41e6b9d3b...\n",
      "abstract            Knowing the up-to-date information about the e...\n",
      "id                                                            1672284\n",
      "clustered_labels                                                    3\n",
      "Name: 1672284, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b8fcf612c6b62e5e8b991   score: 0.96995664   abstract: The paper presents an environment, called LiFe-Support, for facilitating the caregiver staff at health service centers in getting live feedback of an emergency situation that may occur to an elderly person at home. The two main components of the system include an autonomous mobile robot, called ARTOS, for serving elderly people in their homes and a visual platform for the caregivers to control communication and navigation of the robot in case of an emergency situation. A preliminary evaluation of the LiFe-Support environment has been carried out and promising results indicate usefulness of the system.\n",
      "\n",
      "2. id: 558ad9f5612c41e6b9d3b894   score: 0.84645087   abstract: Situational awareness in rescue operations can be provided by teams of autonomous mobile robots. Human operators are required to teleoperate the current generation of mobile robots for such applications; however, teleoperation is increasingly difficult as the number of robots is expanded. As the number of robots is increased, each robot may also interfere with one another and eventually decrease mapping performance. As presented here, through careful consideration of robot team coordination and exploration strategy, large numbers of mobile robots can be allocated to accomplish the mapping task more quickly and accurately. We present both the coordination and exploration strategies and present results from experiments in simulation as well as with up to nine mobile platforms.\n",
      "\n",
      "3. id: 5390bf1320f70186a0f5095f   score: 0.8398654   abstract: Crisis Management requires communication and collaboration among of people. Approaches to support information sharing through mobile devices may not have built an appropriate interaction on this domain. This paper discusses an approach to provide a multimodal interface on mobile devices to work with emergency response teams.\n",
      "\n",
      "4. id: 5390979920f70186a0dff0e0   score: 0.761893   abstract: We demonstrate a same-time different-place collaboration system for managing crisis situations using geospatial information. Our system enables distributed spatial decision-making by providing a multimodal interface to team members. Decision makers in front of large screen displays and/or desktop computers, and emergency responders in the field with tablet PCs can engage in collaborative activities for situation assessment and emergency response.\n",
      "\n",
      "5. id: 53909fbd20f70186a0e42599   score: 0.71032697   abstract: This project aims to further our understanding of collaboration practices in small, collocated teams that work in dynamic, time-and safety critical environments. We use emergency room as a natural laboratory for investigating information gathering, sharing and archiving, as well as decision making in trauma teams as they conduct trauma resuscitations. We have observed and analyzed six trauma resuscitations and conducted interviews with trauma team members. We found that every worker has different information needs, which are role specific and change rapidly over time. Information gathering and sharing within trauma teams often become inefficient due to the urgency of situation and the lack of information support technologies. Our future efforts will focus on deriving system requirements for collaborative technologies that could support trauma teams more effectively.\n",
      "\n",
      "6. id: 5390a06e20f70186a0e4bf5b   score: 0.6441132   abstract: This paper presents the EU project SHARE which is developing a mobile service architecture to support large-scale rescue operations with multimedia communication and information services. The task of planning and controlling large-scale rescue operations requires flexible and robust tools which help the rescue forces to do their search and rescue work with maximum efficiency. Today the main channels of communication are analog radio and paper text forms. The SHARE system introduces an advanced multimedia communication and information system to the highly mobile working environment which supports well established command hierarchies of the rescue organizations. Different operation scenarios for mobile networks using WiMAX, WLAN or UMTS are discussed regarding their usability for large-scale rescue operations.\n",
      "\n",
      "7. id: 5390a1d420f70186a0e57478   score: 0.63624007   abstract: Recent advances in mobile computing technologies and platform-independent information systems have enabled to realize a ubiquitous environment. Community computing is developed to as a useful tool for realizing collaborative services in a ubiquitous environment. In this paper, we present a formal model of a ubiquitous space that takes community concept into consideration and propose two management frameworks that prevent conflicts among communities. To demonstrate the validity of the proposed frameworks, a prototype system for coordinating medical emergency system is provided.\n",
      "\n",
      "8. id: 5390b95420f70186a0f2dda8   score: 0.62589556   abstract: The development of autonomous robotic systems has experienced a remarkable boost within the last years. Away from stationary manufacturing units, current robots have grown up into autonomous, mobile systems that not only interact with real world environments, but also fulfill mission critical tasks in collaboration with human individuals on a reliable basis. Typical fields of application are unmanned vehicles for exploration but also for transportation, reconnaissance and search-and-rescue in hazardous environments, and ambient assisted living for elderly or disabled people.\n",
      "\n",
      "9. id: 5390a45520f70186a0e71804   score: 0.5885344   abstract: When an emergency occurs within a building, it may be initially safer to send autonomous mobile nodes, instead of human responders, to explore the area and identify hazards and victims. Exploring all the area in the minimum amount of time and reporting back interesting findings to the human personnel outside the building is an essential part of rescue operations. Our assumptions are that the area map is unknown, there is no existing network infrastructure, long-range wireless communication is unreliable and nodes are not location-aware. We take into account these limitations, and propose an architecture consisting of both mobile nodes (robots, called agents) and stationary nodes (inexpensive smart devices, called tags). As agents enter the emergency area, they sprinkle tags within the space to label the environment with states. By reading and updating the state of the local tags, agents \n",
      "\n",
      "10. id: 5390a1d420f70186a0e56a99   score: 0.5410451   abstract: This paper describes outcome from a project aimed at creating an instance of integrated environment endowed with heterogeneous software and robotic agents to actively assist an elderly person at home. Specifically, a proactive environment for continuous daily activity monitoring has been created in which an autonomous robot acts as the main interactor with the person. This paper describes how the synergy of different technologies guarantees an overall intelligent behavior capable of personalized and contextualized interaction with the assisted person.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1706911\n",
      "index                                        55323b8645cec66b6f9da122\n",
      "title               Lightweight Location Verification in Air Traff...\n",
      "authors             Martin Strohmeier, Vincent Lenders, Ivan Marti...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 1st ACM Workshop on Cyber-P...\n",
      "references                                   558ce1f20cf23fdd601e0ed2\n",
      "abstract            In this work, we develop a realistic threat mo...\n",
      "id                                                            1706911\n",
      "clustered_labels                                                    3\n",
      "Name: 1706911, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909f2c20f70186a0e37608   score: 0.63578796   abstract: Because the knowledge of sensors' locations is very important to many location-based applications proposed for Wireless Sensor Networks, many secure localization and location verification schemes have been proposed to provide robust location estimations for sensors. We propose two lightweight location verification algorithms, namely, Greedy Filtering by Matrix (GFM) and Trustability Indicator (TI). Unlike other schemes, our algorithms do not require any specialized hardware or deployment knowledge. In GFM algorithm, the Verification Center( VC) calculates several matrices based on sensors' estimated locations and their neighborhood observations, and uses these matrixes to identify and revoke inconsistent locations. In TI algorithm, VC calculates trustability indicators for the sensors and reject those whose indicators are below a threshold. Simulation results demonstrated the effectivene\n",
      "\n",
      "2. id: 5390a30b20f70186a0e6af32   score: 0.5911333   abstract: Due to the widespread adoption of the Global Positioning System (GPS), many systems have been designed to use the location information of participants. When these systems confer rights (such as access rights) based on location, such claim must be securely verified in order to prevent attackers from gaining access to resources that should be restricted. Substantial effort has been made on secure location verification; however, previous work does not address the impact of collusion attacks where adversaries share their private keys nor do they address a possible jamming attack where attackers inject a high amount of noise to prevent successful challenge and response receptions. In this paper, we propose a secure multilateration scheme that provides maximal security achievable by any time-of-flight based system that does not employ other verification methods.\n",
      "\n",
      "3. id: 5390b56a20f70186a0f0593c   score: 0.57821   abstract: Due to the open nature of a sensor network, it is relatively easy for an adversary to eavesdrop and trace packet movement in the network in order to capture the receiver physically. After studying the adversary's behavior patterns, we present countermeasures to this problem. We propose a locationprivacy routing protocol (LPR) that is easy to implement and provides path diversity. Combining with fake packet injection, LPR is able to minimize the traffic direction information that an adversary can retrieve from eavesdropping. By making the directions of both incoming and outgoing traffic at a sensor node uniformly distributed, the new defense system makes it very hard for an adversary to perform analysis on locally gathered information and infer the direction to which the receiver locates. We evaluate our defense system based on three criteria: delivery time, privacy protection strength, a\n",
      "\n",
      "4. id: 5390a9a520f70186a0ea62bb   score: 0.53540206   abstract: Most of the state-of-the-art localization algorithms in wireless sensor networks (WSNs) are vulnerable to attacks from malicious or compromised network nodes, whereas the secure localization schemes proposed so far are too complex to be applied to power constrained WSNs. This paper provides a novel secure scheme \"Bilateration\" which is derived from multilateration but can be calculated more accurately and quickly to resolve the positions of unknown nodes without explicitly distinguishing what kind of location attacks the WSN is facing. This paper also compares Bilateration with three existing multilateration solutions that optimize the location estimation accuracy via LS, LMS and LLMS respectively in a simulated threat environment. The experiment results show that Bilateration gets the best tradeoff among estimation error, filtering ability and computational complexity.\n",
      "\n",
      "5. id: 5390b0ca20f70186a0ed970c   score: 0.4329023   abstract: In some networks, a mobile node must be located securely as well as accurately. One way to locate is for base stations to use time-difference-of-arrival multilateration. An attacker that can transmit at different times in different directions may falsify its position with respect to known base stations. Previous work proposed hiding the base stations, following a model of \"security through obscurity.\" We propose randomly selecting a set of base stations such that even if their locations are known, the attacker has at most a 50% chance of succeeding in one trial. Through iteration we reduce the likelihood of success to an arbitrarily small probability.\n",
      "\n",
      "6. id: 5390a88c20f70186a0e9973c   score: 0.36082682   abstract: Trustworthy location information is important because it is a critical input to a wide variety of location-based applications. However, the localization infrastructure is vulnerable to physical attacks and consequently the localization results are affected. In this paper, we focus on achieving robust wireless localization when attacks are present on access points. We first investigate the effects of attacks on localization. We then derive an attack-resistant scheme that can be integrated with existing localization algorithms and are not algorithm-specific. Our attack-resistant scheme are based on K-means clustering analysis. We examined our approach using received signal strength (RSS) in widely used lateration-based algorithms. We validated our method in the ORBIT testbed with an IEEE 802.11 (Wi-Fi) network. Our experimental results demonstrate that our proposed approach can achieve com\n",
      "\n",
      "7. id: 539099a220f70186a0e1909f   score: 0.32359383   abstract: Many sensor applications are being developed that require the location of wireless devices, and localization schemes have been developed to meet this need. However, as location-based services become more prevalent, the localization infrastructure will become the target of malicious attacks. These attacks will not be conventional security threats, but rather threats that adversely affect the ability of localization schemes to provide trustworthy location information. This paper identifies a list of attacks that are unique to localization algorithms. Since these attacks are diverse in nature, and there may be many unforseen attacks that can bypass traditional security countermeasures, it is desirable to alter the underlying localization algorithms to be robust to intentionally corrupted measurements. In this paper, we develop robust statistical methods to make localization attack-tolerant.\n",
      "\n",
      "8. id: 5390bda020f70186a0f45db7   score: 0.31341976   abstract: This work studies the security of next generation air traffic surveillance technology based on Automatic Dependent Surveillance --- Broadcast (ADS-B). ADS-B is already supported by a majority of international aircraft and will become mandatory in 2020 for most airspaces worldwide. While it is known that ADS-B might be susceptible to different spoofing attacks, the complexity and impact of launching these attacks has been debated controversially by the air traffic control community. Yet, the literature remains unclear on the requirements of launching ADS-B attacks in real-world environments, and on the constraints which affect their feasibility. In this paper, we take a scientific approach to systematically evaluate realistic ADS-B attacks. Our objective is to shed light on the practicability of different threats and to quantify the main factors that impact the success of such attacks. Ou\n",
      "\n",
      "9. id: 5390ada620f70186a0ec243c   score: 0.25571522   abstract: Internet Threat Monitoring (ITM) systems are a widely deployed facility to detect, analyze, and characterize dangerous Internet threats such as worms and distributed denial-of-service (DDoS) attacks. Nonetheless, an ITM system can also become the target of attacks. In this paper, we address localization attacks against ITM systems in which an attacker impairs the effectiveness of an ITM system by identifying the locations of ITM monitors. We propose an information-theoretic framework that models localization attacks as communication channels. Based on this model, we generalize all existing attacks as \"temporal attacks,” derive closed formulas of their performance, and propose an effective attack detection approach. The information-theoretic model also inspires a new attack called a spatial attack and motivates the corresponding detection approach. We show simulation results that support \n",
      "\n",
      "10. id: 5390bfa220f70186a0f54f0d   score: 0.2516477   abstract: Automatic Dependent Surveillance-Broadcast (ADS-B) is one of the key components of the next generation air transportation system. Since ADS-B will become mandatory by 2020 for most airspaces, it is important that aspects such as capacity, applications, and security are investigated by an independent research community. However, large-scale real-world data was previously only accessible to a few closed industrial and governmental groups because it required specialized and expensive equipment. To enable researchers to conduct experimental studies based on real data, we developed OpenSky, a sensor network based on low-cost hardware connected over the Internet. OpenSky is based on off-the-shelf ADS-B sensors distributed to volunteers throughout Central Europe. It covers 720,000 sq km2, is able to capture more than 30% of the commercial air traffic in Europe, and enables researchers to analyz\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698007\n",
      "index                                        55922e950cf2ceaae74c8e2e\n",
      "title               Cycle-based Model to Evaluate Consistency Prot...\n",
      "authors             Hamza Chaker, Loïc Cudennec, Safae Dahmani, Gu...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 International Workshop...\n",
      "references          558b4ea5612c41e6b9d48c33;5390a25820f70186a0e60...\n",
      "abstract            Many-core processors are made by hundreds to t...\n",
      "id                                                            1698007\n",
      "clustered_labels                                                    3\n",
      "Name: 1698007, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558fd8fc612c29c89cd7b7fe   score: 0.91228   abstract: We analyze the scalability of six memory consistency models in network-on-chip (NoC)-based distributed shared memory multicore systems: 1) protected release consistency (PRC); 2) release consistency (RC); 3) weak consistency (WC); 4) partial store ordering (PSO); 5) total store ordering (TSO); and 6) sequential consistency (SC). Their realizations are based on a transaction counter and an address-stack-based approach. The scalability analysis is based on different workloads mapped on various sizes of networks using different problem sizes. For the experiments, we use Nostrum NoC-based configurable multicore platform with a 2-D mesh topology and a deflection routing algorithm. Under the synthetic workloads, the average execution time for the PRC, RC, WC, PSO, and TSO models in the 8$\\\\,\\\\times\\\\,$8 network (64-cores) is reduced by 32.3%, 28.3%, 20.1%, 13.8%, and 9.9% over the SC model, re\n",
      "\n",
      "2. id: 53908b2120f70186a0db6710   score: 0.8000679   abstract: Distributed Shared Memory (DSM) systems typically support one consistency protocol [3,5,6]. However, recent work [1,11,12,14,17] proposes the use of adaptive consistency based on a heuristical analysis of recent access patterns. Although heuristic-based approaches can significantly improve runtime, the access pattern alone does not necessarily define the most appropriate consistency protocol. The size of updates and other factors related to the computing environment, such as heavily loaded links, heavily loaded nodes, bursty traffic patterns, and network latency all affect performance. Multiple access patterns within the application also make it difficult to select the most appropriate consistency protocol. This paper presents a measurement-based approach to the problem of selecting the most appropriate consistency protocol for the current application in the current runtime environment. \n",
      "\n",
      "3. id: 5390b2fc20f70186a0eefad0   score: 0.7634837   abstract: As we enter an era of exascale multicores, the question of efficiently supporting a shared memory model has become of paramount importance. On the one hand, programmers demand the convenience of coherent shared memory; on the other, growing core counts place higher demands on the memory subsystem and increasing on-chip distances mean that interconnect delays are becoming a significant part of memory access latencies. In this article, we first review the traditional techniques for providing a shared memory abstraction at the hardware level in multicore systems. We describe two new schemes that guarantee coherent shared memory without the complexity and overheads of a cache coherence protocol, namely execution migration and library cache coherence. We compare these approaches using an analytical model based on average memory latency, and give intuition for the strengths and weaknesses of e\n",
      "\n",
      "4. id: 5390a93b20f70186a0ea0ee0   score: 0.7620701   abstract: This paper studies realization of relaxed memory consistency models in the network-on-chip based distributed shared memory (DSM) multi-core systems. Within DSM systems, memory consistency is a critical issue since it affects not only the performance but also the correctness of programs. We investigate the scalability of the relaxed consistency models (weak, release consistency) implemented by using transaction counters. Our experimental results compare the average and maximum code, synchronization and data latencies of the two consistency models for various network sizes with regular mesh topologies. The observed latencies rise for both the consistency models as the network size grows. However, the scaling behaviors are different. With the release consistency model these latencies grow significantly slower than with the weak consistency due to better optimization potential by means of ov\n",
      "\n",
      "5. id: 5390b36120f70186a0ef08c0   score: 0.71611935   abstract: Recently, Intel has introduced a research prototype many core processor called the Single-chip Cloud Computer (SCC). The SCC is an experimental processor created by Intel Labs. It contains 48 cores in a single chip and each core has its own L1 and L2 caches without any hardware support for cache coherence. It allows maximum 64GB size of external memory that can be accessed by all cores and each core dynamically maps the external memory into their own address space. In this paper, we introduce the design and implementation of an OpenCL framework (i.e., runtime and compiler) for such many core architectures with no hardware cache coherence. We have found that the OpenCL coherence and consistency model fits well with the SCC architecture. The OpenCL's weak memory consistency model requires relatively small amount of messages and coherence actions to guarantee coherence and consistency betwe\n",
      "\n",
      "6. id: 5390995d20f70186a0e162e5   score: 0.70161086   abstract: A problem with running distributed shared memory applications in heterogeneous environments is that making optimal use of available resources often requires significant changes to the application. In this paper we present a model, dubbed the view model, that provides an abstraction of shared data and separates the concerns of programming model, consistency, and communication. Separating these concerns makes it possible for applications to easily be adapted to different execution environments, allowing them to take full advantages of resources such as high speed interconnects and hardware-based memory coherence, and to be optimised for specific network topologies. Furthermore, it allows different data consistency protocol implementations to be used without requiring changes to the application code itself. We also present an implementation of the view model and provide experimental results\n",
      "\n",
      "7. id: 5390b95420f70186a0f2e732   score: 0.690872   abstract: We propose a novel hardware support for three relaxed memory models, Release Consistency (RC), Partial Store Ordering (PSO) and Total Store Ordering (TSO) in Network-on-Chip (NoC) based distributed shared memory multicore systems. The RC model is realized by using a Transaction Counter and an Address Stack based approach to enforce the required global orders on the shared memory operations. The PSO and TSO models are realized by using a Write Transaction Counter and a Write Address Stack based approach to enforce the required global orders on the shared memory operations. In the experiments, we use a configurable platform based on a 2D mesh NoC using deflection routing policy. The results show that under synthetic workloads, the average execution time for the RC, PSO and TSO models in 8x8 network (64 cores) is reduced by 35.8%, 22.7% and 16.5% over the sequential consistency (SC) model, \n",
      "\n",
      "8. id: 539087c720f70186a0d56e6e   score: 0.6369179   abstract: We compare the performance of software-supported shared memory on a general-purpose network to hardware-supported shared memory on a dedicated interconnect.Up to eight processors, our results are based on the execution of a set of application programs on a SGI 4D/480 multiprocessor and on TreadMarks, a distributed shared memory system that runs on a Fore ATM LAN of DECstation-5000/240s. Since the DECstation and the 4D/480 use the same processor, primary cache, and compiler, the shared-memory implementation is the principal difference between the systems. Our results show that TreadMarks performs comparably to the 4D/480 for applications with moderate amounts of synchronization, but the difference in performance grows as the synchronization frequency increases. For applications that require a large amount of memory bandwidth, TreadMarks can perform better than the SGI 4D/480.Beyond eight \n",
      "\n",
      "9. id: 5390980720f70186a0e033ff   score: 0.6353356   abstract: This extended abstract presents models to derive timing and resource usage numbers for an application when distant, shared memories are used in an important class of future embedded platforms, namely network-on-chip-based multiprocessors.\n",
      "\n",
      "10. id: 5390ad5620f70186a0ebcbc0   score: 0.60743034   abstract: Multi-core architectures also referred to as Chip Multiprocessors (CMPs) have emerged as the dominant architecture for both desktop and high-performance systems. CMPs introduce many challenges that need to be addressed to achieve the best performance. One of the big challenges comes with the shared-memory model observed in such architectures which is the cache coherence overhead problem. Contemporary architectures employ write-invalidate based protocols which are known to generate coherence misses that yield to latency issues. On the other hand, write-update based protocols can solve the coherence misses problem but they tend to generate excessive network traffic which is especially not desirable for CMPs. Previous studies have shown that a single protocol approach is not sufficient for many sharing patterns. As a solution, this paper evaluates an adaptive protocol which targets write-up\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1718700\n",
      "index                                        55323c8b45cec66b6f9dc51f\n",
      "title               Nash Feature Package of an Integrated Finance ...\n",
      "authors             Masoud Rabbani, Sina Keyhanian, Maryam Ghazanf...\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Strategic Decision Sc...\n",
      "references                                   558aff50612c41e6b9d3fdc8\n",
      "abstract            Higher inflation rate and subsequently, higher...\n",
      "id                                                            1718700\n",
      "clustered_labels                                                    1\n",
      "Name: 1718700, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b63320f70186a0f17438   score: 0.3528705   abstract: Two make-to-order firms, each modelled as a single-server queue, compete for a common stream of (potential) customers by setting their service capacities (rates) and service prices. Each customer maximizes her expected return by getting service from a firm or by balking. We completely characterize the Nash equilibrium of the competition.\n",
      "\n",
      "2. id: 5390975920f70186a0dfcee4   score: 0.32348695   abstract: We describe two sales strategies used by our agent, MinneTAC, for the 2003 Supply Chain Management Trading Agent Competition (TAC SCM). Both strategies estimate, as the game progresses, the probability of receiving a customer order for different prices and compute the expected profit. We empirically analyze the effect of the discount given by suppliers on orders made the first day of the game, and show that in high-demand games there is a strong correlation between the performance of an agent in the game and the offers it receives from suppliers the first day of the game.\n",
      "\n",
      "3. id: 53909fca20f70186a0e445bf   score: 0.3226327   abstract: When a new investment opportunity of purchasing a new device occurs, the investor must decide whether or not and when to buy this device in an online fashion. This problem which generalizes the basic leasing problem has been in- troduced by Y.Azar et al, and then two special cases have been studied by P.Damaschke. In the so-called equal prices model a 2-competitive algorithms is devised and a 1.618 lower bound is given. Here we make use of an averag- ing technique and obtain a better tight lower bound of 2, that is, this lower bound can not be improved. Further- more, another special case which considers two-stage de- vice replacement is studied in this paper. We introduce the risk-reward model to analyze this problem and improve the competitive ratio.\n",
      "\n",
      "4. id: 5390aa7620f70186a0eab93c   score: 0.31090358   abstract: We model a market for a single product that may be composed of sub-products that face horizontal and vertical competition. Each firm, offering all or some portion of the product, adopts a price function proportional to its costs by deciding on the size of a markup. Customers then choose a set of providers that offers the lowest total cost. We characterize equilibria of the two-stage game and study the efficiency resulting from the competitive structure of the market.\n",
      "\n",
      "5. id: 5390be6620f70186a0f4cf47   score: 0.297572   abstract: The paper considers a dynamic game with a single manufacturer who supplies two retailers. The manufacturer determines his production rate of a specific product, the rate of quality improvement efforts as well as the rate of advertising for the product. Each retailer controls her purchasing rate and the consumer sales price. Payments from a retailer to the manufacturer are determined by a wholesale price or a revenue-sharing scheme. The retailers operate in the same consumer market in which they compete in prices for the consumer demand. Nash equilibrium conditions are derived and numerical methods are employed to characterize equilibrium behavior of the players in a differential game of fixed and finite duration.\n",
      "\n",
      "6. id: 5390a40520f70186a0e6eadc   score: 0.265309   abstract: We consider a market with two suppliers and a set of buyers in search of procurement contracts with one of the suppliers. In particular, each buyer needs to process a certain volume of work, and each supplier's ability to process the customers' requests is constrained by a production capacity. The procurement contracts include guarantees that the products will be available when needed, and the buyers select a supplier based on their service delivery offers. The suppliers are modeled as make-to-stock queues and compete for the buyers' business. The main objective of this paper is to determine how the procurement contracts are established between buyers and suppliers. Because each buyer selects a single supplier to establish the sourcing relationship, the game fails to have a pure-strategy Nash equilibrium. Instead, an equilibrium is defined as the limit equilibrium of some discrete action\n",
      "\n",
      "7. id: 5390a74f20f70186a0e8b91a   score: 0.19390947   abstract: In this paper, we apply the game theory to study some strategic actions for retailers to fight a price war. We start by modeling a noncooperative pure pricing game among multiple competing retailers who sell a certain branded product under price-dependent stochastic demands. A unique Nash equilibrium is proven to exist under some mild conditions. We demonstrate mathematically the incentives for retailers to start a price war. Based on a strategic framework via the game theory, we illustrate the use of service level to build price walls which can prevent a huge drop in price, as well as profit. Three kinds of price walls are proposed, and the respective strengths and weaknesses have been studied. Analytical conditions, under which a price wall can effectively prevent big drops in both market share and profit, are developed. Aside from the proposed price walls, two other pricing strategies\n",
      "\n",
      "8. id: 5390b7fe20f70186a0f27409   score: 0.1859472   abstract: We incorporate the effects of churn, which refers to customers switching to competing brands, in a dynamic model of advertising for oligopoly markets. Each firm's market share depends not only on its own and competitors' advertising decisions, but also on market churn. Applying differential game theory, we derive a feedback Nash equilibrium under symmetric and asymmetric competition. We obtain explicit solutions and discover the counter-intuitive result that, as market churn increases, firms should decrease advertising rather than increase it to counteract the impact of churn.\n",
      "\n",
      "9. id: 5390b9d520f70186a0f31c51   score: 0.17412923   abstract: We consider dynamic pricing competition between two firms offering vertically differentiated products to strategic customers who are intertemporal utility maximizers. We show that price skimming arises as the unique pure-strategy Markov perfect equilibrium in the game under a simple condition. Our results highlight the asymmetric effect of strategic customer behavior on quality-differentiated firms. Even though the profit of either firm decreases as customers become more strategic, the low-quality firm suffers substantially more than the high-quality firm. Furthermore, we show that unilateral commitment to static pricing by either firm generally improves profits of both firms. Interestingly, both firms enjoy higher profit lifts when the high-quality firm commits rather than when the low-quality firm commits. This paper was accepted by Yossi Aviv, operations management.\n",
      "\n",
      "10. id: 5390ad8920f70186a0ec147a   score: 0.17217185   abstract: Game theory is a theory that study the rational decision-makers’ decision-making behavior under the condition of strategic interaction between the parties. The traditional economic management theory focuses on the profit-maximizing of a single enterprise with the resource constraints, but the game theory play particular emphasis on market profit equalization that formed in modern competition. This kind of profit equalization is that the parties do no want to achieve through changing his own strategy unilateral and in response equilibrium conditions. Competition among enterprises is actually the game among the various strategies. In the competitive decision-making game process, each firm will make decisions according to the competitors’ decision to maximize their own profit. This paper uses the game model innovatively to analyze the competitive decision-making process between enterprises,\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698343\n",
      "index                                        5591690b0cf2e89307ca9a25\n",
      "title               Reconstructable Software Appliances with Kameleon\n",
      "authors             Cristian Ruiz, Salem Harrache, Michael Mercier...\n",
      "year                                                           2015.0\n",
      "venue               ACM SIGOPS Operating Systems Review - Special ...\n",
      "references          5390b86b20f70186a0f28594;53909ee020f70186a0e33...\n",
      "abstract            A software appliance builder bundles together ...\n",
      "id                                                            1698343\n",
      "clustered_labels                                                    3\n",
      "Name: 1698343, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390962020f70186a0df4bce   score: 0.81227237   abstract: In this paper, we describe our experiences with developing two middleware infrastructures for networked home appliances on commodity software platforms to show what future distributed system designers need to take into account.\n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3d605   score: 0.80240077   abstract: Almost ten years after its premises, the Grid'5000 platform has become one of the most complete testbeds for designing or evaluating large-scale distributed systems. Initially dedicated to the study of High Performance Computing, the infrastructure has evolved to address wider concerns related to Desktop Computing, the Internet of Services and more recently the Cloud Computing paradigm. In this paper, we present the latest mechanisms we designed to enable the automated deployment of the major open-source IaaS cloudkits (i.e., Nimbus, OpenNebula, CloudStack, and OpenStack) on Grid'5000. Providing automatic, isolated and reproducible deployments of cloud environments lets end-users study and compare each solution or simply leverage one of them to perform higher-level cloud experiments (such as investigating Map/Reduce frameworks or applications).\n",
      "\n",
      "3. id: 5390a5b020f70186a0e7d181   score: 0.7218412   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "4. id: 5390a5b020f70186a0e7d187   score: 0.7218412   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "5. id: 5390a5b020f70186a0e7d1b8   score: 0.7218412   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "6. id: 5390a5b020f70186a0e7d197   score: 0.7218412   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "7. id: 5390a5b020f70186a0e7d15f   score: 0.7216451   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "8. id: 5390a5b020f70186a0e7d141   score: 0.7216451   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "9. id: 5390a5b020f70186a0e7d1c1   score: 0.7216451   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "10. id: 5390a5b020f70186a0e7d172   score: 0.7216451   abstract: We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1711694\n",
      "index                                        559131f50cf2127aa930c175\n",
      "title               How hot is piping hot?: lower energy consumpti...\n",
      "authors             Yong Sun, Md Anindya Prodhan, Erin Griffiths, ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 14th International Conferen...\n",
      "references          558af2fc612c41e6b9d3e450;5390a6b120f70186a0e83...\n",
      "abstract            In typical US homes, water heating is the larg...\n",
      "id                                                            1711694\n",
      "clustered_labels                                                    3\n",
      "Name: 1711694, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b95520f70186a0f2f929   score: 0.8842393   abstract: After space heating and cooling, water heating is typically the largest energy consumer in the U.S. homes, accounting for approximately 17% of total energy consumption. Current water heating systems waste up to 20% of their energy due to poor insulation in pipes or water tanks, but improving this insulation is too costly to be practical for energy savings. In this paper, we build on recent fixture and water flow monitoring systems to create the Hot Water DJ, which provides hot water to any fixture based on the requirement of the fixture. In our experiment, we deployed sensors in a real home to learn about the accurate temperature model for each of the fixtures in the house. Whenever any of the fixture asks for hot water, Hot Water DJ would provide hot water as hot as the appliance typically require. Our result shows, with our approach we can save 10% of water heater energy with limited i\n",
      "\n",
      "2. id: 5390aa7620f70186a0eac267   score: 0.61207783   abstract: The high energy required by home appliances (like white goods, audio/video devices and communication equipments) and air conditioning systems (heating and cooling), makes our homes one of the most critical areas for the impact of energy consumption on natural environment. In this paper we present a work in progress within the European project AIM for the design of a system that can minimize energy waste in home environments efficiently managing devices operation modes. In our architecture we use a wireless sensor network to monitor physical parameters (like light and temperature) as well as the presence of users at home and in each of its rooms. With gathered data our system creates profiles of the behavior of house inhabitants and through a prediction algorithm is able to automatically set system parameters in order to optimize energy consumption and cost while guaranteeing the required\n",
      "\n",
      "3. id: 5390bf1320f70186a0f50510   score: 0.5490362   abstract: Energy is one of the most important resources required by modern human society. In 2010, energy expenditures represented 10% of global gross domestic product (GDP). By 2035, global energy consumption is expected to increase by more than 50% from current levels. The increased pace of global energy consumption leads to significant environmental and socioeconomic issues: (i) carbon emissions, from the burning of fossil fuels for energy, contribute to global warming, and (ii) increased energy expenditures lead to reduced standard of living. Efficient use of energy, through energy conservation measures, is an important step toward mitigating these effects. Residential and commercial buildings represent a prime target for energy conservation, comprising 21% of global energy consumption and 40% of the total energy consumption in the United States.This thesis describes techniques for the analysi\n",
      "\n",
      "4. id: 5390b9d520f70186a0f319c0   score: 0.3869951   abstract: We developed an integrated inter-building physical and human network model to predict the energy conservation for an assumed urban residential block. We utilized an Artificial Neural Network to predict hourly energy consumption in both the first physical and second human stage. In the first stage, simulated data were exported from EnergyPlus, and the optimal scenario was found to consume 12.28% less energy than the base scenario. In the second stage, the human network closeness index was obtained from a residential experiment to represent occupants' network connections. We found that energy consumption can be further reduced up to 51.75%. Finally, hour-by-hour energy consumption prediction under various levels of occupant networks was examined, and we found the block exhibits a potential of conserving 57.68% of the original energy consumption. An integrated understanding of physical and \n",
      "\n",
      "5. id: 5390bb1d20f70186a0f3e1c6   score: 0.38664764   abstract: Reducing the large energy consumption of temperature regulation systems is a challenge for researchers and practitioners alike. In this paper, we explore and compare two common types of solutions: A manual systems that encourages reduced energy use, and an intelligent automatic control system. We deployed an eco-feedback system with the ability to remotely control one's thermostat to ten participants for three months. Participants appreciated the ability to remotely control the thermostat, and controlled their heating system with 78.8% accuracy, a 6.3% improvement over not having this system. However, despite having feedback and remote control, they still wasted a lot of energy heating when away from home for the day. Using data from our deployment, we developed TherML, an occupancy prediction algorithm that uses GPS data from a user's smartphone to automatically control the indoor tempe\n",
      "\n",
      "6. id: 5390ac1820f70186a0eb4549   score: 0.34764785   abstract: Energy consumption represents a major concern, considering the limited resources and latest targets for lower emissions of carbon dioxide. Therefore design of electric heating elements for household and industry are more and more subject to optimization, in order to improve efficiency and minimize losses. An analysis has been made using the finite elements method to optimize the efficiency of an instant electric heater used for production of hot water. The actual solution is used now for a large scale production of heaters, and is based on considerable and mostly empirical experience. However some problems have been reported during exploitation and maintenance of industrial equipment provided with hot water by such instant heaters. Some new technical solutions have been analyzed to improve the classic design. Optimized solutions have been based on simulation of heat transfer and water fl\n",
      "\n",
      "7. id: 558b7872612c6b62e5e8947a   score: 0.3457677   abstract: In this paper, we propose an power reduction method in a house which considers comfort. The proposed method assumes sensor networks which consist of smart taps, temperature and humidity sensors, light sensors, sleeping detection sensors, and human detection sensors. Furthermore, we made a modeling of the sensor networks and developed a simulation system to verify an effectiveness of the proposed method. From the evaluation by simulation, we verified effectiveness of the proposed method.\n",
      "\n",
      "8. id: 558ae6da612c41e6b9d3ce2e   score: 0.32691577   abstract: Current approaches for benchmarking building energy consumption are either too data intensive to be feasible in practice or too data agnostic to be useful. We present a limited data approach where in, instead of using minutiae required for accurate HVAC modeling, we model the heating/cooling loads, the drivers for HVAC. This allows us to see how a building's (i) weather independent consumption compares to the optimal value and (ii) weather dependent consumption compares with its expected heating/cooling loads. Based on this two dimensional metric, we benchmark 94 geographically diverse supermarket stores and present our findings.\n",
      "\n",
      "9. id: 5390bda020f70186a0f469b4   score: 0.30342212   abstract: Residential buildings contribute significantly to the overall energy usage across the world. Real deployments, and collected data thereof, play a critical role in providing insights into home energy consumption and occupant behavior. Existing datasets from real residential deployments are all from the developed countries. Developing countries, such as India, present unique opportunities to evaluate the scalability of existing research in diverse settings. Building upon more than a year of experience in sensor network deployments, we undertake an extensive deployment in a three storey home in Delhi, spanning 73 days from May-August 2013, measuring electrical, water and ambient parameters. We used 33 sensors across the home, measuring these parameters, collecting a total of approx. 400 MB of data daily. We discuss the architectural implications on the deployment systems that can be used fo\n",
      "\n",
      "10. id: 558b0afd612c41e6b9d413cf   score: 0.2850733   abstract: As the carbon dioxide emission and global warming become the world serious issues, many researches are performed to reduce the energy consumption. Therefore, there are many new devices for energy saving scheme in the smart home area, such as smart plug, AMI, and IHD. Using them, users can recognize and reduce the amount of energy consumption, and the appliances can be controlled considering the energy efficiency. In this paper, we propose a home energy management system for an energy-efficient smart home. First, the data of energy consumption for a smart home is collected and analyzed. And energy conservation measures are generated and performed while maintaining comfort of users. Using these methods, energy management services for smart homes can be provided more easily and efficiently.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698159\n",
      "index                                        559166170cf2e89307ca98f7\n",
      "title               A Profile Guided Approach to Optimize Branch D...\n",
      "authors                                Santonu Sarkar, Sayantan Mitra\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 8th India Software Engineer...\n",
      "references          5390a4d020f70186a0e76411;53909fca20f70186a0e45...\n",
      "abstract            GPUs offer a powerful bulk synchronous program...\n",
      "id                                                            1698159\n",
      "clustered_labels                                                    3\n",
      "Name: 1698159, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba3820f70186a0f36e18   score: 0.9114953   abstract: Branch divergence can incur a high performance penalty on GPGPU programs. We propose a software optimization, called loop merging, that aims to reduce divergence due to varying trip-count of a loop across warp threads. This optimization merges the divergent loop with one or more outer surrounding loops into one loop. In this way, warp threads do not have to wait for each other in each outer loop iteration, thus improving execution efficiency. We implement loop merging in LLVM. Our evaluation on a Fermi GPU shows that it improves the performance of a synthetic benchmark and five application benchmarks by up to 1.6X and 4.3X respectively.\n",
      "\n",
      "2. id: 5390aa7620f70186a0eac1d9   score: 0.9114953   abstract: Because of their tremendous computing power and remarkable cost efficiency, GPUs (graphic processing unit) have quickly emerged as a kind of influential platform for high performance computing. However, as GPUs are designed for massive data-parallel computing, their performance is subject to the presence of condition statements in a GPU application. On a conditional branch where threads diverge in which path to take, the threads taking different paths have to run serially. Such divergences often cause serious performance degradations, impairing the adoption of GPU for many applications that contain non-trivial branches or certain types of loops. This paper presents a systematic investigation in the employment of runtime thread-data remapping for solving that problem. It introduces an abstract form of GPU applications, based on which, it describes the use of reference redirection and data\n",
      "\n",
      "3. id: 5390b44620f70186a0ef7fb3   score: 0.8897206   abstract: Due to their massive computational power, graphics processing units (GPUs) have become a popular platform for executing general purpose parallel applications. GPU programming models allow the programmer to create thousands of threads, each executing the same computing kernel. GPUs exploit this parallelism in two ways. First, threads are grouped into fixed-size SIMD batches known as warps, and second, many such warps are concurrently executed on a single GPU core. Despite these techniques, the computational resources on GPU cores are still underutilized, resulting in performance far short of what could be delivered. Two reasons for this are conditional branch instructions and stalls due to long latency operations. To improve GPU performance, computational resources must be more effectively utilized. To accomplish this, we propose two independent ideas: the large warp microarchitecture and\n",
      "\n",
      "4. id: 53909fbd20f70186a0e42de3   score: 0.87212974   abstract: Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in commodity desktop computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high perfor- mance with minimal overhead incurred by control hard- ware. Scalar threads are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set archi- tecture and programs using these instructions may experi- ence reduced performance due to the way branch execution is supported by hardware. One approach is to add a stack to allow different SIMD processing elements to execute dis- tinct program paths after a branch instruction. The occur- rence of diverging branch outcomes for different proc\n",
      "\n",
      "5. id: 558b103c612c41e6b9d41ec5   score: 0.8659059   abstract: General-purpose computing on graphics processing unit (GPGPU) architectures rely on data locality and regular computation to leverage parallel resources to achieve performance benefits over multi-core systems. Current GPUs often cannot effectively accommodate irregular algorithms and non-uniform memory accesses. Irregular execution corrupts the efficiency of the rigid GPU groups of threads that lead to idle execution execution and workload disparity in the GPU architecture. While hardware aspects of branch divergence, local memory bank conflicts, and non-coalesced memory accesses cause workload disparity, the greater issue is the lost of execution potential from other GPU workgroups. This paper sets out to characterize the workload disparity and toexplore data-driven models for executing irregular computations on GPU architectures.\n",
      "\n",
      "6. id: 558b3c00612c41e6b9d474f1   score: 0.85729086   abstract: General purpose computing using graphics processing units (GPGPUs) is an attractive option to achieve power efficient throughput computing. But the power efficiency of GPGPUs can be significantly curtailed in the presence of divergence. This paper evaluates two important facets of this problem. First, we study the branch divergence behavior of various GPGPU workloads. We show that only a few branch divergence patterns are dominant in most workloads. In fact only five branch divergence patterns account for 60% of all the divergent instructions in our workloads. In the second part of this work we exploit this branch divergence pattern bias to propose a new divergence pattern aware warp scheduler, called PATS. PATS prioritizes scheduling warps with the same divergence pattern so as to create long idleness windows for any given execution lane. The long idleness windows are then exploited for\n",
      "\n",
      "7. id: 5390af8820f70186a0ecefc2   score: 0.85269034   abstract: Branch divergence has a significant impact on the performance of GPU programs. We propose two novel software-based optimizations, called iteration delaying and branch distribution that aim to reduce branch divergence. Iteration delaying targets a divergent branch enclosed by a loop within a kernel. It improves performance by executing loop iterations that take the same branch direction and delaying those that take the other direction until later iterations. Branch distribution reduces the length of divergent code by factoring out structurally similar code from the branch paths. We conduct a preliminary evaluation of the two optimizations using both synthetic benchmarks and a highly-optimized real-world application. Our evaluation shows that they improve the performance of the synthetic benchmarks by as much as 30% and 80% respectively, and that of the real-world application by 12% and 16\n",
      "\n",
      "8. id: 558b498d612c41e6b9d48428   score: 0.7550956   abstract: Hardware parallelism should be exploited to improve the performance of computing systems. Single instruction multiple data (SIMD) architecture has been widely used to maximize the throughput of computing systems by exploiting hardware parallelism. Unfortunately, branch divergence due to branch instructions causes underutilization of computational resources, resulting in performance degradation of SIMD architecture. Graphics processing unit (GPU) is a representative parallel architecture based on SIMD architecture. In recent computing systems, GPUs can process general-purpose applications as well as graphics applications with the help of convenient APIs. However, contrary to graphics applications, general-purpose applications include many branch instructions, resulting in serious performance degradation of GPU due to branch divergence. In this paper, we propose concurrent warp execution (\n",
      "\n",
      "9. id: 5390a06e20f70186a0e4c336   score: 0.73354733   abstract: GPUs are a class of specialized parallel architectures with tremendous computational power. The new Compute Unified Device Architecture (CUDA) programming model from NVIDIA facilitates programming of general purpose applications on their GPUs. However, manual development of high-performance parallel code for GPUs is still very challenging. In this paper, a number of issues are addressed towards the goal of developing a compiler framework for automatic parallelization and performance optimization of affine loop nests on GPGPUs: 1) approach to program transformation for efficient data access from GPU global memory, using a polyhedral compiler model of data dependence abstraction and program transformation; 2) determination of optimal padding factors for conflict-minimal data access from GPU shared memory; and 3) model-driven empirical search to determine optimal parameters for unrolling an\n",
      "\n",
      "10. id: 5390adfc20f70186a0ec4d3b   score: 0.6914973   abstract: The increasing programability and the high computational power of Graphical Processing Units (GPU) make them attractive to general purpose programming. However, taking full bene t of this execution environment is a challenging task. One of these challenges stem from divergences, a phenomenon that occurs when threads that execute in lock-step are forced to take di erent program paths due to branches in the code. In face of divergences, some threads will have to wait, idly, while their diverging siblings execute. Optimizing the code to avoid divergences is diffcult, because this task demands a deep understanding of programs that might be large and convoluted. In order to facilitate the detection of divergences, this paper introduces the divergence map, a data structure that indicates the location and the volume of divergences in a program. We build this map via dynamic profiling techniques\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1728641\n",
      "index                                        55323d6c45cec66b6f9de468\n",
      "title               An emotion detection system based on multi lea...\n",
      "authors                       Divya Tomar, Divya Ojha, Sonali Agarwal\n",
      "year                                                           2015.0\n",
      "venue                             Advances in Artificial Intelligence\n",
      "references          559257ad0cf28b1a968ffcec;55323add45cec66b6f9d8...\n",
      "abstract            Posttraumatic stress disorder (PTSD), bipolar ...\n",
      "id                                                            1728641\n",
      "clustered_labels                                                    2\n",
      "Name: 1728641, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac5720f70186a0eb4dd3   score: 0.7446563   abstract: In this paper, we present a comparative analysisof three classifiers for speech signal emotion recognition.Recognition was performed on emotional Berlin Database.This work focuses on speaker and utterance (phrase)dependent and independent framework. One hundred thirtythree (133) sound/speech features were extracted from Pitch,Mel Frequency Cepstral Coefficients, Energy and Formantsand were evaluated in order to create a feature set sufficient todiscriminate between seven emotions in acted speech. A set of26 features was selected by statistical method and MultilayerPercepton, Probabilistic Neural Networks and Support VectorMachine were used for the Emotion Classification at sevenclasses: anger, happiness, anxiety/fear, sadness, boredom,disgust and neutral. In speaker dependent framework,Probabilistic Neural Network classifier reached very highaccuracy of 94%, whereas in speaker independen\n",
      "\n",
      "2. id: 5390bb7b20f70186a0f40fcf   score: 0.73866904   abstract: The success of suicide prevention, a major public health concern worldwide, hinges on adequate suicide risk assessment. Online platforms are increasingly used for expressing suicidal thoughts, but manual monitoring is unfeasible given the information overload experts are confronted with. We investigate whether the recent advances in natural language processing, and more specifically in sentiment mining, can be used to accurately pinpoint 15 different emotions, which might be indicative of suicidal behavior. A system for automatic emotion detection was built using binary support vector machine classifiers. We hypothesized that lexical and semantic features could be an adequate way to represent the data, as emotions seemed to be lexicalized consistently. The optimal feature combination for each of the different emotions was determined using bootstrap resampling. Spelling correction was app\n",
      "\n",
      "3. id: 53908b2120f70186a0db6d7b   score: 0.6200466   abstract: This paper describes an experimental study on the detection of emotion from speech. As computer-based characters such as avatars and virtual chat faces become more common, the use of emotion to drive the expression of the virtual characters becomes more important. This study utilizes a corpus containing emotional speech with 721 short utterances expressing four emotions: anger, happiness, sadness, and the neutral (unemotional) state, which were captured manually from movies and teleplays. We introduce a new concept to evaluate emotions in speech. Emotions are so complex that most speech sentences cannot be precisely assigned to a particular emotion category; however, most emotional states nevertheless can be described as a mixture of multiple emotions. Based on this concept we have trained SVMs (support vector machines) to recognize utterances within these four categories and developed a\n",
      "\n",
      "4. id: 53909e8b20f70186a0e2ebc3   score: 0.6020031   abstract: In this study, the robustness of approaches to the automatic classification of emotions in speech is addressed. Among the many types of emotions that exist, two groups of emotions are considered, adult-to-adult acted vocal expressions of common types of emotions like happiness, sadness, and anger and adult-to-infant vocal expressions of affective intents also known as ''motherese''. Specifically, we estimate the generalization capability of two feature extraction approaches, the approach developed for Sony's robotic dog AIBO (AIBO) and the segment-based approach (SBA) of [Shami, M., Kamel, M., 2005. Segment-based approach to the recognition of emotions in speech. In: IEEE Conf. on Multimedia and Expo (ICME05), Amsterdam, The Netherlands]. Three machine learning approaches are considered, K-nearest neighbors (KNN), Support vector machines (SVM) and Ada-boosted decision trees and four emot\n",
      "\n",
      "5. id: 5390b0ca20f70186a0eda1bb   score: 0.59643316   abstract: In this paper, we adopt a supervised machine learning approach to recognize six basic emotions (anger, disgust, fear, happiness, sadness and surprise) using a heterogeneous emotion-annotated dataset which combines news headlines, fairy tales and blogs. For this purpose, different features sets, such as bags of words, and N-grams, were used. The Support Vector Machines classifier (SVM) performed significantly better than other classifiers, and it generalized well on unseen examples.\n",
      "\n",
      "6. id: 5390a1bc20f70186a0e55a22   score: 0.48402944   abstract: This article is dedicated to Real-life emotion detection using a corpus of real agent-client spoken dialogs from a medical emergency call center. Emotion annotations have been done by two experts with twenty verbal classes organized in eight macro-classes. Two studies are reported in this paper with the four macro classes: Relief, Anger, Fear and Sadness: the first investigates automatic emotion detection using linguistic information whith a detection score of about 78% and a very good detection of Relief, whereas the second investigates emotion detection with paralinguistic cues with 60% of good detection, Fear being best detected.\n",
      "\n",
      "7. id: 5390b19020f70186a0edf569   score: 0.46626836   abstract: The HCI community is actively seeking novel methodologies to gain insight into the user's experience during interaction with both the application and the content. We propose an emotional recognition engine capable of automatically recognizing a set of human emotional states using psychophysiological measures of the autonomous nervous system, including galvanic skin response, respiration, and heart rate. A novel pattern recognition system, based on discriminant analysis and support vector machine classifiers is trained using movies' scenes selected to induce emotions ranging from the positive to the negative valence dimension, including happiness, anger, disgust, sadness, and fear. In this paper we introduce an emotion recognition system and evaluate its accuracy by presenting the results of an experiment conducted with three physiologic sensors.\n",
      "\n",
      "8. id: 5390bed320f70186a0f4f6db   score: 0.41679665   abstract: Here we propose an approach for developing a diagnosis system for mood disorders, such as depression and bipolar disorder, based on language analysis from speech and text. Our system is based on the Mood State Indicator algorithm (MSI) for real-time analysis of a patient's mental state. MSI is designed to give a quantitative measure of cognitive state based on axiological values and time orientation of lexical features. MSI's multi-layered analytic engine consists of multiple information processing modules to systematically retrieve, parse and process features of a patient's discourse. Gold standard clinical criteria will be used to match language analysis indicators to mood disorder diagnosis.\n",
      "\n",
      "9. id: 5390a1d420f70186a0e56b0c   score: 0.35734323   abstract: In the context of affective computing, a significant trend in multi modal human-computer interaction is focused to determine emotional status of the users. For a constructive and natural human-computer interaction, the computers should be able to adapt to the user's emotional state and respond appropriately. This work proposes few simple and robust features in the framework of determining emotions from speech. Our approach is suitable for voice based applications, such as call centers or interactive voice systems, which are dependent on telephone conversations. For a typical call center application, it is crucial to recognize and classify agitation (anger, happiness, fear, and disgust) and calm (neutral, sadness, and boredom) callers, for the systems to respond appropriately. For instance, in a typical voice based application, the system should be able to either apologize or appreciate t\n",
      "\n",
      "10. id: 558bce10612cf642427588ec   score: 0.31226513   abstract: There is an enormous number of potential applications of the system which is capable to recognize human emotions. Such opportunity can be useful in various applications, e.g., improvement of Spoken Dialogue Systems (SDSs) or monitoring agents in call-centers. Therefore, the Emotion Recognition In The Wild Challenge 2014 (EmotiW 2014) is focused on estimating emotions in real-world situations. This study presents the results of multimodal emotion recognition based on support vector classifier. The described approach results in 41.77% of overall classification accuracy in the multimodal case. The obtained result is more than 17% higher than the baseline result for multimodal approach.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707931\n",
      "index                                        55323b9345cec66b6f9da320\n",
      "title               High-performance hybrid CPU and GPU parallel a...\n",
      "authors                     Mark Gates, Michael T Heath, John Lambros\n",
      "year                                                           2015.0\n",
      "venue               International Journal of High Performance Comp...\n",
      "references          5390a25820f70186a0e60030;539087fe20f70186a0d73...\n",
      "abstract            We present a hybrid Message Passing Interface ...\n",
      "id                                                            1707931\n",
      "clustered_labels                                                    1\n",
      "Name: 1707931, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bd1520f70186a0f44fad   score: 0.7772999   abstract: We develop speed, efficiency, and accuracy improvements to a three-dimensional (3D) digital volume correlation (DVC) algorithm, which measures displacement and strain fields throughout the interior of a material. Our goal is to perform DVC with resolution comparable to that achieved in 2D digital image correlation, in time that is commensurate with the image acquisition time. This represents a significant improvement over the current state-of-the-art available in the literature. Using an X-ray micro-CT scanner, we can resolve features at the 5 micron scale, generating 3D images with up to 36 billion voxels. We utilize linear and quadratic shape functions with tricubic spline interpolation to achieve high accuracy. We improve the algorithm’s speed and robustness through an improved coarse search, efficient implementation of spline interpolation, and using smoothing splines to address nois\n",
      "\n",
      "2. id: 5390bfa220f70186a0f5410b   score: 0.7189892   abstract: In this study a robust strategy for 3D-Volume Digital Image Correlation DIC is presented, apt to provide accurate kinematic measurements within a loaded sample on the basis of three-dimensional digital images by X-ray computed micro-tomography. In the framework of a Galerkin, finite element discretization of the displacement field, the inverse problem of estimating 3D motion inside the bulk material is solved recursively on a hierarchical family of grids, linked by suitable restriction and prolongation operators. Such structured grids are defined over an image pyramid, which is generated starting from the raw tomographic reconstructions by a reiterated application of average filters and sub-sampling operators. To achieve robust estimates of the underlying displacement fields, multi-grid cycles are performed ascending and descending along the pyramid in a selected sequence. At each scale,\n",
      "\n",
      "3. id: 53908e0020f70186a0dd47c5   score: 0.60929173   abstract: Abstract: Three dimensional computed tomography is a computationally intensive procedure, requiring large amounts of RAM and processing power. Parallel methods for two dimensional computed tomography have been studied, but not extended or applied to 3D cone beam tomography. A discussion on how the 3D cone beam tomography problem can be implemented in parallel using MPI is given. We show an improvement in performance from 58.2% to 81.8% processor utilization in a heterogeneous cluster of workstations by load balancing. A 96.8% efficiency was seen using a 2 processor SMP.\n",
      "\n",
      "4. id: 5390a54620f70186a0e7860c   score: 0.43980747   abstract: With recent developments in the field of high speed image acquisition systems, especially increase in speed and image quality, and with concurrent enhancement of 2D and 3D Digital Image Correlation Method (DICM) capabilities, it is now possible to measure surface shape and deformations in a wide range of applications, including biomechanics, fracture mechanics, non-destructive evaluation and material property measurements. The method provides the ability to measure displacements with sub-pixel accuracy from digital images by accurately matching subsets from images obtained from the undeformed and deformed states. Up until recently, DIC has been limited to mostly quasi-static and moderately slow strain rate applications. In this work, an effort has been made to characterize and calibrate ultra high-speed image acquisition systems using (a) a single camera to acquire 2D surface deformation\n",
      "\n",
      "5. id: 553bcbf20cf276b98ef215c3   score: 0.37536293   abstract: Reconstruction of displacement and strain fields in geomechanical structures from surface images is a challenging task. Digital Image Correlation (DIC) is a well known technique to achieve these tasks if deformation is continuous but it fails in the presence of discontinuities. This paper investigates the application of the DIC technique to displacement and strain field reconstruction in the presence of discontinuities, and presents a post-processing algorithm that leverages the convergence results in DIC to reconstruct displacement and strain fields around discontinuities with high accuracy. The proposed algorithm uses the results obtained from DIC and concentrates on the area where DIC fails. Pattern matching is conducted on the area around the discontinuities and associated displacement is found for each pixel. The proposed algorithm is tested using two different discontinuity scenari\n",
      "\n",
      "6. id: 5390b1d220f70186a0ee3209   score: 0.15495056   abstract: This paper presents a parallel multi-resolution volume rendering algorithm based on graphics processing unit (GPU). The algorithm is based on several important criteria, rendering is done adaptively by selecting high-resolution cells close to a center of attention and low-resolution cells away from this area. We employ the 3D texture mapping capability commonly available in modern GPU as a core rendering engine and take advantage of visibility tests and adaptive level of detail (LOD) selection to accelerate the whole rendering process. Experimental results show that our method achieve real-time performance for very large seismic data (10GB) on standard PC hardware.\n",
      "\n",
      "7. id: 5390a7f620f70186a0e950ea   score: 0.13892843   abstract: The biomedical imaging chain is continuously being challenged to reconstruct, analyze, and visualize increasing amounts of data in shorter amounts of time. Parallel computing on multi-core devices and clustered computers has allowed for continued innovation of compute and processing technologies but not without facing serious constraints of cost, space, and power consumption. Over the last three years the graphics processing unit (GPU) and its increased programmability has played an integral role in defining a new dimension to parallel computing with its single chip, many-core architecture as well as evolving the graphics pipeline to enhance visualization techniques. Image reconstruction, segmentation and registration algorithms architected to take advantage of the GPU parallel architecture not only realize massive processing speedups but also set the stage for scalability. High resoluti\n",
      "\n",
      "8. id: 5390a4cc20f70186a0e75a7d   score: 0.114368536   abstract: Digital Image Correlation (DIC) is a powerful technique to provide full-field displacement measurements for mechanical tests of materials and structures. The displacement fields may be further processed as an entry for identification procedures giving access to parameters of constitutive laws. A new implementation of a Finite Element based Integrated Digital Image Correlation (I-DIC) method is presented, where the two stages (image correlation and mechanical identification) are coupled. This coupling allows one to minimize information losses, even in case of low signal-to-noise ratios. A case study for elastic properties of a composite material illustrates the approach, and highlights the accuracy of the results. Implementations on GPUs (using CUDA) leads to high speed performance while preserving the versatility of the methodology.\n",
      "\n",
      "9. id: 5390bae520f70186a0f3ac68   score: 0.08093671   abstract: While much work has been done on applying GPU technology to computed tomography (CT) reconstruction algorithms, many of these implementations focus on smaller datasets that are better suited for medical applications. This paper proposes an irregular approach to the algorithm design which utilizes the GPU hardware's unique cache structure and employs small x-ray image data prefetches on the host to upload to the GPUs while the devices are operating on large contiguous subvolumes of the reconstruction. This approach will improve the overall cache hit-rates and thus improve the performance of the massively multithreaded environment of the GPU. Overall, utilizing small prefetches of x-ray image data improved the volumetric pixel (voxel) processing rate when compared to utilizing large data prefetches which would minimize data transfers and kernel launches. Additionally, this approach does no\n",
      "\n",
      "10. id: 5390a5b020f70186a0e7da3e   score: 0.05531065   abstract: Image Correlation for Shape, Motion and Deformation Measurements provides a comprehensive overview of data extraction through image analysis. Readers will find and in-depth look into various single- and multi-camera models (2D-DIC and 3D-DIC), two- and three-dimensional computer vision, and volumetric digital image correlation (VDIC). Fundamentals of accurate image matching are described, along with presentations of both new methods for quantitative error estimates in correlation-based motion measurements, and the effect of out-of-plane motion on 2D measurements. Thorough appendices offer descriptions of continuum mechanics formulations, methods for local surface strain estimation and non-linear optimization, as well as terminology in statistics and probability. With equal treatment of computer vision fundamentals and techniques for practical applications, this volume is both a reference\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1650973\n",
      "index                                        559172530cf2e89307ca9de2\n",
      "title               Connected Through Crisis: Emotional Proximity ...\n",
      "authors             Y. Linlin Huang, Kate Starbird, Mania Orand, S...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390893e20f70186a0d93648;5390881d20f70186a0d82...\n",
      "abstract            During crises, the ability to access relevant ...\n",
      "id                                                            1650973\n",
      "clustered_labels                                                    3\n",
      "Name: 1650973, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390baa120f70186a0f3964e   score: 0.97800976   abstract: In this paper, we examined information sharing behavior in social media when one was taking the perspective of self versus other. We found that imagining self in a disaster center, Fukushima, Japan, increased the likelihood of sharing crisis information relative to imagining another person, John, in the same place. People's intention to share crisis information by default, without being asked to take any perspective, paralleled the intention to share when taking another person's perspective. Moreover, when the information was associated with negative feelings, such as worry or fear, it was more likely to be shared, when the information was perceived confusing or uninteresting, it was less likely to be shared.\n",
      "\n",
      "2. id: 5390b78a20f70186a0f24ed7   score: 0.876423   abstract: The use of social media for communication and interaction is becoming more and more frequent, which is also the case during crises. To monitor social media may therefore be a useful capability from a crisis management perspective, both for detecting new or emergent crises, as well as for getting a better situation awareness of how people react to a particular crisis. The work presented in this paper is part of the EU research project Alert4All, having the overall goal of improving the effectiveness of alert and communication toward the population in crises.\n",
      "\n",
      "3. id: 558b5dec612caa4dd659f096   score: 0.8632764   abstract: People share information with their peers using social media services (e.g. sharing their latest news over Facebook or Twitter) in order to inform the peers about their current situation. This has become a huge part of our social life. During crises this behaviour becomes even more acute because it allows people to reassure their peers (followers and friends) of their well being expeditiously. Of late, social media services have been also used for another purpose during crises: that of informing oneself over the current evolution of the crises. However obtaining relevant information from social media can be a difficult challenge as the bar for posting information, good or bad, is very low. Filtering the flow of messages such that only relevant information is remaining is critical in times of crises. To aid in this, we propose a spatial-temporal model that collects the data from Twitter. \n",
      "\n",
      "4. id: 5390b52620f70186a0f044f3   score: 0.84247476   abstract: Research on the use of social media during disaster events has gained attention in recent years. Prior research paid attention to warnings, response activities and the dissemination of information through social media during times of crisis. This study focuses on the use of social media during crisis from different perspectives. In this paper, the implications of symbols that emerged in YouTube videos were examined. Social media can be a valuable medium immediately after disaster events for people in crisis and for others who are indirectly affected to seek spiritual and emotional support and to reconstruct their cultural value system and identity. This study also addresses that persistence and replay are crucial attributes in the design of social media tools for users to seek emotional support and to engage with others in larger space and time.\n",
      "\n",
      "5. id: 55913ed80cf232eb904fb6fb   score: 0.73278314   abstract: Addressing crises sometimes requires grappling with sophisticated technical or scientific content. To make sense of the BP DeepWater Horizon Oil Spill people had to grapple with uncertain and sometimes contentious, complex information. This empirical study shows that an emergent, connected crowd interacted to surface, share, question and discuss these complexities. While studies have observed collective sensemaking taking place via social media in other kinds of crises, this study extends our understanding of emergent crowd work as collective sensemaking where members of the public assemble and interpret evidence on complex topics in a crisis context, perhaps performing a kind emergent citizen science.\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3d205   score: 0.7055822   abstract: Social media attract attention for sharing information, especially Twitter, which is now being used in times of disasters. In this paper, we perform regional analysis of user interactions on Twittter during the Great East Japan Earthquake and arrived at the following two conclusions:People diffused much more information after the earthquake, especially in the heavily-damaged areas; People communicated with nearby users but diffused information posted by distant users. We conclude that social media users changed their behavior to widely diffuse information.\n",
      "\n",
      "7. id: 5536867d0cf2dbb77a8169f5   score: 0.6754436   abstract: The paper explores the role of social media during large scale crises and disasters, as investigated by the COSMIC project. Based on a typology of crises and on case studies, we examine communication challenges and societal dynamics in conjunction with interactions among new media, officials and first responders, and the public. A number of technology issues associated with new media applications, such as their vast size, fast updates and semantic richness--albeit at semi-structured representations--are considered. Finally, we refer to features of the relationship between social media and the wider public, the associated ethics, risks and benefits and the potential role of citizens as first responders/volunteers, social activists and journalists/reporters.\n",
      "\n",
      "8. id: 5390bae520f70186a0f3bc83   score: 0.56427854   abstract: Social media is gaining popularity as a medium of communication before, during, and after crises. In several recent disasters, it has become evident that social media sites like Twitter and Facebook are an important source of information, and in cases they have even assisted in relief efforts. We propose a novel approach to identify a subset of active users during a crisis who can be tracked for fast access to information. Using a Twitter dataset that consists of 12.9 million tweets from 5 countries that are part of the \"Arab Spring\" movement, we show how instant information access can be achieved by user identification along two dimensions: user's location and the user's affinity towards topics of discussion. Through evaluations, we demonstrate that users selected by our approach generate more information and the quality of the information is better than that of users identified using s\n",
      "\n",
      "9. id: 5390b68720f70186a0f1d292   score: 0.5573635   abstract: The purpose of this study is to understand how microblogging communications change and contribute to collective sense-making over time during a crisis. Using B. Dervin's (1983) theory of sense-making applied to crises and communications during crises, we examined 7, 184 microblogging communications sent in response to three violent crises that occurred on U. S. college campuses. The analysis of patterns of microblogging communications found that information-sharing behaviors dominated the early response phase of violent crises, and opinion sharing increased over time, peaking in the recovery phase of the crises. The analysis of individual microblogging communications identified various themes in the conversation threads that not only helped individual contributors make sense of the situation but also helped others who followed the conversation. The results of this study show that microbl\n",
      "\n",
      "10. id: 5390ba3820f70186a0f3671a   score: 0.51350075   abstract: Social media is a powerful medium for rapidly sharing information and organizing response in times of crisis or extreme events. Twitter users have adopted a convention of hashtags to support this and other uses of microblogging services. Using Twitter data from the 2011 London riots, we analyze emergent social networks directly relating to response to crisis. We examine networks of riot response oriented around cleanup or prayer activities. These networks differ in size, structure, general membership, and prominent actors. We explore whether temporal patterns observed in social media, such as hashtag \"lifespan,\" may relate to observed social processes and behaviors.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710764\n",
      "index                                        55323be345cec66b6f9dac85\n",
      "title               Discriminative training using non-uniform crit...\n",
      "authors                                  Chao Weng, Biing-Hwang Juang\n",
      "year                                                           2015.0\n",
      "venue               IEEE/ACM Transactions on Audio, Speech and Lan...\n",
      "references          5390b52620f70186a0f020eb;5390b68720f70186a0f1d...\n",
      "abstract            In this work, we formulate the problem of keyw...\n",
      "id                                                            1710764\n",
      "clustered_labels                                                    4\n",
      "Name: 1710764, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b68720f70186a0f1d1df   score: 0.9014011   abstract: During the last decade, weighted finite-state transducers (WFSTs) have become popular in speech recognition. While their main field of application remains hidden Markov model (HMM) decoding, the WFST framework is now also seen as a brick in solutions to many other central problems in automatic speech recognition (ASR). These solutions are less known, and this work aims at giving an overview of the applications of WFSTs in large-vocabulary continuous speech recognition (LVCSR) besides HMM decoding: discriminative acoustic model training, Bayes risk decoding, and system combination. The application of the WFST framework has a big practical impact: we show how the framework helps to structure problems, to develop generic solutions, and to delegate complex computations to WFST toolkits. In this paper, we review the literature, discuss existing approaches, and provide new insights into WFST e\n",
      "\n",
      "2. id: 53909ee020f70186a0e34164   score: 0.89274967   abstract: The minimum classification error/generalized probabilistic descent (MCE/GPD) framework has been applied to several recognizer frameworks, such as hidden Markov models, prototype based systems, and systems based on artificial neural networks. However, to our knowledge, the MCE/CPD framework has not yet been applied to a working online speech recognition system in a realistic application environment. We describe the application of MCE/GPD to a telephone-based multi-speaker speech recognition system that accepts spoken Japanese names and forwards calls to any of up to 400 staff members. Points of interest include the automatic collection and labeling of new training data and the use of MCE/GPD training to improve recognizer performance.\n",
      "\n",
      "3. id: 5390b52620f70186a0f03c0d   score: 0.8578872   abstract: The minimum classification error (MCE) framework for discriminative training is a simple and general formalism for directly optimizing recognition accuracy in pattern recognition problems. The framework applies directly to the optimization of hidden Markov models (HMMs) used for speech recognition problems. However, few if any studies have reported results for the application of MCE training to large-vocabulary, continuous-speech recognition tasks. This article reports significant gains in recognition performance and model compactness as a result of discriminative training based on MCE training applied to HMMs, in the context of three challenging large-vocabulary (up to 100 k word) speech recognition tasks: the Corpus of Spontaneous Japanese lecture speech transcription task, a telephone-based name recognition task, and the MIT Jupiter telephone-based conversational weather information t\n",
      "\n",
      "4. id: 5390a9a520f70186a0ea5a4c   score: 0.8244619   abstract: The goal of keyword spotting is to detect the presence of specific spoken words in unconstrained speech. The majority of keyword spotting systems are based on generative hidden Markov models and lack discriminative capabilities. However, discriminative keyword spotting systems are currently based on frame-level posterior probabilities of sub-word units. This paper presents a discriminative keyword spotting system based on recurrent neural networks only, that uses information from long time spans to estimate word-level posterior probabilities. In a keyword spotting task on a large database of unconstrained speech the system achieved a keyword spotting accuracy of 84.5%\n",
      "\n",
      "5. id: 5390a2be20f70186a0e65e7b   score: 0.70058763   abstract: The classical Bayes decision theory [3] is the foundation of statistical pattern recognition. In [4], we have addressed the issue of non-uniform error criteria in statistical pattern recognition, and generalized the Bayes decision theory for pattern recognition tasks where errors over different classes have varying degrees of significance. We further introduced the weighted minimum classification error (MCE) method for a practical design of a statistical pattern recognition system to achieve empirical optimality when non-uniform error criteria are prescribed. However, one key issue in the weighted MCE method, the methodology of building a suitable non-uniform error cost function given the user’s requirements, has not been addressed yet. In this paper, we propose some viable techniques for the design of the non-uniform error cost function in the context of automatic speech recognition (AS\n",
      "\n",
      "6. id: 5390a55520f70186a0e7b0ee   score: 0.6897238   abstract: In this paper we show how common training criteria like for example MPE or MMI can be extended to incorporate a margin term. In addition, a transducer-based training implementation is presented, which covers a large variety of discriminative training criteria for ASR, including the standard MMI, MPE, and MCE criteria, as well as the modifications to these criteria presented here. The modified criteria are directly related with the conventional large margin formulation of SVMs. In the proposed approach, we can take advantage of the generalization guarantees of large margin classifiers while keeping the existing framework for the discriminative training, including the efficient algorithms for conventional MPE or MMI. On the conceptual side, this allows for a direct evaluation of the margin term. Finally, experimental results are presented for different large vocabulary continuous speech re\n",
      "\n",
      "7. id: 5390b5ed20f70186a0f0e107   score: 0.68836373   abstract: Over the past few years, there has been a resurgence of interest in designing high-accuracy automatic speech recognition (ASR) systems due to the key rule they can play in many real-world applications, such as voice print for biometric identification, language identification, and call-scanning. Improving current state-of-the-art technology is therefore vital for the success of those aforementioned applications, yet this is not simple with the standard technology based on hidden Markov models (HMMs) trained on short-term spectral features. This paper offers an innovative prospective on how two novel prominent approaches to ASR, namely speech attribute detection and discriminative training, can be combined into a unified framework with beneficial effects on the overall speech recognition performance. This goal is achieved by embedding phonetic feature detection into a penalized logistic re\n",
      "\n",
      "8. id: 53909f8220f70186a0e3e1e6   score: 0.68658024   abstract: Discriminative training has been a leading factor for improving automatic speech recognition (ASR) performance over the last decade. The traditional discriminative training, however, has been aimed to minimize empirical error rates on training sets, which may not be well generalized to test sets. Many attempts have been made recently to incorporate the principle of large margin (PLM) into the training of hidden Markov models (HMMs) in ASR to improve the generalization abilities. Significant error rate reduction on the test sets has been observed on both small vocabulary and large vocabulary continuous ASR tasks using large-margin discriminative training (LMDT) techniques. In this paper, we introduce the PLM, define the concept of margin in the HMMs, and survey a number of popular LMDT algorithms proposed and developed recently. Specifically, we review and compare the large-margin minimum\n",
      "\n",
      "9. id: 5390aeba20f70186a0ecaeb5   score: 0.63148063   abstract: The Prototype-Based Minimum Error Classifier (PBMEC) that we previously described achieved high recognition rates for isolated word recognition problems. However, it was left unclear how PBMEC could be trained in a continuous speech recognition task. Here we describe a straightforward application of PBMEC training to existing techniques for handling continuous speech. Furthermore, we define a new MCE/GPD loss function that can incorporate word spotting errors and other measures of symbolic distance between correct and incorrect categories. Classification consists in a time-synchronous DTW pass through a finite state machine; adaptation makes use of an A* based N-best algorithm and consists in propagating the derivative of the loss over the N best paths through the finite state machine. The key feature is that the loss function being optimized closely reflects the actual recognition perfo\n",
      "\n",
      "10. id: 53909f2c20f70186a0e37189   score: 0.62079406   abstract: The general goal of this thesis is to improve the performance of state-of-the-art statistical automatic speech recognition (ASR) operating in real time conditions. Such operating environment dictates that the system adapts to acoustic and linguistic mismatch between the development and deployment conditions. In order for this to be practically possible the system will need to cope with sparse available new training data, and perform in real time with modest computational and memory usage. Towards this goal, this investigation proceeded within the framework of statistical speech recognition and focuses on two aspects of ASR: (a) statistical model adaptation and its extension to acoustic model adaptation and (b) pronunciation adaptation. The basic strategy is to adjust the parameters of the pre-trained system, based on the information from new data, to fit characteristics of the new enviro\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707534\n",
      "index                                        5591485a0cf232eb904fb97a\n",
      "title               Embodied Technology: Unraveling Bodily Action ...\n",
      "authors             Laurens Boer, Robb Mitchell, Agnese Caglio, An...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390a25820f70186a0e5f431;5390b64020f70186a0f19...\n",
      "abstract            Interactive artifacts are normative, as they m...\n",
      "id                                                            1707534\n",
      "clustered_labels                                                    0\n",
      "Name: 1707534, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a93b20f70186a0ea0bea   score: 0.87212974   abstract: Popular practices with non-digital artifacts were explored in order to reveal qualities for design of interaction that allow for full body experiences, and engagement of a rich array of our senses and bodily capabilities for being-in and moving-in the world. For successful design of movement-based and bodily interactive artifacts, we have to include qualities that allow users to connect their actions with the artifact to the surrounding physical and social world.\n",
      "\n",
      "2. id: 5390a37f20f70186a0e6c341   score: 0.84972197   abstract: We propose a new perspective, seeing interactivity that is the immaterial part of an interactive artifact as something concretely describable and perceivable as we do with physical materials. In order to examine the validity of this proposal, we extracted a set of interactivity attributes to be used as a design language for thinking and describing interactivity in a new way, and conducted an online survey with 14 Flash prototypes representing pairs of values of 7 interactivity attributes we extracted. The result showed that all the interactivity attributes were significant, and participants experienced distinctive and meaningful emotional effects for different interactivity attributes.\n",
      "\n",
      "3. id: 55323b2945cec66b6f9d9705   score: 0.8113773   abstract: We bring forward a novel interaction model to complement the conventional models, which cannot fully describe the interactive process of wearable devices. By using this model as a method of thinking and design tool, designers can have clearer line of thought during implementation of their interaction designs.\n",
      "\n",
      "4. id: 5390b24420f70186a0ee7312   score: 0.7735587   abstract: Understanding contexts is an important challenge that is made harder for designers by the increasing speed at which contexts change. To assist designers, three types of contextual dynamism are distinguished: physical, ontological and social. To inform understanding ontological dynamism and social dynamism, \"social contraptions\" - a form of socially interactive design experimentation is proposed. This paper focuses on cryanic social contraptions in which unseen users interact through a human surrogate that they guide via radio transmissions. Observations from initial trials are reported along with a discussion of themes arising for design and an appraisal of this approach's potential as a design tool.\n",
      "\n",
      "5. id: 53909a0220f70186a0e1fb32   score: 0.5788648   abstract: We used human movement as the basis for designing a collaborative aesthetic design environment. Our intention was to promote social interaction and creative expression. We employed off-the-shelf computer vision technology. Movement became the basis for the choreography of gestures, the development of gesture recognition, and the development of imagery and visualization. We discovered that the design of clear affordances is no less important in movement-based than in mouse-based systems. Through an integrated and iterative design process, we developed a new type of affordance, the choreographic button, which integrates choreography, gesture recognition, and visual feedback. Jumping, a quick movement, and crouching, a sustained gesture, were choreographed to form a vocabulary that is personally expressive, and which also facilitates automatic recognition.How can we evaluate socially motiva\n",
      "\n",
      "6. id: 5390afc920f70186a0ed23e1   score: 0.47939703   abstract: This paper is motivated by the increasing significance of form in design and use of interactive artifacts. The objective of this paper is to conceptualize what we mean by form in the context of interaction design and HCI research and how we can approach it in regard to emerging type of digital materiality. To do this, we first examine conceptual dimensions of form in interactive artifacts through the lens of three existing perspectives with their respective focus on: material, meaning, and making. We then apply these perspectives in our analysis of specific forms of interactive artifacts. Based on this analysis, we suggest a model of four different types of forms: the cognitive, embodied, expressive, and exploratory forms. Reflecting on this model, we propose form-driven interaction design research with its epistemological and methodological implications.\n",
      "\n",
      "7. id: 5390b8d620f70186a0f2abec   score: 0.42310047   abstract: We explore social norms embedded in interaction design, how different identity roles are made relevant during a specific design project and how norm-critical efforts are made by different actors during this design process. We have studied the development of the Swedish National Youth Counselling site to illustrate how interaction design may construct meaning, norms and values in design. We present an ethnographic study, the development of the Love Animation. Examples are shown in which interaction design unintentionally discourages the purpose of the intended message which suggests that there is a need for further understanding of how the content and the interaction design relates to each other. Using Science and Technology theories, the research join the emergent critical tradition in HCI and a critical perspective on technology as a co-constructing agent is applied.\n",
      "\n",
      "8. id: 539089d320f70186a0d9b9a4   score: 0.41667795   abstract: I describe an ongoing design process for expanding a user interface involving advanced interaction techniques (marking menus and toolglasses). My goal is to investigate evaluation and reuse of design guidelines for untraditional interfaces in a participatory design process.\n",
      "\n",
      "9. id: 5390ad0620f70186a0eba097   score: 0.3414727   abstract: What is it that is makes swinging a club to hit a ball so captivating and fun that people spend their whole lives perfecting that one movement? In this paper we present how we, rather than to invent something off-line in a lab, have returned to the real world to get inspiration and studied full body movement activities with non-digital artefacts that have track records of ensnaring and hooking practitioners for a life time, golf and skateboarding. We have also looked at a new interactive movement device called the BodyBug. We explore how the skilled use of the artefacts puts people in contact with and let them experience the world in an essentially new way. We identify and present 8 design qualities for Whole Body Interaction, based on people's performances in these activities. The interdependency between user, artefact and physical environment was a primary driving forces behind rich, s\n",
      "\n",
      "10. id: 5390a37f20f70186a0e6c94c   score: 0.31679183   abstract: We describe a new application of interactive participatory performance in interaction design. Our pragmatic strategy permits us to use performance as an investigatory tool in the exploration of user behavior. By taking a holistic view of the evaluation of the interplay between the designed artifact (the performance content) and the people who interact and relate to it, we can extract insights from the performance with the intention of informing the process of designing interaction mechanisms for more conventional public interfaces.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717604\n",
      "index                                        55914c720cf232eb904fba71\n",
      "title               An energy-efficient custom architecture for th...\n",
      "authors             Leandro Fiorin, Erik Vermij, Jan Van Lunteren,...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 12th ACM International Conf...\n",
      "references          5390bb1d20f70186a0f3e34a;5390b00c20f70186a0ed4...\n",
      "abstract            The Square Kilometre Array (SKA) will be the b...\n",
      "id                                                            1717604\n",
      "clustered_labels                                                    3\n",
      "Name: 1717604, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b1d220f70186a0ee1e86   score: 0.8978745   abstract: The U.S. design concept for the Square Kilometre Array (SKA) program is based on utilizing a large number of small-diameter dish antennas in the 12 to 15 meter diameter range. 12The Technology Development Project (TDP) is planning to design and build the first of these antennas to provide a demonstration of the technology and a solid base on which to estimate costs. The latest considerations for selecting both the optics and feed design are presented.\n",
      "\n",
      "2. id: 558b0013612c41e6b9d3ffa8   score: 0.7577944   abstract: In the quest to operationalise the next-generation infrastructure that pioneers advancement in state-of-the-art, as regards synthesis array radio telescopes of global repute, the square kilometre array (SKA) assumes significance as the. The science case for the SKA stems from addressing some of the fundamental mysteries in physics at the micro- and macrocosm levels, in the form of Key Science Projects (KSP)s. SKA design and evaluation tasks over novel challenges to the study of array configurations in the spatial. In this paper, the various instrument design specifications are discussed for relative merits and adaptability for the SKA, by invoking well-founded and established array-design and optimization principles offered on a dedicated user-friendly software platform.\n",
      "\n",
      "3. id: 55323b9245cec66b6f9da305   score: 0.52880716   abstract: The Square Kilometre Array (SKA) will be the most sensitive radio telescope in the world. This unprecedented sensitivity will be achieved by combining and analyzing signals from 262,144 antennas and 350 dishes at a raw datarate of petabits per second. The processing pipeline to create useful astronomical data will require hundreds of peta-operations per second, at a very limited power budget. We analyze the compute, memory and bandwidth requirements for the key algorithms used in the SKA. By studying their implementation on existing platforms, we show that most algorithms have properties that map inefficiently on current hardware, such as a low compute-bandwidth ratio and complex arithmetic. In addition, we estimate the power breakdown on CPUs and GPUs, analyze the cache behavior on CPUs, and discuss possible improvements. This work is complemented with an analysis of supercomputer trend\n",
      "\n",
      "4. id: 554505670cf21e970c06b342   score: 0.40344933   abstract: The Square Kilometre Array SKA will be the most sensitive radio telescope in the world. This unprecedented sensitivity will be achieved by combining and analyzing signals from 262,144 antennas and 350 dishes at a raw datarate of petabits per second. The processing pipeline to create useful astronomical data will require exa-operations per second, at a very limited power budget. We analyze the compute, memory and bandwidth requirements for the key algorithms used in the SKA. By studying their implementation on existing platforms, we show that most algorithms have properties that map inefficiently on current hardware, such as a low compute-bandwidth ratio and complex arithmetic. In addition, we estimate the power breakdown on CPUs and GPUs, analyze the cache behavior on CPUs, and discuss possible improvements. This work is complemented with an analysis of supercomputer trends, which demons\n",
      "\n",
      "5. id: 5390b1d220f70186a0ee1df9   score: 0.14426938   abstract: Taking into consideration space applications' demands for higher bandwidth and processing power, we propose to efficiently apply upcoming many-core processors and other Commercial-Off-The-Shelf (COTS) products to improve the on-board processing power.1 2 A combination of traditional hardware and software-implemented fault-tolerant techniques, addresses the reliability of the system. We first describe common requirements and design challenges presented by future space applications like the High Resolution Wide Swath Synthetic Aperture Radar (HRWS SAR). After proposing the High Performance Computing (HPC) architecture, we compare between most suitable hardware technologies and give some rough performance estimations based on their features. For benchmarking purposes we have manually converted the Scalable Synthetic Compact Application (SSCA#3) from Matlab to C and parallelized it using Ope\n",
      "\n",
      "6. id: 5390a45520f70186a0e71a2a   score: 0.11536139   abstract: A recent development in radio astronomy is to replace traditional dishes with many small antennas. The signals are combined to form one large, virtual telescope. The enormous data streams are cross-correlated to filter out noise. This is especially challenging, since the computational demands grow quadratically with the number of data streams. Moreover, the correlator is not only computationally intensive, but also very I/O intensive. The LOFAR telescope, for instance, will produce over 100 terabytes per day. The future SKA telescope will even require in the order of exaflops, and petabits/s of I/O. A recent trend is to correlate in software instead of dedicated hardware. This is done to increase flexibility and to reduce development efforts. Examples include e-VLBI and LOFAR. In this paper, we evaluate the correlator algorithm on multi-core CPUs and many-core architectures, such as NVID\n",
      "\n",
      "7. id: 5390b60d20f70186a0f1316c   score: 0.087720014   abstract: Next generation radio telescopes will require tremendous amounts of compute power. With the current state of the art, the Square Kilometer Array (SKA), currently entering its pre-construction phase, will require in excess of one ExaFlop/s in order to process and reduce the massive amount of data generated by the sensors. The nature of the processing involved means that conventional high performance computing (HPC) platforms are not ideally suited. Consequently, the SKA project requires active and intensive involvement from both the high performance computing research community, as well as industry, in order to make sure a suitable system is available when the telescope is built. In this paper we present a first analysis of the processing required, and a tool that will facilitate future analysis and external involvement.\n",
      "\n",
      "8. id: 5390b72e20f70186a0f21f45   score: 0.085403666   abstract: Traditional radio telescopes use large steel dishes to observe radio sources. The largest radio telescope in the world, LOFAR, uses tens of thousands of fixed, omni-directional antennas instead, a novel design that promises ground-breaking research in astronomy. Where traditional tele-scopes use custom-built hardware, LOFAR uses software to do signal processing in real time. This leads to an instrument that is inherently more flexible. However, the enormous data rates and processing requirements (tens to hundreds of teraflops) make this extremely challenging. The next-generation telescope, the SKA, will require exa flops. Unlike traditional instruments, LOFAR and SKA can observe in hundreds of directions simultaneously, using beam forming. This is useful, for example, to search the sky for pulsars (i.e. rapidly rotating highly magnetized neutron stars). Beam forming is an important techn\n",
      "\n",
      "9. id: 5390a25820f70186a0e5f6b4   score: 0.049865905   abstract: Array-based, direct-sampling radio telescopes have computational and communication requirements unsuited to conventional computer and cluster architectures. Synchronization must be strictly maintained across a large number of parallel data streams, from A/D conversion, through operations such as beamforming, to dataset recording. FPGAs supporting multigigabit serial I/O are ideally suited to this application. We describe a recently-constructed radio telescope called ETA having all-sky observing capability for detecting low frequency pulses from transient events such as gamma ray bursts and primordial black hole explosions. Signals from 24 dipole antennas are processed by a tiered arrangement of 28 commercial FPGA boards and 4 PCs with FPGA-based data acquisition cards, connected with custom I/O adapter boards supporting InfiniBand and LVDS physical links. ETA is designed for unattended o\n",
      "\n",
      "10. id: 5390a7f520f70186a0e939ba   score: 0.041773878   abstract: LOFAR is the first of a new generation of radio telescopes.Rather than using expensive dishes, it forms a distributed sensor network that combines the signals from many thousands of simple antennas. Its revolutionary design allows observations in a frequency range that has hardly been studied before. Another novel feature of LOFAR is the elaborate use of software to process data, where traditional telescopes use customized hardware. This dramatically increases flexibility and substantially reduces costs, but the high processing and bandwidth requirements compel the use of a supercomputer. The antenna signals are centrally combined, filtered, optionally beam-formed, and correlated by an IBM Blue Gene/P. This paper describes the implementation of the so-called correlator. To meet the real-time requirements, the application is highly optimized, and reaches exceptionally high computational a\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710646\n",
      "index                                        55323be345cec66b6f9dac7e\n",
      "title               Unsegmented dialogue act annotation and decodi...\n",
      "authors             Carlos D. Martínez-Hinarejos, José-Miguel Bene...\n",
      "year                                                           2015.0\n",
      "venue               IEEE/ACM Transactions on Audio, Speech and Lan...\n",
      "references          558fedeb612c29c89cd7c519;53909a0220f70186a0e1f...\n",
      "abstract            Most studies on dialogue corpora, as well as m...\n",
      "id                                                            1710646\n",
      "clustered_labels                                                    4\n",
      "Name: 1710646, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390994d20f70186a0e129c3   score: 0.59690315   abstract: While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our re\n",
      "\n",
      "2. id: 5390b8d720f70186a0f2c790   score: 0.59372705   abstract: This book constitutes the refereed proceedings of the 15th International Conference on Text, Speech and Dialogue, TSD 2012, held in Brno, Czech Republic, in September 2012. The 82 papers presented together with 2 invited talks were carefully reviewed and selected from 173 submissions. The papers are organized in topical sections on corpora and language resources, speech recognition, tagging, classification and parsing of text and speech, speech and spoken language generation, semantic processing of text and speech, integrating applications of text and speech processing, machine translation, automatic dialogue systems, multimodal techniques and modeling.\n",
      "\n",
      "3. id: 5390b04120f70186a0ed6e21   score: 0.5714079   abstract: This paper presents an update semantic for dialogue acts, defined in terms of combinations of very simple 'elementary update functions'. This approach allows fine-grained distinctions to be made between related types of dialogue acts, and relations like entailment and exclusion between dialogue acts to be established. The approach is applied to dialogue act representations as defined in the Dialogue Act Markup Language (DiAML), part of the recently proposed ISO standard 24617-2 for dialogue act annotation.\n",
      "\n",
      "4. id: 5390962020f70186a0df3b6a   score: 0.5249579   abstract: The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multitutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the firs\n",
      "\n",
      "5. id: 5390a63c20f70186a0e81281   score: 0.5200546   abstract: In this paper, we present an integrated model of the two central tasks of dialog management: interpreting user actions and generating system actions. We model the interpretation task as a classification problem and the generation task as a prediction problem. These two tasks are interleaved in an incremental parsing-based dialog model. We compare three alternative parsing methods for this dialog model using a corpus of human-human spoken dialog from a catalog ordering domain that has been annotated for dialog acts and task/subtask information. We contrast the amount of context provided by each method and its impact on performance.\n",
      "\n",
      "6. id: 5390979920f70186a0e00644   score: 0.51581806   abstract: We propose a statistical dialogue analysis model to determine discourse structures as well as speech acts using maximum entropy model. The model can automatically acquire probabilistic discourse knowledge from a discourse tagged corpus to resolve ambiguities. We propose the idea of tagging discourse segment boundaries to represent the structural information of discourse. Using this representation we can effectively combine speech act analysis and discourse structure analysis in one framework.\n",
      "\n",
      "7. id: 5390994d20f70186a0e12986   score: 0.49658397   abstract: We have developed a discourse level tagging tool for spoken dialogue corpus using machine learning methods. As discourse level information, we focused on dialogue act, relevance and discourse segment. In dialogue act tagging, we have implemented a transformation-based learning procedure and resulted in 70% accuracy in open test. In relevance and discourse segment tagging, we have implemented a decision-tree based learning procedure and resulted in about 75% and 72% accuracy respectively.\n",
      "\n",
      "8. id: 5390bed320f70186a0f4dc94   score: 0.45980373   abstract: This book constitutes the refereed proceedings of the 16th International Conference on Text, Speech and Dialogue, TSD 2013, held in Pilsen, Czech Republic, in September 2013. The 65 papers presented together with 5 invited talks were carefully reviewed and selected from 148 submissions. The main topics of this year's conference was corpora, texts and transcription, speech analysis, recognition and synthesis, and their intertwining within NL dialogue systems. The topics also included speech recognition, corpora and language resources, speech and spoken language generation, tagging, classification and parsing of text and speech, semantic processing of text and speech, integrating applications of text and speech processing, as well as automatic dialogue systems, and multimodal techniques and modelling.\n",
      "\n",
      "9. id: 558ad0c0612c41e6b9d3abe4   score: 0.450722   abstract: This book constitutes the refereed proceedings of the 17th International Conference on Text, Speech and Dialogue, TSD 2013, held in Brno, Czech Republic, in September 2014. The 70 papers presented together with 3 invited papers were carefully reviewed and selected from 143 submissions. They focus on topics such as corpora and language resources; speech recognition; tagging, classification and parsing of text and speech; speech and spoken language generation; semantic processing of text and speech; integrating applications of text and speech processing; automatic dialogue systems; as well as multimodal techniques and modelling.\n",
      "\n",
      "10. id: 5390a77d20f70186a0e8de1c   score: 0.34687307   abstract: To construct an expandable and adaptable dialog system which handles multiple tasks, we proposes a dialog system using a weighted finite-state transducer (WFST) in which users concept and system action tags are input and output of the transducer, respectively. To test the potential of the WFST-based dialog management (DM) platform using statistical DM models, we construct a dialog system using a human-to-human spoken dialog corpus for hotel reservation, which is annotated with Interchange Format (IF). A scenario, a Spoken Language Understanding (SLU) and a Sentence Generation (SG) WFSTs are obtained from the corpus and then composed together and optimized to generate a Dialog Management (DM) WFST. We evaluate the detection accuracy of the system next actions using Mean Reciprocal Ranking (MRR). We evaluated how WFST optimization operations contribute to dialog systems and confirmed the o\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691394\n",
      "index                                        5592551f0cf2aff368683bac\n",
      "title               Subgradient Projection Algorithms for Convex F...\n",
      "authors                                  X. M. Wang, C. Li, J. C. Yao\n",
      "year                                                           2015.0\n",
      "venue                 Journal of Optimization Theory and Applications\n",
      "references          539087d920f70186a0d6181d;5390882720f70186a0d8a...\n",
      "abstract            Under the assumption that the sectional curvat...\n",
      "id                                                            1691394\n",
      "clustered_labels                                                    1\n",
      "Name: 1691394, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b9d520f70186a0f303e4   score: 0.96210754   abstract: An algorithm for solving convex feasibility problem for a finite family of convex sets is considered. The acceleration scheme of De Pierro (em Methodos de projeção para a resolução de sistemas gerais de equações algébricas lineares. Thesis (tese de Doutoramento), Instituto de Matemática da UFRJ, Cidade Universitária, Rio de Janeiro, Brasil, 1981), which is designed for simultaneous algorithms, is used in the algorithm to speed up the fully sequential cyclic subgradient projections method. A convergence proof is presented. The advantage of using this strategy is demonstrated with some examples.\n",
      "\n",
      "2. id: 5390bda020f70186a0f46a4e   score: 0.9425068   abstract: In the present paper we study convergence of subgradient projection algorithms for solving convex feasibility problems in a Hilbert space. Our goal is to obtain an approximate solution of the problem in the presence of computational errors. We show that our subgradient projection algorithm generates a good approximate solution, if the sequence of computational errors is bounded from above by a constant.\n",
      "\n",
      "3. id: 5390a40520f70186a0e6f257   score: 0.91655886   abstract: We study subgradient projection type methods for solving non-differentiable convex minimization problems and monotone variational inequalities. The methods can be viewed as a natural extension of subgradient projection type algorithms, and are based on using non-Euclidean projection-like maps, which generate interior trajectories. The resulting algorithms are easy to implement and rely on a single projection per iteration. We prove several convergence results and establish rate of convergence estimates under various and mild assumptions on the problem’s data and the corresponding step-sizes.\n",
      "\n",
      "4. id: 5390990f20f70186a0e10439   score: 0.8934954   abstract: We study convergence properties of a modified subgradient algorithm, applied to the dual problem defined by the sharp augmented Lagrangian. The primal problem we consider is nonconvex and nondifferentiable, with equality constraints. We obtain primal and dual convergence results, as well as a condition for existence of a dual solution. Using a practical selection of the step-size parameters, we demonstrate the algorithm and its advantages on test problems, including an integer programming and an optimal control problem.\n",
      "\n",
      "5. id: 53909e7c20f70186a0e2c116   score: 0.83508044   abstract: The cyclic projections algorithm is an important method for determining a point in the intersection of a finite number of closed convex sets in a Hilbert space. That is, for determining a solution to the \"convex feasibility\" problem. We study the rate of convergence for the cyclic projections algorithm. The notion of angle between convex sets is defined, which generalizes the angle between linear subspaces. The rate of convergence results are described in terms of these angles.\n",
      "\n",
      "6. id: 5390a25820f70186a0e5f3df   score: 0.80700475   abstract: We study some methods of subgradient projections for solving a convex feasibility problem with general (not necessarily hyperplanes or half-spaces) convex sets in the inconsistent case and propose a strategy that controls the relaxation parameters in a specific self-adapting manner. This strategy leaves enough user flexibility but gives a mathematical guarantee for the algorithm's behavior in the inconsistent case. We present the numerical results of computational experiments that illustrate the computational advantage of the new method.\n",
      "\n",
      "7. id: 5390a25820f70186a0e5ff8d   score: 0.80609053   abstract: The cyclic projections algorithm is an important method for determining a point in the intersection of a finite number of closed convex sets in a Hilbert space. That is, for determining a solution to the ''convex feasibility'' problem. This is the third paper in a series on a study of the rate of convergence for the cyclic projections algorithm. In the first of these papers, we showed that the rate could be described in terms of the ''angles'' between the convex sets involved. In the second, we showed that these angles often had a more tractable formulation in terms of the ''norm'' of the product of the (nonlinear) metric projections onto related convex sets. In this paper, we show that the rate of convergence of the cyclic projections algorithm is also intimately related to the ''linear regularity property'' of Bauschke and Borwein, the ''normal property'' of Jameson (as well as Bakan, \n",
      "\n",
      "8. id: 539089ab20f70186a0d96df7   score: 0.77081   abstract: In this paper, we use a unified approach to analyze the local convergence behavior of a wide class of projection-type methods for solving variational inequality problems. Under certain conditions, it is shown that, in a finite number of iterations, either the sequence of iterates terminates at a solution of the concerned problem or all iterates enter and remain in the relative interior of the optimal face and, hence, the subproblem reduces to a simpler form.\n",
      "\n",
      "9. id: 5390962020f70186a0df46b4   score: 0.7177049   abstract: We present a unified convergence framework for approximate subgradient methods that covers various stepsize rules (including both diminishing and nonvanishing stepsizes), convergence in objective values, and convergence to a neighborhood of the optimal set. We discuss ways of ensuring the boundedness of the iterates and give efficiency estimates. Our results are extended to incremental subgradient methods for minimizing a sum of convex functions, which have recently been shown to be promising for various large-scale problems, including those arising from Lagrangian relaxation.\n",
      "\n",
      "10. id: 539099a220f70186a0e170e8   score: 0.68510735   abstract: We study the behavior of subgradient projections algorithms for the quasiconvex feasibility problem of finding a point x* ∈ Rn that satisfies the inequalities f1(x*)≤0, f2(x*)≤0,..., fm(x*)≤0, where all functions are continuous and quasiconvex. We consider the consistent case when the solution set is nonempty. Since the Fenchel-Moreau subdifferential might be empty we look at different notions of the subdifferential and determine their suitability for our problem. We also determine conditions on the functions, that are needed for convergence of our algorithms. The quasiconvex functions on the left-hand side of the inequalities need not be differentiable but have to satisfy a Lipschitz or a Hölder condition.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1716162\n",
      "index                                        55323c6045cec66b6f9dbedf\n",
      "title               A model for the classification and survey of c...\n",
      "authors                              Amulya Ratna Swain, R.C. Hansdah\n",
      "year                                                           2015.0\n",
      "venue                                                 Ad Hoc Networks\n",
      "references                                   558b84d1612c6b62e5e8a835\n",
      "abstract            Clock synchronization in wireless sensor netwo...\n",
      "id                                                            1716162\n",
      "clustered_labels                                                    3\n",
      "Name: 1716162, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b0ca20f70186a0ed971a   score: 0.990732   abstract: Clock synchronization is an extremely important requirement of wireless sensor networks(WSNs). There are many application scenarios such as weather monitoring and forecasting etc. where external clock synchronization may be required because WSN itself may consists of components which are not connected to each other. A usual approach for external clock synchronization in WSNs is to synchronize the clock of a reference node with an external source such as UTC, and the remaining nodes synchronize with the reference node using an internal clock synchronization protocol. In order to provide highly accurate time, both the offset and the drift rate of each clock with respect to reference node are estimated from time to time, and these are used for getting correct time from local clock reading. A problem with this approach is that it is difficult to estimate the offset of a clock with respect to\n",
      "\n",
      "2. id: 5390b9d520f70186a0f31868   score: 0.990732   abstract: Clock synchronisation is an important requirement for various applications in wireless sensor networks (WSNs). Most of the existing clock synchronisation protocols for WSNs use some hierarchical structure that introduces an extra overhead due to the dynamic nature of WSNs. Besides, it is difficult to integrate these clock synchronisation protocols with sleep scheduling scheme, which is a major technique to conserve energy. In this paper, we propose a fully distributed peer-to-peer based clock synchronisation protocol, named Distributed Clock Synchronisation Protocol (DCSP), using a novel technique of pullback for complete sensor networks. The pullback technique ensures that synchronisation phases of any pair of clocks always overlap. We have derived an exact expression for a bound on maximum synchronisation error in the DCSP protocol, and simulation study verifies that it is indeed less \n",
      "\n",
      "3. id: 5390ba0a20f70186a0f333f5   score: 0.98340684   abstract: Clock synchronization is an important requirement of wireless sensor networks WSNs. Synchronization is crucial to maintain data consistency, coordination, and perform fundamental operations. Many application scenarios exist where external clock synchronization may be required because WSN itself may not consist of an infrastructure for distributing the clock reference. In distributed systems the clock of a reference node is synchronized with GPS time tag or UTC as conventional external clock sources. The rest of the nodes estimate the offset and drift based on a synchronization protocol. For vast WSN, where the topology introduces propagation delay and fast drift rate of clock over sampling periods, synchronizing the WSN nodes and maintaining the synchronization is difficult. To maintain an accurate synchronization across the WSN, the authors propose a cooperative synchronization method, \n",
      "\n",
      "4. id: 5390a1f720f70186a0e5bd22   score: 0.9781772   abstract: In the wireless sensor networks (WSNs), Synchronousapproaches share the schedule information that specifiesthe cycle of active and sleep period by the control packets. On the other hand, asynchronous approaches do notexchange the synchronization information to send or receivedata. Instead, they employ preamble sampling to do that. In this paper, we compare and analyze synchronous and asynchronous MAC protocols for WSNs with our two proposed schemes (AD-MAC and AS-MAC). Our proposed schemes present an enhanced adaptive duty cycling and high low power listening algorithm for energy efficiency, respectively.The performance of those two MAC mechanisms with our proposed protocols will be investigated through simulation and analysis to design the effective MAC protocol in WSNs.\n",
      "\n",
      "5. id: 5390a1f720f70186a0e5bd4e   score: 0.97520185   abstract: Sensor nodes are small-size, low-cost embedded systems capable of local processing and RF communication. In a sensor network, nodes need to organize their operations to perform distributed sensing tasks, and need therefore to be time synchronized to a common reference. In this paper we present a time-synchronization approach for Wireless Sensor Networks (WSNs), denoted as Accuracy-Driven Synchronization Protocol (ADSP). ADSP is based on the \"always-on\" model of time synchronization, and offers customizable accuracy level, using a novel method to reduce the transmit-to-receive time delay. The time reference is provided by a master node frequently broadcasting packets that the nodes use to synchronize themselves, and then to improve their synchronization. Nodes estimate their own clock characteristics with respect to the master node, thus keeping themselves synchronized even when the maste\n",
      "\n",
      "6. id: 53909f6920f70186a0e3a443   score: 0.97409195   abstract: Wireless sensor networks consist of many nodes that collect real-world data, process them, and transmit the data by radio. Wireless sensor networks represent a new, rapidly developing direction in the field of organization of computer networks of free configuration. Sensor networks are used for monitoring a parameter field, where it is often required to fix time of an event with high accuracy. High accuracy of local clocks is also necessary for operation of network protocols (for energy-saving purposes, the nodes spend most of the time in the sleeping mode and communicate only occasionally). In the paper, base techniques used in the existing time synchronization schemes are analyzed; models of local clock behavior and models of interaction of the network devices are described; classification of the synchronization problems is presented; and a survey of the existing approaches to synchron\n",
      "\n",
      "7. id: 5390ab8820f70186a0eb160b   score: 0.9730365   abstract: Recent advances in the development of wireless sensor networks(WSNs) have considerably increased the interest in their applications for a wide range of problems such as environmental monitoring, target tracking, habitat monitoring etc. Many of these applications and the wireless sensor network (WSN) itself require that the clocks of the sensor nodes are synchronized with certain accuracy. Existing approaches to clock synchronization in WSNs are mostly hierarchical in nature. A hierarchical structure is usually difficult to maintain, and it results in longer synchronization phase and reduced synchronization accuracy for large WSNs. Traditional internal clock synchronization protocols that have been proposed for distributed systems assume that the network is complete, i.e., every node can communicate with every other node directly. But WSNs, in general, are not a complete network, and henc\n",
      "\n",
      "8. id: 5390bda020f70186a0f479af   score: 0.9697854   abstract: Wireless Sensor Networks (WSNs) are widely used in different kinds of environments. They may encounter lots of stochastic uncertainties and disturbances like message loss and node dynamics. Thus, it is critical to ensure the correctness of low level protocols in WSNs and evaluate their performance under different circumstances. In this paper, we propose a new method to analyze and evaluate WSN protocols based on stochastic timed automata and statistical model checking. For modeling, the work flow of a WSN protocol can be modeled with classical timed automata. Then, to model the uncertainties such as message loss and node dynamics, which are common in realistic circumstances, the timed automata can be extended by stochastic transitions, resulting in the stochastic timed automata. For analysis, the correctness of the protocol can be answered by classical model checking on the timed automat\n",
      "\n",
      "9. id: 5390be6620f70186a0f4cac8   score: 0.9689738   abstract: A Wireless Sensor Network WSN consists of numerous nodes gathering observations and combining these observations. Often, the timing of these observations is of importance when processing sensor data. Thus, a need for clock synchronisation arises in WSNs. The Clock Sampling Mutual Network Synchronisation CS-MNS algorithm has been proposed to fulfil this role. This paper compares simulation results and testbed results for CS-MNS. The simulations were done using Matlab, the testbed implementation was done in TinyOS 2.1, running on a mix of TelosB and MCIAz motes. The results demonstrate good qualitative agreement between simulation and experimentation in most cases. Quantitatively, the testbed results converge slower and achieve less synchronisation accuracy, however. Using the testbed, we also compare CS-MNS against FTSP, the clock synchronisation protocol provided with TinyOS 2.1. In all \n",
      "\n",
      "10. id: 5390b00c20f70186a0ed645e   score: 0.96795994   abstract: Clock synchronization is critical for Wireless Sensor Networks (WSNs) due to the need of inter-node coordination and collaborative information processing. Although many message passing protocols can achieve satisfactory clock synchronization accuracy, they incur prohibitively high overhead when the network scales to more than tens of nodes. An alternative approach is to take advantage of the global time reference induced by existing infrastructures including GPS, timekeeping radio stations, or power grid. However, high power consumption and geographic constraints present them from being widely adopted in WSNs. In this paper, we propose ROCS, a new clock synchronization approach exploiting the Radio Data System (RDS) of FM radios. First, we design a new hardware FM receiver that can extract a periodic pulse from FM broadcasts, referred to as RDS clock. We then conduct a large-scale measur\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675392\n",
      "index                                        559131220cf232eb904fb2da\n",
      "title               CoFaçade: A Customizable Assistive Approach fo...\n",
      "authors             Jason Chen Zhao, Richard C. Davis, Pin Sym Foo...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390a80f20f70186a0e9795a;539087d920f70186a0d61...\n",
      "abstract            We present CoFaçade, a novel approach to helpi...\n",
      "id                                                            1675392\n",
      "clustered_labels                                                    0\n",
      "Name: 1675392, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a4d020f70186a0e761c7   score: 0.96741027   abstract: Researchers are addressing the computing challenges of older individuals, whose needs are different---and too often disregarded.\n",
      "\n",
      "2. id: 53909a0220f70186a0e203c3   score: 0.8800775   abstract: A multidisciplinary team from industry, government, and academia developed prototype email, Web search, and navigation systems for users over 60 years old who were inexperienced in using computers and had never used the Internet. The academics encountered problems in persuading other team members of the specific challenges of designing for and working with older people. A number of ways of overcoming such challenges were implemented, and the final “radically simple” systems evaluated by a team of older people. The collaboration highlighted the conflicting pressures of the commercial world and the time and patience needed to design for older users.\n",
      "\n",
      "3. id: 5390b2d720f70186a0eed084   score: 0.80131465   abstract: There has been a steady growth in the global population of elderly people, challenging researchers in the HCI community to design technologies to help them remain independent and preserve their quality of life. One approach has been to create assistive technology solutions using Personal Digital Assistants (PDAs). However, some have questioned whether older people can use PDAs because of age related problems with dexterity, coordination, and vision. This paper presents an initial usability study that shows there are no major differences in performance between older and younger users when physically interacting with PDAs and completing conventional (e.g. pressing buttons, viewing icons, recording messages) and non-conventional tasks (e.g. scanning bar codes).\n",
      "\n",
      "4. id: 5390a6d920f70186a0e87f7c   score: 0.7704648   abstract: This paper describes a study that was conducted to learn more about how older adults use the tools in a GUI to undertake tasks in Windows applications. The objective was to gain insight into what people did and what they found most difficult. File and folder manipulation, and some aspects of formatting presented difficulties, and these were thought to be related to a lack of understanding of the task model, the correct interpretation of the visual cues presented by the interface, and the recall and translation of the task model into a suitable sequence of actions.\n",
      "\n",
      "5. id: 5390881820f70186a0d80f54   score: 0.73866904   abstract: In this panel we will present four strategies for providing computing for the elderly. We hope to generate discussion and ideas of the plusses and minuses of these strategies.\n",
      "\n",
      "6. id: 5390893e20f70186a0d93b93   score: 0.7242856   abstract: This paper presents a possible solution framework for offering advanced self-help and service to the more proficient part of the academic IT user base. This can relieve the helpdesk staff of some work load and better satisfy the more demanding and experienced user base by allowing them to find solutions on their own.\n",
      "\n",
      "7. id: 539089d320f70186a0d9b6bd   score: 0.6276089   abstract: A process is described for producing interface design and training interventions aimed at making new technologies more accessible to older adults. This method has been used to examine the usability of three computerized systems that older adults are likely to encounter. One of the three systems, automatic teller machines (ATMs), is used an as example of how the proposed intervention design and evaluation process has been successfully carried out.\n",
      "\n",
      "8. id: 5390a06e20f70186a0e4bbd1   score: 0.5723045   abstract: This installment of the Requirments column addresses how to apply user-centered design methods to design interactive systems for the elderly. A case study (with its successes and weaknesses) showed that a need exists for more creative and participatory design approaches for this population.\n",
      "\n",
      "9. id: 5390a55520f70186a0e7a1b7   score: 0.55573666   abstract: In this paper, we describe applications of computing assisted technology to home environments of elderly persons.\n",
      "\n",
      "10. id: 5390b00c20f70186a0ed54b6   score: 0.540651   abstract: We hypothesized that users show different behavioral patterns at work when using interactive products, namely execute, engage, evolve and expand. These patterns refer to task accomplishment, persistence, task modification and creation of new tasks, each contributing to the overall work goal. By developing a questionnaire measuring these behavioral patterns we were able to demonstrate that these patterns do occur at work. They are not influenced by the users alone, but primarily by the product, indicating that interactive products indeed are able to support users at work in a holistic way. Behavioral patterns thus are accounted for by the interaction of users and product.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698001\n",
      "index                                        55915bde0cf232eb904fbe7b\n",
      "title               An Evaluation of Memory Sharing Performance fo...\n",
      "authors                    Pirmin Vogel, Andrea Marongiu, Luca Benini\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 International Workshop...\n",
      "references          558acf6b612c41e6b9d3a97f;5390b48420f70186a0efb...\n",
      "abstract            Today's systems-on-chip (SoCs) more and more c...\n",
      "id                                                            1698001\n",
      "clustered_labels                                                    3\n",
      "Name: 1698001, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b5df20f70186a0f0bac5   score: 0.7953404   abstract: Heterogeneous computing that combines CPUs with an accelerator has become a popular architecture. In this paper, we explore different memory design options quantitatively and conclude that partially shared memory space is most promising option.\n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3e81c   score: 0.7668179   abstract: Future many-core processors are likely to concurrently execute a large number of diverse applications. How these applications are mapped to cores largely determines the interference between these applications in critical shared hardware resources. This paper proposes new application-to-core mapping policies to improve system performance by reducing inter-application interference in the on-chip network and memory controllers. The major new ideas of our policies are to: 1) map network-latency-sensitive applications to separate parts of the network from network-bandwidth-intensive applications such that the former can make fast progress without heavy interference from the latter, 2) map those applications that benefit more from being closer to the memory controllers close to these resources. Our evaluations show that, averaged over 128 multiprogrammed workloads of 35 different benchmarks ru\n",
      "\n",
      "3. id: 558acf6b612c41e6b9d3a97f   score: 0.72535694   abstract: Modern high-end embedded systems are designed as sophisticated systems-on-chip (SoC) composed of a virtualization-ready multi-core processor (the host) coupled to programmable manycore accelerators (PMCA). To tackle the increased programming complexity, the roadmap of several major industrial players envisions dedicated HW support to allow host and PMCA to communicate via shared virtual memory. I/O memory management units (IOMMU) and other HW are required to allow coherent virtual memory sharing. Currently no embedded heterogeneous SoC exist that provides such support, and it is unclear if the required HW will fit the tight area and energy budgets of such designs. However, providing the abstraction of a shared memory is very relevant to simplify programming of heterogeneous SoCs, as well as techniques to extend virtualization support to the manycore. We present in this work a software in\n",
      "\n",
      "4. id: 5390bded20f70186a0f4a145   score: 0.63827175   abstract: We present an open SystemC-based virtual platform that fits the design flow of heterogeneous, self-adaptive shared memory-based multicore SoCs by supporting concept validation and verification of equivalent RTL models, early software development of corresponding system drivers and efficient design space exploration at an early stage of the design. We examine connectivity, functionality and interaction among its components, such as CPU, memory and NoC, and outline innovative features related to supporting system-level monitoring via time-driven and event-based shared memory primitives. As a case-study, we consider co-simulation of shared memory-based array processing using cycle-approximate ARM Cortex-A9 processor models in the presence of application load balancing and best-effort memory bandwidth sharing, e.g. arising from a service-level agreement. Besides improving performance (10% to\n",
      "\n",
      "5. id: 5390bd1520f70186a0f450d5   score: 0.5077661   abstract: Heterogeneous multi-cores—platforms comprised of both general purpose and accelerator cores—are becoming increasingly common. Contemporary processor designs have also shown a rapid increase in the number of cores per chip. Recently, this trend has begun to include functional and performance asymmetries to balance power vs. performance requirements. Effective use of such heterogeneous resources, however, requires suitable system abstractions and methods to manage them for the dynamic and diverse workloads imposed by applications. This poses new challenges in platform resource management, which are further exacerbated by the need for runtime power budgeting and by the increased dynamics in workload behavior observed in consolidated data-center and cloud-computing systems. Hence, the first problem addressed by this dissertation, for both heterogeneous and asymmetric future chip architecture\n",
      "\n",
      "6. id: 558b951b612c6b62e5e8c132   score: 0.4567429   abstract: With an increasing number of cores per chip, it is becoming harder to guarantee optimal performance for parallel shared memory applications due to interference caused by kernel threads, interrupts, bus contention, and temperature management schemes (referred to as jitter). We demonstrate that the performance of parallel programs gets reduced (up to 35.22 percent) in large CMP based systems. In this paper, we characterize the jitter for large multi-core processors, and evaluate the loss in performance. We propose a novel jitter measurement unit that uses a distributed protocol to keep track of the number of wasted cycles. Subsequently, we try to compensate for jitter by using DVFS across a region of timing critical instructions called a frame. Additionally, we propose an OS cache that intelligently manages the OS cache lines to reduce memory interference. By performing detailed cycle accu\n",
      "\n",
      "7. id: 539095bb20f70186a0df298e   score: 0.40697983   abstract: In embedded system-on-a-chip (SoC) applications, the demand for integrating heterogeneous processors onto a single chip is increasing. An important issue in integrating multiple heterogeneous processors on the same chip is to maintain the coherence of their data caches. In this paper, we propose a hardware/software methodology to make caches coherent in heterogeneous multiprocessor platforms with shared memory. Our approach works with any combination of processors that support invalidation-based protocols. As shown in our experiments, up to 58% performance improvement can be achieved with low miss penalty at the expense of adding simple hardware, compared to a pure software solution. Speedup can be improved even further as the miss penalty increases. In addition, our approach provides embedded system programmers a transparent view of shared data, removing the burden of software synchroni\n",
      "\n",
      "8. id: 5390aeba20f70186a0ecbb87   score: 0.29370826   abstract: With the rapid progress in semiconductor technologies, more and more accelerators can be integrated onto a single SoC chip. In SoCs, accelerators often require deterministic data access. However, as more and more applications are running simultaneous, latency can vary significantly due to contention. To address this problem, we propose a template-based memory access engine (MAE) for accelerators in SoCs. The proposed MAE can handle several common memory access patterns observed for near-future accelerators. Our evaluation results show that the proposed MAE can significantly reduce memory access latency and jitter, thus very effective for accelerators in SoCs.\n",
      "\n",
      "9. id: 5390bd1520f70186a0f43a31   score: 0.28676808   abstract: New SOC like the Xilinx Zynq 7045 allow researchers and developers to combine the advantages of writing software for control functionality and having accelerators in the FPGA logic for the number crunching. The dual core Cortex-A9 ARM processor runs with up to 1 GHz and the FPGA has up to 900 DSP slices allowing a performance of up to 1, 334 GMACs. SCS is porting a lot of algorithms like SGM stereo, Stixel clustering or an optical flow to such devices allowing new cars to see their environment and react appropriately. The new developed SCS Zynq 7045 module will allow accelerated development using this technology.\n",
      "\n",
      "10. id: 53908a4020f70186a0d9dc1d   score: 0.2510964   abstract: This paper develops and validates an efficient analytical model for evaluating the performance of shared memory architectures with ILP processors. First, we instrument the SimOS simulator to measure the parameters for such a model and we find a surprisingly high degree of processor memory request heterogeneity in the workloads. Examining the model parameters provides insight into application behaviors and how they interact with the system. Second, we create a model that captures such heterogeneous processor behavior, which is important for analyzing memory system design tradeoffs. Highly bursty memory request traffic and lock contention are also modeled in a significantly more robust way than in previous work. With these features, the model is applicable to a wide range of architectures and applications. Although the features increase the model complexity, it is a useful design tool beca\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1725030\n",
      "index                                        55323d1545cec66b6f9dd831\n",
      "title               A Generation Method of Network Security Harden...\n",
      "authors             Chao Zhao, Huiqiang Wang, Junyu Lin, Hongwu Lv...\n",
      "year                                                           2015.0\n",
      "venue                  International Journal of Web Services Research\n",
      "references                                   558bfcb50cf25dbdbb050a12\n",
      "abstract            Analyzing attack graphs can provide network se...\n",
      "id                                                            1725030\n",
      "clustered_labels                                                    1\n",
      "Name: 1725030, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b72e20f70186a0f212aa   score: 0.99486035   abstract: Attack graph analysis has been established as a powerful tool for analyzing network vulnerability. However, previous approaches to network hardening look for exact solutions and thus do not scale. Further, hardening elements have been treated independently, which is inappropriate for real environments. For example, the cost for patching many systems may be nearly the same as for patching a single one. Or patching a vulnerability may have the same effect as blocking traffic with a firewall, while blocking a port may deny legitimate service. By failing to account for such hardening interdependencies, the resulting recommendations can be unrealistic and far from optimal. Instead, we formalize the notion of hardening strategy in terms of allowable actions, and define a cost model that takes into account the impact of interdependent hardening actions. We also introduce a near-optimal approxim\n",
      "\n",
      "2. id: 558ac907612c41e6b9d3a219   score: 0.99209267   abstract: Attack graphs are instrumental to analyze security vulnerabilities in networks. They provide decision support to security analysts via identification of critical exploitation scenarios. However, they may suffer from scalability issues due to network size and potential vulnerabilities, and become difficult to analyze. In this paper, we propose a heuristic method to find a cost-effective solution to protect a network using compact attack graphs. First, we extract all possible attack paths which reach predetermined critical resources embedded in the network. The exploit or initial condition which contributes the most to attack paths with least cost is selected to be removed. This process continues iteratively and a security analyst can stop it when the total cost exceeds the allocated budget. The experimental results show that the algorithm scales gracefully with the size of the graphs and \n",
      "\n",
      "3. id: 53908bcc20f70186a0dc62b9   score: 0.9883128   abstract: An integral part of modeling the global view of network security isconstructing attack graphs.In practice, attack graphs areproduced manually by Red Teams.Construction by hand, however, istedious, error-prone, and impractical for attack graphs larger than ahundred nodes.In this paper we present an automated technique forgenerating and analyzing attack graphs.We base our technique onsymbolic model checking algorithms,letting us construct attack graphs automatically and efficiently.Wealso describe two analyses to help decide which attacks would be mostcost-effective to guard against.We implemented our technique in atool suite and tested it on a small network example, which includesmodels of a firewall and an intrusion detection system.\n",
      "\n",
      "4. id: 5390a88c20f70186a0e9a30e   score: 0.97414124   abstract: Attack graphs play important roles in analyzing network security vulnerabilities, and previous works have provided meaningful conclusions on the generation and security measurement of attack graphs. However, it is still hard for us to understand attack graphs in a large network, and few suggestions have been proposed to prevent inside malicious attackers from attacking networks. To address these problems, we propose a novel approach to generate and describe attack graphs. Firstly, we construct a two-layer attack graph, where the upper layer is a hosts access graph and the lower layer is composed of some host-pair attack graphs. Compared with previous works, our attack graph has simpler structures, and reaches the best upper bound of computation cost in O(N2). Furthermore, we introduce the adjacency matrix to efficiently evaluate network security, with overall evaluation results presented\n",
      "\n",
      "5. id: 558b19ff612c41e6b9d43500   score: 0.9709644   abstract: Traditional vulnerability scan tools cannot show the associations among vulnerabilities, and thus the security administrators have the difficulty to comprehensively understand the risks in networks according to the vulnerabilities sources. With the number of vulnerabilities growing rapidly, repairing all vulnerabilities costs much. In order to mitigate this problem, we propose a method using attack graph analysis, which provides network security hardening strategies in a cost effective way. For such a purpose, we construct attack graphs by software, and analyze the potential risks in networks by preprocessing them. Further, we calculate low-cost network security hardening strategies via modified ant-colony optimization. In case that the algorithm falls into local optima, a node-hidden mechanism with the highest selected probability is introduced. We have evaluated the performance of the \n",
      "\n",
      "6. id: 5390a5dc20f70186a0e809a9   score: 0.96777767   abstract: Attack graphs are important tools for analyzing network security vulnerabilities. Recently, the generation method of attack graphs is a hot topic to the security researchers. As previous works encounter the scalability problem and inaccurate input information problem, we propose a novel method to automatic construction of attack graphs based on probability. After introducing prior-probability, match-probability,and transition-probability into attack graphs generation process, we develop a new attack model and relevant generation algorithms. Our method uses threshold and key states to control the scale of result attack graphs with important attack paths reserved. The following experiments show our approach could get meaningful results with less time and space, especially when one wants to get a few shortest attack paths quickly.\n",
      "\n",
      "7. id: 5390a2e820f70186a0e66356   score: 0.95447797   abstract: The attack graph, a typical model-based method, is widely used in the field of network security evaluation. The biggest disadvantage of attack graph method is its exponential growth of the state space. This paper presents an efficient algorithm based on the malefactor’s access level vector in every host of the network to generate a reduced attack graph in polynomial compute complexity. In this algorithm, the state space is reduced to , where is the number of nodes and is the whole number of vulnerabilities in the network. We also present a standard method to generate attack templates from the vulnerabilities.\n",
      "\n",
      "8. id: 5390b8d720f70186a0f2ba6f   score: 0.95362186   abstract: Existing network security analysis methods such as using tools like attack graphs or attack trees to compute risk probabilities did not consider the concrete running environment of the target network, which may make the obtained results deviate from the true situation. In this paper, we propose a network security analysis method taking into account the usage information of the target network. We design usage sensors in each host to get the usage information in the network. Combining with attack graph generation tool which gets all the vulnerabilities in the network in the graph form, we evaluation the network using the usage information and the vulnerabilities information, and get more accurate evaluation results.\n",
      "\n",
      "9. id: 559256db0cf28b1a968ffcb6   score: 0.952131   abstract: In network security hardening a network administrator may need to use limited resources (such as honeypots) to harden a network against possible attacks. Attack graphs are a common formal model used to represent possible attacks. However, most existing works on attack graphs do not consider the reactions of attackers to different defender strategies. We introduce a game-theoretic model of the joint problem where attacker's strategies are represented using attack graphs, and defender's strategies are represented as modifications of the attack graph. The attack graphs we use allow for sequential attack actions with associated costs and probabilities of success/failure. We present an algorithm for an computing attack policy that maximizes attacker's expected reward and empirical results demonstrating our methods on a case study network.\n",
      "\n",
      "10. id: 5390bfa220f70186a0f54f4d   score: 0.95013416   abstract: This Springer Brief examines the tools based on attack graphs that help reveal network hardening threats. Existing tools detail all possible attack paths leading to critical network resources. Though no current tool provides a direct solution to remove the threats, they are a more efficient means of network defense than relying solely on the experience and skills of a human analyst. Key background information on attack graphs and network hardening helps readers understand the complexities of these tools and techniques. A common network hardening technique generates hardening solutions comprised of initially satisfied conditions, thereby making the solution more enforceable. Following a discussion of the complexity issues in this technique, the authors provide an improved technique that considers the dependencies between hardening options and employs a near-optimal approximation algorithm\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674252\n",
      "index                                        55916b890cf2e89307ca9b09\n",
      "title               Applying the CASSM Framework to Improving End ...\n",
      "authors               Marco Gillies, Andrea Kleinsmith, Harry Brenton\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 20th International Conferen...\n",
      "references          5390a1d420f70186a0e56ad8;5390a05920f70186a0e49...\n",
      "abstract            This paper presents an application of the CASS...\n",
      "id                                                            1674252\n",
      "clustered_labels                                                    0\n",
      "Name: 1674252, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558bea290cf2e30013db30d1   score: 0.40145314   abstract: Although interaction techniques have been extensively studied under controlled laboratory conditions, little is known about their capabilities during unconstrained free tasks. Understanding real world use is particularly important for older adults, as some may find computer input more challenging due to age-related declines in motor skill. As a step towards addressing this gap, we studied the feasibility of using machine-learning techniques. In this study, we emphasized on identifying errors from sub-movement behavior, using pen-based data from younger and older adults. We tested four machine-learning algorithms: Decision Trees, Neural Networks, Naïve Bayesian Networks, and Rule Induction. Each yielded an accuracy rate around 90%. Moreover, the experiments have identified some useful metrics to classify errors among older adult users.\n",
      "\n",
      "2. id: 5390b19020f70186a0ee03d4   score: 0.38375676   abstract: End-user interactive machine learning is a promising tool for enhancing human capabilities with large data. Recent work has shown that we can create end-user interactive machine learning systems for specific applications. However, we still lack a generalized understanding of how to design effective end-user interaction with interactive machine learning systems. My dissertation work aims to advance our understanding of this question by investigating new techniques that move beyond naïve or ad-hoc approaches and balance the needs of both end-users and machine learning algorithms. Although these explorations are grounded in specific applications, we endeavored to design strategies independent of application or domain specific features. As a result, our findings can inform future end-user interaction with machine learning systems.\n",
      "\n",
      "3. id: 53908b4920f70186a0dbce3b   score: 0.2928986   abstract: We have designed and evaluated Nautilus -- a group game played in interactive virtual space. This was a study about new kinds of computer games with new types of user interfaces. Our aim was to reduce the boundaries between the surrounding physical space and the virtual space designed to appeal to users' senses. We utilized the iterative Human-Centred Design (HCD) approach in the study. We created a new way to experience and play computer games, where players use their natural body movements and interact with each other. We have received information on the use of bodily and spatial user interfaces for location-based entertainment (LBE) solutions.\n",
      "\n",
      "4. id: 559167bc0cf2e89307ca999d   score: 0.21239533   abstract: On-body interaction techniques are gaining traction, and opening up new avenues to control interactive systems. At the same time, they reveal potential to increase the accessibility of systems like touch based smartphones and other mobile devices for visually impaired users. However, for this potential to be realised, it is paramount that these techniques can be used in a multitude of contextual settings, and, ideally, do not impose training and calibration procedures. Our approach intends to optimize signal filtering, feature extraction parameters and classifier configurations for each defined gesture. The results show a 98.35% accuracy for the optimized classifier. We proceeded to conduct a validation study (15 participants) in three contexts: seated, standing and walking. Our findings show that, despite the gesture being trained by someone not participating in the study, the average a\n",
      "\n",
      "5. id: 558ae163612c41e6b9d3c37a   score: 0.2109288   abstract: This special issue highlights research articles that apply machine learning to robots and other systems that interact with users through more than one modality, such as speech, gestures, and vision. For example, a robot may coordinate its speech with its actions, taking into account (audio-)visual feedback during their execution. Machine learning provides interactive systems with opportunities to improve performance not only of individual components but also of the system as a whole. However, machine learning methods that encompass multiple modalities of an interactive system are still relatively hard to find. The articles in this special issue represent examples that contribute to filling this gap.\n",
      "\n",
      "6. id: 5390a79f20f70186a0e90f98   score: 0.17638768   abstract: In recent years, game design has made great progress in human-computer interaction, but the methods and amount of information exchange between players and computer are still quite limited, which results in the interaction and entertainment of game are much more restricted. Based on this context and taking the human-computer interaction (HCI) theory as analysis tool, this paper puts forward and gives details on the principles of human-computer interaction in game design--- “Simple, natural, friendly and consistent”.\n",
      "\n",
      "7. id: 53909f8220f70186a0e3c6fd   score: 0.1490353   abstract: The results of a study of two computer games, that use human movement as direct input, were analysed using four existing frameworks and approaches, drawn from different disciplines that relate to interaction and movement. This enabled the exploration of the relationships between bodily actions and the corresponding responses from technology. Interaction analysis, two design frameworks and Laban movement analysis were chosen for their ability to provide different perspectives on human movement in interaction design. Each framework and approach provided a different, yet still useful, perspective to inform the design of movement-based interaction. Each allowed us to examine the interaction between the player and the game technology in quite distinctive ways. Each contributed insights that the others did not.\n",
      "\n",
      "8. id: 55323b8145cec66b6f9da0dd   score: 0.12830654   abstract: Designing intelligent computer interfaces requires human intelligence, which can be captured through multimodal sensors during human-computer interactions. These data modalities may involve users' language, vision, and body signals, which shed light on different aspects of human cognition and behaviors. I propose to integrate multimodal data to more effectively understand users during interactions. Since users' manipulation of big data (e.g., texts, images, videos) through interfaces can be computationally intensive, an interactive machine learning framework will be constructed in an unsupervised manner.\n",
      "\n",
      "9. id: 558b18e2612c41e6b9d43217   score: 0.11516222   abstract: In developing an interactive system, the part of the design corresponding to the specification of the functionality is commonly directed by the requirements. The specifications of these requirements are based on templates that classify activity in a way that in most cases, developers are not conscious and therefore do not understand how wrong classification due to methodological weaknesses, affects the understanding that has the user in relation with the system he is using. An ideal situation is that the developer fully understands what the impact of wrong classification and helps him to recognize the limits to define a scope for usability, safety or any other aspect. Our proposal is to define an activity classification model and make explicit the need for it to serve as a framework to promote software quality mainly it has to do with the Human Computer Interaction (HCI).\n",
      "\n",
      "10. id: 5390b8d720f70186a0f2b782   score: 0.07423137   abstract: We explore motion capture as a means for generating expressive bodily interaction between humans and virtual characters. Recorded interactions between humans are used as examples from which rules are formed that control reactions of a virtual character to human actions. The author of the rules selects segments considered important and features that best describe the desired interaction. These features are motion descriptors that can be calculated in real-time such as quantity of motion or distance between the interacting characters. The rules are authored as mappings from observed descriptors of a human to the desired descriptors of the responding virtual character. Our method enables a straightforward process of authoring continuous and natural interaction. It can be used in games and interactive animations to produce dramatic and emotional effects. Our approach requires less example mo\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1718906\n",
      "index                                        55323c8d45cec66b6f9dc5d4\n",
      "title               Overview of Recent Trans-Institutional Health ...\n",
      "authors             Maren Juhr, Reinhold Haux, Takahiro Suzuki, Ka...\n",
      "year                                                           2015.0\n",
      "venue                                      Journal of Medical Systems\n",
      "references                                   558ae335612c41e6b9d3c6f9\n",
      "abstract            Worldwide populations are aging and countries ...\n",
      "id                                                            1718906\n",
      "clustered_labels                                                    1\n",
      "Name: 1718906, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390882720f70186a0d898d2   score: 0.81316423   abstract: From the Publisher:'Transforming Health Care Through Information': Case Studies draws upon the experience of the foremost Medical Informatics users throughout the world to present the complex challenges faced when implementing technological changes as well as specific, practical solutions employed to overcome them.\n",
      "\n",
      "2. id: 53909f8c20f70186a0e40e57   score: 0.7360213   abstract: Because most health information exchange (HIE) initiatives are as yet immature, formative evaluation is recommended so that what is learned through evaluation can be immediately applied to assist in HIE development efforts. Qualitative methods can be especially useful for formative evaluation because they can guide ongoing HIE growth while taking context into consideration. This paper describes important HIE-related research questions and outlines appropriate qualitative research techniques for addressing them.\n",
      "\n",
      "3. id: 5390b95520f70186a0f2f3ff   score: 0.5926666   abstract: A multitude of projects are under way that support the transition to electronic health records, which enable the exchange of health information among healthcare-related parties while maintaining patient privacy and offering security protections.\n",
      "\n",
      "4. id: 5390b13020f70186a0edc52f   score: 0.40444866   abstract: There is a pronounced trend of aging of population. The fight against aging is a global, interdisciplinary process, which involves not only various medical branches, but also different research domains and the fast development of modern and adapted technologies and devices. There is an urgent need of a much broader implementation of Information and Communication Technology that aim to facilitate a dignified aging, an independent living and to improve the efficiency and quality of elderly care. The increasing number of medical or non medical information and the changes in the anti-aging domain lead to the necessity to create specialised Health Information Systems (like AgingNice or AGECVD presented in this paper) in which the general public and professionals should be able to find the latest information in the field of anti-aging.\n",
      "\n",
      "5. id: 5390b44620f70186a0ef91d8   score: 0.38479656   abstract: As the population ages and healthcare costs continue to soar, the focus of the nation and the healthcare industry turns to reducing costs and making the delivery process more efficient. Demonstrating how improvements in information systems can lead to improved patient care, Information and Communication Technologies in Healthcare explains how to create a holistic Medical Records System as a core component to addressing the issues affecting the U.S. healthcare system. Examining the impact of our aging population on healthcare, the book describes the range of systems that support key segments of the industry, including: hospitals, physicians, imaging, and nursing. It considers patient records, the physicians office, emerging home-monitoring networks, the recording and information submitting process, and hospice/nursing home use. Leaving no stone unturned, this reference investigates: Healt\n",
      "\n",
      "6. id: 539089ab20f70186a0d96a03   score: 0.2758119   abstract: From the Publisher:This book has been thoroughly revised and updated to reflect the vast technological changes in the field for 2-year or 4-year health management programs. This text focuses on health data, its collection and use. It emphasizes the deployment of information technology and the role of the HIM professional in the development of the electronic health record.\n",
      "\n",
      "7. id: 5390a2be20f70186a0e64832   score: 0.2757144   abstract: This paper describes support for three public health practice domains in demonstrations of a model health information exchange (HIE): biosurveillance, case reporting, and communication from public health to providers through integrated decision support. The model HIE implements interoperability through the use of existing semantic and syntactic standards specified as part of Integration Profiles to support specific data transfer use cases. We implemented these profiles in several public health applications using a service-orientated architecture approach. Methods were validated for each public health domain in national showcase demonstrations. We believe that this work has implications for the integration of public health functions into any HIE, regardless of its architecture, because our informatics methods support a distributed environment. This approach may be extended to strengthen d\n",
      "\n",
      "8. id: 5390a25820f70186a0e5f297   score: 0.23686911   abstract: In this paper, we describe and evaluate a new distributed architecture for clinical decision support called SANDS (Service-oriented Architecture for NHIN Decision Support), which leverages current health information exchange efforts and is based on the principles of a service-oriented architecture. The architecture allows disparate clinical information systems and clinical decision support systems to be seamlessly integrated over a network according to a set of interfaces and protocols described in this paper. The architecture described is fully defined and developed, and six use cases have been developed and tested using a prototype electronic health record which links to one of the existing prototype National Health Information Networks (NHIN): drug interaction checking, syndromic surveillance, diagnostic decision support, inappropriate prescribing in older adults, information at the p\n",
      "\n",
      "9. id: 5390981d20f70186a0e04630   score: 0.20024471   abstract: Countries across this world are taking many and varied approaches to incorporating information and communications technologies into their health care systems. Accordingly, a great deal can be learned from each other, despite notable differences in such widely variable factors as geography, population density, culture, and nature of health care provision. This paper discusses differences and similarities in health information challenges in China and Canada, focuses on the electronic health record as the core component of the health infostructure, briefly highlights Canadian activities to date and concludes with considerations around potential key learnings for China.\n",
      "\n",
      "10. id: 53909f8c20f70186a0e40e59   score: 0.18462045   abstract: Health information exchange (HIE) initiatives are in various stages of development across the United States. They aim to bring previously unavailable clinical data from patients' disparate health records, which may be spread over multiple provider and payer networks, to the point of care where clinicians and their patients need it most. The implications of these initiatives on public health are numerous. This article provides general evaluation methods for measuring the impact of HIE on public health in six use cases: (1) mandated reporting of laboratory diagnoses, (2) mandated reporting of physician-based diagnoses, (3) public health investigation, (4) disease-based non-reportable laboratory data, (5) antibiotic-resistant organism surveillance, and (6) population-level quality monitoring.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674928\n",
      "index                                        558f83960cf23638afbe69c5\n",
      "title               Demand in My Pocket: Mobile Devices and the Da...\n",
      "authors             Carolynne Lord, Mike Hazas, Adrian K. Clear, O...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558bd2c60cf20e727d0f2433;558bd0460cf23f2dfc593...\n",
      "abstract            This paper empirically explores the role that ...\n",
      "id                                                            1674928\n",
      "clustered_labels                                                    0\n",
      "Name: 1674928, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390990f20f70186a0e10686   score: 0.7132319   abstract: Mobile devices, applications and services have become integrated into people's daily lives on a personal and professional level. Although traditional research methods are being used to understand the use of mobile devices and applications, methodological challenges still exist. Researchers have responded to these challenges in a range of ways, with an emphasis on developing methods that enable new ways of accessing, making available and collecting, data about mobile technology use. This paper identifies, defines, describes and presents, a preliminary framework for understanding the methodological responses emerging in current Mobile Human Computer Interaction (Mobile HCI) research.\n",
      "\n",
      "2. id: 558afb89612c41e6b9d3f85c   score: 0.63624007   abstract: Internet connected mobile devices are an increasingly ubiquitous part of our everyday lives and we present here the results from unobtrusive audio-video recordings of iPhone use -- over 100 days of device use collected from 15 users. The data reveals for analysis the everyday, moment-by-moment use of contemporary mobile phones. Through video analysis of usage we observed how messages, social media and internet use are integrated and threaded into daily life, interaction with others, and everyday events such as transport, delays, establishment choice and entertainment. We document various aspects of end-user mobile device usage, starting with understanding how it is occasioned by context. We then characterise the temporal and sequential nature of use. Lastly, we discuss the social nature of mobile phone usage. Beyond this analysis, we reflect on how to draw these points into ideas for des\n",
      "\n",
      "3. id: 5390bf1320f70186a0f515d6   score: 0.49034238   abstract: This report presents preliminary results from an unobtrusive video study of iPhone use -- totalling over 100 days of everyday device usage. The data gives us a uniquely detailed view on how messages, social media and internet use are integrated and threaded into daily life, our interaction with others, and everyday events such as transport, communication and entertainment. These initial results seek to address the when, who and what of situated mobile phone use -- beginning with understanding the impact of context. We then characterise three key modes of use found in the data: micro-breaks, digital knitting and reading. Finally we consider the multi-party and shared nature of phone use and who is involved. We reflect on analysis to date, designing from understanding use and future work -- our data provides the resource and scope for further analysis of the moment-by-moment use of contemp\n",
      "\n",
      "4. id: 5390bf1320f70186a0f52277   score: 0.4023334   abstract: Smartphones have become an integral part of our everyday lives. Having in-depth insights on how people use and deal with the devices, services and applications has increasingly become a necessity for many actors in mobile sector. As such, in the current study, an empirical study regarding the smartphone usage of 88 respondents has been conducted aiming to provide understandings on how smartphones are being used. To do so, a survey-based questionnaire supported by a diary study has been used to collect the log data. The results show there is hardly a correlation between what people use and what they perceive they do. The findings show that the respondents were accurate in assessing only few services, whereas for the majority of services, their assessments were not accurate.\n",
      "\n",
      "5. id: 5390bb1d20f70186a0f3deac   score: 0.3930355   abstract: The use of smartphones and tablets is set to surpass the use of PCs for Internet access, media experiences, and social networking. Recognizing the growing \"bring your own device\" trend, enterprises are increasingly embracing the use of multi-platform mobile devices. And for millions of users in many parts of the world, mobile devices are emerging as the primary means to participate in an increasingly online and digital world. The above trends have given rise to, and been facilitated by, the development and adoption of mobile applications, which leverage the unique capabilities of the mobile device including its location awareness and the numerous on-device sensors. However, mobile applications can also be limited by the processing and battery capabilities of mobile devices, which despite recent progress, shows a widening gap with the growing computing/power requirements of emerging Inter\n",
      "\n",
      "6. id: 5390a93b20f70186a0ea0aa1   score: 0.30424836   abstract: We present results from a qualitative study examining how professionals living and working in Nairobi, Kenya regularly use ICT in their everyday lives. There are two contributions of this work for the HCI community. First, we provide empirical evidence demonstrating constraints our participants encountered when using technology in an infrastructure-poor setting. These constraints are limited bandwidth, high costs, differing perceptions of responsiveness, and threats to physical and virtual security. Second, we use our findings to critically evaluate the \"access, anytime and anywhere\" construct shaping the design of future technologies. We present an alternative vision called deliberate interactions--a planned and purposeful interaction style that involves offline preparation and discuss ways ICT can support this online usage behavior.\n",
      "\n",
      "7. id: 5390be6620f70186a0f4c4b3   score: 0.25571522   abstract: To date, research in sustainable HCI has dealt with eco-feedback, usage and recycling of appliances within the home, and longevity of portable electronics such as mobile phones. However, there seems to be less awareness of the energy and greenhouse emissions impacts of domestic consumer electronics and information technology. Such awareness is needed to inform HCI sustainability researchers on how best to prioritise efforts around digital media and IT. Grounded in inventories, interview and plug energy data from 33 undergraduate student participants, our findings provide the context for assessing approaches to reducing the energy and carbon emissions of media and IT in the home. In the paper, we use the findings to discuss and inform more fruitful directions that sustainable HCI research might take, and we quantify how various strategies might have modified the energy and emissions impac\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3e1f9   score: 0.23528403   abstract: We investigate how technology usage in homes has changed with the increasing prevalence of mobile devices including Tablets and Smart Phones. We logged Internet usage from 86 Belgium households to determine their six most common Internet Activities. Next, we surveyed households about what devices they own, how they share those devices, and which device they use for different Internet activities. We then conducted semi-structured interviews with 18 of 55 households that responded to the survey in which participants explained their device usage patterns and where they use technology in their home. Our findings suggest that the nature of online activity and social context influence device preference. Many participants reported that their Desktop PC is now a special purpose device, which they use only for specific activities such as working from home or online gaming. Compared to past studie\n",
      "\n",
      "9. id: 5390b0ca20f70186a0edb8f5   score: 0.23423143   abstract: It is well known that the types of information needs that arise while mobile differ significantly from the types of information needs that arise in desktop environments. Limited devices and interactions as well as dynamically changing contexts all have a role to play. However, to date, studies exploring mobile information needs have be relatively small in terms of scale and duration. The goal of this work is to understand more about mobile information needs on a larger-scale. In this paper we outline a study that employs intelligent experience sampling, an online diary and SMS technology as a means to gather insights into the types of needs that occur while mobile. Rather than reporting results, we discuss our experiences, the lessons learned and the challenges faced in terms of deployment, interacting with our participants as well as in analyzing the dataset we generated.\n",
      "\n",
      "10. id: 559168e60cf2e89307ca9a11   score: 0.20402452   abstract: As people possess increasing numbers of information devices, situations where several devices are combined and used together have become more common. We present a user study on people's current practices in combining multiple information devices in their everyday lives, ranging from pragmatic tasks to leisure activities. Based on diaries and interviews of 14 participants, we characterize the usage practices of the most common devices, including smartphones, computers, tablets, and home media centers. We analyze 123 real-life multi-device use cases and identify the main usage patterns, including Sequential Use, Resource Lending, Related Parallel Use, and Unrelated Parallel Use. We discuss the practical challenges of using several information devices together. Finally, we identify three levels of decisions that determine which devices are used in a particular situation, including acquiring\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1667917\n",
      "index                                        559168cc0cf2e89307ca9a04\n",
      "title               Challenges, Benefits and Best Practices of Per...\n",
      "authors                                           Wolfgang Gottesheim\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 4th International Workshop ...\n",
      "references                                   558bde220cf23f2dfc594913\n",
      "abstract            Did you know that just a handful of root cause...\n",
      "id                                                            1667917\n",
      "clustered_labels                                                    3\n",
      "Name: 1667917, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b24420f70186a0ee8117   score: 0.86499614   abstract: Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5% of the causes are related to so\n",
      "\n",
      "2. id: 5390bae520f70186a0f3b720   score: 0.83973396   abstract: Performance is crucial for the success of an application. To build responsive and cost efficient applications, software engineers must be able to detect and fix performance problems early in the development process. Existing approaches are either relying on a high level of abstraction such that critical problems cannot be detected or require high manual effort. In this paper, we present a novel approach that integrates performance regression root cause analysis into the existing development infrastructure using performance-aware unit tests and the revision history. Our approach is easy to use and provides software engineers immediate insights with automated root cause analysis. In a realistic case study based on the change history of Apache Commons Math, we demonstrate that our approach can automatically detect and identify the root cause of a major performance regression.\n",
      "\n",
      "3. id: 5390bb1d20f70186a0f3de6c   score: 0.81227237   abstract: Root Cause Analysis (RCA) is the process of identifying project issues, correcting them and taking preventive actions to avoid occurrences of such issues in the future. Issues could be variance in schedule, effort, cost, productivity, expected results of software, performance parameters and customer satisfaction. RCA also involves collecting valid data, analyzing it, deriving metrics and finding root causes using RCA methods. In this paper we will do Root cause analysis of some severe software failures that happened in the past and of some failures in ongoing projects in the software Industry. We will also describe various RCA methods and processes used in the software Industry to reduce the chances of software failure.\n",
      "\n",
      "4. id: 5390baa120f70186a0f384a4   score: 0.69699055   abstract: Performance problems such as high response times in software applications have a significant effect on the customer's satisfaction. However, detecting performance problems is still a highly manual and cumbersome process requiring deep expertise in performance engineering. Uncovering performance problems and finding their root causes are two challenging problems which are not solved yet. Existing approaches either focus on certain types of performance problems, do not conduct root cause analysis or consider performance only under average load scenarios. In this PhD research proposal, we pursue the goal to support software engineers in uncovering performance problems and identifying their root causes. Based on a novel way of structuring the knowledge about performance problems, we propose an automatic, experimentation-based approach for diagnostics of performance problems. Utilizing monito\n",
      "\n",
      "5. id: 5390a88c20f70186a0e99232   score: 0.6928498   abstract: Sources of errors can be categorized into 4 different types: 1. Software bugs, 2. Human mistake during configuration and deployment of applications, and during maintenance of machines, 3. System hardware failures, and 4. Network problems. Software bugs have direct impact to system resource availability, for instance, memory leaks. Human mistakes usually result in decreacing of application availability. As thousands of computers connected together to form an application and to serve network traffic, hardware failures become common, such as RAID failures, file system issues, disk failed, etc. An example of Network problem is network switch failed. The ugly thing is that the switch usually is partially failed. Before this problem switch is identified, many other application timeouts, intermittent application availibility are already making people doing trouble shooting crazy. Embeded networ\n",
      "\n",
      "6. id: 558b21c6612c41e6b9d445d2   score: 0.6734065   abstract: To design effective tools for detecting and recovering from software failures requires a deep understanding of software bug characteristics. We study software bug characteristics by sampling 2,060 real world bugs in three large, representative open-source projects--the Linux kernel, Mozilla, and Apache. We manually study these bugs in three dimensions--root causes, impacts, and components. We further study the correlation between categories in different dimensions, and the trend of different types of bugs. The findings include: (1) semantic bugs are the dominant root cause. As software evolves, semantic bugs increase, while memory-related bugs decrease, calling for more research effort to address semantic bugs; (2) the Linux kernel operating system (OS) has more concurrency bugs than its non-OS counterparts, suggesting more effort into detecting concurrency bugs in operating system code;\n",
      "\n",
      "7. id: 5390a2e920f70186a0e673e6   score: 0.65742844   abstract: Application problem diagnosis in complex enterprise environments is a challenging problem, and contributes significantly to the growth in IT management costs. While application problems have a large number of possible causes, failures due to runtime interactions with the system environment (e.g., configuration files, resource limitations, access permissions) are one of the most common categories. Troubleshooting these problems requires extensive experience and time, and is very difficult to automate. In this paper, we propose a black-box approach that can automatically diagnose several classes of application faults using applications' runtime behaviors. These behaviors along with various system states are combined to create signatures that serve as a baseline of normal behavior. When an application fails, the faulty behavior is analyzed against the signature to identify deviations from e\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3cebe   score: 0.64567864   abstract: Performance problems pose a significant risk to software vendors. If left undetected, they can lead to lost customers, increased operational costs, and damaged reputation. Despite all efforts, software engineers cannot fully prevent performance problems being introduced into an application. Detecting and resolving such problems as early as possible with minimal effort is still an open challenge in software performance engineering. In this paper, we present a novel approach for Performance Problem Diagnostics (PPD) that systematically searches for well-known performance problems (also called performance antipatterns) within an application. PPD automatically isolates the problem's root cause, hence facilitating problem solving. We applied PPD to a well established transactional web e-Commerce benchmark (TPC-W) in two deployment scenarios. PPD automatically identified four performance probl\n",
      "\n",
      "9. id: 5390a8db20f70186a0e9d08b   score: 0.61566573   abstract: Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors. Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened durin\n",
      "\n",
      "10. id: 5390a2e920f70186a0e674cc   score: 0.5843898   abstract: We present a three-part approach for diagnosing bugs and performance problems in production distributed environments. First, we introduce a novel execution monitoring technique that dynamically injects a fragment of code, the agent, into an application process on demand. The agent inserts instrumentation ahead of the control flow within the process and propagates into other processes, following communication events, crossing host boundaries, and collecting a distributed function-level trace of the execution. Second, we present an algorithm that separates the trace into user-meaningful activities called flows. This step simplifies manual examination and enables automated analysis of the trace. Finally, we describe our automated root cause analysis technique that compares the flows to help the analyst locate an anomalous flow and identify a function in that flow that is a likely cause of t\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1688368\n",
      "index                                        55915b3f0cf232eb904fbe4d\n",
      "title               Hybrid User-level Sandboxing of Third-party An...\n",
      "authors             Yajin Zhou, Kunal Patel, Lei Wu, Zhi Wang, Xux...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 10th ACM Symposium on Infor...\n",
      "references          558b1af1612c41e6b9d43663;558bd1570cf25dbdbb04d...\n",
      "abstract            Users of Android phones increasingly entrust p...\n",
      "id                                                            1688368\n",
      "clustered_labels                                                    3\n",
      "Name: 1688368, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bfa220f70186a0f5371f   score: 0.9451974   abstract: In these days there are many malicious applications that collect sensitive information owned by third-party applications by escalating their privileges to the higher level on the Android operating system. An attack of obtaining the root-level privilege in the Android operating system can be a serious threat to users because it can break down the whole system security. This paper proposes a new Android security framework that can meet the following three goals: (1) preventing privilege escalation attacks, (2) maintaining system integrity, and (3) protecting users' personal information. To achieve these goals, our proposed framework introduces three mechanisms: Root Privilege Protection (RPP), Resource Misuse Protection (RMP), and Private Data Protection (PDP). RPP keeps track of a list of trusted programs with root-level privileges and can detect and respond to malware that illegally trie\n",
      "\n",
      "2. id: 5390b78a20f70186a0f24148   score: 0.90681833   abstract: As mobile devices become more widespread and powerful, they store more sensitive data, which includes not only users' personal information but also the data collected via sensors throughout the day. When mobile applications have access to this growing amount of sensitive information, they may leak it carelessly or maliciously. Google's Android operating system provides a permissions-based security model that restricts an application's access to the user's private data. Each application statically declares the sensitive data and functionality that it requires in a manifest, which is presented to the user upon installation. However, it is not clear to the user how sensitive data is used once the application is installed. To combat this problem, we present AndroidLeaks, a static analysis framework for automatically finding potential leaks of sensitive information in Android applications on \n",
      "\n",
      "3. id: 5390adfd20f70186a0ec5ec4   score: 0.7989722   abstract: Today's smartphone operating systems frequently fail to provide users with adequate control over and visibility into how third-party applications use their private data. We address these shortcomings with TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid provides realtime analysis by leveraging Android's virtualized execution environment. TaintDroid incurs only 14% performance overhead on a CPU-bound micro-benchmark and imposes negligible overhead on interactive third-party applications. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, we found 68 instances of potential misuse of users' private information across 20 applications. Monitoring sensitive data with TaintDroid provides informed use of third-party applications for phone users a\n",
      "\n",
      "4. id: 558b1af1612c41e6b9d43663   score: 0.7874412   abstract: Android applications often include third-party libraries written in native code. However, current native components are not well managed by Android's security architecture. We present NativeGuard, a security framework that isolates native libraries from other components in Android applications. Leveraging the process-based protection in Android, NativeGuard isolates native libraries of an Android application into a second application where unnecessary privileges are eliminated. NativeGuard requires neither modifications to Android nor access to the source code of an application. It addresses multiple technical issues to support various interfaces that Android provides to the native world. Experimental results demonstrate that our framework works well with a set of real-world applications, and incurs only modest overhead on benchmark programs.\n",
      "\n",
      "5. id: 5390a1d420f70186a0e58f5d   score: 0.55410856   abstract: The current security model for a third-party application running on a mobile device requires its user to trust that application's vendor and whilst mechanisms exist to mediate this relationship, they cannot guarantee complete protection against the threats posed. This work introduces a security architecture that prevents a third-party application deviating from its intended behaviour, defending devices against previously unseen malware more effectively than existing security measures.\n",
      "\n",
      "6. id: 558af39d612c41e6b9d3e636   score: 0.5167174   abstract: Today’s smartphone operating systems frequently fail to provide users with visibility into how third-party applications collect and share their private data. We address these shortcomings with TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid enables realtime analysis by leveraging Android’s virtualized execution environment. TaintDroid incurs only 32&percnt; performance overhead on a CPU-bound microbenchmark and imposes negligible overhead on interactive third-party applications. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, in our 2010 study we found 20 applications potentially misused users’ private information; so did a similar fraction of the tested applications in our 2012 study. Monitoring the flow of privacy-sensitive data wit\n",
      "\n",
      "7. id: 5390b78a20f70186a0f22f83   score: 0.51308906   abstract: The increasing popularity of Google's mobile platform Android makes it the prime target of the latest surge in mobile malware. Most research on enhancing the platform's security and privacy controls requires extensive modification to the operating system, which has significant usability issues and hinders efforts for widespread adoption. We develop a novel solution called Aurasium that bypasses the need to modify the Android OS while providing much of the security and privacy that users desire. We automatically repackage arbitrary applications to attach user-level sandboxing and policy enforcement code, which closely watches the application's behavior for security and privacy violations such as attempts to retrieve a user's sensitive information, send SMS covertly to premium numbers, or access malicious IP addresses. Aurasium can also detect and prevent cases of privilege escalation atta\n",
      "\n",
      "8. id: 558b159f612c41e6b9d42953   score: 0.49365264   abstract: As Android has become the most prevalent operating system in mobile devices, privacy concerns in the Android platform are increasing. A mechanism for efficient runtime enforcement of information-flow security policies in Android apps is desirable to confine privacy leakage. The prior works towards this problem require firmware modification (i.e., modding) and incur considerable runtime overhead. Besides, no effective mechanism is in place to distinguish malicious privacy leakage from those of legitimate uses. In this paper, we take a bytecode rewriting approach. Given an unknown Android app, we selectively insert instrumentation code into the app to keep track of private information and detect leakage at runtime. To distinguish legitimate and malicious leaks, we model the user's decisions with a context-aware policy enforcement mechanism. We have implemented a prototype called Capper and\n",
      "\n",
      "9. id: 5390b19020f70186a0ee0485   score: 0.44384146   abstract: We examine two privacy controls for Android smartphones that empower users to run permission-hungry applications while protecting private data from being exfiltrated: (1) covertly substituting shadow data in place of data that the user wants to keep private, and (2) blocking network transmissions that contain data the user made available to the application for on-device use only. We retrofit the Android operating system to implement these two controls for use with unmodified applications. A key challenge of imposing shadowing and exfiltration blocking on existing applications is that these controls could cause side effects that interfere with user-desired functionality. To measure the impact of side effects, we develop an automated testing methodology that records screenshots of application executions both with and without privacy controls, then automatically highlights the visual differ\n",
      "\n",
      "10. id: 5390bae620f70186a0f3c07f   score: 0.41335875   abstract: We present πBox, a new application platform that prevents apps from misusing information about their users. To strike a useful balance between users' privacy and apps' functional needs, πBox shifts much of the responsibility for protecting privacy from the app and its users to the platform itself. To achieve this, πBox deploys (1) a sandbox that spans the user's device and the cloud, (2) specialized storage and communication channels that enable common app functionalities, and (3) an adaptation of recent theoretical algorithms for differential privacy under continual observation.We describe a prototype implementation of πBox and show how it enables a wide range of useful apps with minimal performance overhead and without sacrificing user privacy.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1729762\n",
      "index                                        55323d7e45cec66b6f9de7f3\n",
      "title               Cost and energy aware service provisioning for...\n",
      "authors                                         Li Chunlin, Li Layuan\n",
      "year                                                           2015.0\n",
      "venue                                   The Journal of Supercomputing\n",
      "references          558b5837612c41e6b9d49b28;558b109c612c41e6b9d41fb5\n",
      "abstract            Currently, mobile devices are becoming the pop...\n",
      "id                                                            1729762\n",
      "clustered_labels                                                    1\n",
      "Name: 1729762, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bded20f70186a0f49d08   score: 0.9457011   abstract: Cloud is the kind of distributed computing which disperses and processes data distributed by virtualized manner. Cloud provides resources and computing infrastructure on demand basis to cloud consumers. The combination of cloud computing into the mobile computing environment is developed as a capable technology for mobile services. In this paper we emphasis on scenarios for availability of cloud resources and services on mobile devices. Further need of mobile cloud computing, real time mobile cloud applications, a comparative analysis on mobile cloud enablement technologies and role of middleware system are discussed as counter part of this research.\n",
      "\n",
      "2. id: 5390bd1520f70186a0f43aba   score: 0.9293122   abstract: With the popularity of smart phones and tablets, as well as the maturity of cloud computing techniques, mobile cloud computing becomes a promising area where people's business and daily life can be facilitated by adopting mobile cloud applications. However, there are also many challenges when utilizing cloud computing through mobile devices, including limited energy, disconnection of networking, lack of mobile cloud services and heterogeneity in mobile platform. Since there is still no mature architecture design approach for mobile cloud computing, in this paper, we propose such an approach by enhancing the basic CCRA (Cloud Computing Reference Architecture). Based on the enhanced architecture for mobile cloud computing, an enterprise-level mobile cloud platform is proposed. A mobile cloud application named Mobile Workflow is illustrated to verify feasibility of the proposed architecture\n",
      "\n",
      "3. id: 5390b8d720f70186a0f2d135   score: 0.8467045   abstract: Cloud computing and mobile computing are two of the most influential technologies that look set to change the face of computing in the coming years. Combination of the two provides us with an unprecedented opportunity to provide highly portable and yet content-rich and computation-intensive services to the end user. In this paper we investigate the possibility of using code/task offload techniques between mobile and cloud in order to reduce the energy cost of workflows deployed on mobile devices. We first present a vision in which mobile devices are coordinated over a network, which is equipped with a layer of cloud-like infrastructures which we term cloudlets, whose computational resources can be leveraged by the mobile devices to host the execution of mission-critical mobile workflows in an energy-aware manner. We then build a model that encompasses various characteristics of the workf\n",
      "\n",
      "4. id: 5390bb7b20f70186a0f3f88d   score: 0.83907574   abstract: In a cloud computing environment, users prefer to migrate their locally processing workloads onto the cloud where more resources with better performance can be expected. ProtoGENI [1] and PlanetLab [17] have further improved the current Internet-based resource outsourcing by allowing end users to construct a virtual network system through virtualization and programmable networking technologies. However, to the best of our knowledge, there is no such general service or resource provisioning platform designated for mobile devices. In this paper, we present a new design and implementation of MobiCloud that is a geo-distributed mobile cloud computing platform. The discussions of the system components, infrastructure, management, implementation flow, and service scenarios are followed by an example on how to experience the MobiCloud system.\n",
      "\n",
      "5. id: 5536867d0cf2dbb77a8169f6   score: 0.78481424   abstract: Mobile Cloud Computing (MCC) is emerging as a main ubiquitous computing platform which enables to leverage the resource limitations of mobile devices and wireless networks by offloading data-intensive computation tasks from resource-poor mobile devices to resource-rich clouds. In this paper, we consider an online location-aware offloading problem in a two-tiered mobile cloud computing environment consisting of a local cloudlet and remote clouds, with an objective to fair share the use of the cloudlet by consuming the same proportional of their mobile device energy, while keeping their individual SLA, for which we devise an efficient online algorithm. We also conduct experiments by simulations to evaluate the performance of the proposed algorithm. Experimental results demonstrate that the proposed algorithm is promising and outperforms other heuristics.\n",
      "\n",
      "6. id: 5390ad5620f70186a0ebea44   score: 0.7599387   abstract: With the recent widespread use of smart mobile devices, as well as the increasing availability of fast and reliable wireless Internet connections for mobile devices, there is increased interest in mobile applications where the majority of the processing occurs on the server side. The flexibility, stability and scalability offered by cloud services make them an ideal architecture to use in client applications in a resource limited mobile environment. This is because mobile application usage patterns tend to be uneven, with various usage spikes according to time and location. However, the mobile setting presents a set of new challenges that cloud service discovery methods developed for non-mobile environments cannot address. The requirements a mobile client device will have from a cloud service may change due to changes in the context of the device, which may include hardware resources, en\n",
      "\n",
      "7. id: 5390bb1d20f70186a0f3efa4   score: 0.7541916   abstract: Accessing Cloud via mobile device proves to be costly because of issues with wireless network. Due to communication overhead, offloading of application execution to Cloud consumes more energy than executing in the device itself. This paper proposes a novel framework in which application execution is offloaded to both Cloud and mobile ad hoc Cloud, in order to reduce this communication overhead. Distributed/Parallel execution of tasks is done to reduce the waiting time of mobile device, provided the cost of offloading is less, compared to cost of executing the application in device. Seamless service provisioning is achieved in this framework, by measuring the signal strength of the wireless medium. Once the signal strength threshold is reached, interim results are got from the device to which task is offloaded. This paper proposes the framework for energy efficient seamless service with f\n",
      "\n",
      "8. id: 558bd0f90cf25dbdbb04dc21   score: 0.7407374   abstract: Mobile cloud computing combines wireless access service and cloud computing to improve the performance of mobile applications. Mobile cloud computing can balance the application distribution between the mobile device and the cloud, in order to achieve faster interactions, battery savings and better resource utilization. To support mobile cloud computing, the paper proposes a phased scheduling model of mobile cloud such that mobile device's users experience lower interaction times and extended battery life. The phased scheduling optimization is solved by two subproblems: mobile device's batch application optimization and mobile device's job level optimization. At the first stage, the mobile cloud global scheduling optimization implements the allocation of the cloud resources to the mobile device's batch applications. At the second stage, mobile device's job level optimization adjusts the \n",
      "\n",
      "9. id: 5390b86b20f70186a0f28677   score: 0.71967924   abstract: Despite increasing usage of mobile computing, exploiting its full potential is difficult due to its inherent problems such as resource scarcity, frequent disconnections, and mobility. Mobile cloud computing can address these problems by executing mobile applications on resource providers external to the mobile device. In this paper, we provide an extensive survey of mobile cloud computing research, while highlighting the specific concerns in mobile cloud computing. We present a taxonomy based on the key issues in this area, and discuss the different approaches taken to tackle these issues. We conclude the paper with a critical analysis of challenges that have not yet been fully met, and highlight directions for future work.\n",
      "\n",
      "10. id: 5390bded20f70186a0f49bb1   score: 0.7157221   abstract: Mobile cloud computing has become a new paradigm, because of rapid growth of mobile devices such as smart mobile phone performance and cloud computing. This mobile cloud environment, with various kinds of service and IT resources according to the user's requirements, effectively provides services and IT resources are necessary. Therefore, in this paper we propose the use of frequency mobile cloud based analysis and filtering method of computing resources to distribute resources and services to users based on context-awareness information.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1683490\n",
      "index                                        55924377612cfc7d8b21467a\n",
      "title               Research on the computer network security eval...\n",
      "authors                                                      Yu Zhang\n",
      "year                                                           2015.0\n",
      "venue               Journal of Intelligent & Fuzzy Systems: Applic...\n",
      "references          558b2ae8612c41e6b9d456d2;558aee63612c41e6b9d3d...\n",
      "abstract            The problem of evaluating the computer network...\n",
      "id                                                            1683490\n",
      "clustered_labels                                                    1\n",
      "Name: 1683490, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b8dc6612c6b62e5e8b676   score: 0.9960083   abstract: In this paper, we investigate the multiple attribute decision making with hesitant fuzzy information. Motivated by the ideal of dependent aggregation, in this paper, we develop some dependent hesitant fuzzy aggregation operators: the dependent hesitant fuzzy ordered weighted averaging (DHFOWA) operator and the dependent hesitant fuzzy ordered weighted geometric (DHFOWG) operator, in which the associated weights only depend on the aggregated hesitant fuzzy arguments and can relieve the influence of unfair hesitant fuzzy arguments on the aggregated results by assigning low weights to those “false” and “biased” ones and then apply them to develop some approaches for multiple attribute group decision making with hesitant fuzzy information. Finally, an illustrative example for supplier selection is given to verify the developed approach and to demonstrate its practicality and effectiveness.\n",
      "\n",
      "2. id: 558aee63612c41e6b9d3daff   score: 0.9906238   abstract: In this paper, we investigate the multiple attribute decision making MADM problem based on the aggregation operators with dual hesitant fuzzy information. Then, motivated by the ideal of arithmetic and geometric operation, we have developed some aggregation operators for aggregating dual hesitant fuzzy information: dual hesitant fuzzy weighted average DHFWA operator, dual hesitant fuzzy weighted geometric DHFWG operator, dual hesitant fuzzy ordered weighted average DHFOWA operator, dual hesitant fuzzy ordered weighted geometric DHFOWG operator, dual hesitant fuzzy hybrid average DHFHA operator and dual hesitant fuzzy hybrid geometric DHFHG operator. The prominent characteristic of these proposed operators are studied. Then, we have utilized these operators to develop some approaches to solve the dual hesitant fuzzy multiple attribute decision making problems. Finally, a practical example\n",
      "\n",
      "3. id: 5390b52620f70186a0f0304e   score: 0.98498523   abstract: In this paper, we investigate the hesitant fuzzy multiple attribute decision making (MADM) problems in which the attributes are in different priority level. Motivated by the ideal of prioritized aggregation operators [R.R. Yager, Prioritized aggregation operators, International Journal of Approximate Reasoning 48 (2008) 263-274], we develop some prioritized aggregation operators for aggregating hesitant fuzzy information, and then apply them to develop some models for hesitant fuzzy multiple attribute decision making (MADM) problems in which the attributes are in different priority level. Finally, a practical example about talent introduction is given to verify the developed approaches and to demonstrate its practicality and effectiveness.\n",
      "\n",
      "4. id: 558b03f8612c41e6b9d40714   score: 0.9843364   abstract: With respect to multiple attribute decision making (MADM) problems in which the attributes are inter-dependent and take the form of dual hesitant fuzzy elements, a new MADM method with dual hesitant fuzzy information is investigated in this paper. Firstly, by using the Choquet integral, some new aggregation operators are developed for aggregating the dual hesitant fuzzy information, such as the dual hesitant fuzzy Choquet ordered average (DHFCOA) operator, the dual hesitant fuzzy Choquet ordered geometric (DHFCOG) operator, the generalized dual hesitant fuzzy Choquet ordered average (GDHFCOA) operator and the generalized dual hesitant fuzzy Choquet ordered geometric (GDHFCOG) operator. Then, some special cases, desirable properties of these operators and the relationships between them are discussed. Furthermore, based on the DHFCOA operator, an approach to MADM is proposed under dual hes\n",
      "\n",
      "5. id: 5390adfd20f70186a0ec583c   score: 0.979354   abstract: To study the problem of multiple attribute decision making in which the decision making information values are triangular fuzzy number, a new group decision making method is proposed. Then the calculation steps to solve it are given. As the key step, a new operator called fuzzy induced ordered weighted harmonic mean (FIOWHM) operator is proposed and a method based on the fuzzy weighted harmonic mean (FWHM) operator and FIOWHM operators for fuzzy MAGDM is presented. The priority based on possibility degree for the fuzzy multiple attribute decision making problem is proposed. At last, a numerical example is provided to illustrate the proposed method. The result shows the approach is simple, effective and easy to calculate.\n",
      "\n",
      "6. id: 5390bed320f70186a0f4e5c7   score: 0.9791955   abstract: In this paper, we develop a series of induced generalized aggregation operators for hesitant fuzzy or interval-valued hesitant fuzzy information, including induced generalized hesitant fuzzy ordered weighted averaging (IGHFOWA) operators, induced generalized hesitant fuzzy ordered weighted geometric (IGHFOWG) operators, induced generalized interval-valued hesitant fuzzy ordered weighted averaging (IGIVHFOWA) operators, and induced generalized interval-valued hesitant fuzzy ordered weighted geometric (IGIVHFOWG) operators. Next, we investigate their various properties and some of their special cases. Furthermore, some approaches based on the proposed operators are developed to solve multiple attribute group decision making (MAGDM) problems with hesitant fuzzy or interval-valued hesitant fuzzy information. Finally, some numerical examples are provided to illustrate the developed approaches\n",
      "\n",
      "7. id: 558ad0f4612c41e6b9d3ac49   score: 0.97520185   abstract: The dual hesitant fuzzy set is an efficient mathematical approach to study imprecise, uncertain or incomplete information or knowledge. In this paper, we study the aggregation methods of the dual hesitant fuzzy information. Several aggregation operators are proposed, such as the generalized dual hesitant fuzzy weighted averaging operator (GDHFWA), the generalized dual hesitant fuzzy ordered weighted averaging operator (GDHFOWA) and the generalized dual hesitant fuzzy hybrid averaging operator (GDHFHA) operator for aggregating the dual hesitant fuzzy elements and establish various properties of these operators. A method is also proposed for multi-criteria decision making under dual hesitant fuzzy environment and applied it to teaching quality assessment.\n",
      "\n",
      "8. id: 5390b20120f70186a0ee41e4   score: 0.97204566   abstract: Considered in this paper is the group decision making problem with inter-dependent or interactive attributes, where evaluation values of decision makers are in linguistic arguments. By using the Choquet integral, some new aggregation operators are introduced, including the 2-tuple correlated averaging operator, the 2-tuple correlated geometric operator and the generalized 2-tuple correlated averaging operator. The proposed operators can better reflect the correlations among the elements. After investigating properties of these operators, a new multiple attribute decision making method based on the new operators is proposed. Finally, a numerical example is provided to illustrate the feasibility and efficiency of the proposed method.\n",
      "\n",
      "9. id: 558ad5ad612c41e6b9d3b290   score: 0.9717254   abstract: We investigate the multiple attribute decision-making problems for evaluating the computer network security with intuitionistic trapezoidal fuzzy information. We utilize the intuitionistic trapezoidal fuzzy weighted average (ITFWA) operator to aggregate the intuitionistic trapezoidal fuzzy information corresponding to each alternative and get the overall value of the alternatives and then rank the alternatives and select the most desirable one(s) according to the distance between the overall value of the alternatives and ideal solution. Finally, an illustrative example for evaluating the computer network security is given.\n",
      "\n",
      "10. id: 5390bfa220f70186a0f541da   score: 0.9562273   abstract: Hesitant fuzzy set, as a new generalized type of fuzzy set, is an efficient and powerful structure in expressing uncertainty and vagueness and has attracted more and more scholars' attention. The aim of this paper is to develop some new aggregation operators to fuse hesitant fuzzy information. The hesitant fuzzy hybrid arithmetical averaging HFHAA operator, the hesitant fuzzy hybrid arithmetical geometric HFHAG operator, the quasi HFHAA operator and the quasi HFHAG operator are proposed and their properties are investigated. On the basis of these proposed operators, some algorithms are introduced to aid multi-criteria single person decision making and multi-criteria group decision making respectively. Some examples are provided to illustrate the practicality and validity of our proposed procedures.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1612577\n",
      "index                                        558bcbc9612cf642427585d5\n",
      "title               Sound Localization on Tabletop Computers: A Co...\n",
      "authors             Bill Kapralos, Jonathan Lam, Karen Collins, An...\n",
      "year                                                           2015.0\n",
      "venue               Computers in Entertainment (CIE) - Theoretical...\n",
      "references          539087c320f70186a0d552e3;539087cf20f70186a0d5c...\n",
      "abstract            Tabletop computers (also known as surface comp...\n",
      "id                                                            1612577\n",
      "clustered_labels                                                    0\n",
      "Name: 1612577, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b44620f70186a0ef8e89   score: 0.9737447   abstract: Table-top computing has been growing in popularity slowly for the last decade and is poised to make in-roads into the consumer market soon, opening up another new market for the games industry. However, before surface computers become widely accepted, there are many questions with respect to sound production and reception for these devices that need to be explored. Here, we describe two experiments that examine sound localization on a horizontal (table-top computer) surface. In the first experiment we collect \"ground truth\" data regarding physical sound source localization by employing a computer controlled grid of 25 equally spaced loudspeakers. In the second experiment we investigate virtual sound source localization using bilinear interpolation amplitude panning method and a modified quadraphonic loudspeaker configuration whereby four loudspeakers are positioned at each corner of the \n",
      "\n",
      "2. id: 53909ed120f70186a0e31c4c   score: 0.88363826   abstract: In a CAVE-like virtual environment spatial audio is typically reproduced with amplitude panning on loudspeakers behind the screens. We arranged a localization experiment where the subjects' task was to point to the perceived location of a sound source. Measured accuracy for a static source was as good as the accuracy in previous headphone experiments using head-related transfer functions. We also measured the localization accuracy of a moving auditory stimulus. The accuracy was decreased by an amount comparable to the minimum audible movement angle.\n",
      "\n",
      "3. id: 5390b60d20f70186a0f11306   score: 0.8767399   abstract: In this paper, we propose an enhanced constant power panning (CPP) law which reduces the sound localization error in pair-wise panning for multi-channel loudspeaker layouts. In conventional CPP law, the phantom sound direction is usually different from the panning angle, especially when two loudspeakers are spread out widely. To overcome this problem, we propose an enhanced CPP law using directional psychoacoustic criteria (DPC). Analysis results show that the proposed method makes the same phantom sound direction as the panning angle in the audible frequency range. In addition, its performance is verified by means of a subjective sound localization test using a real sound source.\n",
      "\n",
      "4. id: 5390b0ca20f70186a0eda2c6   score: 0.72700745   abstract: This demo presents PANDAA, a zero-configuration automatic spatial localization technique for networked devices based on ambient sound sensing. We will demonstrate that after initial placement of the devices, ambient sounds, such as human speech, music, footsteps, finger snaps, hand claps, or coughs and sneezes, can be used to autonomously resolve the spatial relative arrangement of devices, such as mobile phones, using trigonometric bounds and successive approximation.\n",
      "\n",
      "5. id: 5390bed320f70186a0f4f761   score: 0.7243831   abstract: Vector-based amplitude panning in three dimensional sound reproduction aims to preserve both sound image direction and distance perception. While in the estimation process, the loudspeakers are supposed to place on a sphere. It is possible that this requirement cannot be met in home environment. An alternative method to estimate gain factors in vector-based amplitude panning is proposed to preserve distance perception in this study. The experiments confirm that listeners do not perceive obvious distance differences when panning and confirm the validation of the proposed method.\n",
      "\n",
      "6. id: 55323b3e45cec66b6f9d99a7   score: 0.58266914   abstract: In recent years, a great deal of research within the field of sound localization has been aimed at finding the acoustic cues that human listeners use to localize sounds and understanding the mechanisms by which they process these cues. In this paper, we propose a complementary approach by constructing an ideal-observer model, by which we mean a model that performs optimal information processing within a Bayesian context. The model considers all available spatial information contained within the acoustic signals encoded by each ear. Parameters for the optimal Bayesian model are determined based on psychoacoustic discrimination experiments on interaural time difference and sound intensity. Without regard as to how the human auditory system actually processes information, we examine the best possible localization performance that could be achieved based only on analysis of the input informa\n",
      "\n",
      "7. id: 5390a2e820f70186a0e66bba   score: 0.5814219   abstract: Traditional videoconferencing systems that work on the basis of face-to-face configurations allow the use of standard 2-channel stereo loudspeakers in order to achieve sound-image localization by the users. In contrast, immersive systems that do not constraint the users to a face-to-face viewpoint introduce new challenges in their acoustic design. This paper discusses the major problems that appear in a novel videoconferencing system in which the use of ordinary 2-channel loudspeakers is not suitable to achieve sound-image localization due to the inherent characteristics of the room space. In efforts to overcome these problems, a loudspeaker-embedded design is proposed and demonstrated with numerical simulations and experiments that show the steady-state sound field radiated by this new loudspeaker system.\n",
      "\n",
      "8. id: 5390990f20f70186a0e0ef37   score: 0.5493687   abstract: In our 2003 ICAD Paper “Optimizing the Spatial Configuration of a Seven Talker Speech Display,” we described a hybrid near-far spatial configuration that maximizes performance in a multitalker listening task. In this brief addendum, we describe our scientific motivation for that study, describe a supplementary experiment that extended the earlier work to two additional spatial configurations, and place the results of the study in the context of current and future research in audio displays.\n",
      "\n",
      "9. id: 5390a37f20f70186a0e6ce16   score: 0.54873395   abstract: A sound localization method in the proximal region is proposed, which is based on a low-cost 3D sound localization algorithm with the use of head-related transfer functions (HRTFs). The auditory parallax model is applied to the current algorithm so that more accurate HRTFs can be used for sound localization in the proximal region. In addition, head-shadowing effects based on rigid-sphere model are reproduced in the proximal region by means of a second-order IIR filter. A subjective listening test demonstrates the effectiveness of the proposed method. Embedded system implementation of the proposed method is also described claiming that the proposed method improves sound effects in the proximal region only with 5.1% increase of memory capacity and 8.3% of computational costs.\n",
      "\n",
      "10. id: 5390a2e820f70186a0e66b9a   score: 0.540742   abstract: A conventional 3D sound field reproduction system using wave field synthesis places a lot of loudspeakers around the listener. However, since such a system is very expensive and loudspeakers come into the listener's field of vision, it is very difficult to construct an audio-visual system with it. We developed and evaluated a 3D sound field reproduction system using eight loudspeakers placed at the vertex of cube and wave field synthesis. We compared the sound localization of a loudspeaker array with that of seventeen loudspeakers placed around the listener and found that their localization capabilities of twelve directions were good.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691511\n",
      "index                                        558e40710cf2af9ee80eb1a5\n",
      "title               Sub-1 V band-gap based and MOS threshold-volta...\n",
      "authors             Dalton Martini Colombo, Gilson Wirth, Sergio B...\n",
      "year                                                           2015.0\n",
      "venue                Analog Integrated Circuits and Signal Processing\n",
      "references          5390962020f70186a0df5d68;5390878320f70186a0d32269\n",
      "abstract            Portable and implantable device applications r...\n",
      "id                                                            1691511\n",
      "clustered_labels                                                    2\n",
      "Name: 1691511, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a1bc20f70186a0e566f9   score: 0.8710367   abstract: A low-voltage low power voltage reference realized in 0.35μm CMOS technology will be presented. In order to achieve two important goals of low-power high performances bandgap references realized in the newer CMOS technologies --- a low supply voltage and a small value of the temperature coefficient, a modified structure using dynamic MOS transistors (equivalent with a virtually lowering of the material bandgap) and a square-root curvature-correction will be implemented. The accuracy of the output voltage will be increased using an Offset Voltage Follower Block as PTAT voltage generator, with the advantage that matched resistors are replaced by matched transistors. The low-power operation of the circuit will be achieved by using exclusively subthreshold-operated MOS devices. Experimental results confirm the theoretical estimations, showing a minimum supply voltage of 2.5V and a temperatur\n",
      "\n",
      "2. id: 5390b86b20f70186a0f29138   score: 0.85342485   abstract: A low-voltage low power voltage reference realized in 0.35µm CMOS technology will be presented. In order to achieve two important goals of low-power high performances bandgap references realized in the newer CMOS technologies - a low supply voltage and a small value of the temperature coefficient, a modified structure using dynamic MOS transistors (equivalent with a virtually lowering of the material bandgap) and a square-root curvaturecorrection will be implemented. The accuracy of the output voltage will be increased using an Offset Voltage Follower Block as PTAT voltage generator, with the advantage that matched resistors are replaced by matched transistors. The low-power operation of the circuit will be achieved by using exclusively subthreshold-operated MOS devices. Experimental results confirm the theoretical estimations, showing a minimum supply voltage of 2.5V and a temperature c\n",
      "\n",
      "3. id: 5390b95520f70186a0f2ed1a   score: 0.83815056   abstract: A voltage reference generator with ultra-low power consumption, low voltage supply and low temperature coefficient has been proposed. An average reference voltage of 242.6mV is achieved with all the transistors working in sub threshold region. The supply voltage ranges from 0.65V to 3V and the power consumption can be as low as 9.6nW at minimum supply voltage. The TC is 3.4ppm/°C at 0.65V and 7.3ppm/°C at 3V. The power supply rejection ratio remains-42dB at 100Hz and -27dB at 1kHz. The process variation of the threshold voltage thus the voltage reference is suppressed by means of subtraction.\n",
      "\n",
      "4. id: 53908d6520f70186a0dd191d   score: 0.83748704   abstract: As the operating supply voltage for commercial CMOS devices falls below 2 V, research activities are underway to develop CMOS integrated circuits that can operate at supply voltages well under 1 V. Although dramatic power reductions can be achieved using low supply voltages in high performance applications, the increased subthreshold leakage that results when transistor threshold voltages are lowered can render some conventional logic circuit styles unusable. Furthermore, some low voltage circuits are not robust when faced with normal variations in threshold voltage. This paper examines the design considerations for logic and memory circuits in very low voltage CMOS, and compares simulated behavior with measurements of fabricated test circuits. These circuit examples were chosen because they illustrate the unique design challenges of low voltage CMOS.\n",
      "\n",
      "5. id: 5390ada620f70186a0ec2dfe   score: 0.8206132   abstract: A CMOS bandgap reference (BGR) capable of operating at supply voltage of 0.9V was proposed in this paper. To guarantee its normal operation at ultra low supply voltage, the substrate bias techniques is used to reduce the threshold voltage. Meanwhile, an amplifier biased by subthreshold current is designed. This circuit is designed in tsmc 0.35μm CMOS technology. An average reference voltage of 658mV with a temperature coefficient of 18.5 ppm/_ is achieved.\n",
      "\n",
      "6. id: 5390a5dc20f70186a0e7f633   score: 0.81331253   abstract: A low-voltage bandgap reference source design in CMOS AMS 0.35μm is presented. For establishing a voltage reference of less than 1V, the current-mode technique is used. The current consumption of the proposed design is 84μA; the supply voltage is 1.2V, and presents a small die area of 0.015mm2. The average measurement results are: temperature coefficient of 114ppm/°C between [-25°C to 100°C], output voltage of 718mV and line regulation of 1.7mV/V.\n",
      "\n",
      "7. id: 5390985d20f70186a0e06fe2   score: 0.80942667   abstract: This paper describes a bandgap architecture with 0.35 and 0.55 V reference voltages that operates at a supply voltage below 0.8 V--the lowest yet reported for 0.6 um standard CMOS technology. The technique uses a well-known self-biased low-supply-voltage (PTAT) regulator and a novel current-controlled summing regulator. Both are combined into the new self-regulating current and voltage-mode architecture. The proposed architecture solves the problem of a wide operating-temperature range (驴50 to 160°C).Two references have been processed: the first has vertical bipolar devices incorporated into a CMOS topology and the second is a pure CMOS solution. The temperature coefficient (TC), measured on 20 samples from one wafer, ranges from 7 to 20 ppm/°C and 20 to 38 ppm/°C respectively in the temperature range from 驴40 to 120°C, without trimming the TC.Good matching was found between the samples.\n",
      "\n",
      "8. id: 53909eef20f70186a0e353b5   score: 0.8003801   abstract: We propose a new bandgap reference topology for supply voltages as low as one diode drop (~0.8V). In conventional low-voltage references, supply voltage is limited by the generated reference voltage. Also, the proposed topology generates the reference voltage at the output of the feedback amplifier. This eliminates the need for an additional output buffer, otherwise required in conventional topologies. With the bandgap core biased from the reference voltage, the new topology is also suitable for a low-voltage shunt reference. We fabricated a 1V, 0.35mV/潞C reference occupying 0.013mm2 in a 90nm CMOS process.\n",
      "\n",
      "9. id: 5390980720f70186a0e02508   score: 0.78032786   abstract: A design of a low voltage bandgap reference (BGR), for SOC application with high accuracy, is described and the chip layout is presented. To compensate the error caused by the current-mirror mismatch, the chopper modulator is used in the BGR to decrease the offset of the opamp. The design features a reference voltage of 0.256V~0.768V with supply voltage range from 1V to 1.5V and temperature range from -20 °C to 60 °C. The maximum supply current is 4 µ A and the area of layout is 0.3 脳 0.4mm虏 with a standard 0.25µm 2P5M n-well CMOS process.\n",
      "\n",
      "10. id: 5390aeba20f70186a0ecbb7e   score: 0.76838577   abstract: A low-power current reference circuit was developed in a 0.35-μm standard CMOS process. The proposed circuit utilizes an offset-voltage generation subcircuit consisting of sub-threshold MOS resistor ladder and generates temperature compensated reference current. Experimental results demonstrated that the proposed circuit generated a 95-nA reference current, and that the total power dissipation was 586 nW. The temperature coefficient of the reference current can be kept small within 523ppm/°C in a temperature range from -20 to 100°C.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707897\n",
      "index                                        55323b9345cec66b6f9da334\n",
      "title               A stable bio-inspired resource assignment stra...\n",
      "authors                                               Pejman Goudarzi\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Computational Vision ...\n",
      "references          5390878e20f70186a0d3959c;539087a520f70186a0d48...\n",
      "abstract            Nature-inspired methods have attracted many at...\n",
      "id                                                            1707897\n",
      "clustered_labels                                                    2\n",
      "Name: 1707897, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bfa220f70186a0f5446f   score: 0.98707944   abstract: Many difficult engineering problems have found natural solutions which have been inspired from biological behaviors of the living kinds. Some important examples include neural networks, Genetic Algorithm GA, DNA computing, artificial immune systems etc. Fair resource allocation strategies which are developed by many researchers are based on solving a form of constrained optimization problem. However, they are not necessarily lead to high-speed and stable solutions. There are plenty of high-speed fair rate allocation methods in the literature, some of them are based on fuzzy controllers for improving the convergence speed and are not necessarily optimal. Hence, in the current research, the GA has been adopted for finding the optimum membership functions which must be used in the fuzzy controller. Stability analysis is presented to guarantee the convergence property of the algorithm. After\n",
      "\n",
      "2. id: 5390b19020f70186a0edfce2   score: 0.6729768   abstract: Artificial Immune Systems (AIS) have attracted enormous attention among researchers because the algorithms are able to improve global searching ability and efficiency. Nevertheless, the rate of convergence for AIS is relatively slow compared to other metaheuristic algorithms. On the other hand, genetic algorithms (GAs) and particle swarm optimization (PSO) have been used successfully in solving optimization problems, although they tend to converge prematurely. Therefore, the good attributes of AIS and PSO are merged in order to reduce this limitation. It is observed that the proposed hybrid AIS (HAIS) achieved better performances in terms of convergence rate, accuracy, and stability against GA and AIS by comparing the optimization results of the mathematical functions. A similar result was achieved by HAIS in the engineering problem when compared to GA, PSO, and AIS.\n",
      "\n",
      "3. id: 558b3df1612c41e6b9d4765a   score: 0.61277324   abstract: The purpose of the current research is to introduce a novel heuristic natural inspired optimisation algorithm based on the annual migration of salmons and common the menaces that lie behind their pathways. This simulation provides a powerful tool for optimising complex multi-dimensional and multi-modal problems. For demonstrating the high robustness and acceptable quality of the great salmon run TGSR, it is compared with some well-known optimisation techniques such as genetic algorithm GA, particle swarm optimisation PSO and artificial bee colony ABC. Simulated experiments are conducted on several benchmark problems and one real-life engineering problem. The obtained results confirm the high performance of the proposed method in both robustness and quality for different optimisation problems.\n",
      "\n",
      "4. id: 5390aa7620f70186a0eaa0a2   score: 0.46362638   abstract: Genetic Algorithm is an important optimization technique, though its application in Fuzzy system is usually limited by problems like local optimal and premature convergence. With an aim to improve the performance of simple Genetic Algorithm, we propose a multi-population genetic algorithm MP-GA which uses two populations collaborating with each other, and apply it to fuzzy controller design to optimize its control rules. The simulation results of Inverted Pendulum demonstrate the effectiveness of this proposed method.\n",
      "\n",
      "5. id: 5390a40520f70186a0e6f33b   score: 0.43896556   abstract: In recent years, various heuristic optimization methods have been developed. Many of these methods are inspired by swarm behaviors in nature. In this paper, a new optimization algorithm based on the law of gravity and mass interactions is introduced. In the proposed algorithm, the searcher agents are a collection of masses which interact with each other based on the Newtonian gravity and the laws of motion. The proposed method has been compared with some well-known heuristic search methods. The obtained results confirm the high performance of the proposed method in solving various nonlinear functions.\n",
      "\n",
      "6. id: 5390a6b120f70186a0e85a5c   score: 0.36556992   abstract: Modern metaheuristic algorithms such as bee algorithms and harmony search start to demonstrate their power in dealing with tough optimization problems and even NP-hard problems. This book reviews and introduces the state-of-the-art nature-inspired metaheuristic algorithms in optimization, including genetic algorithms, bee algorithms, particle swarm optimization, simulated annealing, ant colony optimization, harmony search, and firefly algorithms. We also briefly introduce the photosynthetic algorithm, the enzyme algorithm, and Tabu search. Worked examples with implementation have been used to show how each algorithm works. This book is thus an ideal textbook for an undergraduate and/or graduate course. As some of the algorithms such as the harmony search and firefly algorithms are at the forefront of current research, this book can also serve as a reference book for researchers.\n",
      "\n",
      "7. id: 5390ad5620f70186a0ebef37   score: 0.3617282   abstract: Modern metaheuristic algorithms such as particle swarm optimization and cuckoo search start to demonstrate their power in dealing with tough optimization problems and even NP-hard problems. This book reviews and introduces the state-of-the-art nature-inspired metaheuristic algorithms for global optimization, including ant and bee algorithms, bat algorithm, cuckoo search, differential evolution, firefly algorithm, genetic algorithms, harmony search, particle swarm optimization, simulated annealing and support vector machines. In this revised edition, we also include how to deal with nonlinear constraints. Worked examples with implementation have been used to show how each algorithm works. This book is thus an ideal textbook for an undergraduate and/or graduate course as well as for self study. As some of the algorithms such as the cuckoo search and firefly algorithms are at the forefront \n",
      "\n",
      "8. id: 5390b95420f70186a0f2d88a   score: 0.3484234   abstract: Gravitational search algorithms (GSAs) yield high performances in solving optimization problems, but require time-consuming computations for the total force on each mass which makes the speed of optimization very low. However, replacing a GSA's sequential approach with a multiagent system could improve a GSA's speed considerably while maintaining the high performance level.\n",
      "\n",
      "9. id: 5390b0ca20f70186a0edb4ab   score: 0.3198645   abstract: This paper deals with a new stochastic heuristic searching algorithm inspired by the fundamental biological principles of survival. It presents a very promising version of a commonly known genetic algorithm denoted as GAHC and an algorithm denoted as HC12. Global optimization properties of these algorithms are illustrated with several nonlinear optimization problems. These problems are also solved by sophisticated solvers in general algebraic modelling system to increase objectivity and to compare different methods. Presented optimization algorithms are implemented in our own optimization toolbox GATE in Matlab environment.\n",
      "\n",
      "10. id: 5390b9d520f70186a0f32165   score: 0.31668615   abstract: This paper deals with a new stochastic heuristic searching algorithm inspired by the fundamental biological principles of survival. It presents a very promising version of a commonly known genetic algorithm denoted as GAHC and an algorithm denoted as HC12. Global optimization properties of these algorithms are illustrated with several nonlinear optimization problems. These problems are also solved by sophisticated solvers in general algebraic modelling system to increase objectivity and to compare different methods. Presented optimization algorithms are implemented in our own optimization toolbox GATE in Matlab environment.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1700489\n",
      "index                                        5591e5dd0cf2ba339a6a49d8\n",
      "title                   On the complexity of the BKW algorithm on LWE\n",
      "authors             Martin R. Albrecht, Carlos Cid, Jean-Charles F...\n",
      "year                                                           2015.0\n",
      "venue                                 Designs, Codes and Cryptography\n",
      "references          53908d6520f70186a0dd1b3f;5390b4c320f70186a0efd...\n",
      "abstract            This work presents a study of the complexity o...\n",
      "id                                                            1700489\n",
      "clustered_labels                                                    2\n",
      "Name: 1700489, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bb1d20f70186a0f3d438   score: 0.9593071   abstract: We show that the Learning with Errors (LWE) problem is classically at least as hard as standard worst-case lattice problems. Previously this was only known under quantum reductions. Our techniques capture the tradeoff between the dimension and the modulus of LWE instances, leading to a much better understanding of the landscape of the problem. The proof is inspired by techniques from several recent cryptographic constructions, most notably fully homomorphic encryption schemes.\n",
      "\n",
      "2. id: 553e6e9c0cf2cadf4a069411   score: 0.9552357   abstract: Most lattice-based cryptographic schemes are built upon the assumed hardness of the Short Integer Solution (SIS) and Learning With Errors (LWE) problems. Their efficiencies can be drastically improved by switching the hardness assumptions to the more compact Ring-SIS and Ring-LWE problems. However, this change of hardness assumptions comes along with a possible security weakening: SIS and LWE are known to be at least as hard as standard (worst-case) problems on euclidean lattices, whereas Ring-SIS and Ring-LWE are only known to be as hard as their restrictions to special classes of ideal lattices, corresponding to ideals of some polynomial rings. In this work, we define the Module-SIS and Module-LWE problems, which bridge SIS with Ring-SIS, and LWE with Ring-LWE, respectively. We prove that these average-case problems are at least as hard as standard lattice problems restricted to module\n",
      "\n",
      "3. id: 5390ac1820f70186a0eb479c   score: 0.9552357   abstract: In this survey we describe the Learning with Errors (LWE) problem, discuss its properties, its hardness, and its cryptographic applications.\n",
      "\n",
      "4. id: 5390b24420f70186a0ee7f78   score: 0.9546474   abstract: We present a fully homomorphic encryption scheme that is based solely on the(standard) learning with errors (LWE) assumption. Applying known results on LWE, the security of our scheme is based on the worst-case hardness of ``short vector problems'' on arbitrary lattices. Our construction improves on previous works in two aspects:\\begin{enumerate}\\item We show that ``somewhat homomorphic'' encryption can be based on LWE, using a new {\\em re-linearization} technique. In contrast, all previous schemes relied on complexity assumptions related to ideals in various rings. \\item We deviate from the \"squashing paradigm'' used in all previous works. We introduce a new {\\em dimension-modulus reduction} technique, which shortens the cipher texts and reduces the decryption complexity of our scheme, {\\em without introducing additional assumptions}. \\end{enumerate}Our scheme has very short cipher text\n",
      "\n",
      "5. id: 5390a54620f70186a0e77c41   score: 0.90878   abstract: Our main result is a reduction from worst-case lattice problems such as GapSVP and SIVP to a certain learning problem. This learning problem is a natural extension of the “learning from parity with error” problem to higher moduli. It can also be viewed as the problem of decoding from a random linear code. This, we believe, gives a strong indication that these problems are hard. Our reduction, however, is quantum. Hence, an efficient solution to the learning problem implies a quantum algorithm for GapSVP and SIVP. A main open question is whether this reduction can be made classical (i.e., nonquantum). We also present a (classical) public-key cryptosystem whose security is based on the hardness of the learning problem. By the main result, its security is also based on the worst-case quantum hardness of GapSVP and SIVP. The new cryptosystem is much more efficient than previous lattice-based\n",
      "\n",
      "6. id: 5390a28020f70186a0e61cc6   score: 0.906156   abstract: Building upon a famous result due to Ajtai, we propose a sequence of lattice bases with growing dimension, which can be expected to be hard instances of the shortest vector problem (SVP) and which can therefore be used to benchmark lattice reduction algorithms.The SVPis the basis of security for potentially post-quantum cryptosystems. We use our sequence of lattice bases to create a challenge, which may be helpful in determining appropriate parameters for these schemes.\n",
      "\n",
      "7. id: 558af6bb612c41e6b9d3eca9   score: 0.8891444   abstract: In this paper we propose the first provably secure public key encryption scheme based on the Learning with Errors (LWE) problem, in which secrets and errors are sampled uniformly at random from a relatively small set rather than from the commonly used discrete Gaussian distribution. Using a uniform distribution, instead of a Gaussian, has the potential of improving computational efficiency a great deal due to its simplicity, thus making the scheme attractive for use in practice. At the same time our scheme features the strong security guarantee of being based on the hardness of worst-case lattice problems. After presenting the construction of our scheme we prove its security and propose asymptotic parameters. Finally, we compare our scheme on several measures to one of the most efficient LWE-based encryption schemes with Gaussian noise. We show that the expected efficiency improvement is\n",
      "\n",
      "8. id: 5390b44620f70186a0ef98da   score: 0.883236   abstract: The “learning with errors” (LWE) problem is to distinguish random linear equations, which have been perturbed by a small amount of noise, from truly uniform ones. The problem has been shown to be as hard as worst-case lattice problems, and in recent years it has served as the foundation for a plethora of cryptographic applications. Unfortunately, these applications are rather inefficient due to an inherent quadratic overhead in the use of LWE. A main open question was whether LWE and its applications could be made truly efficient by exploiting extra algebraic structure, as was done for lattice-based hash functions (and related primitives). We resolve this question in the affirmative by introducing an algebraic variant of LWE called ring-LWE, and proving that it too enjoys very strong hardness guarantees. Specifically, we show that the ring-LWE distribution is pseudorandom, assuming that \n",
      "\n",
      "9. id: 5390bded20f70186a0f48221   score: 0.883236   abstract: The “learning with errors” (LWE) problem is to distinguish random linear equations, which have been perturbed by a small amount of noise, from truly uniform ones. The problem has been shown to be as hard as worst-case lattice problems, and in recent years it has served as the foundation for a plethora of cryptographic applications. Unfortunately, these applications are rather inefficient due to an inherent quadratic overhead in the use of LWE. A main open question was whether LWE and its applications could be made truly efficient by exploiting extra algebraic structure, as was done for lattice-based hash functions (and related primitives). We resolve this question in the affirmative by introducing an algebraic variant of LWE called ring-LWE, and proving that it too enjoys very strong hardness guarantees. Specifically, we show that the ring-LWE distribution is pseudorandom, assuming that \n",
      "\n",
      "10. id: 53908b9320f70186a0dbfce2   score: 0.87418467   abstract: We survey some recent developments in the study of the complexity of lattice problems. After a discussion of some problems on lattices which can be algorithmically solved efficiently, our main focus is the recent progress on complexity results of intractability. We will discuss Ajtai's worst-case/ average-case connections, NP-hardness and non-NP-hardness, transference theorems between primal and dual lattices, and the Ajtai-Dwork cryptosystem.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672801\n",
      "index                                        55913b2e0cf232eb904fb5e1\n",
      "title                 Can Deep Learning Revolutionize Mobile Sensing?\n",
      "authors                              Nicholas D. Lane, Petko Georgiev\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 16th International Workshop...\n",
      "references          558b98b7612c6b62e5e8c634;558b20a4612c41e6b9d44...\n",
      "abstract            Sensor-equipped smartphones and wearables are ...\n",
      "id                                                            1672801\n",
      "clustered_labels                                                    3\n",
      "Name: 1672801, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539089ab20f70186a0d9503e   score: 0.950411   abstract: Although mobile devices keep getting smaller and more powerful, their interface with the user is still based on that of the regular desktop computer. This implies that interaction is usually tedious, while interrupting the user is not really desired in ubiquitous computing. We propose adding an array of hardware sensors to the system that, together with machine learning techniques, make the device aware of its context while it is being used. The goal is to make it learn the context-descriptions from its user on the spot, while minimising user-interaction and maximising reliability.\n",
      "\n",
      "2. id: 5390ba3820f70186a0f36725   score: 0.9184803   abstract: Mobile phones are quickly becoming the primary source for social, behavioral, and environmental sensing and data collection. Today's smartphones are equipped with increasingly more sensors and accessible data types that enable the collection of literally dozens of signals related to the phone, its user, and its environment. A great deal of research effort in academia and industry is put into mining this raw data for higher level sense-making, such as understanding user context, inferring social networks, learning individual features, and behavior prediction. In this work we investigate the properties of learning and inferences of real world data collected via mobile phones. In particular, we look at the dynamic learning process over time with various sizes of sampling groups and examine the interplay between these two parameters. We validate our model using extensive simulations carried \n",
      "\n",
      "3. id: 53908cde20f70186a0dcd48f   score: 0.91102153   abstract: We apply machine-learning techniques to the task of context-awareness, or inferring aspects of the user's state given a stream of inputs from sensors worn by the person. We focus on the task of indoor navigation, and show that by integrating information from accelerometers, magnetometers, temperature and light sensors, we can collect enough information to infer the user's location. However, our navigation algorithm performs very poorly, with almost 50% error, if we use only the raw sensor signals. Instead, we introduce a ``data cooking'' module that computes appropriate high-level features from the raw sensor data. By introducing these high-level features, we are able to reduce the error rate to 2% in our example environment.\n",
      "\n",
      "4. id: 5390bd1520f70186a0f449ad   score: 0.90414387   abstract: With the proliferation of sensor-enabled smartphones, significant attention has been attracted to develop sensing-driven mobile systems. Current research on sensing-driven mobile systems can be classified into two categories, based on the purpose of sensing. In the first category, smartphones are used to sense personal context information, such as locations, activities, and daily habits to enable applications such as location-aware systems and virtual reality systems. In the second category, smartphones are exploited to collect sensing data of the physical world and enable applications including traffic monitoring, environmental monitoring, and others. As smartphones become blossomed in popularity and ubiquity, new problems have emerged in both categories of mobile sensing systems. In this thesis, we investigate three core challenges by answering the following fundamental questions: firs\n",
      "\n",
      "5. id: 5390bda020f70186a0f46176   score: 0.8958876   abstract: In recent years we have seen the emergence of context-aware mobile sensing apps which employ machine learning algorithms on real-time sensor data to infer user behaviors and contexts. These apps are typically optimized for power and performance on the app processors of mobile platforms. However, modern mobile platforms are sophisticated system on chips (SoCs) where the main app processors are complemented by multiple co-processors. Recently chip vendors have undertaken nascent efforts to make these previously hidden co-processors such as the digital signal processors (DSPs) programmable. In this paper, we explore the energy and performance implications of off-loading the computation associated with machine learning algorithms in context-aware apps to DSPs embedded in mobile SoCs. Our results show a 17% reduction in a TI OMAP4 based mobile platform's energy usage from off-loading context \n",
      "\n",
      "6. id: 5390b63320f70186a0f16f71   score: 0.87599933   abstract: We propose ACE (Acquisitional Context Engine), a middleware that supports continuous context-aware applications while mitigating sensing costs for inferring contexts. ACE provides user's current context to applications running on it. In addition, it dynamically learns relationships among various context attributes (e.g., whenever the user is Driving, he is not AtHome). ACE exploits these automatically learned relationships for two powerful optimizations. The first is inference caching that allows ACE to opportunistically infer one context attribute (AtHome) from another already-known attribute (Driving), without acquiring any sensor data. The second optimization is speculative sensing that enables ACE to occasionally infer the value of an expensive attribute (e.g., AtHome) by sensing cheaper attributes (e.g., Driving). Our experiments with two real context traces of 105 people and a Wind\n",
      "\n",
      "7. id: 5390bb1d20f70186a0f3e1c1   score: 0.8727817   abstract: Smartphones are excellent mobile sensing platforms, with the microphone in particular being exercised in several audio inference applications. We take smartphone audio inference a step further and demonstrate for the first time that it's possible to accurately estimate the number of people talking in a certain place -- with an average error distance of 1.5 speakers -- through unsupervised machine learning analysis on audio segments captured by the smartphones. Inference occurs transparently to the user and no human intervention is needed to derive the classification model. Our results are based on the design, implementation, and evaluation of a system called Crowd++, involving 120 participants in 10 very different environments. We show that no dedicated external hardware or cumbersome supervised learning approaches are needed but only off-the-shelf smartphones used in a transparent manne\n",
      "\n",
      "8. id: 558b19eb612c41e6b9d434bb   score: 0.86726075   abstract: A deep neural network (DNN) pre-trained via stacking restricted Boltzmann machines (RBMs) demonstrates high performance. The binary RBM is usually used to construct the DNN. However, a continuous probability of each node is used as real value state, although the state of the binary RBM's node should be represented by a random binary variable. One of main reasons of this abuse is that it works. One of others is to reduce a computational cost. In this paper, we propose a novel inference of the RBM, considering that the input of the RBM is the random binary variable. Straight forward derivation of the proposed inference is intractable. Then, we also propose the closed-form approximation of it. We convince that the proposed inference is more reasonable than a conventional algorithm of the RBM. Experimental comparisons demonstrate that the proposed inference improves the performance of the DN\n",
      "\n",
      "9. id: 558b3639612c41e6b9d46cfb   score: 0.8645393   abstract: Modern sensor-equipped smartphones have attracted significant research interest in the pervasive community for recognizing and creating context-aware applications at a personal or community scale level. In this paper, we propose a proof of concept Do-Not-Disturb (DND) service that can a) determine a user's context relevant for DND service from the built-in smartphone sensors and b) correctly predict the DND status based on the given context such as being in a meeting, sleeping, or working at the office. In this preliminary study, we investigate whether sensor data can be clustered to represent user contexts. We use standard machine learning techniques to learn the relationship between a user's context and the corresponding DND status (available or unavailable). Given a user's current context, the DND service predicts a DND status and configures the mobile device accordingly. Our prelimin\n",
      "\n",
      "10. id: 5390bb7b20f70186a0f4002a   score: 0.8596637   abstract: Mobile applications can offer improved user experience through the use of novel modalities and user context. However, these new input dimensions often require recognition-based techniques, with which mobile app developers or designers may not be familiar. Furthermore, the recruiting, data collection and labeling, necessary for using these techniques, are usually time-consuming and expensive. We present CrowdLearner, a framework based on crowdsourcing to automatically generate recognizers using mobile sensor input such as accelerometer or touchscreen readings. CrowdLearner allows a developer to easily create a recognition task, distribute it to the crowd, and monitor its progress as more data becomes available. We deployed CrowdLearner to a crowd of 72 mobile users over a period of 2.5 weeks. We evaluated the system by experimenting with 6 recognition tasks concerning motion gestures, tou\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1713520\n",
      "index                                        55323c2b45cec66b6f9db60a\n",
      "title               A novel hybrid artificial bee colony algorithm...\n",
      "authors             Xiaohui Yan, Yunlong Zhu, Hanning Chen, Hao Zhang\n",
      "year                                                           2015.0\n",
      "venue                     Natural Computing: an international journal\n",
      "references                                   558f7bd20cf2cb5aa7674262\n",
      "abstract            Artificial Bee Colony (ABC) algorithm is one o...\n",
      "id                                                            1713520\n",
      "clustered_labels                                                    1\n",
      "Name: 1713520, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558f7bd20cf2cb5aa7674262   score: 0.9905874   abstract: Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms. ABC simulates the intelligent foraging behaviour of a honeybee swarm. In this work, ABC is used for optimizing a large set of numerical test functions and the results produced by ABC algorithm are compared with the results obtained by genetic algorithm, particle swarm optimization algorithm, differential evolution algorithm and evolution strategies. Results show that the performance of the ABC is better than or similar to those of other population-based algorithms with the advantage of employing fewer control parameters.\n",
      "\n",
      "2. id: 5390b7fe20f70186a0f25b3a   score: 0.9896718   abstract: The artificial bee colony (ABC) algorithm is a swarm intelligence optimization algorithm inspired by the intelligent foraging behavior of honeybees. ABC algorithm gets the new solution by searching the neighborhood of the current solution in the search process and the scope searched is small, which leads to slow convergence and easily gets stuck to the local optimal solution. In this paper, an improved ABC algorithm is proposed based on multi-exchange neighborhood (MNABC) by exchanging neighborhood in the search process. The simulation experiment comparing MNABC with the basic ABC and PSO algorithms, shows that the proposed method can improve the convergence speed and global searching capability of ABC algorithm.\n",
      "\n",
      "3. id: 53909f6a20f70186a0e3b3dd   score: 0.98662347   abstract: Artificial bee colony (ABC) algorithm is an optimization algorithm based on a particular intelligent behaviour of honeybee swarms. This work compares the performance of ABC algorithm with that of differential evolution (DE), particle swarm optimization (PSO) and evolutionary algorithm (EA) for multi-dimensional numeric problems. The simulation results show that the performance of ABC algorithm is comparable to those of the mentioned algorithms and can be efficiently employed to solve engineering problems with high dimensionality.\n",
      "\n",
      "4. id: 5390c04b20f70186a0f5873d   score: 0.98651993   abstract: Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms used in optimization problems. ABC simulates the intelligent foraging behavior of a honeybee swarm. In this paper, two aspects of ABC algorithm are modified and new configurations are used. The modified versions are tested on some well-known benchmark functions. Results show that the new changes have positive effects on the performance of ABC algorithm.\n",
      "\n",
      "5. id: 5390b44620f70186a0ef8dd0   score: 0.98228765   abstract: Swarm intelligence is a research field that models the collective intelligence in swarms of insects or animals. Many algorithms that simulates these models have been proposed in order to solve a wide range of problems. The Artificial Bee Colony algorithm is one of the most recent swarm intelligence based algorithms which simulates the foraging behaviour of honey bee colonies. In this work, modified versions of the Artificial Bee Colony algorithm are introduced and applied for efficiently solving real-parameter optimization problems.\n",
      "\n",
      "6. id: 5390b20120f70186a0ee5489   score: 0.98024315   abstract: Artificial bee colony algorithm (ABC), which is inspired by the foraging behavior of honey bee swarm, is a biological-inspired optimization. It shows more effective than genetic algorithm (GA), particle swarm optimization (PSO) and ant colony optimization (ACO). However, ABC is good at exploration but poor at exploitation, and its convergence speed is also an issue in some cases. For these insufficiencies, we propose an improved ABC algorithm called I-ABC. In I-ABC, the best-so-far solution, inertia weight and acceleration coefficients are introduced to modify the search process. Inertia weight and acceleration coefficients are defined as functions of the fitness. In addition, to further balance search processes, the modification forms of the employed bees and the onlooker ones are different in the second acceleration coefficient. Experiments show that, for most functions, the I-ABC has \n",
      "\n",
      "7. id: 5390baa120f70186a0f39d71   score: 0.9768908   abstract: An new swarm intelligence algorithm based on the behavior of honeybee swarms has been proposed for some years, namely, Artificial Bee Colony(ABC). In this work, ABC is used for optimizing a set of split in a given integration interval by simulating the foraging activities of bees and reflecting the changing trends of the integrand. At the same time, this work also compares the performance of ABC algorithm with that of Trapezoidal, Simpson, Differential Evolution(DE), particle swarm optimization(PSO) for numerical integration. The simulation results show that the mentioned algorithm for numerical integration outperforms the other algorithm.\n",
      "\n",
      "8. id: 555b70f80cf2cb3a163d38cd   score: 0.97622   abstract: Graphical abstractDisplay Omitted Highlights31 components of ABC algorithm are tested with a systematic experimental study.Each component impact on algorithm performance is identified.Two benchmark sets, SOCO and CEC05, are used in experimental study.Two new variants of ABC algorithms are proposed for each of the two benchmark sets.The best components are selected for each step of the proposed ABC algorithms. The artificial bee colony (ABC) algorithm is a swarm intelligence algorithm inspired by the intelligent foraging behavior of a honeybee swarm. In recent years, several ABC variants that modify some components of the original ABC algorithm have been proposed. Although there are some comparison studies in the literature, the individual contribution of each proposed modification is often unknown. In this paper, the proposed modifications are tested with a systematic experimental study \n",
      "\n",
      "9. id: 53909f8220f70186a0e3c36b   score: 0.9759923   abstract: Swarm intelligence is a research branch that models the population of interacting agents or swarms that are able to self-organize. An ant colony, a flock of birds or an immune system is a typical example of a swarm system. Bees' swarming around their hive is another example of swarm intelligence. Artificial Bee Colony (ABC) Algorithm is an optimization algorithm based on the intelligent behaviour of honey bee swarm. In this work, ABC algorithm is used for optimizing multivariable functions and the results produced by ABC, Genetic Algorithm (GA), Particle Swarm Algorithm (PSO) and Particle Swarm Inspired Evolutionary Algorithm (PS-EA) have been compared. The results showed that ABC outperforms the other algorithms.\n",
      "\n",
      "10. id: 5390bb7b20f70186a0f3f7e9   score: 0.97334224   abstract: In recent years, swarm intelligence has proven its importance for the solution of those problems that cannot be easily dealt with classical mathematical techniques. The foraging behaviour of honey bees produces an intelligent social behaviour and falls in the category of swarm intelligence. Artificial bee colony ABC algorithm is a simulation of honey bee foraging behaviour, established by Karaboga in 2005. Since its inception, a lot of research has been carried out to make ABC more efficient and to apply it on different types of problems. This paper presents a review on ABC developments, applications, comparative performance and future research perspectives.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707466\n",
      "index                                        559124bf0cf232eb904faf78\n",
      "title               Captchat: A Messaging Tool to Frustrate Ubiqui...\n",
      "authors             Paul Dunphy, Johannes Schöning, James Nicholso...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558b051e612c41e6b9d40866;5390b13020f70186a0ede...\n",
      "abstract            There is currently a widespread uncertainty re...\n",
      "id                                                            1707466\n",
      "clustered_labels                                                    0\n",
      "Name: 1707466, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b19020f70186a0ee0459   score: 0.7606507   abstract: We carry out a systematic study of existing visual CAPTCHAs based on distorted characters that are augmented with anti-segmentation techniques. Applying a systematic evaluation methodology to 15 current CAPTCHA schemes from popular web sites, we find that 13 are vulnerable to automated attacks. Based on this evaluation, we identify a series of recommendations for CAPTCHA designers and attackers, and possible future directions for producing more reliable human/computer distinguishers.\n",
      "\n",
      "2. id: 5390aefc20f70186a0ecd3ed   score: 0.2148561   abstract: Captchas are a standard defense on commercial websites against undesirable or malicious Internet bot programs, but widely deployed schemes can be broken with simple but novel attacks. Applying security engineering expertise to the design of Captchas can significantly improve their robustness.\n",
      "\n",
      "3. id: 5390b44620f70186a0ef88e0   score: 0.19852993   abstract: We present an implementation of CAPTCHA image generation as a REST-style web service, currently available at http://captchaservice.org. We argue that CAPTCHA generation is well suited to a web services approach, particularly one powered by open-source code, and discuss techniques for using such a service to protect weblogs from comment spam attacks. We describe the captchservice.org API by example, and detail the workings of the two image-distortion techniques that the service offers. We also discuss accessibility objections to visual CAPTCHAs, describe our early attempts at non-visual alternatives, and summarize future development directions.\n",
      "\n",
      "4. id: 53909f8220f70186a0e3c64b   score: 0.18359363   abstract: CAPTCHAs are widely used by websites for security and privacy purposes. However, traditional text-based CAPTCHAs are not suitable for individuals with visual impairments. We proposed and developed a new form of CAPTCHA that combines both visual and audio information to allow easy access by users with visual impairments. A preliminary evaluation suggests strong potential for the new form of CAPTCHA for both blind and visual users.\n",
      "\n",
      "5. id: 5390ada620f70186a0ec3c11   score: 0.17147702   abstract: CAPTCHA is a technique that is used to prevent automatic programs from being able to acquire free e-mail or online service accounts. However, as many researchers have already reported, conventional CAPTCHA could be overcome by state-of-the-art malware since the capabilities of computers are approaching those of humans. Therefore, CAPTCHA should be based on even more advanced human-cognitive-processing abilities. In addition, it is also important to keep in mind that answering CAPTCHA is an added annoyance for users, who feel troublesome to prove that they are human at every Web access. So, CAPTCHA should be enjoyable for users. To cope with these issues, this paper focuses on using the human ability to understand humor, which represents the ultimate in human cognitive processing abilities, and propose a new CAPTCHA that uses four-panel cartoons.\n",
      "\n",
      "6. id: 5390aca920f70186a0eb94b0   score: 0.15649123   abstract: We systematically study the design of image recognition CAPTCHAs (IRCs) in this paper. We first review and examine all existing IRCs schemes and evaluate each scheme against the practical requirements in CAPTCHA applications, particularly in large-scale real-life applications such as Gmail and Hotmail. Then we present a security analysis of the representative schemes we have identified. For the schemes that remain unbroken, we present our novel attacks. For the schemes for which known attacks are available, we propose a theoretical explanation why those schemes have failed. Next, we provide a simple but novel framework for guiding the design of robust IRCs. Then we propose an innovative IRC called Cortcha that is scalable to meet the requirements of large-scale applications. It relies on recognizing objects by exploiting the surrounding context, a task that humans can perform well but co\n",
      "\n",
      "7. id: 539098dd20f70186a0e0eaf8   score: 0.1491592   abstract: We propose IMAGINATION (IMAge Generation for INternet AuthenticaTION), a system for the generation of attack-resistant, user-friendly, image-based CAPTCHAs. In our system, we produce controlled distortions on randomly chosen images and present them to the user for annotation from a given list of words. The distortions are performed in a way that satisfies the incongruous requirements of low perceptual degradation and high resistance to attack by content-based image retrieval systems. Word choices are carefully generated to avoid ambiguity as well as to avoid attacks based on the choices themselves. Preliminary results demonstrate the attack-resistance and user-friendliness of our system compared to text-based CAPTCHAs.\n",
      "\n",
      "8. id: 558b5357612c41e6b9d4940e   score: 0.14767806   abstract: CAPTCHAs have become a standard security mechanism that are used to deter automated abuse of online services intended for humans. However, many existing CAPTCHA schemes to date have been successfully broken. As such, a number of CAPTCHA developers have explored alternative methods of designing CAPTCHAs. 3D CAPTCHAs is a design alternative that has been proposed to overcome the limitations of traditional CAPTCHAs. These CAPTCHAs are designed to capitalize on the human visual system's natural ability to perceive 3D objects from an image. The underlying security assumption is that it is difficult for a computer program to identify the 3D content. This paper investigates the robustness of text-based 3D CAPTCHAs. In particular, we examine three existing text-based 3D CAPTCHA schemes that are currently deployed on a number of websites. While the direct use of Optical Character Recognition (OCR\n",
      "\n",
      "9. id: 5390b13020f70186a0edc3a7   score: 0.14511536   abstract: Completely Automatic Public Turning test to tell Computers and Humans Apart, or CAPTCHA, is a security measure that guards a system from exploitation by the discrimination between a real human being and an automated computer program via the method of presenting to the unknown user the challenges that are hard for computer yet easy for human. Focusing on text-based CAPTCHA, this study conducted an experiment to study the effect of age groups and distortion types on the CAPTCHA task. Twenty-four participants were recruited to take part in the experiment, where twelve of them were in the senior group and twelve in the young group. Participants were observed to use three general steps: recognition, rehearsal, and motor response. With the inevitability of the security measure and the increasing population of senior netizens, this study has important implications for the design of CAPTCHA syst\n",
      "\n",
      "10. id: 5390bf1320f70186a0f509a8   score: 0.14211279   abstract: A CAPTCHA is a Turing test to distinguish human users from automated scripts to defend against internet adversarial attacks. As text-based CAPTCHAs (TBC) have become increasingly difficult to solve, image-based CAPTCHAs, and particularly face recognition CAPTCHAs (FRC), offer a chance to overcome TBC limitations. In this paper, we systematically design and implement a practical FRC, informed by psychological findings. We use gray-scale and binary images, which are computationally inexpensive to generate and deploy. Furthermore, our FRC complies with CAPTCHA design guidelines, thereby ensuring its robustness.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652277\n",
      "index                                        559257470cf28b1a968ffcd0\n",
      "title               How much impact can be made in a week?: Design...\n",
      "authors                                 Grace Ngai, Stephen C.F. Chan\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 46th ACM Technical Symposiu...\n",
      "references          5390a63c20f70186a0e8168c;5390b44620f70186a0ef8...\n",
      "abstract            Service learning has been gaining attention in...\n",
      "id                                                            1652277\n",
      "clustered_labels                                                    0\n",
      "Name: 1652277, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539095ba20f70186a0df13df   score: 0.99025387   abstract: Service learning is an educational experience that enables students to apply material learned in the classroom by volunteering in a real-world situation. This paper provides a brief review of service learning and describes two models that the computer science department at Saint Anselm College implemented successfully.\n",
      "\n",
      "2. id: 5390958a20f70186a0def50f   score: 0.9811669   abstract: Service-learning is a form of experiential learning that integrates classroom concepts with related community service. Computer science faculty and students stand to reap great benefit from the integration of service-learning practice and philosophy into curricula. Many faculty are already doing so, yet computer science is not very visible in the service-learning community. Similarly, service-learning is not very visible in the computer science education community. It is imperative for those computer science faculty involved in service-learning to develop, apply and disseminate effective frameworks for integrating service learning into undergraduate computer science curricula so that its benefits may be more fully realized.\n",
      "\n",
      "3. id: 5390bded20f70186a0f496fe   score: 0.9561454   abstract: Many computer science students and faculty want to participate in service projects; these projects might include service learning in the classroom, service experiences in the local community, or service projects at a domestic or international site. Developing service projects that incorporate computer science and/or technology skills that are both interesting and meaningful often presents a challenge. During this BOF participants will discuss and answer questions about service projects in which they have participated, their successes and failures, techniques for finding appropriate partners, logistical issues, etc. This session will be helpful to faculty having no prior service experience, as well as veteran faculty who have organized multiple service trips.\n",
      "\n",
      "4. id: 5390b4da20f70186a0effce3   score: 0.9513234   abstract: Service learning is a rapidly growing pedagogy in higher education and has come under increased visibility within engineering and computer science. Service-learning has been cited as a potential tool for increasing student engagement as well as diversity among our engineering, technology and computing student bodies. It has opportunities to excite the next generation of engineers, addressing issues including retention and diversity as well as learning in our classrooms. This workshop will introduce interested faculty, students and professionals to service-learning models and will provide resources for them to implement such programs at their respective institutions.\n",
      "\n",
      "5. id: 539087fe20f70186a0d752e6   score: 0.941007   abstract: Service learning is an educational philosophy that promotes active learning through community service. We have recently applied this approach in our computer science curriculum, specifically to our software engineering course. In order that other computer science departments can benefit from our experience, we have developed a primer one can follow to establish a program for service learning in the computer sciences. We also describe and assess our experience after one year of applying service learning to software engineering.\n",
      "\n",
      "6. id: 5390a01420f70186a0e474c1   score: 0.9253649   abstract: Service Learning is rising in popularity as a way to provide practical experience for our students while contributing to the local community. Service Learning can take many forms. The purpose of this panel is to present the various types of Service Learning, and then to explore 2 of them in more depth. The panel members come from Point Loma Nazarene University where Service Learning is conducted as a year-long course, and California State University, Northridge where Service Learning was a component of another course.\n",
      "\n",
      "7. id: 53909e8b20f70186a0e2dec1   score: 0.9190633   abstract: Service learning can play an important role in computer science education: it can address declining enrollments and increase current student satisfaction. Although computer science poses some serious problems for effectively implementing service learning, these can be surmounted by reconfiguring a course to include a substantial treatment of accessibility. I describe such a course, explain how it overcomes the problems, and discuss the results of the course from a student perspective.\n",
      "\n",
      "8. id: 5390a8b220f70186a0e9c9af   score: 0.8740772   abstract: Service Learning is a program where students utilize skills gained in coursework to benefit members of the community. Students gain practical experience, and those being served gain valuable assistance. Service Learning programs range from volunteer hours added as a requirement to a traditional course to semester or year-long classes dedicated to Service Learning. Each model has pros and cons and the effectiveness of a particular model can vary based on the characteristics of and resources available at the individual university. This paper presents the evolving model of a year-long course dedicated to Service Learning and housed in the Mathematical, Information and Computer Sciences department of Point Loma Nazarene University. PLNU is a relatively small liberal arts university without an administrative department overseeing Service Learning. The course is one of three options for gainin\n",
      "\n",
      "9. id: 5390981d20f70186a0e057bc   score: 0.8446654   abstract: While service-learning is becoming more common in college curriculums, it is still noticeably absent from many computer science programs. This paper describes the concept of service-learning and its implementation into a computer science course. It then describes a successful collaboration of service-learning between students in a CS I course and inmates at a local county prison.\n",
      "\n",
      "10. id: 5390a1bc20f70186a0e54f39   score: 0.83801806   abstract: Service-Learning is the delivery of a service to the community within the context of an educational program of study. Going further, it is a form of experiential learning, applying what was learned within a classroom or laboratory setting to problems of the real world. Using service-learning, students in information technology can experience the unstructured problems of a real world situation while having the structure of a university course. While one major goal of service-learning is service, the delivery of some benefit to the community partner, the other major goal is for the students to achieve the learning that is part of the department's education objectives. It is therefore a pedagogy which should be investigated for efficacy as any other pedagogy would be. This paper investigates how information technology based service-learning could be measured for both service and learning. F\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717059\n",
      "index                                        55323c6f45cec66b6f9dc126\n",
      "title               Supporting caregivers in assisted living facil...\n",
      "authors             Sebastián Aced López, Fulvio Corno, Luigi De R...\n",
      "year                                                           2015.0\n",
      "venue                     Universal Access in the Information Society\n",
      "references                                   558fd51c612c29c89cd7b64f\n",
      "abstract            Research activities on designing healthcare su...\n",
      "id                                                            1717059\n",
      "clustered_labels                                                    0\n",
      "Name: 1717059, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b78a20f70186a0f240ff   score: 0.953188   abstract: In the article the general concept of system supporting patients, disabled and the elderly was presented. The assumptions of the system is to improve the quality of life and comfort those who are not able to function without assistance. The system consists of network infrastructure and mobile devices used by beneficiaries of support. Developing a detailed specification requires the cooperation of specialists in many fields, especially doctors, therapists, senior caregivers and disabled people themselves, because it is of critical importance to design the system according to the needs of the potential system users. Despite all the problems, it seems that the development of the abovedescribed systems is a job worth doing.\n",
      "\n",
      "2. id: 5390aca920f70186a0eb8e8a   score: 0.85512674   abstract: To identify the design issues that should be addressed for developing Ambient Information Systems (AIS) that effectively assist elderly with their ADLs (i.e. medicating), a case study was carried out to identify the kind of support that elderly may need for medicating. The proposed AIS provides the elderly with ambient aids to: remind them to medicate (Remind-Me system); guide the medication (GUIDE-Me system), and encourage elders to medicate (CARe-Me system). These AIS will be evaluated to determine their utility and the users' acceptance, which will enable us to conclude whether the identified design dimensions of AIS are appropriate to assist elderly.\n",
      "\n",
      "3. id: 5390a9a520f70186a0ea6d38   score: 0.72115445   abstract: We present the findings of an in situ field study conducted using our assisted living system, I-Living, that aims to enable seniors to live in a cost-effective manner independently. Basing the study on both interviews and diaries provided valuable and well-rounded data. Some of the main findings revealed that seniors will wear small health sensors if designed carefully. The study further reveals that delicate and complicated social structures influence the design space in such communities. The primary contribution of this paper is the pilot study conducted at an assisted living facility. It paints a compelling picture of day-to-day life in a healthcare institution and uncovers broad design implications that apply to a wide range of technologies.\n",
      "\n",
      "4. id: 5390a2be20f70186a0e644ad   score: 0.6512439   abstract: With today's technology, elderly users could be supported in living independently in their own homes for a prolonged period of time. Commercially available products enable remote monitoring of the state of the user, enhance social networks, and even support elderly citizens in their everyday routines. Whereas technology seems to be in place to support elderly users, one might question the value of present solutions in terms of solving real user problems such as loneliness and self-efficacy. Furthermore, products tend to be complex in use and do not relate to the reference framework of elderly users. Consequently, acceptability of many present solutions tends to be low. This paper presents a design vision of assisted living solutions that elderly love to use. Based on earlier work, five concrete design goals have been identified that are specific to assisted living services for elderly us\n",
      "\n",
      "5. id: 5390b4c420f70186a0efe775   score: 0.6406359   abstract: This paper describes the extension of a previously developed architecture for an Ambient Assisted Living (AAL) environment, called AmbienNet, to provide access to ubiquitous services. The former AAL system generates adaptive instructions to support elderly people at home. This extension includes an adaptive model-based user interface generator that was created for ubiquitous applications. This approach involves the extension of the supportive user interface from home supervision and support to allow access to ubiquitous applications outside the home. The present challenge is how to include adaptive supportive instructions for accessing ubiquitous services outside the home that are coherent with the previously designed home support system.\n",
      "\n",
      "6. id: 5390a40520f70186a0e6f4c3   score: 0.631026   abstract: Ambient Assisted Living (AAL) aims at supporting elderly people in their daily lives, allowing them to grow old at home. In order to provide easy remote control over the rapidly growing number of assistance services from anywhere in the apartment, many AAL environments offer a universal control device. However, the problem of structuring the numerous services for intuitive usage has not been solved satisfactorily yet. This paper introduces a spatial metaphor for universal control devices to structure available services based on the elderly person's own apartment. We carried out a study with 18 younger elderly people using a prototype to evaluate the appropriateness and acceptance of this metaphor. The results included in this paper show that this apartment metaphor is appropriate and accepted by this main target group of AAL.\n",
      "\n",
      "7. id: 5390a7f520f70186a0e93ba0   score: 0.62658125   abstract: As they age, older adult's present losses in their functional capabilities which cause them can't continue performing their activities of daily living (ADL) independently at home. We propose Ambient Information Systems (AIS) as appropriate pervasive devices to promote their independent living. Therefore our aim is to determine the utility and usability of AIS to support the independent life of older adults by helping them to perform their activities. In this paper we present preliminary results of a case study that we carried out for understanding the problems and needs that older adults face in doing some of their activities of daily living. In particular, we present results regarding the elderly problems to adhere to their medication prescription. Based on these results we propose AIS to support older adults to medicate. Finally, we present the design attributes incorporated into this \n",
      "\n",
      "8. id: 5390a5dc20f70186a0e7f54f   score: 0.606848   abstract: This paper presents a design of an innovative framework to support continuous monitoring and assistance for ageing people affected by disabilities or chronic diseases during their stay in a structured environment such as home or hospital.\n",
      "\n",
      "9. id: 5390aca920f70186a0eb9c87   score: 0.58930284   abstract: Cognitive assistance systems are aimed to support people loosing their autonomy while completing activities of daily living (ADL). The range of activities to support, the dissemination of interactive devices in the environment and the variability of users' cognitive capacities raise real challenges when designing assistance in such systems. This paper addresses the assistance issues and presents guidelines for assistive systems design. Especially, based on an experiment of the assistance provided while completing ADL, we suggest a principle of scaled assistance. The principle was applied to Archipel, a cognitive orthesis. This application and the results analysis of a 12 sujects experiment led to the evaluation and the improvement of some ergonomic guidelines about ambient assisted living.\n",
      "\n",
      "10. id: 53909a0220f70186a0e20156   score: 0.57164705   abstract: This paper reports on a study of a newly developed system for assisted living, which was implemented in the homes of seven elderly residents. Based on these findings we point out three fundamental issues that will enrich and improve the use of technology for assisted living in the home. Firstly, we argue that the technology must co-evolve with the elderly people as their needs change, thereby building on an existing familiarity with a given system or artifact. Secondly, we argue that there is a need to seriously take into account the qualities of the domestic setting in both design and deployment, and that social as well as clinical aspects must be considered when designing for assisted living. Thirdly, we argue that technology must be much easier to deploy, use and comprehend for the elder users. We then outline our future work on developing technology for assisted living.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1668408\n",
      "index                                        559256e30cf28b1a968ffcba\n",
      "title               UP-GNIV: an expeditious high utility pattern m...\n",
      "authors                 Kannimuthu Subramanian, Premalatha Kandhasamy\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Information Technolog...\n",
      "references          5591e23b0cf2ba339a6a474e;539089d220f70186a0d9a...\n",
      "abstract            Traditionally, frequent pattern mining dealt i...\n",
      "id                                                            1668408\n",
      "clustered_labels                                                    1\n",
      "Name: 1668408, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909f8c20f70186a0e40548   score: 0.96160597   abstract: Frequent pattern mining discovers patterns in transaction databases based only on the relative frequency of occurrence of items without considering their utility. For many real world applications, however, utility of itemsets based on cost, profit or revenue is of importance. The utility mining problem is to find itemsets that have higher utility than a user specified minimum. Unlike itemset support in frequent pattern mining, itemset utility does not have the anti- monotone property and so efficient high utility mining poses a greater challenge. Recent research on utility mining has been based on the candidate-generation- and-test approach which is suitable for sparse data sets with short patterns, but not feasible for dense data sets or long patterns. In this paper we propose a new algorithm called CTU-Mine that mines high utility itemsets using the pattern growth approach. We have tes\n",
      "\n",
      "2. id: 5591e23b0cf2ba339a6a474e   score: 0.9240048   abstract: Utility itemsets typically consist of items with different values such as utilities, and the aim of utility mining is to identify the itemsets with highest utilities. In the past studies on utility mining, the values of utility itemsets were considered as positive. In some applications, however, an itemset may be associated with negative item values. Hence, discovery of high utility itemsets with negative item values is important for mining interesting patterns like association rules. In this paper, we propose a novel method, namely HUINIV (High Utility Itemsets with Negative Item Values)-Mine, for efficiently and effectively mining high utility itemsets from large databases with consideration of negative item values. To the best of our knowledge, this is the first work that considers the concept of negative item values in utility mining. The novel contribution of HUINIV-Mine is that it \n",
      "\n",
      "3. id: 5390aeba20f70186a0eca45f   score: 0.9001793   abstract: In the past, many algorithms were proposed to mine association rules, most of which were based on item frequency values. Considering a customer may buy many copies of an item and each item may have different profits, mining frequent patterns from a traditional database is not suitable for some real-world applications. Utility mining was thus proposed to consider costs, profits and other measures according to user preference. In this paper, the high utility pattern tree (HUP tree) is designed and the HUP-growth mining algorithm is proposed to derive high utility patterns effectively and efficiently. The proposed approach integrates the previous two-phase procedure for utility mining and the FP-tree concept to utilize the downward-closure property and generate a compressed tree structure. Experimental results also show that the proposed approach has a better performance than Liu et al.'s t\n",
      "\n",
      "4. id: 5390985d20f70186a0e0704a   score: 0.84959716   abstract: In this paper, we present a novel algorithm for mining complete frequent itemsets. This algorithm is referred to as the TM algorithm from hereon. In this algorithm, we employ the vertical representation of a database. Transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space thus reducing the number of intersections. When the compression coefficient becomes smaller than the average number of comparisons for intervals intersection, the algorithm switches to transaction id intersection. We have evaluated the algorithm against two popular frequent itemset mining algorithms -- FP-growth and dEclat using a variety of data sets with short and long frequent patterns. Experimental data show that the TM algorithm outperforms these two algorithms.\n",
      "\n",
      "5. id: 5390a4cc20f70186a0e73ff5   score: 0.84156543   abstract: There have been many studies on mining frequent itemset (or pattern) in the data mining field because of its broad applications in mining association rules, correlations, graph patterns, constraint based frequent patterns, sequential patterns, and many other data mining tasks. One of major challenges in frequent pattern mining is a huge number of result patterns. As the minimum threshold becomes lower, an exponentially large number of itemsets are generated. Therefore, pruning unimportant patterns effectively in mining process is one of main topics in frequent pattern mining. In weighted frequent pattern mining, not only support but also weight are used and important patterns can be detected. In this paper, we propose two efficient algorithms for mining weighted frequent itemsets in which the main approaches are to push weight constraints into the Apriori algorithm and the pattern growth\n",
      "\n",
      "6. id: 558b19ee612c41e6b9d434c4   score: 0.83155376   abstract: Mining of association rules or frequent patterns has become an important topic in the research of data mining. However the classical Apriori algorithm is used to mine a frequent pattern which is based on Support-Confidence criteria. But it does not mine significant frequent patterns from the transactional database if Quantity, Profit and weight attributes are there. So, this paper introduces a new approach which extracts significant frequent patterns by considering quantity attributes and by applying Q-factor and S-factor to the transactional database. Q-ratio is the ratio of quantity of particular items throughout all transaction to the total quantity of all items of all transaction and S-factor is the product of Q-ratio of particular items and the frequency of particular items throughout all transaction.\n",
      "\n",
      "7. id: 5390aefc20f70186a0ecdae3   score: 0.8168451   abstract: Association rule mining among frequent items has been widely studied in data mining field. Many researches have improved the algorithm for generation of all the frequent itemsets. In this paper, we proposed a new algorithm to mine all frequents itemsets from a transaction database. The main features of this paper are: (1) the database is scanned only one time to mine frequent itemsets; (2) the new algorithm called the JoinFI-Mine algorithm which use mathematics properties to reduces huge of subsequence mining; (3) the proposed algorithm mines frequent itemsets without generation of candidate sets; and (4) when the minimum support threshold is changed, the database is not require to scan. We have provided definitions, algorithms, examples, theorem, and correctness proving of the algorithm.\n",
      "\n",
      "8. id: 5390aeba20f70186a0ecbd45   score: 0.80363655   abstract: Frequent patterns are an important class of regularities that exist in a transaction database. Certain frequent patterns with low minimum support (minsup) value can provide useful information in many real-world applications. However, extraction of these frequent patterns with single minsup-based frequent pattern mining algorithms such as Apriori and FP-growth leads to \"rare item problem.\" That is, at high minsup value, the frequent patterns with low minsup are missed, and at low minsup value, the number of frequent patterns explodes. In the literature, \"multiple minsups framework\" was proposed to discover frequent patterns. Furthermore, frequent pattern mining techniques such as Multiple Support Apriori and Conditional Frequent Pattern-growth (CFP-growth) algorithms have been proposed. As the frequent patterns mined with this framework do not satisfy downward closure property, the algori\n",
      "\n",
      "9. id: 5390a1bc20f70186a0e55d48   score: 0.7844842   abstract: Traditional association rules mining (ARM) only concerns the frequency of itemsets, which may not bring large amount of profit. Utility mining only focuses on itemsets with high utilities, but the number of rich-enough customers is limited. To overcome the weakness of the two models, we propose a novel model, called general utility mining, which takes both frequency and utility into consideration simultaneously. By adjusting the weight of the frequency factor or the utility factor, this model can meet the different preferences of different applications. It is flexible and practicable in a broad range of applications. We evaluate our proposed model on a real-world database. Experimental results demonstrate that the mining results are valuable in business decision making.\n",
      "\n",
      "10. id: 5390a1e620f70186a0e5ac68   score: 0.77814394   abstract: Mining frequent patterns in transaction databases has been studied extensively in data mining research. However, most of the existing frequent pattern mining algorithms do not consider the time stamps associated with the transactions. In this paper, we extend the existing frequent pattern mining framework to take into account the time stamp of each transaction and discover patterns whose frequency dramatically changes over time. We define a new type of patterns, called transitional patterns, to capture the dynamic behavior of frequent patterns in a transaction database. Transitional patterns include both positive and negative transitional patterns. Their frequencies increase/decrease dramatically at some time points of a transaction database. We introduce the concept of significant milestones for a transitional pattern, which are time points at which the frequency of the pattern changes \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1652104\n",
      "index                                        559253ab0cf28b1a968ffc08\n",
      "title                        Self-Representation in Girard's System U\n",
      "authors                                     Matt Brown, Jens Palsberg\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 42nd Annual ACM SIGPLAN-SIG...\n",
      "references          5390878320f70186a0d32118;5390b78a20f70186a0f23...\n",
      "abstract            In 1991, Pfenning and Lee studied whether Syst...\n",
      "id                                                            1652104\n",
      "clustered_labels                                                    3\n",
      "Name: 1652104, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908cde20f70186a0dcde2e   score: 0.34565726   abstract: Abstract We consider the problems of typability and type checking in the Girard/Reynolds second-order polymorphic typed lambda calculus, for which we use the short name ``System F'''' and which we use in the ``Curry style'''' where types are assigned to pure lambda terms. These problems have been considered and proven to be decidable or undecidable for various restrictions and extensions of System F and other related systems, and lower-bound complexity results for System F have been achieved, but they have remained ``embarrassing open problems'''' for System F itself. We first prove that type checking in System F is undecidable by a reduction from semi-unification. We then prove typability in System F is undecidable by a reduction from type checking. Since the reverse reduction is already known, this implies the two problems are equivalent. The second reduction uses a novel method of con\n",
      "\n",
      "2. id: 5390958920f70186a0dee9ac   score: 0.2786492   abstract: We propose a type system MLF that generalizes ML with first-class polymorphism as in System F. Expressions may contain second-order type annotations. Every typable expression admits a principal type, which however depends on type annotations. Principal types capture all other types that can be obtained by implicit type instantiation and they can be inferred.All expressions of ML are well-typed without any annotations. All expressions of System F can be mechanically encoded into MLF by dropping all type abstractions and type applications, and injecting types of lambda-abstractions into MLF types. Moreover, only parameters of lambda-abstractions that are used polymorphically need to remain annotated.\n",
      "\n",
      "3. id: 5390972920f70186a0dfad46   score: 0.16518871   abstract: We define the typed lambda calculus F^ω (F-omega-meet), a natural generalization of Girard's system Fω (F-omega) with intersection types and bounded polymorphism. A novel aspect of our presentation is the use of term rewriting techniques to present intersection types, which clearly splits the computational semantics (reduction rules) from the syntax (inference rules) of the system. We establish properties such as Church-Rosser for the reduction relation on types and terms, and strong normalization for the reduction on types. We prove that types are preserved by computation (subject reduction), and that the system satisfies the minimal types property. We define algorithms for type checking and subtype checking. The development culminates with the proof of decidability of typing in F^ω, containing the first proof of decidability of subtyping of a higher-order lambda calculus with subtyping\n",
      "\n",
      "4. id: 558af8e9612c41e6b9d3f23b   score: 0.16132024   abstract: We propose a type system MLF that generalizes ML with first-class polymorphism as in System F. Expressions may contain secondorder type annotations. Every typable expression admits a principal type, which however depends on type annotations. Principal types capture all other types that can be obtained by implicit type instantiation and they can be inferred. All expressions of ML are welltyped without any annotations. All expressions of System F can be mechanically encoded into MLF by dropping all type abstractions and type applications, and injecting types of lambda-abstractions into MLF types. Moreover, only parameters of lambda-abstractions that are used polymorphically need to remain annotated.\n",
      "\n",
      "5. id: 5390972920f70186a0dfbdbe   score: 0.11777492   abstract: We propose an imperative version of the Rewriting-calculus, a calculus based on pattern-matching, pattern-abstraction, and side-effects, which we call iRho.We formulate a static and a call-by-value dynamic semantics of iRho like that of Gilles Kahn's Natural Semantics. The operational semantics is deterministic, and immediately suggests how to build an interpreter for the calculus. The static semantics is given via a first-order type system based on a form of product-type, which can be assigned to iRho-terms like structures (i.e. pairs).The calculus is à la Church, i.e. pattern-abstractions are decorated with the types of the free variables of the pattern.iRho is a good candidate for a core or an intermediate language, because it can safely access and modify a (monomorphic) typed store, and because fixed-points can be defined.Properties like determinism of the interpreter, and subject re\n",
      "\n",
      "6. id: 559133f10cf232eb904fb39a   score: 0.11008789   abstract: This article is the second part of a two articles series about the definition of higher order polymorphic functions in a type system with recursive types and set-theoretic type connectives (unions, intersections, and negations). In the first part, presented in a companion paper, we defined and studied the syntax, semantics, and evaluation of the explicitly-typed version of a calculus, in which type instantiation is driven by explicit instantiation annotations. In this second part we present a local type inference system that allows the programmer to omit explicit instantiation annotations for function applications, and a type reconstruction system that allows the programmer to omit explicit type annotations for function definitions. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic f\n",
      "\n",
      "7. id: 5390981d20f70186a0e064cc   score: 0.10818896   abstract: We present a type-preserving translation of System F (the polymorphic lambda calculus) into a forthcoming revision of the C&sharp; programming language supporting parameterized classes and polymorphic methods. The forthcoming revision of Java in JDK 1.5 also makes a suitable target. We formalize the translation using a subset of C&sharp; similar to Featherweight Java. We prove that the translation is fully type-preserving and that it preserves behaviour via the novel use of environment-style semantics for System F.We observe that whilst parameterized classes alone are sufficient to encode the parameterized datatypes and let-polymorphism of languages such as ML and Haskell, it is the presence of dynamic dispatch for polymorphic methods that supports the encoding of the ‘first-class polymorphism’ found in System F and recent extensions to ML and Haskell. Copyright © 2004 John Wiley & Sons,\n",
      "\n",
      "8. id: 5390b13020f70186a0eddafe   score: 0.10557884   abstract: Self-interpreters can be roughly divided into two sorts: self-recognisers that recover the input program from a canonical representation, and self-enactors that execute the input program. Major progress for statically-typed languages was achieved in 2009 by Rendel, Ostermann, and Hofer who presented the first typed self-recogniser that allows representations of different terms to have different types. A key feature of their type system is a type:type rule that renders the kind system of their language inconsistent. In this paper we present the first statically-typed language that not only allows representations of different terms to have different types, and supports a self-recogniser, but also supports a self-enactor. Our language is a factorisation calculus in the style of Jay and Given-Wilson, a combinatory calculus with a factorisation operator that is powerful enough to support the \n",
      "\n",
      "9. id: 5390b44620f70186a0ef8039   score: 0.08284563   abstract: We introduce a polymorphic λ-calculus that features inductive types and that enforces termination of recursive definitions through typing. Then, we define a sound and complete type inference algorithm that computes a set of constraints to be satisfied for terms to be typable. In addition, we show that Subject Reduction fails in a naive use of typed-based termination for a λ-calculus à la Church, and we propose a general solution to this problem.\n",
      "\n",
      "10. id: 53908b2a20f70186a0db84c2   score: 0.07978202   abstract: In this paper, we propose a common theoretical framework for type based static analyses. The aim is the study of relationships between typing and program analysis. We present a variant of Girard's System F called F≤:Π. We prove standard properties of F≤:Π. We show how it can be used to formalize various program analyses like binding time and dead code. We relate our work to previous analyses in terms of expressivness (often only simply typed calculi are considered) and power (more information can be inferred). F≤:Π features polymorphism as well as subtyping at the level of universe extending previous author work where only universe polymorphism (on a simply typed calculus) was considered.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1651709\n",
      "index                                        559037380cf28fa9103179e7\n",
      "title               Mobility Increases Localizability: A Survey on...\n",
      "authors             Zheng Yang, Chenshu Wu, Zimu Zhou, Xinglin Zha...\n",
      "year                                                           2015.0\n",
      "venue                                    ACM Computing Surveys (CSUR)\n",
      "references          558b2c0f612c41e6b9d4599f;558b8d09612c6b62e5e8b...\n",
      "abstract            Wireless indoor positioning has been extensive...\n",
      "id                                                            1651709\n",
      "clustered_labels                                                    3\n",
      "Name: 1651709, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b269c612c41e6b9d44e84   score: 0.9811307   abstract: Indoor positioning has been intensively studied recently due to the exploding demands of indoor mobile applications. While numerous works have employed wireless signals or dead-reckoning techniques, wearable computing poses new opportunities as well as challenges to the localization problem. This research studies the wearable localization problem by proposing a particle filter-based scheme to fuse the inputs from wearable inertial and visual sensors on human body. Specifically, the filter takes inertial measurements, wireless signals, visual landmarks, and indoor floor plans as inputs for location tracking. The inertial signals imply human body movements, the wireless signals indicate a rough absolute region inside a building, while the visual landmarks provide relative angles viewed from particular positions to these markers. Furthermore, a head-mounted display provides intuitive and fr\n",
      "\n",
      "2. id: 5390baa120f70186a0f3814d   score: 0.96131647   abstract: Despite of several years of innovative research, indoor localization is still not mainstream. Existing techniques either employ cumbersome fingerprinting, or rely upon the deployment of additional infrastructure. Towards a solution that is easier to adopt, we propose CUPID, which is free from these restrictions, yet is comparable in accuracy. While existing WiFi based solutions are highly susceptible to indoor multipath, CUPID utilizes physical layer (PHY) information to extract the signal strength and the angle of only the direct path, successfully avoiding the effect of multipath reflections. Our main observation is that natural human mobility, when combined with PHY layer information, can help in accurately estimating the angle and distance of a mobile device from an wireless access point (AP). Real-world indoor experiments using off-the-shelf wireless chipsets confirm the feasibility\n",
      "\n",
      "3. id: 5390bfa220f70186a0f54f12   score: 0.9186263   abstract: While location is one of the most important context information in mobile and ubiquitous computing, large-scale deployment of indoor localization system remains elusive. In this work, we propose PiLoc, an indoor localization system that utilizes opportunistically sensed data contributed by users. Our system does not require manual calibration, prior knowledge and infrastructure support. The key novelty of PiLoc is that it merges walking segments annotated with displacement and signal strength information from users to derive a map of walking paths annotated with radio signal strengths. We evaluate PiLoc over 4 different indoor areas. Evaluation shows that our system can achieve an average localization error of 1.5m.\n",
      "\n",
      "4. id: 5390baa120f70186a0f3851b   score: 0.9158089   abstract: In this demo, we present an efficient hybrid indoor positioning solution that uses multi-sensory location-oriented observations, including WiFi, accelerometer, gyroscope and digital compass data, that are widely available on Android smartphones.\n",
      "\n",
      "5. id: 5390a4cc20f70186a0e7554a   score: 0.90312344   abstract: Proliferating mobile phones provide a foundation for revolutionary innovations in peoplecentric computing. Numerous applications are on the rise, many of which exploit the phone's location as the primary indicator of context. We argue that existing physical localization schemes based on GPS/WiFi/GSM have limitations which make them impractical for use in such applications. Instead, in this poster we describe a means of localization where phones sense their surroundings, and use this ambient information to classify their location. Put differently, we postulate that different surroundings have photo-acoustic fingerprints, that can be sensed and used for localization. We demonstrate the feasibility using Tmote Invent motes that have light and sound sensors. Our ongoing work is extending SurroundSense to the mobile phone platform, and exploiting additional sensors (such as accelerometers and\n",
      "\n",
      "6. id: 5390a6d920f70186a0e87b46   score: 0.9027811   abstract: Localization in wireless sensor network is an important issue as GPS is not accessible in an indoor environment.Because coarse accuracy is sufficient for most sensor network applications, solutions in range-free localization are being pursued as a cost-effective alternative to more expensive range-based approaches.In this paper, we present an Accelerometer based Positioning Scheme (APS) for tracking objects in indoor environments.The main idea of the APS is to compute an object’s displacement by transforming the information of an accelerometer. Given an original coordinates of an object, the final position could be estimated according to the directional displacement.To evaluate the effectiveness of the proposed method, both theoretical validation and simulations were conducted in this study. The simulation results reflect that the APS is a feasible approach in developing indoor localizat\n",
      "\n",
      "7. id: 5390baa120f70186a0f38518   score: 0.9005298   abstract: Indoor localization has attracted more and more attention with the growth of emerging location-based services (LBS), e.g. microblogging, location-based content sharing, and interactive indoor multimedia display. In the literature, wireless-based indoor positioning for hand-held mobile devices is in meter-level precision, including WLAN (Wi-Fi), Bluetooth, and GSM-based approaches. A smartphone-based LBS fusing various sensors, such as accelerometer, digital compass, Wi-Fi, and GPS, can achieve the localization at a better room-level accuracy. In addition, the scanned Wi-Fi access point can determine the room-level identification for a mobile user. However, when multiple human subjects locating in the same coverage area for the same Wi-Fi access points, e.g. in the same room, the individuals often cannot be identified from each other. Therefore, in order to achieve a high precision in a c\n",
      "\n",
      "8. id: 558b29fa612c41e6b9d454b2   score: 0.89256257   abstract: Indoor localization using mobile devices such as smartphones remains a challenging problem as GPS (Global Positioning System) does not work inside buildings and the accuracy of other localization techniques typically comes at the expense of additional infrastructure or cumbersome war-driving. For such environments, we propose a localization scheme which uses motion information from the smartphone's accelerometer, magnetometer, and gyroscope sensors to detect steps and estimate direction changes. At the same time, we use a Wi-Fi based fingerprinting technique for independent position estimation. These measurements along with an internal representation of the environment are combined using a Bayesian filter. This system will allow us to reduce the amount of training required and work in sparse Wi-Fi environments. We test our approach in two real-world environments to show the benefits of i\n",
      "\n",
      "9. id: 5390b78a20f70186a0f24770   score: 0.8889517   abstract: This paper addresses reliable and accurate indoor localization using inertial sensors commonly found on commodity smartphones. We believe indoor positioning is an important primitive that can enable many ubiquitous computing applications. To tackle the challenges of drifting in estimation, sensitivity to phone position, as well as variability in user walking profiles, we have developed algorithms for reliable detection of steps and heading directions, and accurate estimation and personalization of step length. We've built an end-to-end localization system integrating these modules and an indoor floor map, without the need for infrastructure assistance. We demonstrated for the first time a meter-level indoor positioning system that is infrastructure free, phone position independent, user adaptive, and easy to deploy. We have conducted extensive experiments on users with smartphone devices\n",
      "\n",
      "10. id: 5390b36120f70186a0ef18ae   score: 0.8750414   abstract: A wireless tracking system such as the Global Positioning System (GPS) is the most effective in relatively open and flat outdoor environments but is much less effective in non-line-of-sight (NLOS) environments such as hilly, mountainous, or built-up areas. In recent years, IEEE 802.11 Wireless Local Area Networks (WLANs) have been widely deployed and are now fuelling a wide range of location-aware computing applications. The localization of devices in these networks (i.e., estimating their location) makes use of the Wi-Fi signal strength. There are two location-sensing techniques for indoor environments, propagation-based and location-fingerprinting techniques. Although these two techniques can locate WLAN-enabled devices, recent research on indoor localization system is still not satisfactory. There are five major problems that lead to inaccurate and low efficiency localization systems.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1713322\n",
      "index                                        55323c2445cec66b6f9db54c\n",
      "title               Towards the interactive transcription of handw...\n",
      "authors                  Björn Gottfried, Marius Wegner, Mathias Lawo\n",
      "year                                                           2015.0\n",
      "venue               International Journal on Document Analysis and...\n",
      "references          558ff609612c29c89cd7c976;5590b6f40cf2ce4b6f39f...\n",
      "abstract            This paper introduces the anytime anywhere doc...\n",
      "id                                                            1713322\n",
      "clustered_labels                                                    1\n",
      "Name: 1713322, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390aaf920f70186a0eacfd6   score: 0.466238   abstract: Paleography experts spend many hours transcribing historic documents, and state-of-the-art handwritten text recognition systems are not suitable for performing this task automatically. In this paper we present the modifications on a previously developed interactive framework for transcription of handwritten text. This system, rather than full automation, aimed at assisting the user with the recognition-transcription process.\n",
      "\n",
      "2. id: 5390b0ca20f70186a0edac05   score: 0.4449868   abstract: An effective approach to handwriting transcription of (old) documents is to follow a sequential, line-by-line transcription of the whole document, in which a continuously retrained system interacts with the user. In the case of multilingual documents, however, a minor yet important issue for this interactive approach is to first identify the language of the current text line image to be transcribed. In this paper, we propose a probabilistic framework and three techniques for this purpose. Empirical results are reported on an entire 764-page multilingual document for which previous empirical tests were limited to its first 180 pages, written only in Spanish.\n",
      "\n",
      "3. id: 53909f8220f70186a0e3dc76   score: 0.37399015   abstract: To date, automatic handwriting recognition systems are far from being perfect and often they need a post editing where a human intervention is required to check and correct the results of such systems. We propose to have a new inter- active, on-line framework which, rather than full automa- tion, aims at assisting the human in the proper recognition- transcription process; that is, facilitate and speed up their transcription task of handwritten texts. This framework combines the efficiency of automatic handwriting recogni- tion systems with the accuracy of the human transcriptor. The best result is a cost-effective perfect transcription of the handwriting text images.\n",
      "\n",
      "4. id: 5390a1bc20f70186a0e566d6   score: 0.36127737   abstract: Paleography experts spend many hours transcribing ancient documents and state-of-the-art handwritten text recognition systems are not suitable for performing this task automatically. We propose here a new interactive, on-line framework which, rather than full automation, aims at assisting the experts in the proper recognition-transcription process; that is, facilitate and speed up the transcription of old documents. This framework combines the efficiency of automatic handwriting recognition systems with the accuracy of the experts, leading to a cost-effective perfect transcription of ancient manuscripts.\n",
      "\n",
      "5. id: 5390ae2e20f70186a0ec7ea3   score: 0.34798014   abstract: We present in this paper a new method of analysis and decomposition of handwritten documents into glyphs (graphemes) and their associated code book. The different techniques that are involved in this paper are inspired by image processing methods in a large sense and mathematical models implying graph coloring. Our approaches provide firstly a rapid and detailed characterization of handwritten shapes based on dynamic tracking of the handwriting (curvature, thickness, direction, etc.) and also a very efficient analysis method for the categorization of basic shapes (graphemes). The tools that we have produced enable paleographers to study quickly and more accurately a large volume of manuscripts and to extract a large number of characteristics that are specific to individual writer or specific era.\n",
      "\n",
      "6. id: 5390b86b20f70186a0f29b9a   score: 0.34048516   abstract: Paleography experts spend many hours transcribing ancient documents and state-of-the-art handwritten text recognition systems are not suitable for performing this task automatically. We propose here a new interactive, online framework which, rather than full automation, aims at assisting the experts in the proper recognition-transcription process; that is, facilitate and speed up the transcription of old documents. This framework combines the efficiency of automatic handwriting recognition systems with the accuracy of the experts, leading to a cost-effective perfect transcription of ancient manuscripts.\n",
      "\n",
      "7. id: 5390b78a20f70186a0f22b58   score: 0.33025515   abstract: This paper reports on the analysis of different approaches in order to search for glyphs within handwritten mediaeval documents. As layout analysis methods are difficult to apply to the documents at hand, template matching methods are employed. A number of different shape descriptions are used to filter out false positives, since the application of correlation coefficients alone results in too many matches. The overall goal consists in the interactive support of an editor who is transcribing a given handwriting. For this purpose, the automatic spotting of glyphs enables the editor to compare glyphs within different contexts.\n",
      "\n",
      "8. id: 53909fbd20f70186a0e43503   score: 0.28846887   abstract: Recognition of handwritten documents has been a success in well-constrained domains, such as automatic recognition of handwritten postal addresses and validating bank check amounts. However, unconstrained offline handwriting recognition is still a very challenging task when lexicon size is very large. Thus, the large amount of handwritten material of the world, historical manuscripts and contemporary documents, are still inaccessible by search engines. We will describe our seminal work in real-time handwriting recognition which paved the way for the first deployment of end-to-end handwritten address interpretation system. We will also present loosely constrained applications such as indexing and searching hand-filled forms and unconstrained handwritten documents. We will conclude with an overview of our current research in transcription of historical manuscripts and using handwriting for\n",
      "\n",
      "9. id: 5390ada620f70186a0ec1e42   score: 0.18742996   abstract: We present a study of the application of Computer Assisted Transcription of Text Images (CATTI) to a task which is much closer to real applications than other tasks previously studied. The new task consists in the transcription of a new publicly available historic handwritten document, called GERMANA. A detailed analysis of the main factors inﬂuencing the system performance are exposed and some strategies to circumvent them are proposed.\n",
      "\n",
      "10. id: 5390b20120f70186a0ee4b8f   score: 0.17441028   abstract: Transcription of handwritten words in historical documents is still a difficult task. When processing huge amount of pages, document-centered approaches are limited by the trade-off between automatic recognition errors and the tedious aspect of human user annotation work. In this article, we investigate the use of inter page dependencies to overcome those limitations. For this, we propose a new architecture that allows the exploitation of handwritten word redundancies over pages by considering documents from a higher point of view, namely the collection level. The experiments we conducted on handwritten word transcription show promising results in terms of recognition error and human user work reductions.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696636\n",
      "index                                        55922e860cf244696a09da5e\n",
      "title               A unified framework for clustering constrained...\n",
      "authors                                            Hu Ding, Jinhui Xu\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558b3e41612c41e6b9d476b3;558b233d612c41e6b9d44...\n",
      "abstract            In this paper, we consider a class of constrai...\n",
      "id                                                            1696636\n",
      "clustered_labels                                                    2\n",
      "Name: 1696636, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b2d720f70186a0eec925   score: 0.98840266   abstract: We generalize the k-means algorithm presented by the authors [14] and show that the resulting algorithm can solve a larger class of clustering problems that satisfy certain properties (existence of a random sampling procedure and tightness). We prove these properties for the k-median and the discrete k-means clustering problems, resulting in O(2(k/ε)O(1)dn) time (1+ε)-approximation algorithms for these problems. These are the first algorithms for these problems linear in the size of the input (nd for n points in d dimensions), independent of dimensions in the exponent, assuming k and ε to be fixed. A key ingredient of the k-median result is a (1+ε)-approximation algorithm for the 1-median problem which has running time O(2(1/ε)O(1)d). The previous best known algorithm for this problem had linear running time.\n",
      "\n",
      "2. id: 5390881820f70186a0d816c8   score: 0.9869795   abstract: In this paper, we show that for several clustering problems one can extract a small set of points, so that using those core-sets enable us to perform approximate clustering efficiently. The surprising property of those core-sets is that their size is independent of the dimension.Using those, we present a (1+ &egr;)-approximation algorithms for the k-center clustering and k-median clustering problems in Euclidean space. The running time of the new algorithms has linear or near linear dependency on the number of points and the dimension, and exponential dependency on 1/&egr; and k. As such, our results are a substantial improvement over what was previously known.We also present some other clustering results including (1+ &egr;)-approximate 1-cylinder clustering, and k-center clustering with outliers.\n",
      "\n",
      "3. id: 53909a9320f70186a0e233f9   score: 0.9850429   abstract: The clustering problem is a well-studied problem in computer science. Given a set of points in metric space, the objective of the clustering problem is to partition the data points into clusters such that the distances between points within the same cluster are small and the distance between points in different cluster are large. As an unsupervised learning procedure, a number of variants of clustering problem have been investigated and many important approaches have been proposed. Among these variants, the Euclidean k-center problem and Euclidean k-median problem have received considerable attentions [2; 4; 10; 11; 13; 34; 36; 40; 81] from the fields of computational geometry and combinatorial optimizations. Such problems are NP-hard, and have been studied from the perspective of worst case performance guarantees. The approaches to design approximation algorithms can be roughly classifi\n",
      "\n",
      "4. id: 5390bfa220f70186a0f542e6   score: 0.9818056   abstract: The problem of clustering with constraints has received considerable attention in the last decade. Indeed, several algorithms have been proposed, but only a few studies have partially compared their performances. In this work, three well-known algorithms for k-means-based clustering with soft constraints --Constrained Vector Quantization Error CVQE, its variant named LCVQE, and the Metric Pairwise Constrained K-Means MPCK-Means --are systematically compared according to three criteria: Adjusted Rand Index, Normalized Mutual Information, and the number of violated constraints. Experiments were performed on 20 datasets, and for each of them 800 sets of constraints were generated. In order to provide some reassurance about the non-randomness of the obtained results, outcomes of statistical tests of significance are presented. In terms of accuracy, LCVQE has shown to be competitive with CVQE\n",
      "\n",
      "5. id: 53909e8b20f70186a0e2df0a   score: 0.97966766   abstract: In this paper, a version of K-median problem, one of the most popular and best studied clustering measures, is discussed. The model using squared Euclidean distances terms to which the K-means algorithm has been successfully applied is considered. A fast and robust algorithm based on DC (Difference of Convex functions) programming and DC Algorithms (DCA) is investigated. Preliminary numerical solutions on real-world databases show the efficiency and the superiority of the appropriate DCA with respect to the standard K-means algorithm.\n",
      "\n",
      "6. id: 5390b0ca20f70186a0eda71d   score: 0.9786719   abstract: Clustering is a fundamental data analysis task that has numerous applications in many disciplines. Clustering can be broadly defined as a process of partitioning a dataset into groups, or clusters, so that elements of the same cluster are more similar to each other than to elements of different clusters.The two main approaches to clustering are hierarchical clustering and partitioning clustering. In this dissertation we focus on partitioning clustering, where the number of clusters is known in advance. Many partitioning clustering methods iteratively update the so-called centroids or cluster centers, and as such, are often referred as center-based clustering methods.Families of center-based clustering methods are capable of handling high dimensional sparse data arising in Text Mining applications. All current center-based algorithms seek to minimize a particular objective function with a\n",
      "\n",
      "7. id: 5390a74e20f70186a0e8b4a7   score: 0.9778412   abstract: We present new approximation algorithms for the $k$-median and $k$-means clustering problems. To this end, we obtain small coresets for $k$-median and $k$-means clustering in general metric spaces and in Euclidean spaces. In $\\mathbb{R}^d$, these coresets are of size with polynomial dependency on the dimension $d$. This leads to $(1+\\varepsilon)$-approximation algorithms to the optimal $k$-median and $k$-means clustering in $\\mathbb{R}^d$, with running time $O(ndk+2^{(k/\\varepsilon)^{O(1)}}d^2\\log^{k+2}n)$, where $n$ is the number of points. This improves over previous results. We use those coresets to maintain a $(1+\\varepsilon)$-approximate $k$-median and $k$-means clustering of a stream of points in $\\mathbb{R}^d$, using $O(d^2k^2\\varepsilon^{-2}\\log^8n)$ space. These are the first streaming algorithms, for those problems, that have space complexity with polynomial dependency on the d\n",
      "\n",
      "8. id: 5390ba3820f70186a0f3550e   score: 0.9773276   abstract: We study algorithms for clustering data that were recently proposed by Balcan et al. (SODA'09: 19th Annual ACM-SIAM Symposium on Discrete Algorithms, pp.聽1068---1077, 2009a) and that have already given rise to several follow-up papers. The input for the clustering problem consists of points in a metric space and a number k, specifying the desired number of clusters. The algorithms find a clustering that is provably close to a target clustering, provided that the instance has the \"(1+驴,驴)-property\", which means that the instance is such that all solutions to the k-median problem for which the objective value is at most (1+驴) times the optimal objective value correspond to clusterings that misclassify at most an 驴 fraction of the points with respect to the target clustering. We investigate the theoretical and practical implications of their results.Our main contributions are as follows. Fi\n",
      "\n",
      "9. id: 5390878720f70186a0d36197   score: 0.97529614   abstract: A new divisive algorithm for multidimensional data clustering is suggested. Based on the minimization of the sum-of-squared-errors, the proposed method produces much smaller quantization errors than the median-cut and mean-split algorithms. It is also observed that the solutions obtained from our algorithm are close to the local optimal ones derived by the k-means iterative procedure.\n",
      "\n",
      "10. id: 53908f5c20f70186a0ddb233   score: 0.9747251   abstract: We give randomized constant-factor approximation algorithms for the k-median problem and an intimately related clustering problem. The input to each of these problems is a metric space with n weighted points and an integer k, 0 k = n. For any such input, let Rsub/subsub/subsub/subsub/sub\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1694907\n",
      "index                                        559248760cf28b1a968ff653\n",
      "title               NCOM: network coding based overlay multicast i...\n",
      "authors                                   Tan Le, Xing Chen, Yong Liu\n",
      "year                                                           2015.0\n",
      "venue                                               Wireless Networks\n",
      "references          5390b5f920f70186a0f0e880;53909f6920f70186a0e39...\n",
      "abstract            The capacities of wireless networks are increa...\n",
      "id                                                            1694907\n",
      "clustered_labels                                                    3\n",
      "Name: 1694907, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ad8920f70186a0ec0f3d   score: 0.9743861   abstract: Opportunistic routing is a recent routing paradigm that achieves high throughput in the face of universally lossy wireless links in wireless mesh networks, likewise, network coding popularly investigated by many institutes is another approach of enhancing the throughput in wireless networks. How to combine the both techniques is an interesting research. To our knowledge, this is still an open issue to implement the multicast routing in wireless mesh networks simultaneously employing the both mechanisms. In this paper, we propose an efficient multicast protocol based on the opportunistic routing and network coding. Additionally, we also provide an ETX (expected number of transmissions) based cost metric formula of the multicast sub-group. Extensive simulation reveals that the performance of the novel presented multicast routing protocol outperforms the conventional multicast routing proto\n",
      "\n",
      "2. id: 5390aa7620f70186a0eaaafa   score: 0.9631613   abstract: In multihop wireless networks, many applications require end-to-end reliable multicast communication with 100% packet delivery ratio. Recently, network coding has been applied to the reliable link-level multicast in wireless networks, where multiple lost packets with distinct intended receivers are XOR-ed together as one packet and forwarded via single retransmission, resulting in a significant reduction of bandwidth consumption in the scenarios of many receivers. However, the simple XOR operation cannot fully exploit the potential coding opportunities. Moreover, for the reliable multicast in multihop wireless networks, the number of receivers of link-level multicast is usually not large and thus this coding-based link-level multicast cannot provide a promising gain. In this work, we propose a new network coding-based reliable multicast scheme which exploits both network coding and the p\n",
      "\n",
      "3. id: 5390b9d520f70186a0f3216b   score: 0.9552357   abstract: Opportunistic routing is a recent routing paradigm that achieves high throughput in the face of universally lossy wireless links in mobile ad hoc networks, likewise, network coding popularly investigated by many institutes is another approach for improving the throughput in wireless networks. How to combine both the techniques is an interesting research. To our knowledge, this is still an open issue to implement the multicast routing in mobile ad hoc networks simultaneously employing both the mechanisms. Due to feedback explosion, most existing wireless network multicast protocols do not guarantee the reliable delivery. In this paper, we propose a novel reliable multicast protocol based on the opportunistic routing and network coding. Additionally, we also provide an expected number of transmission-based cost metric formula of the multicast sub-group. Extensive simulation reveals that th\n",
      "\n",
      "4. id: 5390a45520f70186a0e713dd   score: 0.94957596   abstract: We study the contribution of network coding (NC) in improving the multicast capacity of random wireless ad hoc networks. We consider a network with $n$ nodes distributed uniformly in a unit square, with each node acting as a source for independent information to be sent to a multicast group consisting of m randomly chosen destinations. We consider the physical model, and show that the per-session capacity in the presence of arbitrary NC has a tight bound of θ (1/√mn) when m = O (n/(log(n))3), and θ (1/n) when m = Ω(n/log n)). Prior work has shown that these same order bounds are achievable on the basis of pure routing, which utilizes only traditional store and forward methods. Therefore, our results demonstrate that the NC gain for multi-source multicast and broadcast is bounded by a constant factor.\n",
      "\n",
      "5. id: 53909fbd20f70186a0e4336d   score: 0.9427181   abstract: As a novel transmission mechanism to enhance the per- formance of multicast network, network coding (NC) has received much researcher concern recently. Due to its in- herent characteristic, the process of constructing the rout- ing paths in network coding is different from the tradi- tional IP multicast based on replicating and forwarding. In this paper, we analyze the algebraic model of network coding and conclude that the goal of the routing problem in network coding is to build edge-disjoin paths from the source to each receiver. Using the idea of tree packing, we present a detailed routing algorithm for network cod- ing based multicast. Simulations based on the algorithm show that, for network coding based multicast, performance in terms of achievable throughput and bandwidth consump- tion has been improved significantly with respect to the tra- ditional IP multicast.\n",
      "\n",
      "6. id: 5390b19020f70186a0edee30   score: 0.93451476   abstract: Opportunistic Routing (OR) has been proposed to improve the efficiency of unicast protocols in wireless networks. In contrast to traditional routing, instead of preselecting a single specific node to be the next-hop forwarder, an ordered set of nodes (referred to as candidates) is selected as the next-hop potential forwarders. In this paper, we propose a new multicast routing protocol based on OR for wireless mesh networks, named Multicast OR Protocol (MORP).We compare our proposal with the well known ODMRP Multicast protocol. Our results show that Multicast-OR outperforms ODMRP, reducing the number of transmissions and increasing the packet delivery ratio.\n",
      "\n",
      "7. id: 5390ad8920f70186a0ebfca9   score: 0.92441523   abstract: Network coding, the notion of performing coding operations on the contents of packets while in transit through the network, was originally developed for wired networks; recently, however, it has been also applied with success also to wireless ad hoc networks. In fact, it has been shown that network coding can yield substantial performance gains, e.g., reduced energy consumption, in ad hoc networks. In this paper, we compare, using linear programming formulations, the maximum throughput that a multicast application can achieve with and without network coding in unreliable ad hoc networks; we show that network coding achieves 65% higher throughput than conventional multicast in a typical ad hoc network scenario. The superiority of network coding, already established by the analytic results, is confirmed by simulation experiments.\n",
      "\n",
      "8. id: 5390ada620f70186a0ec1a5c   score: 0.9144437   abstract: The innovative exploitation of network coding technology could bring great design advantages to MANETs. First, wireless links are inherently lossy due to channel fading or interference. Second, the delivery condition of the route from the source to each receiver in a multicast transmission might be significantly distinct. Trying to satisfy the reliability requirement for the poorly-connected receivers may affect the performance of the rest of the receivers. In this paper, we present a tree-based multicast protocol, which exploits the characteristics of network coding to provide efficient and reliable multicast. A metric, named Receiving Probability (RP), is proposed to represent the probability for a node in a multicast tree to successfully receive a packet sent by the multicast source. To mitigate the distinctions among the receivers, the proposed multicast protocol aims to construct mu\n",
      "\n",
      "9. id: 5390a88c20f70186a0e9967f   score: 0.9034647   abstract: Network coding (NC) is an emerging technology area promising to increase the utilization of both wired and wireless networks. It can be seen as an extension to routing by allowing relay nodes to combine the information received from multiple links in the subsequent transmissions. NC was initially proposed in the context of multicast in wired networks where it was proven to achieve the maximum multicast capacity. However, the operation of NC in wireless networks with multiple unicast transmissions and its impact on the different layers has not been fully understood. In this paper we give a thorough insight into the interaction of NC and scheduling in wireless networks and we propose a novel distributed proportional fair (PF) scheduling algorithm that exploits multiuser diversity and coding opportunities to maximize the network throughput while guarantees \"proportional fairness\".\n",
      "\n",
      "10. id: 5390aaf920f70186a0eacc28   score: 0.9027811   abstract: Network coding is a promising technology that can effectively improve the efficiency and capacity of multihop wireless networks by exploiting the broadcast nature of the wireless medium. However, current packet routing schemes do not take advantage of the network coding, and the benefits of network coding have not been fully utilized. To improve the performance gain of network coding, in this paper, we apply network coding over a wireless backbone and investigate the performance of this approach from a theoretical perspective. Our analysis shows that, compared to network coding over ad hoc networks with traditional routing schemes, network coding over the backbone structure exhibits significant advantages. This is because all packets are transmitted over the constructed backbone with pre-specified routes, and consequently the opportunity for coding packets at intermediate nodes can be su\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1668088\n",
      "index                                        559139ae0cf232eb904fb56c\n",
      "title               Classification Framework of MapReduce Scheduli...\n",
      "authors             Nidhi Tiwari, Santonu Sarkar, Umesh Bellur, Ma...\n",
      "year                                                           2015.0\n",
      "venue                                    ACM Computing Surveys (CSUR)\n",
      "references          5390962020f70186a0df5db9;5390baa120f70186a0f37...\n",
      "abstract            A MapReduce scheduling algorithm plays a criti...\n",
      "id                                                            1668088\n",
      "clustered_labels                                                    3\n",
      "Name: 1668088, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bda020f70186a0f46f9a   score: 0.96909106   abstract: MapReduce is a programming model and an associated implementation for processing and generating large data sets. Providing MapReduce as a service is the development future trend. By leveraging the game theory, this paper proposes a scheduling algorithm to deal with the competition for resources between multiple jobs in MapReduce. Firstly, we present a model that could estimate job executing time, and then a utility function of job and an optimization objective are brought forward; thirdly, we present a game model to solve the optimization problem. The proof and the solution are also present. Finally, we implement the algorithm and experiment it in a hadoop cluster. The result shows the present algorithm could schedule jobs rational.\n",
      "\n",
      "2. id: 53909fbd20f70186a0e420b3   score: 0.9020933   abstract: MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.\n",
      "\n",
      "3. id: 55323b2b45cec66b6f9d9761   score: 0.90105337   abstract: MapReduce is a popular parallel data-processing system, and task scheduling is one of the kernel techniques in MapReduce. In many applications, users have requirements that their MapReduce jobs should be completed before specific deadlines. Hence, in this paper, a novel scheduling algorithm based on the most effective sequence (SAMES) is proposed for deadline-constraint jobs in MapReduce. First, according to the characteristics of MapReduce, we propose a novel sequence-based execution strategy for MapReduce jobs and a new concept, the effective sequence (ES). Then, we design some efficient approaches for finding ESes and choose the most effective sequence (MES) for job execution. We also propose methods for MES-updates and exception handling. Finally, we verify the effectiveness of SAMES through experiments. The experimental results show that SAMES is an efficient scheduling algorithm fo\n",
      "\n",
      "4. id: 558af068612c41e6b9d3df74   score: 0.89368117   abstract: The scheduling approach in MapReduce may result in the \\\"long tail\\\" problem because of the unreasonable task assignment and high scheduling overhead because of an amount of task scheduling operations. To address these problems, a new task scheduling approach for MapReduce, named \\\"Iterative Task Scheduling Algorithm\\\", is proposed. The new approach tries to schedule the map tasks according to the solution of the equation for the optimal task assignment. Thus the \\\"long tail\\\" problem can be mitigated effectively and the task scheduling operations can be significantly reduced. To support our new scheduling approach, two approaches are proposed: The first one is adopted to estimate task execution times of nodes and the second one is adopted to produce the optimal task assignment based on the known task execution times of nodes. Comprehensive experiments have been performed with the real l\n",
      "\n",
      "5. id: 5535350f0cf2ca99703de650   score: 0.8784186   abstract: In this paper, the schedulers of MapReduce on the Hadoop platform are analyzed. LATE (Longest Approximate Time to End) scheduling algorithm always launches backup tasks for those tasks which have more remaining time than other tasks. This algorithm redistributes the backward task to fast nodes and does not consider fast node workload, which may result in cluster performance degradation. Aiming at its disadvantages, an improved LATE scheduling algorithm is put forward. The algorithm employs a method of CPU occupancy as a load index by feeding back the load index and adapted load changes dynamically. The experiment results on Hadoop cloud platform show that the improved LATE scheduling algorithm can shorten the mission completion time and improve the whole performance of the cluster.\n",
      "\n",
      "6. id: 5390bda020f70186a0f4787e   score: 0.87768656   abstract: MapReduce is a scalable parallel computing framework for big data processing. It exhibits multiple processing phases, and thus an efficient job scheduling mechanism is crucial for ensuring efficient resource utilization. There are a variety of scheduling challenges within the MapReduce architecture, and this paper studies the challenges that result from the overlapping of the ''map'' and ''shuffle'' phases. We propose a new, general model for this scheduling problem, and validate this model using cluster experiments. Further, we prove that scheduling to minimize average response time in this model is strongly NP-hard in the offline case and that no online algorithm can be constant-competitive. However, we provide two online algorithms that match the performance of the offline optimal when given a slightly faster service rate (i.e., in the resource augmentation framework). Finally, we val\n",
      "\n",
      "7. id: 5536867f0cf2dbb77a816b37   score: 0.8586001   abstract: MapReduce has become one standard for big data processing in Cloud computing environment. However job scheduling in this model is always a challenge for the research fraternity and several job scheduling algorithms have already been proposed by researchers addressing various issues. All such algorithms mostly emphasize on meeting of job deadline without taking cognizance of additional released resources being available through intermediate release of resources during processing. In this paper, we propose a new job scheduling algorithm which satisfies job deadline for all scheduled jobs along with utilization of the unused resources so that more number of jobs can be scheduled and the overall system efficiency will be increased.\n",
      "\n",
      "8. id: 5390be6620f70186a0f4b24a   score: 0.85452086   abstract: MapReduce is a programming model and an associated implementation for processing and generating large data sets. Providing MapReduce as a service is the development future trend. This paper present a queue theory based scheduling scheme for MapReduce framework. The scheme divides users into priority user and non-priority and users respectively corresponding to different queues. The scheme insures priority user has a pre-specified delay and the non-priority queue have a promised utilization ratio. At last, Experiments show the effectiveness of the scheme.\n",
      "\n",
      "9. id: 5390ae2e20f70186a0ec7621   score: 0.8356178   abstract: User constraints such as deadlines are important requirements that are not considered by existing cloud-based data processing environments such as Hadoop. In the current implementation, jobs are scheduled in FIFO order by default with options for other priority based schedulers. In this paper, we extend real time cluster scheduling approach to account for the two-phase computation style of MapReduce. We develop criteria for scheduling jobs based on user specified deadline constraints and discuss our implementation and preliminary evaluation of a Deadline Constraint Scheduler for Hadoop which ensures that only jobs whose deadlines can be met are scheduled for execution.\n",
      "\n",
      "10. id: 5390c04520f70186a0f56563   score: 0.831006   abstract: MapReduce is emerging as an important programming model for massive data processing. A key challenge in MapReduce environments is the ability to efficiently control resource allocation and task scheduling for achieving Service Level Objectives (SLOs) of MapReduce jobs. However, there are few effective task scheduling methods to guarantee MapReduce jobs' SLOs. Therefore, we address this challenge by proposing a SLO-driven task scheduling mechanism in this paper. Based on the MapReduce performance model we build, our mechanism dynamically adjusts resource allocation and task scheduling in order to guarantee the SLOs of jobs and improve global job utility. Experimental results show that our SLO-driven task scheduler effectively meets the specified job latency SLOs and enhances job utility on tested MapReduce programs.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707942\n",
      "index                                        55323b9945cec66b6f9da3a2\n",
      "title               Formal verification and validation of embedded...\n",
      "authors             Luciano Baresi, Gundula Blohm, Dimitrios S. Ko...\n",
      "year                                                           2015.0\n",
      "venue                           Software and Systems Modeling (SoSyM)\n",
      "references          5390b86b20f70186a0f29899;5390b04120f70186a0ed8...\n",
      "abstract            Formal verification and validation activities ...\n",
      "id                                                            1707942\n",
      "clustered_labels                                                    0\n",
      "Name: 1707942, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b36120f70186a0ef0939   score: 0.84859616   abstract: Identifying errors in early design phases leads to a decrease in the cost of repairing such problems if they are discovered only in advanced design phases.This work is a first step toward an automatic verification approach for embedded and real-time systems' high-level specifications, such as UML models.This paper presents a model-driven framework to allow simulating system's behavior already in early design phases, prior to the implementation phase.More specifically, the mentioned framework simulates the behavior specified within UML models, generating a trace of executed actions for the selected behaviors.The achieved results show that early simulation of UML models is practicable, opening room for its usage in different CASE tools for early verification and validation of embedded and real-time systems.\n",
      "\n",
      "2. id: 5390b95420f70186a0f2e212   score: 0.8451773   abstract: In a software development cycle, it is often more than half of the development time that is dedicated to verification activities. Formal methods offer new possibilities for verification. In the specification phase, simulation or model-checking allow users to detect errors in models. In the implementation phase, analysis techniques, like static analysis, make the verification tasks more exhaustive and more automatic. In that context, we propose to take advantage of these methods to improve embedded software development processes based on the V-model.\n",
      "\n",
      "3. id: 539087f920f70186a0d735db   score: 0.81331253   abstract: A widely accepted approach to software development involves successive refinement of design and requirements specifications from a top-level description of the system down to the code level. As the system is refined, it is verified at each phase of development before proceeding to the next phase. In the past, several tools and techniques have been developed to assist in the development and verification process.Tools have been developed and have been in use for many years to examine the testability and system information flow in hardware systems. These problems are approached as a knowledge base verification and validation problem. Several strong analogues exist between hardware and software systems. However, several fundamental differences exist which affect the approach to modeling and verifying the system.This paper briefly describes past efforts in verifying hardware and software syst\n",
      "\n",
      "4. id: 5390893e20f70186a0d94149   score: 0.7613611   abstract: In a perfect world, verification and validation of a software design specification would be possible before any code was generated. Indeed, in a perfect world we would know that the implementation was correct because we could trust the class libraries, the development tools, verification tools and simulations, etc. These features would provide the confidence needed to know that all aspects (complexity, logical and timing correctness) of the design were fully satisfied (i.e., everything was right). Right in the sense that we built it right (it is correct with respect to its specification) and it solves the right problem. Unfortunately, it is not a perfect world, and therefore we must strive to continue to refine, develop and validate useful methods and tools for the creation of safe and correct software. This paper considers the analysis of systems expressed using formal notations. We int\n",
      "\n",
      "5. id: 5390893e20f70186a0d92644   score: 0.7409249   abstract: This paper proposes a simulation-based methodology for validation of a system under design in an early phase of development. The key element of this approach is the visual specification, as Live Sequence Charts (LSCs), of the properties to be checked. The LSCs are automatically translated into the input format for the SystemC-based checker engine, which indicates during simulation, if the property is fulfilled or produces a counter-example, if the property is violated. The entire process from the visual property specification to the checking is largely automated, which makes our approach accessible even for users which have not been trained in formal methods.\n",
      "\n",
      "6. id: 5590b80d0cf25001c36ee138   score: 0.7049733   abstract: Embedded systems are becoming increasingly common in our everyday lives. As technology progresses, these systems become more and more complex, and designers handle this increasing complexity by reusing existing components (Intellectual Property blocks). At the same time, the systems must fulfill strict requirements on reliability and correctness.This paper proposes a formal verification methodology which smoothly integrates with component-based system-level design using a divide and conquer approach. The methodology assumes that the system consists of several reusable components, each of them already formally verified by their designers. The components are considered correct given that the environment satisfies certain properties imposed by the component. The methodology verifies the correctness of the glue logic inserted between the components and the interaction of the components throu\n",
      "\n",
      "7. id: 53908adf20f70186a0dac496   score: 0.6533481   abstract: In this position paper, we mention some of the challenges in specification and verification which are raised by the emerging discipline of embedded systems. The main proposition of the paper is that a feasible solution to the problem of effective, reliable, and dependable construction of embedded systems can be provided by a seamless development process based on a formal specification of the required system, which proceeds by the activities of verification and analysis of the specification at very early stages of the design, and then followed by automatic code generation, preceded if necessary by code distribution and allocation.As a prototype example of such a development process, we quote some experiences from the Sacres project and its follow-up Safeair. Necessary extensions to these preliminary experiments are discussed and evaluated.\n",
      "\n",
      "8. id: 53909a0220f70186a0e201f8   score: 0.6489114   abstract: Symbolic simulation and uninterpreted functions have long been staple techniques for formal hardware verification. In recent years, we have adapted these techniques for the automatic, formal verification of low-level embedded software--specifically, checking the equivalence of different versions of assembly language programs. Our approach, though limited in scalability, has proven particularly promising for the intricate code optimizations and complex architectures typical of high-performance embedded software, such as for DSPs and VLIW processors. Indeed, one of our key findings was how easy it was to create or retarget our verification tools to different, even very complex, machines. The resulting tools automatically verified or found previously unknown bugs in several small sequences of industrial and published example code. This paper provides an introduction to these techniques and \n",
      "\n",
      "9. id: 5390b9d520f70186a0f30f50   score: 0.64880025   abstract: With the demand for new and complicated features, embedded systems are becoming more and more difficult to design and verify. Even if the design of a system is verified, how to guarantee the consistency between the design and its implementation remains a big issue. As a solution, we propose a framework that can help a system designer to model his or her embedded system using a high-level modeling language, verify the design of the system, and automatically generate executable software codes whose behavior semantics are consistent with that of the high-level model. We use two case studies to demonstrate the effectiveness of our framework.\n",
      "\n",
      "10. id: 5390bae520f70186a0f3bae3   score: 0.6021201   abstract: An embedded system is loosely defined as any system that utilizes electronics but is not perceived or used as a general-purpose computer. Traditionally, one or more electronic circuits or microprocessors are literally embedded in the system, either taking up roles that used to be performed by mechanical devices, or providing functionality that is not otherwise possible. The goal of this book is to investigate how formal methods can be applied to the domain of embedded system design. The emphasis is on the specification, representation, validation, and design exploration of such systems from a high-level perspective. The authors review the framework upon which the theories and experiments are based, and through which the formal methods are linked to synthesis and simulation. A formal verification methodology is formulated to verify general properties of the designs and demonstrate that th\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1662818\n",
      "index                                        5591504f0cf232eb904fbb68\n",
      "title               System-Level Characterization of Datacenter Ap...\n",
      "authors             Manu Awasthi, Tameesh Suri, Zvika Guz, Anahita...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 6th ACM/SPEC International ...\n",
      "references          5390bae520f70186a0f3a286;5390b5ed20f70186a0f0c...\n",
      "abstract            In recent years, a number of benchmark suites ...\n",
      "id                                                            1662818\n",
      "clustered_labels                                                    3\n",
      "Name: 1662818, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 559163420cf2e89307ca980e   score: 0.61947125   abstract: With the increasing prevalence of warehouse-scale (WSC) and cloud computing, understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding, this paper presents a detailed microarchitectural analysis of live datacenter jobs, measured on more than 20,000 Google machines over a three year period, and comprising thousands of different applications. We first find that WSC workloads are extremely diverse, breeding the need for architectures that can tolerate application variability without performance loss. However, some patterns emerge, offering opportunities for co-optimization of hardware and software. For example, we identify common building blocks in the lower levels of the software stack. This \\\"datacenter tax\\\" can comprise nearly 30% of cyc\n",
      "\n",
      "2. id: 5390b7fe20f70186a0f265ee   score: 0.5266471   abstract: We provide a summary of the outcomes from the Workshop on Big Data Benchmarking (WBDB2012) held on May 8-9, 2012 in San Jose, CA. The workshop discussed a number of issues related to big data benchmarking definitions and benchmark processes, and was attended by 60 invitees representing 45 different organizations from industry and academia. Attendees were selected based on their experience and expertise in one or more areas of big data, database systems, performance benchmarking, and big data applications. The participants concluded that there exists both a need and an opportunity for defining benchmarks to capture the end-to-end aspects of big data applications. The metrics for such benchmarks would need to include metrics for performance as well as price/performance, and consider several costs including total system cost, setup cost, and energy costs. The next Workshop on Big Data Bench\n",
      "\n",
      "3. id: 5590fd8f0cf2ce4b6f3a0f60   score: 0.503313   abstract: Benchmarking for Big Data is done at the system level, but with processors now being designed specifically for Cloud Computing and Big Data applications, optimization can now be done at the node level. The purpose of this work is to analyze three SPEC CPU2006 Integer benchmarks (libquantum, h264ref and hmmer) that were deemed \\\"highly memory sensitive\\\" in other works to determine their potential as Big Data processor benchmarks. Program characteristics like instruction count, instruction mix, locality, and memory footprint were analyzed. Through this preliminary analysis, these benchmarks were determined to be potential Big Data node-level benchmarks, but more analysis will have to be done in future work.\n",
      "\n",
      "4. id: 558c17b00cf2e30013db539c   score: 0.48332834   abstract: Today's largest datacenters dissipate megawatts of power. Efficiency is rapidly becoming the primary determinant of datacenter capability. To understand microarchitectural factors that affect efficiency, we must study datacenter workloads.\n",
      "\n",
      "5. id: 5534d8bd45cedae85c37970a   score: 0.40380192   abstract: Big Data applications have become more and more important over the last few years. Such applications are focused on the analysis of huge amounts of unstructured information and present a series of differences with traditional High Performance Computing (HPC) applications. For illustrating such dissimilarities, this paper analyzes the behavior of the most scalable version of the Graph500 benchmark when run on a state-of-the-art commodity cluster facility. Our work shows that this new computation paradigm stresses the interconnection subsystem. In this work, we provide both analytical and empirical characterizations of the Graph500 benchmark, showing that its communication needs bound the achieved performance on a cluster facility. Up to our knowledge, our evaluation is the first to consider the impact of message aggregation on the communication overhead and explore the selection of a trad\n",
      "\n",
      "6. id: 558b13df612c41e6b9d42528   score: 0.2903768   abstract: In computing, a benchmark is the result of running a set of computer programs or a computer program, in order to assess relative performance by running a series of standard tests. By doing this, researchers highlight the characteristics of certain systems and are able to rank the system against the rest. On the other hand, BigData is a hot topic. It not only deals with large amounts of data sets and the procedures and tools used to analyze and manipulate them, but also to a computational turn in research and thought. At the same time, Decision Support applications are related to Big Data as they need to deal with large datasets. In this paper we describe two of the most popular benchmarks, one representing Decision Support Systems (TPC-H), and the other represents the Big Data class (YCSB - Yahoo Cloud Serving Benchmark).\n",
      "\n",
      "7. id: 55323c6645cec66b6f9dbffd   score: 0.28517285   abstract: Now we live in an era of big data, and big data applications are becoming more and more pervasive. How to benchmark data center computer systems running big data applications in short big data systems is a hot topic. In this paper, we focus on measuring the performance impacts of diverse applications and scalable volumes of data sets on big data systems. For four typical data analysis applications--an important class of big data applications, we find two major results through experiments: first, the data scale has a significant impact on the performance of big data systems, so we must provide scalable volumes of data sets in big data benchmarks. Second, for the four applications, even all of them use the simple algorithms, the performance trends are different with increasing data scales, and hence we must consider not only variety of data sets but also variety of applications in benchmar\n",
      "\n",
      "8. id: 53909eef20f70186a0e36959   score: 0.20833994   abstract: With the development of new client-server computing models, such as thin clients and network computers, the performance of servers becomes a bottleneck. In these models, servers support a large number of clients. They download significant amounts of data to their clients in the form of graphics, executables (e.g., applets), and video. We present an architecture for building high-performance server systems that can efficiently serve large local clusters of NCs or other clients. The key component in our architecture is a generic cache module that is designed to fully utilize available bus bandwidth. Our experiments show that such a server system can achieve throughput rates of up to 36,000 transactions per second. We detail the design and implementation of the generic cache component, describe its use in the implementation of a sample server system, and show how the architecture can be sca\n",
      "\n",
      "9. id: 5390990f20f70186a0e0fcfe   score: 0.18359363   abstract: With the proliferation of database workloads on servers, much recent research on server architecture has focused on database system benchmarks. The TPC benchmarks for the two most common server workloads, OLTP and DSS, have been used extensively in the database community to evaluate the database system functionality and performance. Unfortunately, these benchmarks fall short of being effective in microarchitecture and memory system research due to several key shortcomings. First, setting up the experimental environment and tuning these benchmarks to match the workload behavior of interest involves extremely complex procedures. Second, the benchmarks themselves are complex and preclude accurate correlation of microarchitecture-and memory-level bottlenecks to dominant workload characteristics. Finally, industrial-grade configurations of such benchmarks are too large and preclude their use \n",
      "\n",
      "10. id: 539099b320f70186a0e1b3e2   score: 0.14378779   abstract: The properties of computer system such as performance, energy, reliability, etc. are commonly evaluated by running benchmarks. However, the benchmarking process is complicated to set-up and use and running the benchmarks takes a substantial amount of time. Furthermore, when designing a computer, architects resort to simulation of the system, increasing the benchmarking time by several orders of magnitude. This problem can be alleviated by reducing the execution time of the benchmark suite. In this paper, we investigate a method to reduce the number of queries in TPC-H, a decision support system benchmark. Our evaluation shows that out of the 22 original queries, a subset of only 6 achieves a high representativeness at only 40% of the original execution time. The results also show that subsets built for a particular database size may be used for evaluating computer systems with other data\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696329\n",
      "index                                        559141740cf232eb904fb7a9\n",
      "title               Beating exhaustive search for quantified boole...\n",
      "authors                                Rahul Santhanam, Ryan Williams\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558b1d8a612c41e6b9d43c60;539087be20f70186a0d53...\n",
      "abstract            We study algorithms for the satisfiability pro...\n",
      "id                                                            1696329\n",
      "clustered_labels                                                    2\n",
      "Name: 1696329, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390adfc20f70186a0ec49f0   score: 0.9860443   abstract: We investigate the possibility of finding satisfying assignments to Boolean formulae and testing validity of quantified Boolean formulae (QBF) asymptotically faster than a brute force search. Our first main result is a simple deterministic algorithm running in time $2^{n - \\Omega(n)}$ for satisfiability of formulae of linear size in $n$, where $n$ is the number of variables in the formula. This algorithm extends to exactly counting the number of satisfying assignments, within the same time bound. Our second main result is a deterministic algorithm running in time $2^{n - \\Omega(n/\\log(n))}$ for solving QBFs in which the number of occurrences of any variable is bounded by a constant. For instances which are ``structured'', in a certain precise sense, the algorithm can be modified to run in time $2^{n - \\Omega(n)}$. To the best of our knowledge, no non-trivial algorithms were known for the\n",
      "\n",
      "2. id: 5390b36120f70186a0ef254f   score: 0.9840936   abstract: We give a randomized algorithm for testing satisfiability of Boolean formulas in conjunctive normal form with no restriction on clause length. Its running time is at most 2n(1−1/α) up to a polynomial factor, where α = ln (m/n) + O(ln ln m) and n, m are respectively the number of variables and the number of clauses in the input formula. This bound is asymptotically better than the previously best known 2n(1−1/log(2m)) bound for SAT.\n",
      "\n",
      "3. id: 5390a74f20f70186a0e8c91b   score: 0.9819447   abstract: In this paper we approach the problem of reasoning with quantified Boolean formulas (QBFs) by combining search and resolution, and by switching between them according to structural properties of QBFs. We provide empirical evidence that QBFs which cannot be solved by search or resolution alone, can be solved by combining them, and that our approach makes a proof-of-concept implementation competitive with current QBF solvers.\n",
      "\n",
      "4. id: 5390a7f520f70186a0e9447a   score: 0.9800149   abstract: Say that an algorithm solving a Boolean satisfiability problem x on n variables is improved if it takes time poly(|x|)2 cn for some constant c i.e., if it is exponentially better than a brute force search. We show an improved randomized algorithm for the satisfiability problem for circuits of constant depth d and a linear number of gates cn: for each d and c, the running time is 2(1 驴 驴)n where the improvement $\\delta\\geq 1/O(c^{2^{d-2}-1}\\lg^{3\\cdot 2^{d-2}-2}c)$, and the constant in the big-Oh depends only on d. The algorithm can be adjusted for use with Grover's algorithm to achieve a run time of $2^{\\frac{1-\\delta}{2}n}$ on a quantum computer.\n",
      "\n",
      "5. id: 5390995d20f70186a0e158b4   score: 0.97458035   abstract: The best currently available solvers for Quantified Boolean Formulas (QBFs) process their input in prenex form, i. e., all the quantifiers have to appear in the prefix of the formula separated from the purely propositional part representing the matrix. However, in many QBFs deriving from applications, the propositional part is intertwined with the quantifier structure. To tackle this problem, the standard approach is to first convert them in prenex form, thereby loosing structural information about the prefix.In this paper we show that conversion to prenex form is not necessary, i. e., that it is relatively easy to extend current search based solvers in order to exploit the original quantifier structure, i. e., to handle non prenex QBFs. Further, we show that the conversion can lead to the exploration of search spaces bigger than the space explored by solvers handling non prenex QBFs. To\n",
      "\n",
      "6. id: 5390980720f70186a0e02499   score: 0.9740426   abstract: In recent years, there has been an increasing interest in Quantified Boolean Formula (QBF) evaluation, since several VLSI CAD problems can be formulated efficiently as QBF instances. Since the original resolution-based methods can suffer from space explosion, existing QBF solvers perform Decision Tree search using the Davis-Putnam Logemann and Loveland (DPLL) procedure. In this paper, we propose a new QBF solver, Q-PREZ, that overcomes the space explosion problem faced in resolution by usingefficient data structures and algorithms, which in turn can outperform DPLL-based QBF solvers. We partition the CNF and store the clauses compactly in Zero-Suppressed Binary Decision Diagrams (ZBDDs). Then, we introduce new and powerful operators to perform existential and universal quantification on the partitioned ZBDD clauses as resolution and elimination procedures. Our preliminary experimental re\n",
      "\n",
      "7. id: 5390b2d720f70186a0eec62f   score: 0.9730877   abstract: Recently Schuler [17] presented a randomized algorithm that solves SAT in expected time at most $2^{n(1-1/{\\rm log}_{2}(2m))}$ up to a polynomial factor, where n and m are, respectively, the number of variables and the number of clauses in the input formula. This bound is the best known upper bound for testing satisfiability of formulas in CNF with no restriction on clause length (for the case when m is not too large comparing to n). We derandomize this algorithm using deterministic k-SAT algorithms based on search in Hamming balls, and we prove that our deterministic algorithm has the same upper bound on the running time as Schuler’s randomized algorithm.\n",
      "\n",
      "8. id: 5390ad0720f70186a0ebb05b   score: 0.9686789   abstract: We describe reductions from the problem of determining the satisfiability of Boolean CNF formulas (CNF-SAT) to several natural algorithmic problems. We show that attaining any of the following bounds would improve the state of the art in algorithms for SAT: • an O(nk-ε) algorithm for k-Dominating Set, for any k ≥ 3, • a (computationally efficient) protocol for 3-party set disjointness with o(m) bits of communication, • an n°(d) algorithm for d-SUM, • an O(n5-ε) algorithm for 2-SAT formulas with m = n1+0(1) clauses, where two clauses may have unrestricted length, and • an O((n + m)k-ε) algorithm for HornSat with k unrestricted length clauses. One may interpret our reductions as new attacks on the complexity of SAT, or sharp lower bounds conditional on exponential hardness of SAT.\n",
      "\n",
      "9. id: 5390b0ca20f70186a0edb325   score: 0.9668514   abstract: I will describe prior and current work on connecting the art of finding good satisfiability algorithms with the art of proving complexity lower bounds: proofs of limitations on what problems can be solved by good algorithms. Surprisingly, even minor algorithmic progress on solving the circuit satisfiability problem faster than exhaustive search can be applied to prove strong circuit complexity lower bounds. These connections have made it possible to prove new complexity lower bounds that had long been conjectured, and they suggest concrete directions for further progress.\n",
      "\n",
      "10. id: 5390b72e20f70186a0f20f5c   score: 0.9656413   abstract: We present a moderately exponential time algorithm for the satisfiability of Boolean formulas over the full binary basis. For formulas of size at most $cn$, our algorithm runs in time $2^{(1-\\mu_c)n}$ for some constant $\\mu_c0$. As a byproduct of the running time analysis of our algorithm, we get strong average-case hardness of affine extractors for linear-sized formulas over the full binary basis.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674932\n",
      "index                                        559152a40cf2127aa930cb53\n",
      "title               Content Destabilization for Head-Mounted Displays\n",
      "authors                       Felix Lauber, Sophia Cook, Andreas Butz\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390979920f70186a0e01367;5390b86b20f70186a0f28c2b\n",
      "abstract            With recent progress in display technology, vi...\n",
      "id                                                            1674932\n",
      "clustered_labels                                                    0\n",
      "Name: 1674932, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b5df20f70186a0f0b119   score: 0.7932662   abstract: This articles talks about the present and future challenges in display technologies.\n",
      "\n",
      "2. id: 5390a72220f70186a0e88fd7   score: 0.6420959   abstract: Two common limitations of modern Head Mounted Displays (HMD): the narrow field of view and limited dynamic range, call for rendering techniques that can circumvent or even take advantage of these factors. We describe a simple practical method of enhancing visual response from HMDs by using view-dependent control over lighting. One example is provided for simulating blinding lights in dark environments.\n",
      "\n",
      "3. id: 5390ad5620f70186a0ebe423   score: 0.5951397   abstract: Physical characteristics and constraints of today's head-mounted displays (HMDs) often impair interaction in immersive virtual environments (VEs). For instance, due to the limited field of view (FOV) subtended by the display units in front of the user's eyes more effort is required to explore a VE by head rotations than for exploration in the real world. In this paper we propose a combination of two augmentation techniques that have the potential to make exploration of VEs more efficient: (1) augmenting the geometric FOV (GFOV) used for rendering the VE, and (2) amplifying head rotations while the user changes her head orientation. In order to identify how much manipulation can be applied without users noticing, we conducted two psychophysical experiments in which we analyzed subjects' ability to discriminate between virtual and real head pitch and roll rotations while three different ge\n",
      "\n",
      "4. id: 558bd1dd0cf20e727d0f2314   score: 0.54598176   abstract: Head-mounted displays (HMDs) have great potential to improve the current situation of car drivers. They provide every benefit of a head-up display (HUD), while at the same time showing more flexibility in usage. We built an infotainment system specifically designed to be displayed in an HMD. With this system, we then conducted a dual task study in a driving simulation, comparing different techniques of content stabilization (head- and cockpit stabilized visualizations). Interaction with the system took place via a physical input device (rotary controller) or indirect pointing gestures. While cockpit-stabilized content generally resulted in a slightly better driving performance, HMD visualizations suffered from technological limitations, partly reflected in the secondary task performance and subjective feedback. Regarding input modality, we found that horizontal gesture interaction signif\n",
      "\n",
      "5. id: 5390be6620f70186a0f4c512   score: 0.4173902   abstract: One unique property of head-mounted displays (HMDs) is that content can easily be displayed at a fixed position within the user's field of view (head-stabilized). This ensures that critical information (e.g. warnings) is continuously visible and can, in principle, be perceived as quickly as possible. We examined this strategy with a physically and visually distracted driver. We ran two consecutive studies in a driving simulator, comparing different warning visualizations in a head-up display (HUD) and a HMD. In an initial study, we found no significant effects of warning type or display technology on the reaction times. In a second study, after modifying our visualization to include a visual reference marker, we found that with only this minor change, reaction times were significantly lower in the HMD when compared to the HUD. Our insights can help others design better head-stabilized no\n",
      "\n",
      "6. id: 5390b20120f70186a0ee3b9e   score: 0.3226327   abstract: The authors investigate whether a high-resolution head-mounted display, roving around a much larger frame buffer image, can give a user the impression of viewing a single very large display screen. A prototype is constructed, consisting of an 1120 /spl times/ 900 pixel head-mounted display, an ultrasonic head-tracker, a 16,386 /spl times/ 6,144 pixel frame buffer, and suitable X-window control software, as a means of studying this question. Applications can write to the large frame buffer using the window system, and the view can navigate around the image rapidly using head rotations. The prototype system, although somewhat awkward to use due to a limited field of view in the head-mounted display, shows that head rotation is a fast, convenient way to switch display contexts.\n",
      "\n",
      "7. id: 5390882d20f70186a0d8e40e   score: 0.24617071   abstract: Perceptual factors that affect monocular, transparent (a.k.a \"see-thru\") head-mounted displays include binocular rivalry, visual interference, and depth of focus. We report the results of an experiment designed to evaluate the effects of these factors on user performance in a table look-up task. Two backgrounds were used. A dynamic moving background was provided by a large screen TV and an untidy bookshelf was used to provide a complex static background. With the TV background large effects were found attributable to both rivalry and visual interference. These two effects were roughly additive. Smaller effects were found with the bookshelf. In conclusion we suggest that monocular transparent HMDs may be unsuitable for use in visually dynamic environments. However when backgrounds are relatively static, having a transparent display may be preferable to having an opaque display.\n",
      "\n",
      "8. id: 5390990f20f70186a0e0fb51   score: 0.23651622   abstract: We built a video see-through head-mounted display with zero eye offset from commercial components and a mount fabricated via rapid prototyping. The orthoscopic HMD's layout was created and optimized with a software simulator. We describe simulator and HMD design, we show the HMD in use and demonstrate zero parallax.\n",
      "\n",
      "9. id: 5390975920f70186a0dfc5d4   score: 0.22712629   abstract: Many applications have used a head-mounted display (HMD), such as in virtual and mixed realities and telepresence. However, the field of view (FOV) of commercial HMD systems is too narrow for feeling immersion. In this paper, we propose a super-wide field of view head-mounted display consisting of an ellipsoidal mirror and a hyperboloidal curved mirror. The horizontal FOV of the proposed HMD is 180 degrees and includes the peripheral vision of humans. It increases the reality and immersion for users.\n",
      "\n",
      "10. id: 53908bde20f70186a0dc73d7   score: 0.21866935   abstract: A Head Mounted Display system suffers largely from the time lag between human motion and the display output. The concept of Reflex HMD to compensate for the time lag is proposed and discussed. Based on this notion, a prototype Reflex HMD is constructed. The rotation movement of the user's head is measured by a gyroscope, modulating the driving signal for the LCD panel, and shifts the viewport within the image supplied from the computer. The derivative distortion is investiated and the dynamic deformation of the watched world was picked up as the essenial demerit. The cylinderical rendering is introduced to solve this problem and is proved to cancel this dynamic deformation and also decrease the static distortion.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1650979\n",
      "index                                        55916fa70cf2e89307ca9ca6\n",
      "title               Using Multiple Contexts to Detect and Form Opp...\n",
      "authors                            Adrian A. de Freitas, Anind K. Dey\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390afc920f70186a0ed2c5f;5390b72e20f70186a0f21...\n",
      "abstract            We present a new technique that allows mobile ...\n",
      "id                                                            1650979\n",
      "clustered_labels                                                    3\n",
      "Name: 1650979, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558f90330cf2cb5aa76744ef   score: 0.9904406   abstract: In this paper, we present the Group Context Framework (GCF), a general-purpose toolkit that allows mobile devices to opportunistically share contextual information. GCF provides a standardized way for developers to request contextual data for their applications. The framework then intelligently groups with other devices to satisfy these requirements. Through two prototypes, we demonstrate how GCF can be used to support a broad range of collaborative and cooperative tasks. We then show how our framework's architecture allows devices to opportunistically detect and collaborate with one another, even when running different applications. Finally, we present two real-world domains that show how GCF's ability to form groups increases users' access to relevant and timely information, and discuss possible incentives and safeguards to context sharing from a user standpoint.\n",
      "\n",
      "2. id: 53909fca20f70186a0e43cd5   score: 0.7666432   abstract: In ubiquitous environments, invisible devices and software are connected to one another to provide convenient services to users. In order to provide such services, we must have mobile devices that connect users and services. However, the types of services available have thus far been limited due to the limited resources of mobile devices. This paper will propose a solution to the resource problem of mobile devices by presenting a context-based collaboration system that allows mobile devices to share various nearby resources. We have tested our context-based collaboration system by using the test scenario. This paper focuses on sharing different devices by recognizing context data in a given situation.\n",
      "\n",
      "3. id: 5390b13020f70186a0edc1b8   score: 0.59684443   abstract: In this paper we present a general concept of a mobile access to a groupware. The central aspect is how to bridge the gap between Mobile Computing and Collaborative Software. Mobile devices have limited capabilities, and therefore only few user interactions are desired. Conversely, groupware requires numerous interactions in order to make virtual collaborative work effective. First, we examine existing approaches and define our specific goal. Then, we present background on our research on user requirements. Afterwards, the general aspects of a prototype we developed are shown, including exemplary examples. After having given information about the first evaluation results, we end with a short conclusion stating our future work.\n",
      "\n",
      "4. id: 5390ad8920f70186a0ebf846   score: 0.5427724   abstract: Mobile phones are increasingly being used collaboratively by social networks of users in spite of the fact that they are primarily designed to support single users and one-to-one communication. It is not well understood how services such as group SMS, SMS-based discussion lists and mobile Instant Messaging (IM) will be used by mobile groups in natural settings. Studying specific instances of common styles of in situ, group interaction may provide a way to see behavior patterns and typical interaction problems. We conducted a study of a mobile, group communication probe used during a rendezvousing activity in an urban environment. Usability problems relating to group usage, phone interface design and context were identified. Several major issues included: multitasking during message composition and reading; speed of text entry; excessive demand on visual attention; and ambiguity of intend\n",
      "\n",
      "5. id: 5390a37f20f70186a0e6da2f   score: 0.49962044   abstract: In this paper we present a context-aware data sharing service over MANet to enable spontaneous collaboration between mobile workers. Context information is used to adapt services and applications to user situation as well as working groups situation. The paper first presents the data sharing system. It then describes the context awareness service. We explain how data sharing uses context information to adapt to changing conditions and how the context service relies on data sharing to store context information afterwards. Context aware and data sharing services maintain a mutually beneficial interaction. Both services are part of a software framework for spontaneous collaboration over MANets that is developed within the Popeye european IST research project.\n",
      "\n",
      "6. id: 5390979920f70186a0dffbbc   score: 0.49649242   abstract: Researchers have noted conflicting trends in collaboration technologies between delivering more information on larger displays and exploiting mobility on smaller devices. Large, shared displays provide greater choice in the presentation of information, but mobile devices offer greater flexibility in the access of information. We describe a platform that leverages the best of both worlds by allowing multiple users to access and interact with a large, shared display using their own personal mobile devices, such as a cell phone, laptop, or wireless PDA. We highlight three applications built on top of the platform that demonstrate its generality and utility in a variety of group settings: namely, web browsing, polling, and entertainment.\n",
      "\n",
      "7. id: 5390975920f70186a0dfe03f   score: 0.4618055   abstract: In this paper, we present an implemented system for supporting group interaction in mobile distributed computing environments. First, an introduction to context computing and a motivation for using contextual information to facilitate group interaction is given. We then present the architecture of our system, which consists of two parts: a subsystem for location sensing that acquires information about the location of users as well as spatial proximities between them, and one for the actual context-aware application, which provides services for group interaction.\n",
      "\n",
      "8. id: 5390bb7b20f70186a0f3fc19   score: 0.4344613   abstract: The importance of mobile groupware systems resides in the specific tasks that they can perform and other systems cannot. On the one hand, groupware systems allow groups of users to work together providing facilities that single-user systems are unable to offer. On the other hand, unlike stationary systems, mobile systems allow users to work on the move. The intersection of these two technologies offers a new support for activities, such as spontaneous collaboration, that could be facilitated neither by stationary groupware systems nor by mobile single-user systems. However, implementations of this new support are uncommon, probably because of the high development effort required and the seemingly little benefit obtained. In this paper, we aim to reduce this effort by facilitating the development of mobile groupware applications that support such activities. Our proposal to achieve this o\n",
      "\n",
      "9. id: 5390975920f70186a0dfe6e7   score: 0.39053392   abstract: The growing diffusion of wireless-enabled portable devices and the recent advances in Mobile Ad-Hoc Networks (MANET) open a new scenario where users can benefit from anywhere/anytime impromptu collaboration. However, the development of collaborative services in MANET environments raises new challenges and calls for novel middleware solutions to handle properly the communication between transiently collaborating partners. The paper proposes AGAPE, a context-aware group communication middleware that permits to select collaborating partners, to schedule incoming messages and to tailor their presentation on the basis of group members context, e.g. depending on member's location, attributes, and device properties.\n",
      "\n",
      "10. id: 558b802c612c6b62e5e8a115   score: 0.3454364   abstract: We present a touch-based method for binding mobile devices for collaborative interactions in a group of collocated users. The method is highly flexible, enabling a broad range of different group formation strategies. We report an evaluation of the method in medium-sized groups of six users. When forming a group, the participants primarily followed viral patterns where they opportunistically added other participants to the group without advance planning. The participants also suggested a number of more systematic patterns, which required the group to agree on a common strategy but then provided a clear procedure to follow. The flexibility of the method allowed the participants to adapt it to the changing needs of the situation and to recover from errors and technical problems. Overall, device binding in medium-sized groups was found to be a highly collaborative group activity and the bind\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710578\n",
      "index                                        55323bdb45cec66b6f9dabdb\n",
      "title                    First-class domain specific aspect languages\n",
      "authors                                   Arik Hadas, David H. Lorenz\n",
      "year                                                           2015.0\n",
      "venue               Companion Proceedings of the 14th Internationa...\n",
      "references                                   558b9031612c6b62e5e8ba20\n",
      "abstract            Programming in a domain specific aspect langua...\n",
      "id                                                            1710578\n",
      "clustered_labels                                                    3\n",
      "Name: 1710578, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55323bd145cec66b6f9daa36   score: 0.9839708   abstract: Domain specific aspect languages (DSALs) are programming languages that are both domain specific and aspect-oriented. However, DSALs seem to be second-class. On the one hand, language workbenches handle only DSLs that are not aspect-oriented, making it difficult to develop new DSALs. On the other hand, development tools for general purpose aspect-oriented languages do not work with DSALs, making it difficult to use them. In this work we present an approach for building a modular DSAL workbench that produces first-class DSALs. A DSAL is said to be first-class if development tools treat it as a general purpose AOP language. Specifically, this means that first-class DSALs for Java can be used with tools that work with AspectJ. For concreteness, we illustrate the approach by describing our implementation of a DSAL workbench, comprising the Spoofax language workbench and the Awesome compositi\n",
      "\n",
      "2. id: 5390ad0620f70186a0eba27f   score: 0.9608783   abstract: Today, language-oriented programming (LOP) is realized by using either language workbenches or internal DSLs, each with their own advantages and disadvantages. In this work, we design a host language for DSLs with language workbench features, thereby combining the two approaches and enjoying the best of both worlds.\n",
      "\n",
      "3. id: 5390a1d420f70186a0e58002   score: 0.9599126   abstract: Domain specific aspect languages (DSALs) are becoming more popular because they can be designed to represent recurring concerns in a way that is optimized for a specific domain. However, the design and implementation of even a limited domain-specific aspect language can be a tedious job. To address this, we propose a framework that offers a fast way to prototype implementations of domain specific aspect languages. A particular goal of the framework is to be general enough to support a wide range of aspect language concepts, such that existing language concepts can be easily used, and new language concepts can be quickly created.We briefly introduce the framework and its underlying model, as well as the workflow used when implementing DSALs. Subsequently, we show mappings of several domain specific aspect languages to demonstrate the framework. Since in our approach the DSALs are mapped t\n",
      "\n",
      "4. id: 5390aefc20f70186a0ecde4a   score: 0.9530133   abstract: It is a great pleasure to welcome you, in the context of the AOSD.11 program, to this sixth edition of DSAL, a series of workshops dedicated to the exploration of the area of domain-specific aspect languages, including language design, enabling technologies and composition issues. We take this opportunity to thank all paper submitters for contributing to the work on domainspecific aspect languages, the authors of accepted papers for providing discussion material, and the program committee for their swift reviews.\n",
      "\n",
      "5. id: 5390ad0620f70186a0eba2a8   score: 0.9508692   abstract: Domain-specific languages (DSLs) provide high expressive power focused on a particular problem domain. They provide linguistic abstractions and specialized syntax specifically designed for a domain, allowing developers to avoid boilerplate code and low-level implementation details. Language workbenches are tools that integrate all aspects of the definition of domain-specific or general-purpose software languages and the creation of a programming environment from such a definition. To count as a language workbench, a tool needs to satisfy basic requirements for the integrated definition of syntax, semantics, and editor services, and preferably also support language extension and composition. Within these requirements there is ample room for variation in the design of a language workbench. In this tutorial, we give an introduction to the state of the art in textual DSLs and language workbe\n",
      "\n",
      "6. id: 5390a30b20f70186a0e69de2   score: 0.9207897   abstract: Like programs written in general-purpose languages, programs written in DSLs may also suffer from tangling and scattering in the presence of domain-specific crosscutting concerns. This paper presents an architecture that supports aspect-oriented features for domain-specific base languages. Both base programs and advices are written in different domain-specific languages. The framework relies on the concept of domain-specific join point.\n",
      "\n",
      "7. id: 5390b44620f70186a0ef8355   score: 0.9036349   abstract: Being able to define and use different aspect languages, including domain-specific aspect languages, to cleanly modularize concerns of a software system represents a valuable perspective. However, combining existing tools leads to unpredictable results, and proposals for experimentation with and integration of aspect languages mostly fail to deal with composition satisfactorily and to provide convenient abstractions to implement new aspect languages. This paper exposes the architecture of a versatile AOP kernel and its Java implementation, Reflex. On top of basic facilities for behavioral and structural transformation, Reflex provides composition handling, including detection of interactions, and language support via a lightweight plugin architecture. We present these facilities and illustrate composition of aspects written in different aspect languages.\n",
      "\n",
      "8. id: 5591715b0cf2e89307ca9d69   score: 0.8688268   abstract: Domain-Specific Aspect Languages (DSALs) are Domain-Specific Languages (DSLs) designed to express crosscutting concerns. Compared to DSLs, their aspectual nature greatly amplifies the language design space. We structure this space in order to shed light on and compare the different domain-specific approaches to deal with crosscutting concerns. We report on a corpus of 36 DSALs covering the space, discuss a set of design considerations, and provide a taxonomy of DSAL implementation approaches. This work serves as a frame of reference to DSAL and DSL researchers, enabling further advances in the field, and to developers as a guide for DSAL implementations.\n",
      "\n",
      "9. id: 5390b44620f70186a0ef92da   score: 0.83508044   abstract: It is our great pleasure to host the seventh edition of the Domain-Specific Aspect Languages workshop (DSAL12), as part of AOSD 2012: Perspectives on Modularity, the 11th International Conference on Aspect-Oriented Software Development. The tendency to raise the abstraction level in programming languages towards a particular domain is also a major driving force in the research domain of aspect-oriented programming languages. As a matter of fact, pioneering work in this field was conducted by devising small domain-specific aspect languages (DSALs) such as COOL for concurrency management and RIDL for serialization, RG, AML, and others. After a dominating focus on general-purpose languages, research in the AOSD community is again taking this path in search of innovative approaches, insights and a deeper understanding of fundamentals behind AOP. Based on the successful DSAL'06-'11 workshops,\n",
      "\n",
      "10. id: 53909ee020f70186a0e33a5d   score: 0.8269914   abstract: Domain-specific aspect languages (DSALs) bring the well-known advantages of domain specificity to the level of aspect code. However, DSALs incur the significant cost of implementing or extending a language processor or weaver. This raises the necessity of an appropriate infrastructure for DSALs. This paper illustrates how the Reflex kernel for multi-language AOP allows for the definition of DSALs, by considering the implementation of a DSAL for advanced transaction management, KALA. We detail the implementation of KALA in Reflex, illustrating the ease of implementation of runtime semantics, syntax, and language translation.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710396\n",
      "index                                        55323bda45cec66b6f9dab91\n",
      "title               An immune optimization based real-valued negat...\n",
      "authors                                Xin Xiao, Tao Li, Ruirui Zhang\n",
      "year                                                           2015.0\n",
      "venue                                            Applied Intelligence\n",
      "references          5390ae2e20f70186a0ec7255;5390a17720f70186a0e51...\n",
      "abstract            Negative selection algorithms are important fo...\n",
      "id                                                            1710396\n",
      "clustered_labels                                                    0\n",
      "Name: 1710396, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a8db20f70186a0e9d7f4   score: 0.9931496   abstract: A new algorithm of detector generation for negative selection algorithm is introduced by adding a big detector to reach a high coverage of non-self space. While the big detector can be variable in different shape using this concept, the paper puts forward an algorithm using a ring-hyper-sphere shaped detector as a big detector. The algorithm is tested using different self set(or training set) and real-word dataset. Preliminary results demonstrate that the new approach enhances the negative selection algorithm in efficiency and reliability without significant increase in complexity.\n",
      "\n",
      "2. id: 5390ae2e20f70186a0ec82b6   score: 0.9920002   abstract: As traditional negative selection algorithms need to calculate the distances between initial detectors and all the self data, the efficiencies of them are too low. In this article, before the creation process of detectors, the self data is pretreated to be a SS-Tree, and then the negative selection process is transformed into the data query process in the self SS-Tree to improve the efficiency of detectors creation.\n",
      "\n",
      "3. id: 5390bf1320f70186a0f503e9   score: 0.9851576   abstract: Negative selection algorithm (NSA) is an important algorithm for the generation of artificial immune detectors. However, the randomly generated candidate detectors have to be compared with the whole self set to exclude self reactive detectors. The inefficiency of the comparing process seriously limited the application of immune algorithms. Therefore, a new negative selection algorithm GF-RNSA is proposed in the paper. Firstly, the feature space is divided into a number of grid cells, and then detectors are separately generated in each cell. As candidate detectors just need to compare with the self antigens located in the same cell rather than with the whole self set, the detector training can be more efficient. The theoretical analysis demonstrated that the time complexity of GF-RNSA is effectively reduced that the exponential relationships between self size and time complexity in tradit\n",
      "\n",
      "4. id: 53908a9620f70186a0da52b0   score: 0.98215127   abstract: In this work we use a simplified model of the immune system to explore the problem solving feature. We consider only two immunological entities, antigens and antibodies, two parameters, and simple immune operators. The experimental results shows how a simple randomized search algorithm coupled with a mechanism for adaptive recognition of hardest constraints, is sufficient to obtain optimal solutions for any combinatorial optimization problem.\n",
      "\n",
      "5. id: 5390b24420f70186a0ee7430   score: 0.9817707   abstract: In order to improve the generating e.ciency of the detector set, a new detection rule called edit distance rule is presented in this paper, based on the negative selection model of Arti.cial Immune System (AIS). Under this rule, edit distance is adopted to measure the similarity between self strings and randomly generated strings. Then a new detector generating algorithm used the new rule is discussed. It is necessary to use the Trie data structure to store the strings in the self set in this new algorithm. Finally, the advantages of the algorithm are given through the theoretical analysis.\n",
      "\n",
      "6. id: 5390985d20f70186a0e07442   score: 0.98087597   abstract: This paper proposes a statistical mechanism to analyze the detector coverage in a negative selection algorithm, namely a quantitative measurement of a detector set's capability to detect nonself data. This novel method has the advantage of statistical confidence in the estimation of the actual coverage. Furthermore, unlike the existing analysis works of negative selection, it doesn't depend on specific detector representation and generation algorithm. Not only can it be implemented as a procedure independent from the steps to generate detectors, the experiments in this paper showed that it can also be tightly integrated into the detector generation algorithm to control the number of detectors.\n",
      "\n",
      "7. id: 5390a88c20f70186a0e98a38   score: 0.97779876   abstract: According to negative selection model in artificial immune system, the paper mainly studies and improves initial linear time detector set generating algorithm. The algorithm constructs two arrays C and C' from two directions, then produces array D by cross product so that the detector can match more Nonself strings, and removes redundant detectors, reduces detector set scale, improves its method of design, performance analysis and testing. The results show that the improved algorithm reduces detector scale and probability of missed detection.\n",
      "\n",
      "8. id: 53909f2c20f70186a0e3749d   score: 0.9772842   abstract: One of the intriguing applications of immune-inspired negative selection algorithm is anomaly detection in the datasets. Such a detection is based on the self/nonself discrimination and its characteristic feature is the ability of detecting nonself samples (anomalies) by using only information about the self, or regular, samples. Thus the problem space (Universe) is splitted into two disjoint subspaces: One of them contains self samples and the second is covered by the samples which activate the detectors generated by the negative selection algorithms. Hence, the efficiency of negative selection algorithms is proportional to the degree of coverage (by the detectors) of nonself subspace. In this paper, we present a simple method of increasing the coverage for real-valued negative selection algorithm.\n",
      "\n",
      "9. id: 5390a2e920f70186a0e66c30   score: 0.97684664   abstract: In this paper, as the generation of naive B-cell in natural immune system, the concept of minimum set of candidate detectors (M) is introduced to cover the whole area of ID problems completely. Then the mechanism of initializing and storing set M are analyzed and provided. Based on M, a new negative selection algorithm (M-Based Negative Selection Algorithm, MBNSA) is proposed. As compared with traditional algorithms, MBNSA generates detectors with higher efficiency. Additionally, to the problems arisen by blind and random detector’s generation, two improved methods of MBNSA is put forward to realize the mechanism \"detecting only when need\". Experimental results demonstrate the efficiency of the proposed schemes.\n",
      "\n",
      "10. id: 5390b2fc20f70186a0eefdfb   score: 0.9765801   abstract: This paper will present a method for generating immune detectors inspired by the working of charged particle swarm optimisation and negative selection in the artificial immune system. The paper will also test the efficiency of the proposed method against a benchmark in order to prove that the proposed method is superior to stochastic generation.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1718393\n",
      "index                                        55323c8445cec66b6f9dc422\n",
      "title               Quantum corrected drift-diffusion model for te...\n",
      "authors             Aritra Acharyya, Jayabrata Goswami, Suranjana ...\n",
      "year                                                           2015.0\n",
      "venue                            Journal of Computational Electronics\n",
      "references                                   558bd0560cf23f2dfc593894\n",
      "abstract            The authors have developed a quantum corrected...\n",
      "id                                                            1718393\n",
      "clustered_labels                                                    1\n",
      "Name: 1718393, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558bd0560cf23f2dfc593894   score: 0.98134655   abstract: Quantum correction is necessary on the classical drift-diffusion (CLDD) model to predict the accurate behavior of high frequency performance of ATT devices at frequencies greater than 200 GHz when the active layer of the device shrinks in the range of 150---350 nm. In the present work, a quantum drift-diffusion model for impact avalanche transit time (IMPATT) devices has been developed by incorporating appropriate quantum mechanical corrections based on density-gradient theory which macroscopically takes into account important quantum mechanical effects such as quantum confinement, quantum tunneling, etc. into the CLDD model. Quantum potentials (synonymous as Bohm potentials) have been incorporated in the current density equations as necessary quantum mechanical corrections for the analysis of millimeter-wave (mm-wave) and Terahertz (THz) IMPATT devices. It is observed that the large-sig\n",
      "\n",
      "2. id: 5390b5c620f70186a0f08722   score: 0.5951397   abstract: The analysis of DDR and DAR IMPATT diodes has been realized on basis of the precise driftdiffusion nonlinear model. The admittance and energy characteristics of the DAR diode were analyzed in very wide frequency band from 30 up to 360 GHz and had been optimized for two high frequency bands near the 200 and 300 GHz.\n",
      "\n",
      "3. id: 55922e330cf244696a09da28   score: 0.5946101   abstract: We propose a device modeling theory based on an improved drift---diffusion solution that is suitable for simulation of the efficiency droop effect in GaN LED. Our theory modifies the drift---diffusion transport by adding a non-local carrier transport component that mimics the effect of hot carriers near the multiple quantum well region. The non-local transport model is supported by recent experimental evidence of Auger-induced hot carriers as well as explaining the experimental low turn-on voltage that conventional drift---diffusion theory fails to predict. A surprising finding from the simulation is that the hot-Auger carriers have a positive effect of reducing the junction resistance of the LED and thus help improve the overall wall-plug efficiency.\n",
      "\n",
      "4. id: 5390aa7620f70186a0eabb91   score: 0.5749915   abstract: The analysis of DDR and DAR IMPATT diodes has been realized on basis of the precise driftdiffusion nonlinear model. The admittance characteristics of the DAR diode were analyzed in very wide frequency band from 30 up to 360 GHz and had been optimized for two high frequency bands near the 200 and 300 GHz.\n",
      "\n",
      "5. id: 539089d320f70186a0d9c2a8   score: 0.49412563   abstract: In this paper, we derive a coupled Schrödinger drift-diffusion self-consistent stationary model for quantum semiconductor device simulations. The device is decomposed into a quantum zone (where quantum effects are expected to be large) and a classical zone (where they are supposed negligible). The Schrödinger equation is solved for scattering states in the quantum zone while a drift-diffusion model is used in the classical zone. The two models are coupled through interface conditions which are derived from those of N. Ben Abdallah (1998, J. Star. Phys. 90, 627) through a diffusion approximation. Numerical tests in the case of a resonant tunneling diode illustrate the validity of the method.\n",
      "\n",
      "6. id: 5390a1f820f70186a0e5dace   score: 0.48979333   abstract: A new empirical nonlinear model of GaN-based electron devices is presented in the article. The model takes into account low-frequency dispersion due to self-heating and charge-trapping phenomena and provides accurate predictions at frequencies where nonquasi-static effects are important. The model is based on the application of a recently proposed equivalent-voltage approach and is identified by using pulsed measurements of drain current characteristics and pulsed S-parameter sets. Full experimental validation on a GaN on SiC PHEMT is provided at both small- and large-signal operating conditions. © 2008 Wiley Periodicals, Inc. Int J RF and Microwave CAE, 2008.\n",
      "\n",
      "7. id: 558b0d08612c41e6b9d41899   score: 0.40503684   abstract: A large-signal method based on non-sinusoidal voltage excitation model is used to study the DC and RF characteristics of Double Avalanche Region (DAR) Silicon Transit Time diode. A large-signal simulation program based on drift-diffusion model is developed for this study. The simulation results show the existence of several distinct negative conductance bands in the admittance characteristics separated by positive conductance. Thus the DAR device is capable of delivering RF power not only at the design frequency but also at several frequency bands higher than the design frequency band in the mm-wave regime. A comparative study with DDR Si device designed to deliver RF power at a particular mm-wave frequency band shows that DAR Si device is capable of delivering significantly higher RF power not only at the designed mm-wave frequency band, but also at higher frequency bands.\n",
      "\n",
      "8. id: 5390af8920f70186a0ed0f36   score: 0.27874735   abstract: The analysis and optimization of the n+pvnp+ avalanche diode structure that includes two avalanche regions have been realized on basis of the nonlinear model and special optimization procedure. The admittance and energy characteristics of the DAR diode were analyzed in very wide frequency band from 30 up to 360 GHz. Output power level was optimized for the second frequency band near the 220 GHz.\n",
      "\n",
      "9. id: 5390a06e20f70186a0e4bdec   score: 0.27717942   abstract: The analysis and optimization of the n+pvnp+ avalanche diode structure that includes two avalanche regions have been realized on basis of the nonlinear model and special optimization procedure. The admittance characteristics of the DAR diode were analyzed in very wide frequency band from 30 up to 360 GHz and had been optimized for the third frequency band near the 300 GHz.\n",
      "\n",
      "10. id: 5390a4d020f70186a0e7611a   score: 0.27717942   abstract: The analysis and optimization of the n+pvnp+ avalanche diode structure that includes two avalanche regions have been realized on basis of the nonlinear model and special optimization procedure. The admittance characteristics of the DAR diode were analyzed in very wide frequency band from 30 up to 360 GHz and had been optimized for the third frequency band near the 300 GHz.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1705639\n",
      "index                                        55323b5e45cec66b6f9d9c9a\n",
      "title               Two Branch Beamforming for the Three User Cons...\n",
      "authors                                   Duckdong Hwang, Tae-Jin Lee\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references          5390b5fa20f70186a0f1047a;5390a88c20f70186a0e99...\n",
      "abstract            We consider the interference alignment (IA) in...\n",
      "id                                                            1705639\n",
      "clustered_labels                                                    1\n",
      "Name: 1705639, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558ced8c0cf2a2c70f68c220   score: 0.9969126   abstract: Interference alignment (IA) is a joint-transmission technique for the interference channel that achieves the maximum degrees-of-freedom and provides linear scaling of the capacity with the number of users for high signal-to-noise ratios (SNRs). Most prior work on IA is based on the impractical assumption that perfect and global channel-state information (CSI) is available at all transmitters. However, to implement IA, each receiver has to feed back CSI to all interferers, resulting in overwhelming feedback overhead. In particular, the sum feedback rate of each receiver scales quadratically with the number of users even if the feedback CSI is quantized. To substantially suppress feedback overhead, this paper focuses on designing efficient arrangements of feedback links, called feedback topologies, under the IA constraint. For the multiple-input multiple-output (MIMO) $K$-user interference\n",
      "\n",
      "2. id: 5390b56a20f70186a0f05a75   score: 0.9737447   abstract: The interference alignment (IA) is a promising technique to effectively mitigate interferences in wireless communication systems. To show the potential benefits of such an IA scheme, this letter focuses on a two-cell multiple-input multiple-output (MIMO) Gaussian interfering broadcast channels (MIMO-IFBC) with M transmit antennas and N receive antennas. It corresponds to a downlink scenario for cellular networks with two base stations (BSs) with M transmit antennas per BS, and two users with N receive antennas per user, on the cell-boundary of each BS. In this scenario, we propose a novel IA technique jointly designing transmit and receive beamforming vectors in a closed-form expression without iterative computation. It is also analytically shown that the proposed IA algorithm achieves the optimal degrees of freedom (DoF) of 2N in the case of [¾N] ≤ M <; 2N. The simulations demonstrate t\n",
      "\n",
      "3. id: 5390b68720f70186a0f1c2ca   score: 0.9641868   abstract: An interference alignment (IA) scheme is presented that allows multiple opportunistic transmitters (secondary users) to use the same frequency band of a pre-existing primary link without generating any interference. The primary and secondary transmit-receive pairs are equipped with multiple antennas. Under power constraints on the primary transmitter, the rate of the primary user is maximized by water-filling on the singular values of its channel matrix leaving some eigen modes unused, and hence, the secondary users can align their transmitted signals to produce a number of interference-free dimensions at each secondary receiver without causing any interference to the primary user. An outer bound is developed on the degrees of freedom (DoF) of the secondary users. In the case of a symmetric secondary network with time-varying channel coefficients having M antennas at each node and operat\n",
      "\n",
      "4. id: 5390ac5720f70186a0eb7353   score: 0.96050966   abstract: Interference alignment (IA) has been shown to achieve linear sum capacity growth, at high SNR, with the number of users in the interference channel by cooperatively precoding transmitted signals to align interference subspaces at the receivers. The theory of IA was derived under assumptions about the richness of the propagation channel; practical channels do not guarantee such ideal decorrelation. This paper presents the first experimental study of IA in measured interference channel and shows that IA achieves the claimed scaling factors in a variety of measured channel settings for a 3 user, 2 antennas per node setup.\n",
      "\n",
      "5. id: 5390adfd20f70186a0ec553e   score: 0.9457011   abstract: We provide inner bound and outer bound for the total number of degrees of freedom of the K user multiple-input multiple-output (MIMO) Gaussian interference channel with M antennas at each transmitter and N antennas at each receiver if the channel coefficients are time-varying and drawn from a continuous distribution. The bounds are tight when the ratio max(M, N)/min(M, N) = R is equal to an integer. For this case, we show that the total number of degrees of freedom is equal to min(M, N) K if K ≤ R and min(M, N) R/R+1 K if K R. Achievability is based on interference alignment. We also provide examples where using interference alignment combined with zero forcing can achieve more degrees of freedom than merely zero forcing for some MIMO interference channels with constant channel coefficients.\n",
      "\n",
      "6. id: 558c128e0cf23f2dfc597460   score: 0.94176143   abstract: This paper analyzes multiple-input, multiple-output interference channels where each receiver knows its channels from all the transmitters and feeds back this information using a limited number of bits to all the other terminals. It is shown that as long as the feedback bit rate scales sufficiently fast with the signal-to-noise ratio, the transmitters can use an interference alignment strategy by treating the quantized channel estimates as being perfect to achieve the sum degrees of freedom of the interference channel attainable with perfect and global channel state information. A tradeoff between the feedback rate and the achievable degrees of freedom is established by showing that a slower scaling of feedback rate for any one user leads to commensurately fewer degrees of freedom for that user alone. It is then shown that under the same fixed transmission strategy but with random quanti\n",
      "\n",
      "7. id: 5390a55520f70186a0e7b349   score: 0.9375548   abstract: Using interference alignment, it has been shown that the number of degrees of freedom in the interference channel scales linearly with the number of users. Unfortunately, closed-form solutions for interference alignment over constant-coefficient channels with more than 3 users are difficult to derive. This paper proposes an algorithm for interference alignment in the MIMO interference channel with an arbitrary number of users, antennas, or spatial streams. The algorithm is an alternating minimization over the precoding matrices at the transmitters and the interference subspaces at the receivers, and is proven to converge. Numerical results show how the algorithm is useful for simulation and can give insight into the limitations of interference alignment.\n",
      "\n",
      "8. id: 5390a88c20f70186a0e99aa1   score: 0.91789335   abstract: An efficient interference alignment (IA) scheme is developed for K-user single-input single-output frequency selective fading interference channels. The main idea is to steer the transmit beamforming matrices such that at each receiver the subspace dimensions occupied by interference-free desired streams are asymptotically the same as those occupied by all interferences. Our proposed scheme achieves a higher multiplexing gain at any given number of channel realizations in comparison with the original IA scheme, which is known to achieve the optimal multiplexing gain asymptotically.\n",
      "\n",
      "9. id: 558ceb7d0cf2a2c70f68c1b3   score: 0.917746   abstract: Consider a multiple input-multiple output (MIMO) interference channel where each transmitter and receiver are equipped with multiple antennas. An effective approach to practically achieving high system throughput is to deploy linear transceivers (or beamformers) that can optimally exploit the spatial characteristics of the channel. The recent work of Cadambe and Jafar (IEEE Trans. Inf. Theory, vol. 54, no. 8) suggests that optimal beamformers should maximize the total degrees of freedom and achieve interference alignment in the high signal-to-noise ratio (SNR) regime. In this paper we first consider the interference alignment problem without channel extension and prove that the problem of maximizing the total achieved degrees of freedom for a given MIMO interference channel is NP-hard. Furthermore, we show that even checking the achievability of a given tuple of degrees of freedom for al\n",
      "\n",
      "10. id: 5390aaf920f70186a0eacd66   score: 0.91352236   abstract: In this paper, we study degrees of freedom for the K-User multiple-input multiple-output (MIMO)-interference channel (IFC) with constant channel coefficients. In this channel, we investigate how many total number of transmit antennas, M1 + M2 + . . . + MK, are required in minimum to achieve di = 1, ∀i degrees of freedom when all receivers have N = 2 antennas, which is a downlink communication scenario. To answer this question, we propose a new interference alignment scheme based on intersection subspace property of the vector space. The proposed interference alignment scheme can be easily generalized regardless of the number of users. In addition, we investigate degrees of freedom for the partially connected MIMOIFC where some arbitrary interfering links are disconnected due to the large path loss or deep fades. In this channel model, we examine how these disconnected links are considere\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691271\n",
      "index                                        559255060cf2aff368683b9d\n",
      "title               Every $$3$$3-equivalenced association scheme i...\n",
      "authors             Mitsugu Hirasaka, Kyoung-Tark Kim, Jeong Rye Park\n",
      "year                                                           2015.0\n",
      "venue               Journal of Algebraic Combinatorics: An Interna...\n",
      "references          5590ae860cf2ce4b6f39ed20;5390893e20f70186a0d93931\n",
      "abstract            For a positive integer $$k$$k we say that an a...\n",
      "id                                                            1691271\n",
      "clustered_labels                                                    1\n",
      "Name: 1691271, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390893e20f70186a0d934fe   score: 0.3262714   abstract: An association scheme (or simply, a scheme) is called thin if each of its basic relations has valency 1. It is easy to see that thin schemes can be viewed as groups and, conversely, groups can be seen as thin schemes. In the present paper, we investigate schemes the basic relations of which have valency 1 or 2. We call these schemes quasi-thin. In order to formulate our results we let (X, R) denote a scheme (in the sense of P.-H. Zieschang). We first offer three sufficient conditions for (X,R) to have an automorphism group acting transitively on X. These conditions are (i) Oθ(R) ∩ Oθ(R) = {1}, (ii) nOθ(R) = 2, (iii) R possesses an element r such that 〈r〉 = R and 〈rr*〉 = 〈r*r〉). We then prove that, if Oθ(R) = Oθ(R) and nOθ(R) = 4, |X|/4 ∈ {3,4,7,8,12,16}. As a consequence of the latter result, we obtain a classification of the quasi-thin schemes with |X|=4p, where p is a prime number.\n",
      "\n",
      "2. id: 539087ef20f70186a0d6dab9   score: 0.3024941   abstract: In this paper, we have a classification of primitive symmetric association schemes with k_1 = 3.\n",
      "\n",
      "3. id: 5390980720f70186a0e02f31   score: 0.16000336   abstract: An association scheme is a combinatorial object derived from the orbitals of a transitive permutation group. Let G be a transitive permutation group acting on a finite set X. Then 驴驴x驴 XGx驴 is a normal subgroup of G where Gx:={g 驴 G 驴 xg=x}. A meta-thin association scheme can be considered as a generalization of the situation where 驴驴x驴 XGx驴 normalizes Gx. In this paper, we consider the automorphism group of a meta-thin association scheme, and obtain a sufficient condition for a meta-thin association scheme to have a transitive automorphism group. This enables us to conclude that every meta-thin association scheme with its thin residue isomorphic to the cyclic group of order pq, where p and q are primes, has a transitive automorphism group.\n",
      "\n",
      "4. id: 539089ab20f70186a0d97642   score: 0.14940722   abstract: We give some observations on association schemes with a relation of valency 2 from the representation theory of Bose-Mesner algebras and the basic structure theory. One of the applications of these observations is the classification of the association schemes with a nonsymmetric relation of valency 2 if the cardinality of the point set is a product of two primes, and another is the proof of Li's conjecture that any finite simple group is a connected 2-DCI-group.\n",
      "\n",
      "5. id: 5390a2e920f70186a0e6884f   score: 0.13262685   abstract: In this paper, we will prove a result which is formally dual to the long-standing conjecture of Bannai and Ito which claims that there are only finitely many distance-regular graphs of valency k for each k2. That is, we prove that, for any fixed m\"12, there are only finitely many cometric association schemes (X,R) with the property that the first idempotent in a Q-polynomial ordering has rank m\"1. As a key preliminary result, we show that the splitting field of any such association scheme is at most a degree two extension of the rationals. All of the tools involved in the proof are fairly elementary yet have wide applicability. To indicate this, a more general theorem is proved here with the result alluded to in the title appearing as a corollary to this theorem.\n",
      "\n",
      "6. id: 5390a45520f70186a0e71c29   score: 0.1309508   abstract: The concept of an association scheme is one of those mathematical concepts which were utilized as technical tools in various different mathematical areas for a long time before becoming the subject of a theory in their own right. The significance of symmetric schemes, for instance, in the design of (statistical) experiments was recognized as early as the first half of the last century. Coding theory has been associated with commutative schemes for more than three decades, and polynomial schemes have provided the language in which major topics in algebraic graph theory are communicated for about twenty years. The notion of a scheme itself, however-a notion which, if considered in its full generality, generalizes not only the notion of a group but also the notion of a Moore geometry and that of a building in the sense of Jacques Tits-has been considered as the subject of an abstract theory\n",
      "\n",
      "7. id: 5390a06e20f70186a0e4c113   score: 0.12656909   abstract: We prove the uniqueness of the two association schemes which appear in recent work of Henry Cohn and others in connection with their study of universally optimal spherical codes in Euclidean spaces: one is the class 4 association scheme with 40 vertices in R^1^0 and the other one is the class 3 association scheme with 64 vertices in R^1^4. We prove the uniqueness mainly by geometric considerations with some computational help.\n",
      "\n",
      "8. id: 5390995d20f70186a0e159f6   score: 0.112405084   abstract: Finite groups of prime order must be cyclic. It is natural to ask what about association schemes of prime order. In this paper, we will give an answer to this question. An association scheme of prime order is commutative, and its valencies of nontrivial relations and multiplicities of nontrivial irreducible characters are constant. Moreover, if we suppose that the minimal splitting field is an abelian extension of the field of rational numbers, then the character table is the same as that of a Schurian scheme.\n",
      "\n",
      "9. id: 53908f5b20f70186a0dd98f3   score: 0.09186979   abstract: Let $P = \\{P_0,...,P_n\\}$ be a partition of a finite abelian group $A$ and $L(P)$ the complex vector space generated by the indicator functions of the sets $P_0,...,P_n$. In [1] it was proved that $P$ defines an abelian association scheme on $A$ if $L(P)$ equals the space $L(P)^*$ of all Fourier transforms of functions in $L(P)$. In this paper it is shown that $P$ defines an abelian association scheme on $A$ if and only if $L(P)^* = L(Q)$ for some partition $Q$ of $A$. Also, several examples illustrating the above result are given.\n",
      "\n",
      "10. id: 558fe5ae0cf23515427183d2   score: 0.08166611   abstract: LetG be a finite abelian group,K a subfield ofC, C[G] regarded as an algebra of matrices.A G K {A¿C[G]| all the entries and eigenvalues ofA are inK} is an association algebra overK. In this paper, the association scheme ofA G K is determined and in the caseK=Q(i), the first eigenmatrix of the association scheme computed. As an application, it is proved thatZ 4×Z 4×Z 4 is the only abelian group admitted as a Singer group by some distance-regular digraph of girth 4 on 64 vertices.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1683521\n",
      "index                                        55923d10612c4fa28ff7a33e\n",
      "title                        Vertex and Edge Dimension of Hypergraphs\n",
      "authors                                  Martín Manrique, S. Arumugam\n",
      "year                                                           2015.0\n",
      "venue                                        Graphs and Combinatorics\n",
      "references                                   5390880720f70186a0d79cfb\n",
      "abstract            Let G = (V, E) be a connected graph and let $$...\n",
      "id                                                            1683521\n",
      "clustered_labels                                                    2\n",
      "Name: 1683521, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390af8920f70186a0ed09c3   score: 0.3710227   abstract: Given a set of vertices S={v\"1,v\"2,...,v\"k} of a connected graph G, the metric representation of a vertex v of G with respect to S is the vector r(v|S)=(d(v,v\"1),d(v,v\"2),...,d(v,v\"k)), where d(v,v\"i), i@?{1,...,k} denotes the distance between v and v\"i. S is a resolving set for G if for every pair of distinct vertices u,v of G, r(u|S)r(v|S). The metric dimension of G, dim(G), is the minimum cardinality of any resolving set for G. Let G and H be two graphs of order n\"1 and n\"2, respectively. The corona product G@?H is defined as the graph obtained from G and H by taking one copy of G and n\"1 copies of H and joining by an edge each vertex from the ith-copy of H with the ith-vertex of G. For any integer k=2, we define the graph G@?^kH recursively from G@?H as G@?^kH=(G@?^k^-^1H)@?H. We give several results on the metric dimension of G@?^kH. For instance, we show that given two connected gr\n",
      "\n",
      "2. id: 5390ad0620f70186a0ebab62   score: 0.24908175   abstract: Let G be a connected graph and d(x,y) be the distance between the vertices x and y. A subset of vertices W={w\"1,w\"2,...,w\"k} is called a resolving set for G if for every two distinct vertices x,y@?V(G), there is a vertex w\"i@?W such that d(x,w\"i)d(y,w\"i). A resolving set containing a minimum number of vertices is called a metric basis for G and the number of vertices in a metric basis is its metric dimension dim(G). A family G of connected graphs is a family with constant metric dimension if dim(G) is finite and does not depend upon the choice of G in G. In this paper, we study the metric dimension of some classes of convex polytopes which are obtained by the combinations of two different graph of convex polytopes. It is shown that these classes of convex polytopes have the constant metric dimension and only three vertices chosen appropriately suffice to resolve all the vertices of these\n",
      "\n",
      "3. id: 5390b56a20f70186a0f06723   score: 0.13649325   abstract: Let H=(V,E) be a hypergraph with vertex set V and edge set E. A dominating set in H is a subset of vertices D@?V such that for every vertex v@?V@?D there exists an edge e@?E for which v@?e and e@?D0@?. The domination number @c(H) is the minimum cardinality of a dominating set in H. It is known that if H is a hypergraph of order n with edge sizes at least three and with no isolated vertex, then @c(H)@?n/3. In this paper, we characterize the hypergraphs achieving equality in this bound.\n",
      "\n",
      "4. id: 5390ba0a20f70186a0f34488   score: 0.122418255   abstract: Let G be a (di)graph. A set W of vertices in G is a resolving set of G if every vertex u of G is uniquely determined by its vector of distances to all the vertices in W. The metric dimension@m(G) of G is the minimum cardinality of all the resolving sets of G. In this paper we study the metric dimension of the line graph L(G) of G. In particular, we show that @m(L(G))=|E(G)|-|V(G)| for a strongly connected digraph G which is not a directed cycle, where V(G) is the vertex set and E(G) is the edge set of G. As a corollary, the metric dimension of de Bruijn digraphs and Kautz digraphs is given. Moreover, we prove that @?log\"2@D(G)@?@?@m(L(G))@?|V(G)|-2 for a simple connected graph G with at least five vertices, where @D(G) is the maximum degree of G. Finally, we obtain the metric dimension of the line graph of a tree in terms of its parameters.\n",
      "\n",
      "5. id: 5390b19020f70186a0ee0543   score: 0.121790156   abstract: Let H=(V,E) be a hypergraph with vertex set V and edge set E of order n\"H=|V| and size m\"H=|E|. A transversal in H is a subset of vertices in H that has a nonempty intersection with every edge of H. The transversal number @t(H) of H is the minimum size of a transversal in H. A dominating set in H is a subset of vertices D@?V such that for every vertex v@?V@?D there exists an edge e@?E for which v@?e and e@?D0@?. The domination number @c(H) is the minimum cardinality of a dominating set in H. A hypergraph H is k-uniform if every edge of H has size k. We establish the following relationship between the transversal number and the domination number of uniform hypergraphs. For any two nonnegative reals a and b and for every integer k=3 the equality sup\"H\"@?\"H\"\"\"k@c(H)/(an\"H+bm\"H)=sup\"H\"@?\"H\"\"\"k\"\"\"-\"\"\"1@t(H)/(an\"H+(a+b)m\"H) holds, where H\"k denotes the class of all k-uniform hypergraphs contai\n",
      "\n",
      "6. id: 53908a4020f70186a0d9df1f   score: 0.08929565   abstract: For an unweighted graph G = (V, E), G′ = (V, E′) is a subgraph if E′ ⊆ E, and G″ = (V″, E″, ω) is a Steiner graph if V ⊆ V″, and for any pair of vertices u, w ∈ V, the distance bet-ween them in G″(denoted dG″, (u, w)) is at least the distance between them in G (denoted da(u, w)).In this paperwe introduce the notion of distance preserver. A subgraph (resp., Steiner graph) G′ of a graph G is a subgraph (resp., Steiner) D-preserver of G if for every pair of vertices u, w ∈ V with dG(u, w) ≥ D, dG′, (u, w) = dG(u, w). We show that anygraph (resp., digraph) has a subgraph D-preserver with at most O(n2/D) edges (resp., arcs), and there are graphs and digraphs for which any undirected Steiner D-preserver contains Ω(n2/D) edges. However, we show that if one allows a directed Steiner (or, shortly, diS-teiner) D-preserver, then these bounds can be improved. Specifically, we show that for any graph\n",
      "\n",
      "7. id: 53909fbd20f70186a0e4241d   score: 0.08897849   abstract: A set of vertices $S$ resolves a graph $G$ if every vertex is uniquely determined by its vector of distances to the vertices in $S$. The metric dimension of $G$ is the minimum cardinality of a resolving set of $G$. This paper studies the metric dimension of cartesian products $G\\,\\square\\,H$. We prove that the metric dimension of $G\\,\\square\\,G$ is tied in a strong sense to the minimum order of a so-called doubly resolving set in $G$. Using bounds on the order of doubly resolving sets, we establish bounds on $G\\,\\square\\,H$ for many examples of $G$ and $H$. One of our main results is a family of graphs $G$ with bounded metric dimension for which the metric dimension of $G\\,\\square\\,G$ is unbounded.\n",
      "\n",
      "8. id: 5390b04120f70186a0ed708c   score: 0.087252244   abstract: The R-set relative to a pair of distinct vertices of a connected graph G is the set of vertices whose distances to these vertices are distinct. This paper deduces some properties of R-sets of connected graphs. It is shown that for a connected graph G of order n and diameter 2 the number of R-sets equal to V(G) is bounded above by $${\\lfloor n^{2}/4\\rfloor}$$. It is conjectured that this bound holds for every connected graph of order n. A lower bound for the metric dimension dim(G) of G is proposed in terms of a family of R-sets of G having the property that every subfamily containing at least r ≥ 2 members has an empty intersection. Three sufficient conditions, which guarantee that a family $${\\mathcal{F}=(G_{n})_{n\\geq 1}}$$ of graphs with unbounded order has unbounded metric dimension, are also proposed.\n",
      "\n",
      "9. id: 5390b4c420f70186a0efe7f7   score: 0.0824015   abstract: Let G be a connected graph. For a vertex v ∈V(G) and an ordered k-partition Π={S1 ,S2 ,...,Sk } of V(G), the representation of v with respect to Π is the k-vector r(v|Π) =(d(v,S1 ),d(v,S2 ),...,d(v,Sk )), where d(v,Si ) denotes the distance between v and Si . The k-partition Π is said to be resolving if the k-vectors r(v|Π), v ∈V(G), are distinct. The minimum k for which there is a resolving k-partition of V(G) is called the partition dimension of G, denoted by pd(G). If each subgraph i induced by Si (1≤i≤k) is required to be connected in G, the corresponding notions are connected resolving k-partition and connected partition dimension of G, denoted by cpd(G). Let the graph J2n be obtained from the wheel with 2n rim vertices W2n by alternately deleting n spokes. In this paper it is shown that for every n≥4 $pd(J_{2n})\\leq 2\\lceil \\sqrt{2n}\\rceil +1$ and cpd(J2n )=⌈(2n+3)/5⌉ applying Cheb\n",
      "\n",
      "10. id: 53908d6520f70186a0dd0a8b   score: 0.08064661   abstract: An $n$-vertex {\\em interval hypergraph} (I-hypergraph, for short) $H$ comprises the set $V _ n~=~\\{ 1,2, \\cdot \\cdot \\cdot , n \\}$ of vertices and a multiset $E(H)$ of hyperedges, each hyperedge a subset of $V _ n$ of the form $\\{k, k+1, \\cdot \\cdot \\cdot , k+r\\}$ $(k \\geq 1, r \\leq n-k)$. The {\\em size} $SIZE(H)$ of the I-hypergraph $H$ is the sum of the cardinalities of its hyperedges. An {\\em embedding} of a graph $G=(V,E)$ in an $n$-vertex I-hypergraph $H$ comprises an injection $\\mu _ {_ V} : V \\rightarrow V _ n$, plus an injection $\\mu _ e :E \\rightarrow E(H)$ satisfying: for all $(u,v) \\in E$, $\\{ \\mu _ {_ V} (u), \\mu _ {_ V} (v)\\} \\subseteq \\mu _ e (u,v)$. The $n$-vertex I-hypergraph $H$ is {\\em strongly universal} for the finite family of graphs $\\Gamma$ if: given any $W \\subseteq V _ n$ and any graph $G=(V,E) \\in \\Gamma$ with $|V| \\leq |W|$, there is an embedding of $G$ in $H$ \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696228\n",
      "index                                        55922dd20cf244696a09d9d7\n",
      "title               Discrete Kernel Preserving Model for 1D Electr...\n",
      "authors                                    Ruo Li, Tiao Lu, Wenqi Yao\n",
      "year                                                           2015.0\n",
      "venue                                 Journal of Scientific Computing\n",
      "references          53908b6c20f70186a0dbd007;5390c04520f70186a0f57b80\n",
      "abstract            We investigate the discretization of of an ele...\n",
      "id                                                            1696228\n",
      "clustered_labels                                                    1\n",
      "Name: 1696228, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909fbd20f70186a0e42900   score: 0.3965355   abstract: We consider two-dimensional scattering problems, formulated as an integral equation defined on the boundary of the scattering obstacle. The oscillatory nature of high-frequency scattering problems necessitates a large number of unknowns in classical boundary element methods. In addition, the corresponding discretization matrix of the integral equation is dense. We formulate a boundary element method with basis functions that incorporate the asymptotic behavior of the solution at high frequencies. The method exhibits the effectiveness of asymptotic methods at high frequencies with only few unknowns, but retains accuracy for lower frequencies. New in our approach is that we combine this hybrid method with very effective quadrature rules for oscillatory integrals. As a result, we obtain a sparse discretization matrix for the oscillatory problem. Moreover, numerical experiments indicate that\n",
      "\n",
      "2. id: 53908d6520f70186a0dd0b74   score: 0.3957762   abstract: The purpose of this paper is to address some difficulties which arise in computing the eigenvalues of the Maxwell''s system by a finite element method. Depending on the used method, the spectrum may be polluted by spurious modes which are difficult to pick out among the approximations of the physically correct eigenvalues. Here we prove, under very general assumptions, that using edge elements the discrete spectrum well approximates the correct one and we give some justification of the spectral pollution which occurs when nodal elements are used. Results of numerical experiments confirming the theory are also reported. EMAIL:: ian@microian.ian.pv.cnr.it\n",
      "\n",
      "3. id: 5390a2be20f70186a0e642b8   score: 0.35947654   abstract: A systematic development of efficient discretization schemes for the numerical evaluation of the eigenvalues of the single-band effective mass equation that describes the motion of electrons in an ideal periodic crystal is presented. The approach presented makes use of the translational symmetry of the crystal lattice and utilizes the quantum mechanical properties of the momentum operator as the generator of spatial translation. Boundary conditions satisfied at the heterointerfaces are explicitly incorporated in the discretization procedure and the effects of this approach in overall accuracy are evaluated by studying a prototype quantum well heterostructure. Copyright © 2008 John Wiley & Sons, Ltd.\n",
      "\n",
      "4. id: 5390b86b20f70186a0f285eb   score: 0.35186765   abstract: Considered are the problems of electromagnetic wave scattering, absorption and emission by several types of two-dimensional and three-dimensional dielectric and metallic objects: arbitrary dielectric cylinder, thin material strip and disk, and arbitrary perfectly electrically conducting surface of rotation. In each case, the problem is rigorously formulated and reduced to a set of boundary integral equations with smooth, singular and hyper-singular kernel functions. These equations are further discretized using Nystrom-type quadrature formulas adapted to the type of kernel singularity and the edge behavior of unknown function. Convergence of discrete models to exact solutions is guaranteed by general theorems. Practical accuracy is achieved by inverting the matrices of the size that is only slightly greater than the maximum electrical dimension of corresponding scatterer. Sample numerica\n",
      "\n",
      "5. id: 539099a220f70186a0e16d73   score: 0.29808256   abstract: The spectrum of the volume integral operator of three-dimensional electromagnetic scattering is analyzed. The operator has both continuous essential spectrum, which dominates at lower frequencies, and discrete eigenvalues, which spread out at higher ones. The explicit expression of the operator's symbol provides an exact outline of the essential spectrum for any inhomogeneous anisotropic scatterer with Hölder continuous constitutive parameters. Geometrical bounds on the location of discrete eigenvalues are derived for various physical scenarios. Numerical experiments demonstrate good agreement between the predicted spectrum of the operator and the eigenvalues of its discretized version.\n",
      "\n",
      "6. id: 55323c7645cec66b6f9dc2d6   score: 0.28957254   abstract: We present a node-centered finite volume method for computing a representative range of eigenvalues and eigenvectors of the Schrödinger operator on a three-dimensional cylindrically symmetric bounded domain with mixed boundary conditions. The three-dimensional Schrödinger operator is reduced to a family of two-dimensional Schrödinger operators distinguished by a centrifugal potential. We consider a uniform, boundary conforming Delaunay mesh, which additionally conforms to the material interfaces. We study how the anisotropy of the effective mass tensor acts on the uniform approximation of the first K eigenvalues and eigenvectors and their sequential arrangement. There exists an optimal uniform Delaunay discretization with matching anisotropy with respect to the effective masses of the host material. For a centrifugal potential one retrieves the theoretically established first-order conve\n",
      "\n",
      "7. id: 5390bd1520f70186a0f437bf   score: 0.28806815   abstract: We present a node-centered finite volume method for computing a representative range of eigenvalues and eigenvectors of the Schrodinger operator on a three-dimensional cylindrically symmetric bounded domain with mixed boundary conditions. The three-dimensional Schrodinger operator is reduced to a family of two-dimensional Schrodinger operators distinguished by a centrifugal potential. We consider a uniform, boundary conforming Delaunay mesh, which additionally conforms to the material interfaces. We study how the anisotropy of the effective mass tensor acts on the uniform approximation of the first K eigenvalues and eigenvectors and their sequential arrangement. There exists an optimal uniform Delaunay discretization with matching anisotropy with respect to the effective masses of the host material. For a centrifugal potential one retrieves the theoretically established first-order conve\n",
      "\n",
      "8. id: 5390b78a20f70186a0f24cc4   score: 0.24780536   abstract: The aperiodic Fourier modal method in contrast-field formulation is a numerical discretization and solution technique for solving scattering problems in electromagnetics. Typically, spectral discretization is used in the finite periodic direction and spatial discretization in the orthogonal direction. In the light of the fact that the structures of interest often have a large width-to-height ratio and that the two discretization approaches have different computational complexities, we propose exchanging the directions for spatial and spectral discretization. Moreover, if the scatterer has repeating patterns, swapping the discretization directions facilitates the reuse of previous computations. Therefore, the new method is suited for scattering from objects with a finite number of periods, such as gratings, memory arrays, metamaterials, etc. Numerical experiments demonstrate a considerabl\n",
      "\n",
      "9. id: 5390995d20f70186a0e15e7f   score: 0.20291664   abstract: We propose a direct solver to the non-stationary Boltzmann-Poisson system for simulating the electron transport in two-dimensional GaAs devices. The GaAs conduction band is approximated by a two-valley model. All of the important scattering mechanisms are taken into account. Our numerical scheme consists of the combination of the multigroup approach to deal with the dependence of the electron distribution functions on the three-dimensional electron wave vectors and a high-order WENO reconstruction procedure for treating their spatial dependences. The electric field is determined self-consistently from the Poisson equation. Numerical results are presented for a GaAs-MESFET. We display electron distribution functions as well as several macroscopic quantities and compare them to those of Monte Carlo simulations. In addition, we study the influence of the used discretization on the obtained \n",
      "\n",
      "10. id: 5390ac5720f70186a0eb5e47   score: 0.13240233   abstract: In this paper we present a numerical method to compute resonances and resonant modes for 2D electromagnetic scattering at a smooth homogeneous dielectric object in free space. The resonances are found as eigenvalues of a non-linear eigenvalue problem which comes from a formulation as a boundary integral equation and subsequent discretization by a Nystrom approach, for which the integral kernels are regularized by singularity subtraction. The eigenvalues are computed by a predictor-corrector strategy, which provides good initial guesses for an iterative corrector procedure. The resonances can be computed with very high accuracy due to an exponentially decreasing discretization error.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1730061\n",
      "index                                        55323d8245cec66b6f9de8e1\n",
      "title               Spiking neural network with RRAM: can we use i...\n",
      "authors             Tianqi Tang, Lixue Xia, Boxun Li, Rong Luo, Yi...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references                                   558bffb80cf20e727d0f528b\n",
      "abstract            The spiking neural network (SNN) provides a pr...\n",
      "id                                                            1730061\n",
      "clustered_labels                                                    0\n",
      "Name: 1730061, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 559151dd0cf232eb904fbbd3   score: 0.95775455   abstract: Inspired by the human brain's function and efficiency, neuromorphic computing offers a promising solution for a wide set of tasks, ranging from brain machine interfaces to real-time classification. The spiking neural network (SNN), which encodes and processes information with bionic spikes, is an emerging neuromorphic model with great potential to drastically promote the performance and efficiency of computing systems. However, an energy efficient hardware implementation and the difficulty of training the model significantly limit the application of the spiking neural network. In this work, we address these issues by building an SNN-based energy efficient system for real time classification with metal-oxide resistive switching random-access memory (RRAM) devices. We implement different training algorithms of SNN, including Spiking Time Dependent Plasticity (STDP) and Neural Sampling meth\n",
      "\n",
      "2. id: 5390ad5620f70186a0ebd517   score: 0.9324533   abstract: The performance analysis of an efficient multiprocessor architecture that allows accelerating the emulation of large-scale Spiking Neural Networks (SNNs) is reported. After describing the architecture and the complex SNN algorithm mapping, the performance study demonstrates that the system can emulate up to 10,000 300-synapse neurons in real time at 64 MHz with conventional FPGAs. Important improvements can be achieved by using advanced technology and increased clock rate or by means of simple architecture modifications. The architecture is flexible enough to be efficiently applied to any SNN model in general.\n",
      "\n",
      "3. id: 5390b7ff20f70186a0f27b15   score: 0.8238959   abstract: Few algorithms for supervised training of spiking neural networks exist that can deal with patterns of multiple spikes, and their computational properties are largely unexplored. We demonstrate in a set of simulations that the ReSuMe learning algorithm can successfully be applied to layered neural networks. Input and output patterns are encoded as spike trains of multiple precisely timed spikes, and the network learns to transform the input trains into target output trains. This is done by combining the ReSuMe learning algorithm with multiplicative scaling of the connections of downstream neurons. We show in particular that layered networks with one hidden layer can learn the basic logical operations, including Exclusive-Or, while networks without hidden layer cannot, mirroring an analogous result for layered networks of rate neurons. While supervised learning in spiking neural networks \n",
      "\n",
      "4. id: 5390bd1520f70186a0f4379d   score: 0.81375694   abstract: Results are presented from several spiking network experiments performed on a novel neuromorphic integrated circuit. The networks are discussed in terms of their computational significance, which includes applications such as arbitrary spatiotemporal pattern generation and recognition, winner-take-all competition, stable generation of rhythmic outputs, and volatile memory. Analogies to the behavior of real biological neural systems are also noted. The alternatives for implementing the same computations are discussed and compared from a computational efficiency standpoint, with the conclusion that implementing neural networks on neuromorphic hardware is significantly more power efficient than numerical integration of model equations on traditional digital hardware.\n",
      "\n",
      "5. id: 5390b9d520f70186a0f3206c   score: 0.7733876   abstract: This paper presents a deterministic and adaptive spike model derived from radial basis functions and a leaky integrate-and-fire sampler developed for training spiking neural networks without direct weight manipulation. Several algorithms have been proposed for training spiking neural networks through biologically-plausible learning mechanisms, such as spike-timing dependent synaptic plasticity and Hebbian plasticity. These algorithms typically rely on the ability to update the synaptic strengths, or weights, directly, through a weight update rule in which the weight increment can be decided and implemented based on the training equations. However, in several potential applications of adaptive spiking neural networks, including neuroprosthetic devices and CMOS/memristor nanoscale neuromorphic chips, the weights cannot be manipulated directly and, instead, tend to change over time by virtu\n",
      "\n",
      "6. id: 5390b36120f70186a0ef15d6   score: 0.71153116   abstract: The simulation of large spiking neural networks (SNN) is still a very time consuming task. Therefore most simulations are limited to rather unrealistic small or medium sized networks (typically hundreds of neurons). In this paper, some methods for the fast simulation of large SNN are discussed. Our results equally amongst others show that event based simulation is an efficient way of simulating SNN, although not all neuron models are suited for an event based approach. We compare some models and discuss several techniques for accelerating the simulation of more complex models. Finally we present an algorithm that is able to handle multi-synapse models efficiently.\n",
      "\n",
      "7. id: 5390a1bc20f70186a0e552e1   score: 0.6124835   abstract: This paper highlights and discusses the current challenges in the implementation of large scale Spiking Neural Networks (SNNs) in hardware. A mixed-mode approach to realising scalable SNNs on a reconfigurable hardware platform is presented. The approach uses compact low power analogue spiking neuron cells, with a weight storage capability, interconnected using Network on Chip (NoC) routers. Results presented show that this route to hardware implementation is promising.\n",
      "\n",
      "8. id: 5390baa120f70186a0f37ed1   score: 0.56025267   abstract: The memory capacity, computational power, communication bandwidth, energy consumption, and physical size of the brain all tend to scale with the number of synapses, which outnumber neurons by a factor of 10,000. Although progress in cortical simulations using modern digital computers has been rapid, the essential disparity between the classical von Neumann computer architecture and the computational fabric of the nervous system makes large-scale simulations expensive, power hungry, and time consuming. Over the last three decades, CMOS-based neuromorphic implementations of “electronic cortex” have emerged as an energy efficient alternative for modeling neuronal behavior. However, the key ingredient for electronic implementation of any self-learning system—programmable, plastic Hebbian synapses scalable to biological densities—has remained elusive. We demonstrate the viability of implement\n",
      "\n",
      "9. id: 5390b72e20f70186a0f20cd1   score: 0.5455279   abstract: We design and implement a key building block of a scalable neuromorphic architecture capable of running spiking neural networks in compact and low-power hardware. Our innovation is a configurable neurosynaptic core that combines 256 integrate-and-fire neurons, 1024 input axons, and 1024x256 synapses in 4.2mm2 of silicon using a 45nm SOI process. We are able to achieve ultra-low energy consumption 1) at the circuit-level by using an asynchronous design where circuits only switch while performing neural updates, 2) at the core-level by implementing a 256 neural fan out in a single operation using a crossbar memory, and 3) at the architecture-level by restricting core-to-core communication to spike events, which occur relatively sparsely in time. Our implementation is purely digital, resulting in reliable and deterministic operation that achieves for the first time one-to-one correspondence\n",
      "\n",
      "10. id: 5390b2d720f70186a0eed3be   score: 0.52737737   abstract: Simulating large networks of spiking neurons is a very common task in the areas of Neuroinformatics and Computational Neurosciences. These simulations are time-consuming but also often intrinsically parallel. The recent advent of powerful and programmable graphic cards seems to be a pertinent solution to the problem: they offer a cheap but efficient possibility to serve as very fast co-processors for the parallel computing that spiking neural networks need. We describe our implementation of three different problems on such a card: two image-segmentation algorithms using spiking neural networks and one multi-purpose spiking neural-network simulator. Using these examples we show the benefits, the challenges and the limits of such an implementation.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1704251\n",
      "index                                        55323b2b45cec66b6f9d975c\n",
      "title               Effect of Islands in Diffusive Properties of t...\n",
      "authors                     Narcís Miguel, Carles Simó, Arturo Vieiro\n",
      "year                                                           2015.0\n",
      "venue                        Foundations of Computational Mathematics\n",
      "references                                   5390878720f70186a0d34d37\n",
      "abstract            In this paper we review, based on massive, lon...\n",
      "id                                                            1704251\n",
      "clustered_labels                                                    2\n",
      "Name: 1704251, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908f5b20f70186a0dd9ab7   score: 0.43422136   abstract: The standard map has a divided phase space in which two dimensional regions of stochasticity are isolated by one dimensional KAM curves which form a barrier to diffusion in action. When two standard maps are coupled together, the two dimensional KAM surfaces no longer divide the four dimensional phase space, and particles diffuse slowly along stochastic layers by the process of Arnold diffusion. We compare an analytic calculation of the rate of localized Arnold diffusion with numerically determined rates in regions having rotational and liberational KAM curves for a single map, in the weak coupling limit for which the three resonance model holds. We then determine the rate of global Arnold diffusion across many cells of the 2pi periodic mapping. The global diffusion rate depends both on the local diffusion rate and on the relative volume occupied by the various stochastically accessible \n",
      "\n",
      "2. id: 53908f5b20f70186a0dd9e07   score: 0.16940588   abstract: When several standard maps are coupled together, KAM surfaces cannot isolate stochastic regions, and particles diffuse along stochastic layers by the process of Arnold diffusion. For the case of two coupled standard maps the rate of Arnold diffusion has previously been calculated both locally around a particular KAM curve and globally across many cells of the 2 pi periodic mapping. When more than two standard maps are coupled, the Arnold diffusion rate increases, depending on the total number of maps, N, and the number of phases in each coupling term, m, where 2 is less than or equal to m is less than or equal to N. As N is increased, the diffusion rate increases as N to the 1/2, the length of the diagonal in the action space. As m is increased, the diffusion rate increases because the phase of the coupling term for a particular map becomes less correlated with the phase of the map itsel\n",
      "\n",
      "3. id: 5390880220f70186a0d77050   score: 0.0946777   abstract: In the models we will consider, space is represented by a grid of sites that can be in one of a finite number of states and that change at rates that depend on the states of a finite number of sites. Our main aim here is to explain an idea of Durrett and Levin (1994): the behavior of these models can be predicted from the properties of the mean field ODE, i.e., the equations for the densities of the various types that result from pretending that all sites are always independent. We will illustrate this picture through a discussion of eight families of examples from statistical mechanics, genetics, population biology, epidemiology, and ecology. Some of our findings are only conjectures based on simulation, but in a number of cases we are able to prove results for systems with \"fast stirring\" by exploiting connections between the spatial model and an associated reaction diffusion equation.\n",
      "\n",
      "4. id: 53908b6c20f70186a0dbd429   score: 0.08122776   abstract: Growth and diffusion phenomena have become a topic of great interest to a number of investigators in various disciplines, such as biology, demography, economy, etc. These processes are usually analysed by means of sigmoidal growth models, being the logistic model commonest among them. The aim of this paper rests on the applicability of a most general model, known as the hyperlogistic model, which is rarely used in applications. We show that this model is suitable for describing the evolution of the number of tourist places in Tenerife, and then we carry out some kind of estimation procedures in order to obtain a proper simulation of the variable.\n",
      "\n",
      "5. id: 5390b0ca20f70186a0ed9bd8   score: 0.08079154   abstract: We do the numerical analysis and simulations for the time fractional radial diffusion equation used to describe the anomalous subdiffusive transport processes on the symmetric diffusive field. Based on rewriting the equation in a new form, we first present two kinds of implicit finite difference schemes for numerically solving the equation. Then we strictly establish the stability and convergence results. We prove that the two schemes are both unconditionally stable and second order convergent with respect to the maximum norm. Some numerical results are presented to confirm the rates of convergence and the robustness of the numerical schemes. Finally, we do the physical simulations. Some interesting physical phenomena are revealed; we verify that the long time asymptotic survival probability @Kt^-^@a, but independent of the dimension, where @a is the anomalous diffusion exponent.\n",
      "\n",
      "6. id: 5390b5ed20f70186a0f0df62   score: 0.061649624   abstract: This paper presents, through numerical simulations, the transformations occuring in the dynamics of a discrete map as one parameter varies within a very small neighborhood of the parameter space. The specific map used is the two-stock, two-location version of the universal map of discrete relative dynamics. This particular neighborhood of the parameter space is analyzed because it reveals the inner structure of quasi-periodicity and its bifurcations. In the spectrum of values examined, the focus is on a general class of transitions involved in and from a quasi-periodic motion.\n",
      "\n",
      "7. id: 539089ab20f70186a0d9627b   score: 0.058131598   abstract: When simulating a dynamical system, the computation is actually of a spatially discretized system, because finite machine arithmetic replaces continuum state space. For chaotic dynamical systems, the discretized simulations often have collapsing effects, to a fixed point or to short cycles. Statistical properties of these phenomena can be modelled with random mappings with an absorbing centre. The model gives results which are very much in line with computational experiments. The effects are discussed with special reference to the family of mappings f_{\\ell}(x)\\,{=}\\,1-|1-2x|^{\\ell}, x\\in[0,1], 1\\,{. Computer experiments show close agreement with predictions of the model.\n",
      "\n",
      "8. id: 5390c04520f70186a0f57023   score: 0.05686155   abstract: In this paper, a spatial epidemic model with protective effect, i.e., the susceptible individual has ability to recognize the infected and keep away from them, which lead the existence of the cross diffusion of the susceptible, is investigated. By both mathematical analysis and numerical simulations, we find that typical dynamics of population density is the formation of isolated groups, i.e., stripe-like or spotted or coexistence of both. The obtained results show that the coefficient of cross diffusion has great influence on the pattern formation of the population. More specifically, as the coefficient decreases, stripe only, coexistence of stripe and spotted, and spotted only emerge successively. As the coefficient of cross diffusion further decreases, the number of the spotted in the spatial domain is decreased. Furthermore, although under the condition that concentration of the infe\n",
      "\n",
      "9. id: 539089d220f70186a0d9ab94   score: 0.05531065   abstract: Spatial data appear in numerous applications, such as GISmultimediaand even traditional databases. Most of the analysis on spatial data has focused on point data, typically using the uniformity assumption, or, more accurately, a fractal distribution. However, no results exist for nonpoint spatial data, like 2D regions (e.g., islands), 3D volumes (e.g., physical objects in the real world), etc. This is exactly the problem we solve in this paper. Based on experimental evidence that real areas and volumes follow a 驴power law,驴 that we named REGAL (REGion Area Law), we show 1) the theoretical implications of our model and its connection with the ubiquitous fractals and 2) the first of its practical uses, namely, the selectivity estimation for range queries. Experiments on a variety of real data sets (islands, lakes, and human-inhabited areas) show that our method is extremely accurate, enjoy\n",
      "\n",
      "10. id: 5390a80f20f70186a0e969e0   score: 0.051845465   abstract: In this work, a semi-analytical solution for the asymptotic Langevin Equation (Random Displacement Equation) applied to the pollutant dispersion in the Planetary Boundary Layer (PBL) is developed and tested. The solution considers as starting point the first-order differential equation for the random displacement, on which is applied the Picard Iterative Method. The new model is parameterized by a turbulent eddy diffusivity derived from the Taylor Statistical Diffusion Theory and a model for the turbulence spectrum, assuming the hypothesis of linear superposition of the mechanical and thermal turbulence mechanisms. We report numerical simulations and comparisons with experimental data and other diffusion models. The main motivation for this work comes from the fact that the round-off error influence and computational time can be reduced in the new method.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1714015\n",
      "index                                        55323c2d45cec66b6f9db6a2\n",
      "title                                                          DESPRO\n",
      "authors             José-Antonio Marcos-García, Alejandra Martínez...\n",
      "year                                                           2015.0\n",
      "venue                                           Computers & Education\n",
      "references                                   558b5580612c41e6b9d497f9\n",
      "abstract            Collaboration analysis methods should be adapt...\n",
      "id                                                            1714015\n",
      "clustered_labels                                                    0\n",
      "Name: 1714015, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a5dc20f70186a0e7ef9f   score: 0.9826909   abstract: Scaffolding learners, i.e. helping learners to attain tasks they could not accomplish without support, entails the notion of fading, i.e. reducing the scaffolding for learners to become more and more self-regulated. Fading implies to tailor support for collaboration, such as collaboration scripts, to the particular needs of the specific collaborators. In computer supported collaborative learning (CSCL) settings, support can be designed in a very restrictive and inflexible fashion; at the same time computerized settings open new possibilities for the realization of adaptive support as they enable automation of analysis and feedback mechanisms. In this symposium we present new technical approaches and latest empirical research on possibilities and limitations of adaptive support for learners in CSCL settings.\n",
      "\n",
      "2. id: 5390b00c20f70186a0ed41b7   score: 0.9800913   abstract: The automated analysis of users' activity in collaborative systems aims at characterizing the collective process established between the participants and contributing to improving the deficiencies that occur during the users' activity and that are identified by means of this characterisation. This type of analysis, however, requires the completion of complex and costly software development tasks such as capturing information about the actions carried out by the users, calculating low and high level variables to characterize the users' work and behaviour and, finally, defining different kinds of interventions to improve the users' experience solving this way all of those problems that have been detected. To enable the automation of these tasks, reducing the required development effort, we propose a framework that allows software developers to specify the processes of analysis of the activ\n",
      "\n",
      "3. id: 5390b19020f70186a0ee0091   score: 0.97821885   abstract: Two strategies have been proposed in CSCL to foster effective collaboration: structuring the learning scenario by means of collaboration scripts and monitoring interactions among participants in order to detect and regulate potential deviations from the initial plan. In order to help teachers in this endeavor, we propose to combine these two approaches by means of a process where design takes into account the especial requirements posed by monitoring, and monitoring is informed by the characteristics of the scripts that must be met to achieve the learning goals. These desired features are obtained from the constraints defined by the collaborative-learning flow patterns on which the scripts are based. The result is an automated and higher-level view about the evolution of the learning process, integrating the data gathered from the different tools. This paper also presents a case study ba\n",
      "\n",
      "4. id: 5390aa0e20f70186a0ea79b7   score: 0.97334224   abstract: In this article, I argue that roles are a key construct for CSCL that demonstrate the interdisciplinary strengths of CSCL as a field. CSCL is a problem-driven field with a history of incorporating different paradigms, and has the advantage of using a design stance to understand phenomena like collaboration and learning that are difficult to study. Roles are understood differently by different disciplines, but the concept of roles serves as a boundary object between the different disciplines within CSCL and highlights potential areas for research.\n",
      "\n",
      "5. id: 5390a5dc20f70186a0e80c50   score: 0.97161794   abstract: Analyzing collaboration data can help revealing important aspects of the collaboration. The results can be used to support both teachers’ intervention and students’ self regulation. This paper presents a social network analysis method to support collaborative learning, using a collaborative knowledge building system as an example. The details of the network analysis process and its integration with a knowledge building system are described.\n",
      "\n",
      "6. id: 5390a74f20f70186a0e8c086   score: 0.97046465   abstract: The aim of the workshop is to look at the different approaches used to document collaborative learning and inform the design of the next generation of CSCL tools. We will be looking into different ways to document collaborative interactions, factors that affect collaboration as well as their effect on learning outcomes, and the development of a community of learners.\n",
      "\n",
      "7. id: 539099b320f70186a0e1968b   score: 0.96795994   abstract: CSCL is seen as a socio-technical process which has to be carefully planned by both students and teachers. These processes can be presented as graphical models which serve as maps to guide the students through their collaboration. In an experimental field study, the participatory development of these models was compared to a condition without models. The data shows the advantages of graphical models for the students' planning coordination. Most of the five hypotheses are confirmed in this study. These findings show just how important a technical concept is which helps to integrate the developed models as a means of coordination and navigation into CSCL-systems.\n",
      "\n",
      "8. id: 539099b320f70186a0e199ae   score: 0.9658352   abstract: In this paper our aim is to present a methodology which can be used in analyzing the interaction processes in a groupware environment. We demonstrate how the social network analysis approach can be used as a method to evaluate the social level structures and processes of a group studying in a CSCL environment. This approach tries to highlight in particular the participatory aspects of collaborative learning processes, but it can also serve as a starting point for more detailed analysis of knowledge building and acquisition processes. The relations between learners and the structure between documents written are the examples studied here.There are some features that make log files especially important in CSCL systems. First, log files can be used automatically, precisely and effectively for data collection. Second, analyzing this information enables evaluative perspective to the collabora\n",
      "\n",
      "9. id: 5390aa0e20f70186a0ea79b5   score: 0.9651191   abstract: The role concept has attracted a lot of attention as a construct for facilitating and analysing interactions in the context of computer-supported collaborative learning (CSCL). So far much of this research has been carried out in isolation and the focus on roles lacks cohesion. In this article we present a conceptual framework to synthesise the contemporary conceptualisation of roles, by discerning three levels of the role concept: micro (role as task), meso (role as pattern) and macro (role as stance). As a first step to further conceptualise 'role as a stance', we present a framework of eight participative stances defined along three dimensions: group size, orientation and effort. The participative stances - Captain, Over-rider, Free-rider, Ghost, Pillar, Generator, Hanger-on and Lurker - were scrutinised on two data sets using qualitative analysis. The stances aim to facilitate meanin\n",
      "\n",
      "10. id: 5390a79f20f70186a0e90eb8   score: 0.96458924   abstract: A CSCL environment provides support to manage collaborative tasks. However, these systems do not usually provide the personalization features required to adapt the learning experience to the student needs, a drawback that can affect the collaboration objective and ultimately a successful learning. To alleviate this disadvantage we propose an architecture that provides adaptive collaboration support for a CSCL environment framed in an open and standards-based LMS. Our proposal combines adaptation rules defined in IMS Learning Design specification and dynamic support through recommendations via an accessible and adaptive guidance system. The implementation offers CSCL courses following a methodology called Collaborative Logical Framework. This system has been tested on a real world scenario at the Madrid Science Week 2009.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674821\n",
      "index                                        55913b980cf232eb904fb604\n",
      "title                Social Group Interactions in a Role-Playing Game\n",
      "authors             Marynel Vázquez, Elizabeth J. Carter, Jo Ana V...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Tenth Annual ACM/IEEE Inter...\n",
      "references          5390aefc20f70186a0ecd8f7;5390be6620f70186a0f4cdee\n",
      "abstract            We present initial findings from an experiment...\n",
      "id                                                            1674821\n",
      "clustered_labels                                                    2\n",
      "Name: 1674821, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558fd3e1612c29c89cd7b5b9   score: 0.6139312   abstract: This paper examines the role of spatial behaviours in building human-robot relationships. A group of 8 participants, involved in a long-term HRI study, interacted with an artificial agent using different embodiments over a period of one and a half months. The robot embodiments had similar interactional and expressive capabilities, but only one embodiment was capable of moving. Participants reported feeling closer to the robot embodiment capable of physical movement and rated it as more likable. Results suggest that while expressive and communicative abilities may be important in terms of building affinity and rapport with human interactants, the importance of physical interactions when negotiating shared physical space in real time should not be underestimated.\n",
      "\n",
      "2. id: 5390ad0720f70186a0ebc6f8   score: 0.5976079   abstract: This paper describes the groundwork for designing a social and emotional interaction between a human and robot in game-playing. We considered that understanding deception in terms of mind reading plays a key role in realistic interactions for social robots. In order to understand the human mind, the humanoid robot observes nonverbal deception cues through multimodal perception during poker playing which is one of human social activities. Additionally, the humanoid manipulates the real environment which includes not only the game but also people to create a feeling of interacting with life-like machine and drive affective responses in determining the reaction.\n",
      "\n",
      "3. id: 5390a06e20f70186a0e4dff6   score: 0.5798763   abstract: A role play activity, in which people become tethered search robots, was trialled. The main constraint was space on the science centre floor, which reduced the effectiveness of the message.\n",
      "\n",
      "4. id: 5390bed320f70186a0f4dd84   score: 0.533519   abstract: This paper presents an experiment which evaluated the added value of a robot in a memory game in three conditions: tablet and robot, robot alone, and tablet alone. Results show that robots may increase game interest. In our experiment, the presence of a robot did not imply additional workload. It seems that people judged themselves more positively when they interacted with the robot. Moreover, people displayed more positive facial expressions with the robot.\n",
      "\n",
      "5. id: 5390b44620f70186a0ef8665   score: 0.5103592   abstract: A 2 x 2 between-subjects experiment was conducted to examine the effects of the type of artificial agent (robot vs. computer) and the role of the agent (ally vs. enemy) on people's perceptions and evaluations of the agent when playing a video game. Participants perceived that playing the game with a robot was more enjoyable and easier than playing with a computer. Regardless of the agent type, participants reported that playing the game was more enjoyable when the agent played as an ally rather than as their opponent. Implications of notable findings are discussed.\n",
      "\n",
      "6. id: 558bd3b70cf2e30013db1906   score: 0.4978504   abstract: Drawing on Kendon's F-formation framework of social interaction, we analysed the game-space activity of collocated players engaged in a tangible multiplayer game. Game input from groups of 3 players interacting competitively in a natural spatial arrangement via balance-boards requiring whole-body movements was logged and analysed quantitatively. The spatial analysis of a range of players' activities in game-space revealed synergistic effects combining perceptual-motor factors with game-strategy behaviour which were reflected in preferred game-board playing regions. The findings illustrate the importance for HCI designers of considering interactions between human spatial behaviour, physical space and virtual game-space as games become increasingly embodied and social.\n",
      "\n",
      "7. id: 558b897d612c6b62e5e8af9a   score: 0.48025015   abstract: For the last decade, robots have been adopted into group work ranging from corporate offices to military operations. While robotic technology has matured enough to allow robots to act as team members, our understanding of how this alters group work is limited. In particular, little work has examined how the adoption of robots might alter group processes and outcomes. The purpose of this workshop is to bring together researchers investigating issues related to the theoretical frameworks and methodological approaches to studying human robot interactions within groups. We expect the workshop will contribute to our understanding of how to better design robots for group interactions.\n",
      "\n",
      "8. id: 5390b36120f70186a0ef1ca3   score: 0.4074513   abstract: This paper presents an experimental test bed for exploring and evaluating human-robot interaction (HRI). Our system is designed around the concept of playing board games involving collaboration between humans and robots in a shared physical environment. Unlike the classic human-versusmachine situation often established in computer-based board games, our test bed takes advantage of the rich interaction opportunities that arise when humans and robots play collaboratively as a team. To facilitate interaction within a shared physical environment, our game is played on a large checkerboard where human and robotic players can be situated and play as game pieces. With meaningful interaction occurring within this controlled setup, various aspects of human-robot interaction can be easily explored and evaluated such as interaction methods and robot behaviour. In this paper we present our test bed \n",
      "\n",
      "9. id: 5591208f0cf232eb904fae03   score: 0.34444344   abstract: The video shows interactions between a robot and team of people during a short group problem-solving task framed as a bomb defusal scenario. We explore how a robot can influence conflict dynamics through repairing negative violations within the team. The video shows three samples of interactions between two participants, a confederate delivering personal violations, and a robot attempting to moderate the team dynamics. These samples highlight interactions from a larger 2 (negative trigger: task-directed vs. personal attack) x 2 (repair: yes or no) between subjects experiment (N = 57 teams, 114 participants). Specifically, the video provides a qualitative look at our finding that a team's sense of personal conflict increases when the robot identifies and intervenes after a personal violation.\n",
      "\n",
      "10. id: 5390aefc20f70186a0ecd536   score: 0.34048516   abstract: We explore deception in the context of a multi-player robotic game. The robot does not participate as a competitor, but is in charge of declaring who wins or loses every round. The robot was designed to deceive game players by imperceptibly balancing how much they won, with the hope this behavior would make them play longer and with more interest. Inducing false belief about who wins the game was accomplished by leveraging paradigms about robot behavior and their better perceptual abilities. Results include the finding that participants were more accepting of lying by our robot than for robots in general. Some participants found the balancing strategy favorable after being debriefed, while others showed less interest due to a perceived level of unfairness.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1720543\n",
      "index                                        559131c10cf2127aa930c166\n",
      "title               Preemptive intrusion detection: theoretical fr...\n",
      "authors             Phuong Cao, Eric Badger, Zbigniew Kalbarczyk, ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Symposium and Bootcamp...\n",
      "references          558bd1e20cf25dbdbb04dd63;5390adfd20f70186a0ec6...\n",
      "abstract            This paper presents a Factor Graph based frame...\n",
      "id                                                            1720543\n",
      "clustered_labels                                                    3\n",
      "Name: 1720543, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558bd1e20cf25dbdbb04dd63   score: 0.97112906   abstract: This paper presents a system named SPOT to achieve high accuracy and preemptive detection of attacks. We use security logs of real-incidents that occurred over a six-year period at National Center for Supercomputing Applications (NCSA) to evaluate SPOT. Our data consists of attacks that led directly to the target system being compromised, i.e., not detected in advance, either by the security analysts or by intrusion detection systems. Our approach can detect 75 percent of attacks as early as minutes to tens of hours before attack payloads are executed.\n",
      "\n",
      "2. id: 5390b1d220f70186a0ee2614   score: 0.8478418   abstract: This paper presents an in-depth study of the forensic data on security incidents that have occurred over a period of 5 years at the National Center for Supercomputing Applications at the University of Illinois. The proposed methodology combines automated analysis of data from security monitors and system logs with human expertise to extract and process relevant data in order to: (i) determine the progression of an attack, (ii) establish incident categories and characterize their severity, (iii) associate alerts with incidents, and (iv) identify incidents missed by the monitoring tools and examine the reasons for the escapes. The analysis conducted provides the basis for incident modeling and design of new techniques for security monitoring.\n",
      "\n",
      "3. id: 5390b44620f70186a0efa15b   score: 0.7218412   abstract: A majority of attacks on computer systems result from a combination of vulnerabilities exploited by an intruder to break into the system. An Attack Graph is a general formalism used to model security vulnerabilities of a system and all possible sequences of exploits which an intruder can use to achieve a specific goal. Attack Graphs can be constructed automatically using off-the-shelf model-checking tools. However, for real systems, the size and complexity of Attack Graphs greatly exceeds human ability to visualize, understand and analyze. Therefore, it is useful to identify relevant portions of an Attack Graph. To achieve this, we propose a ranking scheme for the states of an Attack Graph. Rank of a state shows its importance based on factors like the probability of an intruder reaching that state. Given a Ranked Attack Graph, the system administrator can concentrate on relevant subgrap\n",
      "\n",
      "4. id: 5390b19020f70186a0edf2a8   score: 0.70161086   abstract: Attack graphs have been widely used for attack modeling, alert correlation, and prediction. In order to address the limitations of current approaches - scalability and impact analysis - we propose a novel framework to analyze massive amounts of alerts in real time, and measure the impact of current and future attacks. Our contribution is threefold. First, we introduce the notion of generalized dependency graph, which captures how network components depend on each other, and how the services offered by an enterprise depend on the underlying infrastructure. Second, we extend the classical definition of attack graph with the notion of timespan distribution, which encodes probabilistic knowledge of the attacker's behavior. Finally, we introduce attack scenario graphs, which combine dependency and attack graphs, bridging the gap between known vulnerabilities and the services that could be ult\n",
      "\n",
      "5. id: 55323bbb45cec66b6f9da77e   score: 0.68098474   abstract: Post-incident analysis of a security event is a complex task due to the volume of data that must be assessed, often within tight temporal constraints. System software, such as operating systems and applications, provide a range of opportunities to record data in log files about interactions with the computer that may provide evidence during an investigation. Data visualization can be used to aid data set interpretation and improve the ability of the analyst to make sense of information. This paper posits a novel methodology that visualizes data from a range of log files to aid the investigation process. In order to demonstrate the applicability of the approach, a case study of identification and analysis of attacks is presented.\n",
      "\n",
      "6. id: 53908bcc20f70186a0dc62b9   score: 0.6807725   abstract: An integral part of modeling the global view of network security isconstructing attack graphs.In practice, attack graphs areproduced manually by Red Teams.Construction by hand, however, istedious, error-prone, and impractical for attack graphs larger than ahundred nodes.In this paper we present an automated technique forgenerating and analyzing attack graphs.We base our technique onsymbolic model checking algorithms,letting us construct attack graphs automatically and efficiently.Wealso describe two analyses to help decide which attacks would be mostcost-effective to guard against.We implemented our technique in atool suite and tested it on a small network example, which includesmodels of a firewall and an intrusion detection system.\n",
      "\n",
      "7. id: 53909eef20f70186a0e367f5   score: 0.64790964   abstract: Society's increasing reliance on networked information systems to support critical infrastructures has prompted interest in making the information systems survivable, so that they continue to perform critical functions even in the presence of vulnerabilities susceptible to malicious attacks. To enable vulnerable systems to survive attacks, it is necessary to detect attacks and isolate failures resulting from attacks before they damage the system by impacting functionality, performance or security. The key research problems in this context include: • detecting in-progress attacks before they cause damage, as opposed to detecting attacks after they have succeeded, • localizing and/or minimizing damage by isolating attacked components in real-time, and • tracing the origin of attacks. We address the detection problem by real-time event monitoring and comparison against events known to be un\n",
      "\n",
      "8. id: 53909f8c20f70186a0e3faeb   score: 0.63578796   abstract: In measuring the overall security of a network, a crucial issue is to correctly compose the measure of individual components. Incorrect compositions may lead to misleading results. For example, a network with less vulnerabilities or a more diversified configuration is not necessarily more secure. To obtain correct compositions of individual measures, we need to first understand the interplay between network components. For example, how vulnerabilities can be combined by attackers in advancing an intrusion. Such an understanding becomes possible with recent advances in modeling network security using attack graphs. Based on our experiences with attack graph analysis, we propose an integrated framework for measuring various aspects of network security. We first outline our principles andmethodologies. We then describe concrete examples to buildintuitions. Finally, we present our formal fra\n",
      "\n",
      "9. id: 5390ac5720f70186a0eb6db0   score: 0.6056824   abstract: EVA is an attack graph tool that allows an administrator to assess and analyze a network in a variety of fashions. Unlike other attack graph tools which just focus on visualizing the network or recommending a set of patches to secure the network, EVA goes beyond these modes to fully explore the power of attack graphs for a multitude of administrative and security tasks. EVA can be used to derive a set of hardening measures for a network, to perform strategic analysis of a network, to design a more secure network architecture, to assist in forensic evaluations after a security event and to augment an intrusion detect system with information about the likely targets of an attack. This paper summarizes the framework used by EVA, provides real-world results of using EVA and shows how EVA is scalable to large networks.\n",
      "\n",
      "10. id: 5390979920f70186a0e01036   score: 0.6041654   abstract: We map intrusion events to known exploits in the network attack graph, and correlate the events through the corresponding attack graph distances. From this, we construct attack scenarios, and provide scores for the degree of causal correlation between their constituent events, as well as an overall relevancy score for each scenario. While intrusion event correlation and attack scenario construction have been previously studied, this is the first treatment based on association with network attack graphs. We handle missed detections through the analysis of network vulnerability dependencies, unlike previous approaches that infer hypothetical attacks. In particular, we quantify lack of knowledge through attack graph distance. We show that low-pass signal filtering of event correlation sequences improves results in the face of erroneous detections. We also show how a correlation threshold ca\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1713118\n",
      "index                                        55323c2145cec66b6f9db47d\n",
      "title               Agent vision in multi-agent based simulation s...\n",
      "authors                              Dane M. Kuiper, Rym Z. Wenkstern\n",
      "year                                                           2015.0\n",
      "venue                       Autonomous Agents and Multi-Agent Systems\n",
      "references                                   5590b6060cf2ce4b6f39f152\n",
      "abstract            In this paper, we propose virtual agent vision...\n",
      "id                                                            1713118\n",
      "clustered_labels                                                    2\n",
      "Name: 1713118, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bae620f70186a0f3c752   score: 0.9835338   abstract: In order to create realistic simulations,virtual agents need to learn about their environment through perception. To date, most multi-agent simulation systems that implement some form of perception have focused heavily on a single sense, vision. In this paper we discuss a multi-sense perception system for virtual agents situated in large scale open environments. The perception system consists of modules (i.e., sensors) for visual, audible and olfactory senses. It also includes a perception combination module that combines data received from the multiple sensors into useful knowledge.\n",
      "\n",
      "2. id: 5390bd1520f70186a0f44584   score: 0.9803186   abstract: In order to successfully plan and make decisions, virtual agents situated in open environments must have mechanisms that allow them to gain knowledge regarding their environment. Most Multi-Agent Based Simulation Systems have approached the challenge of perception by providing agents with global environmental knowledge, an approach not suitable for the simulation of realistic scenarios. In this work, we present an agent perception module that integrates vision, auditory and olfactory sensors. We discuss vision and vision obstruction algorithms that approximates realistic vision while maintaining low execution time. We also discuss auditory and olfactory algorithms and present a perception combination system that combines visual, auditory and olfactory sensory data into useful knowledge. Finally, we present experimental results and a case study showing the performance of our perception mo\n",
      "\n",
      "3. id: 558b2e2a612c41e6b9d45db5   score: 0.9724148   abstract: Perception is one of the most important cognitive capabilities of an entity since it determines how an entity perceives its environment. The presented work focuses on providing cost efficient but realistic perceptual processes for intelligent virtual agents (IVAs) or NPCs with the goal of providing a sound information basis for the entities' decision making processes. In addition, an agent-central perception process should rovide a common interface for developers to retrieve data from the IVAs' environment. The overall process is evaluated by applying it to a scenario demonstrating its benefits. The evaluation indicates, that such a realistically simulated perception process provides a powerful instrument to enhance the (perceived) realism of an IVA's simulated behavior.\n",
      "\n",
      "4. id: 5390b13020f70186a0edda0e   score: 0.9468921   abstract: In this paper we discuss virtual agent perception in large scale open environment based MABS.\n",
      "\n",
      "5. id: 5390ada620f70186a0ec3742   score: 0.9199307   abstract: The development of large-scale distributed multi-agent systems in open dynamic environments is a challenge. System behavior is often not predictable and can only be evaluated by execution. This paper proposes a framework to support design and development of such systems: a framework in which both simulation and emulation play an important role. A distributed agent platform (AgentScape) is used to illustrate the potential of the framework.\n",
      "\n",
      "6. id: 539087fe20f70186a0d73946   score: 0.9119669   abstract: In virtual reality simulations the speed of rendering is vitally important. One of the techniques for controlling the frame rate is the assignment of different levels of detail for each object within a scene. The most well-known level of detail assignment algorithms are the Funkhouser[1] algorithm and the algorithm where the level of detail is assigned with respect to the distance of the object from the viewer.We propose an algorithm based on an analogy to a market system where each object does not have an assigned level of detail but has the ownership of a certain amount of time which is can use to be rendered with. The optimization of the levels of detail then becomes a simplistic trading process where objects with large amounts of time that they don't need will trade with objects who have need of extra time.The new algorithm has been implemented to run on the DIVE[2] virtual environme\n",
      "\n",
      "7. id: 5390bed320f70186a0f4f518   score: 0.88059187   abstract: In this paper we present DIVAs 4.0, a framework that supports the development of large-scale agent-based simulation systems where agents are situated in open environments. DIVAs includes high-level abstractions for the definition of agents and open environments, a micro kernel for the management of the simulation workflow, domain-specific libraries for the rapid development of simulations, and reusable, extendable components for the control and visualization of simulations. We illustrate the use of DIVAs through the development of a simple simulator where virtual agents are situated in a virtual city.\n",
      "\n",
      "8. id: 5390bae620f70186a0f3c80b   score: 0.8775817   abstract: In this paper we present DIVAs 4.0, a framework that supports the development of large-scale agent-based simulation systems where agents are situated in open environments. DIVAs includes high-level abstractions for the definition of agents and open environments, a microkernel for the management of the simulation workflow, domain-specific libraries for the rapid development of simulations, and reusable, extendable components for the control and visualization of simulations. We illustrate the use of DIVAs through the development of a simulator where virtual agents are situated in a virtual city and an office environment.\n",
      "\n",
      "9. id: 55323bc345cec66b6f9da893   score: 0.8614217   abstract: When agents are integrated in a game engine for embodiment in a virtual environment, perception often leads to performance issues due to the lack of control over the sensing process. In previous work a perception framework was proposed within CIGA, a middleware facilitating the coupling between a multiagent system and a game engine. It allowed agents to have control over the flow of sensory information generated in their embodiment. In this paper we continue this work by presenting performance optimizations within this framework. Here, the computational complexity of the sensing process in the game engine can be controlled by an agent itself, allowing it to deal with more complex environments. Additionally we provide an overall performance analysis of the framework.\n",
      "\n",
      "10. id: 53909f8c20f70186a0e3fd79   score: 0.85403454   abstract: The virtual vision paradigm features a unique synergy of computer graphics, artificial life, and computer vision technologies. Virtual vision prescribes visually and behaviorally realistic virtual environments as a simulation tool in support of research on large-scale visual sensor networks. Virtual vision has facilitated our research into developing multi-camera control and scheduling algorithms for next-generation smart video surveillance systems.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1663693\n",
      "index                                        559133d70cf232eb904fb393\n",
      "title               Application of Specific Delay Window Routing f...\n",
      "authors                                     Evan Wegley, Qinhai Zhang\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 ACM/SIGDA Internationa...\n",
      "references          539087cb20f70186a0d59036;5390b63320f70186a0f15...\n",
      "abstract            In addition to optimizing for timing performan...\n",
      "id                                                            1663693\n",
      "clustered_labels                                                    3\n",
      "Name: 1663693, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909e8b20f70186a0e2f0fb   score: 0.8862236   abstract: Routing delays dominate other delays in current FPGA designs. We have proposed a novel Globally Asynchronous Locally Synchronous (GALS) FPGA architecture called the GAPLA to deal with this problem. In the GAPLA architecture, The FPGA area is divided into locally synchronous blocks and the communications between them are through asynchronous I/O interfaces. An automatic design flow is developed for the GAPLA architecture. Starting from behavioral description, a design is partitioned into smaller modules and fit to GAPLA synchronous blocks. The asynchronous communications between modules are then sytthesized. The CAD flow is parameterized in modeling the GAPLA architecture. By manipulating the parameters, we could study different factors of the designed GAPLA arcitecturc. Our experimental results show an average of 20% performance improvement could be achieved by the GAPLA architecture.\n",
      "\n",
      "2. id: 539087d420f70186a0d5e3b9   score: 0.88543355   abstract: Abstract: Sequential place and route tools for FPGAs are inherently weak at addressing both wirability and timing optimizations. This is primarily due to the difficulty of accurately predicting wirability and delay during placement. A new performance-driven simultaneous placement/routing technique has been developed for island-style FPGA designs. On a set of industrial designs for Xilinx 4000-series FPGAs, our scheme produces 100% routed designs with 8%-15% improvement in delay when compared to the Xilinx XACT5.0 place and route system.\n",
      "\n",
      "3. id: 5390b63320f70186a0f15ef0   score: 0.85729086   abstract: This paper presents the first published algorithm to simultaneously optimize both short- and long-path timing in a field-programmable gate array (FPGA): the routing cost valleys (RCV) algorithm. RCV consists of the following two components: a new slack-allocation algorithm that determines both a minimum and a maximum delay budget for each circuit connection and a new router that strives to meet and, if possible, surpass these connection-delay constraints. RCV improves both long- and short-path timing slacks significantly versus an earlier computer-aided design system, showing the importance of an integrated approach that simultaneously optimizes considering both types of timing constraints. It is able to meet long- and short-path timing constraints on all 157 peripheral component interconnect cores tested, while an earlier algorithm failed to achieve timing on 75% of the cores. Even in c\n",
      "\n",
      "4. id: 539087d420f70186a0d5e3b8   score: 0.84195566   abstract: We present an empirical routing delay model for estimating interconnection delays in FPGAs. We assume that the routing delay is a function of interPLC distances, circuit size, fanout of the net and routing congestion in the channel. We performed extensive simulations of various circuits to generate a sufficiently large dataset. Our method estimates delays by reading the average value tables and interpolating the values, if necessary. We present a rigorous statistical justification of this delay model. Our results show that our method predicts the delays within 20% of actual and it far outperforms all other existing techniques.\n",
      "\n",
      "5. id: 5390975920f70186a0dfddc2   score: 0.83960253   abstract: Field-Programmable Gate Arrays (FPGAs) have been one of the most popular devices for system prototyping, logic emulation, and reconfigurable computing. Their user-programmable prefabricated logic modules and routing structures provide low manufacturing cost and fast time-to-market implementation solutions to the users. However, the routing delay due to their inherent routing structure has been one of the biggest bottlenecks of their speed performance. As the VLSI fabrication feature size is shrunk to deep submicron dimension in modern technology, the portion taken up by routing in both of area and timing grows even more significantly. In this dissertation, we address issues on routing algorithms to optimize area and timing of an FPGA system. We present a new timing-driven routing algorithm for FPGAs. The algorithm finds a routing solution with minimum critical path delay for a given plac\n",
      "\n",
      "6. id: 5390880720f70186a0d78a91   score: 0.81859183   abstract: In this paper we present a timing-driven router for symmetrical array-based FPGAs. The routing resources in the FPGAs consist of segments of various lengths. Researchers have shown that the number of segments, instead of wirelength, used by a net is the most critical factor in controlling routing delay in an FPGA. Thus, the traditional measure of routing delay on the basis of geometric distance of a signal is not accurate. To consider wirelength and delay simultaneously, we study a model of timing-driven routing rees, arising from the special properties of FPGA routing architectures. Based on the solutions to the routing-tree problem, we present a routing algorithm that is able to utilize various routing segments with global considerations to meet timing constraints. Experimental results show that our approach is very effective in reducing timing violations.\n",
      "\n",
      "7. id: 53908b6c20f70186a0dbe867   score: 0.8149381   abstract: In this paper we present a timing-constrained routing algorithm for symmetrical FPGAs which embodies a novel incremental routing strategy that combines global and detailed routing, and a routing resource allocation algorithm that takes into account both the characteristics of the routing resources and timing information. Experimental results confirm that the algorithm reduces delay along the longest path in the circuit, uses routing resources efficiently, and requires low CPU time.\n",
      "\n",
      "8. id: 53909f8220f70186a0e3c88d   score: 0.7889086   abstract: Delay variation can cause a design to fail its timing specification. Ernst et al. [2003] observe that the worst delay of a design is least probable to occur. They propose a mechanism to detect and correct occasional errors while the design can be optimized for the common cases. Their experimental results show significant performance (or power) gain as compared with the worst-case design. However, the architecture in Ernst et al. [2003] suffers the short path problem, which is difficult to resolve. In this article, we propose a novel error-detecting architecture to solve the short path problem. Our experimental results show considerable performance gain can be achieved with reasonable area overhead.\n",
      "\n",
      "9. id: 539087cb20f70186a0d59036   score: 0.7876047   abstract: Routing FPGAs is a challenging problem because of the relative scarcity of routing resources, both wires and connection points. This can lead either to slow implementations caused by long wiring paths that avoid congestion or a failure to route all signals. This paper presents PathFinder, a router that balances the goals of performance and routability. PathFinder uses an iterative algorithm that converges to a solution in which all signals are routed while achieving close to the optimal performance allowed by the placement. Routability is achieved by forcing signals to negotiate for a resource and thereby determine which signal needs the resource most. Delay is minimized by allowing the more critical signals a greater say in this negotiation. Because PathFinder requires only a directed graph to describe the architecture of routing resources, it adapts readily to a wide variety of FPGA ar\n",
      "\n",
      "10. id: 5390990f20f70186a0e1136d   score: 0.78032786   abstract: As the clock frequencies used in industrial applications increase, the timing requirements on routing problems become tighter, and current routing tools can not successfully handle these constraints any more. We focus on the high-performance single-layer bus routing problem, where the objective is to match the lengths of all nets belonging to each bus. An effective approach to solve this problem is to allocate extra routing resources around short nets during routing; and use those resources for length extension afterwards. We first propose a provably optimal algorithm for routing nets with min-area max-length constraints. Then, we extend this algorithm to the case where minimum constraints are given as exact length bounds. We also prove that this algorithm is optimal within a constant factor. Both algorithms proposed are also shown to be scalable for large circuits, since the respective \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1694860\n",
      "index                                        5592481d0cf28b1a968ff621\n",
      "title               Decentralized dynamic task planning for hetero...\n",
      "authors             Donato Paola, Andrea Gasparri, David Naso, Fra...\n",
      "year                                                           2015.0\n",
      "venue                                               Autonomous Robots\n",
      "references          558ce6830cf23fdd601e1042;5390b68720f70186a0f1d...\n",
      "abstract            In this paper, we propose a decentralized mode...\n",
      "id                                                            1694860\n",
      "clustered_labels                                                    2\n",
      "Name: 1694860, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a4cc20f70186a0e7584c   score: 0.9510514   abstract: The field of robotics is evolving from single monolithic robots to teams of small but interconnected robots that achieve global objectives using local coordination. Coordinated missions for such teams of mobile robots include coordinated estimation, surveillance, and coverage, coordinated satellite alignment, as well as distributed placement and assignment in creating desirable team formations. The fundamental challenge in such problems is the design of local rules, such as distributed controllers and estimators, which by local coordination give rise to the desired global objectives.In this thesis, we present the first distributed, scalable, and verifiable algorithm that achieves dynamic robot placement and assignment using local coordination rules. This is achieved using a combination of multi-destination potential fields and assignment coordination protocols, and results in a dynamic n\n",
      "\n",
      "2. id: 5390bded20f70186a0f4926c   score: 0.9326989   abstract: This paper considers the problem of assigning a set of tasks to a set of heterogeneous agents under the additional assumptions that some tasks must be necessarily allocated and therefore are critical for the assignment problem, and that each agent can execute a limited number of tasks. In order to solve this problem in a decentralized way (i.e., without any form of central supervision), we develop an extension of an algorithm proposed in the recent literature. After analyzing convergence and communication requirement of the algorithm, a set of numerical simulations is provided to confirm the effectiveness of the proposed approach.\n",
      "\n",
      "3. id: 5390bed320f70186a0f4ef99   score: 0.89947516   abstract: This paper describes the task management elements of a framework for coordinating a changing collection of heterogeneous robots operating in complex and dynamic environments such as disaster zones. Our framework allows a team to discover and distribute tasks among its members, in a distributed fashion, where the structure of the team is under regular change. Robots may become lost or fail at any time, and new equipment may arrive at any time. We evaluate our framework through an example implementation where robots perform exploration and search for victims in a simulated disaster environment.\n",
      "\n",
      "4. id: 5390a1bc20f70186a0e54285   score: 0.8576489   abstract: Over the last 5 years, the AI community has shown considerable interest in decentralized control of multiple decision makers or \"agents\" under uncertainty. This problem arises in many application domains, such as multi-robot coordination, manufacturing, information gathering, and load balancing. Such problems must be treated as decentralized decision problems because each agent may have different partial information about the other agents and about the state of the world. It has been shown that these problems are significantly harder than their centralized counterparts, requiring new formal models and algorithms to be developed. Rapid progress in recent years has produced a number of different frameworks, complexity results, and planning algorithms. The objectives of this paper are to provide a comprehensive overview of these results, to compare and contrast the existing frameworks, and \n",
      "\n",
      "5. id: 5390882820f70186a0d8aec4   score: 0.84581506   abstract: From the Publisher:Distributed autonomous robotic systems (DARS) are systems composed of multiple autonomous units such as modules, cells, processors, agents, and robots. Cooperative operation of multiple autonomous units is expected to lead to desirable features such as flexibility, fault tolerance, and efficiency. The DARS is the leading established conference on these topics. All papers have the common goal to contribute solutions to realize robust and intelligent robotic systems.\n",
      "\n",
      "6. id: 5390bda020f70186a0f46c20   score: 0.7980295   abstract: This paper proposes a distributed control algorithm to im- plement dynamic task allocation in a swarm robotics environment. In this context, each robot that integrates the swarm must run the algorithm periodically in order to control the underlying actions and decisions. The algorithm was implemented and extensively tested. The corresponding performance and effectiveness are promising.\n",
      "\n",
      "7. id: 53909f8c20f70186a0e400fc   score: 0.74853617   abstract: The challenges of robotics have led the researchers to develop control architectures composed of distributed, independent and asynchronous behaviors. One way to approach decentralization is through cooperative control, since it allows the development of complex behavior based on several controllers combined to achieve the desired result. Robots, however, require high-level cognitive capacities, and multi-agent architectures provide the appropriate level of abstraction to define them. This article describes a multi-agent architecture combined with cooperative control developed within the agent. The experiments were carried out on an ActivMedia Pioneer 2DX mobile robot.\n",
      "\n",
      "8. id: 5390a88c20f70186a0e9a47b   score: 0.71393055   abstract: Multi agent systems are being applied to a wide range of applications from territorial explorations to unmanned combat missions. Agents operate as distributed problem solvers and work together to accomplish the tasks in the system. Various control frameworks have been developed for execution of tasks by agents in the system. Control frameworks consider that the agents are aware of their tasks and provide for the controls to complete the tasks in the dynamic, changing and uncertain environment of the system. But tasks in multi agent systems are not always predetermined and may evolve over time requiring dynamic planning for not only the controls but also the determination of tasks for the agents. This paper presents an integrated task planning and control framework for multi agent systems. A dynamic planning framework is developed which in turn is integrated with a selected hybrid control\n",
      "\n",
      "9. id: 558b277d612c41e6b9d4513f   score: 0.7128323   abstract: The current trends in the robotics field have led to the development of large-scale multiple robot systems, and they are deployed for complex missions. The robots in the system can communicate and interact with each other for resource sharing and task processing. Many of such systems fail despite the availability of necessary resources. The major reason for this is their poor coordination mechanism. Task planning, which involves task decomposition and task allocation, is paramount in the design of coordination and cooperation strategies of multiple robot systems. Task allocation mechanism allocates the task in a mission to the robots by maximizing the overall expected performance, and thereby reducing the total allocation cost for the team. In this paper, we formulate a heuristic search-based task allocation algorithm for the task processing in heterogeneous multiple robot system, by max\n",
      "\n",
      "10. id: 5390a96f20f70186a0ea45e3   score: 0.69925445   abstract: We present a computational framework for automatic synthesis of decentralized communication and control strategies for a robotic team from global specifications, which are given as temporal and logic statements about visiting regions of interest in a partitioned environment. We consider a purely discrete scenario, where the robots move among the vertices of a graph. However, by employing recent results on invarianec and facet reachability for dynamical systems in environments with polyhedral partitions, the framework from this paper can be directly implemented for robots with continuous dyrtamics. While allowing tor a rich specification language and guaranteeing the correctness of the solution, our approach is conservative in the sense that we might not find a solution, even if one exists. The overall amount of required computation is large. However, most of it is performed offline befor\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1699119\n",
      "index                                        55912fc30cf232eb904fb283\n",
      "title               Joint energy-and-bandwidth spectrum sensing wi...\n",
      "authors             Yanxiao Zhao, Jems Pradhan, Jun Huang, Yu Luo,...\n",
      "year                                                           2015.0\n",
      "venue                             ACM SIGAPP Applied Computing Review\n",
      "references                                   5390b00c20f70186a0ed6301\n",
      "abstract            This paper focuses on an experimental investig...\n",
      "id                                                            1699119\n",
      "clustered_labels                                                    3\n",
      "Name: 1699119, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b09a8612c41e6b9d41060   score: 0.9932288   abstract: Spectrum sensing is an essential process to implement dynamic spectrum access successfully and hence improve the spectrum utilization. This paper focuses on an experimental investigation of spectrum sensing using GNU radio and Universal Software Radio Peripheral (USRP) board. In the most related works, the method of energy detection is widely used for experiments on spectrum sensing, in which the energy amplitude of the received signal is the sole parameter to determine a channel's status. Specifically, if the amplitude of received energy exceeds a given threshold, the channel is detected busy, otherwise the channel is sensed idle. By observing experimental results, we find that the energy amplitudes of unwanted signals, which may be either interference or noise or both, are often at the same level as the desired signal. This will lead to a false alarm if the traditional energy detection\n",
      "\n",
      "2. id: 53909ed120f70186a0e2f723   score: 0.9818056   abstract: Spectrum sensing has been identified as a key enabling functionality to ensure that cognitive radios would not interfere with primary users, by reliably detecting primary user signals. Recent research studied spectrum sensing using energy detection and network cooperation via modeling and simulations. However, there is a lack of experimental study that shows the feasibility and practical performance limits of this approach under real noise and interference sources in wireless channels. In this work, we implemented energy detector on a wireless testbed and measured the required sensing time needed to achieve the desired probability of detection and false alarm for modulated and sinewave-pilot signals in low SNR regime. We measured the minimum detectable signal levels set by the receiver noise uncertainties. Our experimental study also measured the sensing improvements achieved via network\n",
      "\n",
      "3. id: 558ae428612c41e6b9d3c8da   score: 0.98050606   abstract: In a cognitive radio system, spectrum sensing is considered as the key step of the cognition cycle and the primary function of the opportunistic usage of the spectrum. In real world scenarios, spectrum sensing algorithms are sensitive to several factors especially the model uncertainty problem in low signal-to-noise ratio environments. In this paper, a blind detector based on the cyclostationarity feature is implemented and tested on the USRP/GNU Radio platform. The performance of this algorithm is measured in different scenarios at various frequencies to prove the sustainability of this blind technique and its robustness against severe channel conditions. Measurements are conducted on the 802.11a bands, the 3G network, and for USRP transmitted signals. In this study, the detector based on the symmetry property of the cyclic autocorrelation function is compared to the classical energy de\n",
      "\n",
      "4. id: 558b1d2c612c41e6b9d43b5f   score: 0.98050606   abstract: In a cognitive radio system, spectrum sensing is considered as the key step of the cognition cycle and the primary function of the opportunistic usage of the spectrum. In real world scenarios, spectrum sensing algorithms are sensitive to several factors especially the model uncertainty problem in low signal-to-noise ratio environments. In this paper, a blind detector based on the cyclostationarity feature is implemented and tested on the USRP/GNU Radio platform. The performance of this algorithm is measured in different scenarios at various frequencies to prove the sustainability of this blind technique and its robustness against severe channel conditions. Measurements are conducted on the 802.11a bands, the 3G network, and for USRP transmitted signals. In this study, the detector based on the symmetry property of the cyclic autocorrelation function is compared to the classical energy de\n",
      "\n",
      "5. id: 5390a8db20f70186a0e9d3e3   score: 0.9754836   abstract: As the rapid growth of wireless technologies, current spectrum scarcity has become a serious problem as more and more wireless applications compete for very little spectrum. On the other hand, some licensed bands allocated to applications such as TV services show little usage. Then, to improve the utilization of radio spectrum, cognitive radio technology has been proposed, so that devices can sense and use idle bands dynamically by autonomous detection. This paper mainly introduce an enhanced energy detection algorithm based on some usual methods for detecting signals embedded in noise, and then give the simulation results.\n",
      "\n",
      "6. id: 5390b29820f70186a0eea386   score: 0.97344345   abstract: Spectrum sensing is an essential task for cognitive radios (CRs) and next generation wireless networks (NGWNs). In this study, an experimental setup which can emulate a spectrum sensing scheme through the use of energy detector is proposed. Both line--of--sight (LOS) and non--line--of--sight (NLOS) scenarios are considered. NLOS scenarios take into account the light and heavy shadowing cases as well. The impact of having perfect spectral knowledge on the performance of spectrum sensing is also studied. Results and relevant discussions are given. Conclusions and future directions are outlined.\n",
      "\n",
      "7. id: 558b4376612c41e6b9d479a6   score: 0.9674718   abstract: The performance of a spectrum sensing algorithm is dependent on many factors such as total detection time, radio noise floor, and carrier frequency. In order to maximize detection capability, the radio platform for which the spectrum sensor is implemented must be sufficiently configurable. An appropriate platform to address such a problem is the Universal Software Radio Peripheral (USRP) with a SBX daughterboard as it is software configurable and capable of wideband reception over a large range of carrier frequencies. Motivation for characterizing the sensing limits of the USRP/SBX radio was the DARPA Spectrum Challenge (DSC) cooperative event. The DSC cooperative event simulated the scenario where multiple users must share the same channel and each user is not supplied with prior information of strategies employed by the other users. Solutions to this problem typically include spectrum \n",
      "\n",
      "8. id: 5390b29820f70186a0eea368   score: 0.96329963   abstract: Spectrum sensing, in particular, detecting the presence of incumbent users in licensed spectrum, is one of the pivotal task for cognitive radios (CRs). In this paper, we provide solutions to the spectrum sensing problem by using statistical test theory, and thus derive novel spectrum sensing approaches. We apply the classical Kolmogorov-Smirnov (KS) test to the problem of spectrum sensing under the assumption that the noise probability distribution is known. In practice, the exact noise distribution is unknown, so a sensing method for Gaussian noise with unknown noise power is proposed. Next it is shown that the proposed sensing scheme is asymptotically robust and can be applied to non-Gaussian noise distributions. We compare the performance of sensing algorithms with the well-known Energy Detector (ED) and Anderson-Darling (AD) sensing proposed in recent literature. Our paper shows that\n",
      "\n",
      "9. id: 558b789b612c6b62e5e894dc   score: 0.9601375   abstract: In radiomobile contexte, radio frequency spectrum is a ressource that needs to be used with appropriate efficiency. This can be achieved by the mean of spectrum sensing operation. This function consists to analyze the occupancy of the radio frequency spectrum in order to detect which bands are unused. This concept is largely appreciated in cognitive radio where more flexibility is required to adapt to the communication environment. Different techniques are presented in the literature. In this paper, we are interested by the application of the energy detector method for spectrum sensing. This application is performed in cognitive radio systems with the use of random sampling. The performance of this approach is evaluated in term of its receiver operating characteristic curve and compared to the uniform sampling case.\n",
      "\n",
      "10. id: 5390a77d20f70186a0e8ea1c   score: 0.95861524   abstract: One of the unique feature of cognitive radio systems is they can make use of temporarily idle spectral bands in an opportunistic manner. To achieve this however, robust sensing algorithm is required which otherwise incumbent services could suffer from interference. In this paper sensing algorithms based on energy detection and self-correlation detection are presented and compared. The self-correlation detection is shown to provide superior performance with lower or comparable complexity. Computer simulation is provided to validate the concept.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691626\n",
      "index                                        559254160cf2aff368683b13\n",
      "title               Robust to PVT enhanced DC gain amplifier using...\n",
      "authors                                       Héctor Iván Gómez Ortiz\n",
      "year                                                           2015.0\n",
      "venue                Analog Integrated Circuits and Signal Processing\n",
      "references          5390882c20f70186a0d8bfe3;5390b1d220f70186a0ee2d58\n",
      "abstract            This paper presents an operational transconduc...\n",
      "id                                                            1691626\n",
      "clustered_labels                                                    2\n",
      "Name: 1691626, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908b4920f70186a0dbb08d   score: 0.96903247   abstract: A small change to the standard current mirror op amp configuration is shown to improve performance, with few, if any, disadvantages. Adding a pair of fixed current sources allows reduced operating-point current in the output stage, while the resulting “leveraging” effect increases slew rate. For equal total power dissipation, the new configuration improves DC gain and gain-bandwidth (GBW) over conventional current-mirror and folded cascode op amps, as shown by hand analyses and SPICE simulations. Also, because of increased input stage transconductance, the new configuration reduces thermal and flicker noise.\n",
      "\n",
      "2. id: 53908bde20f70186a0dc6fbc   score: 0.9129032   abstract: The technique of mirror biasing is introduced and applied to a very high gain two stage CMOS cascode op-amp, in order to desensitize its output voltage to bias variations. Various performance metrics like low frequency common mode and power supply rejection ratios, slew rate and the sensitivity of the systematic offset are substantially improved. The improved performance is theoretically predicted and substantiated through circuit simulations.\n",
      "\n",
      "3. id: 5390a0b720f70186a0e4ff01   score: 0.90878   abstract: An improved frequency compensation technique is presented for low-power low-voltage three-stage operational amplifiers with high capacitive loads. The technique uses single RC Miller compensation and a direct gain enhanced feedforward path from the input to the output. With a load capacitance of 300 pF, the amplifier nominally achieves a dc gain of 74 dB, a 3-dB bandwidth of 2.9 kHz, a 52 degrees phase margin, and a slew rate of 0.22 V/μs, while consuming 0.24 mW of power with a 1.2-V supply voltage, in a 180 nm CMOS technology. The 3-dB bandwidth is one of the highest reported for a high-gain three-stage CMOS amplifier.\n",
      "\n",
      "4. id: 5390b52620f70186a0f02ec2   score: 0.8916225   abstract: Based on the analysis of the inherent limitations of conventional OTA, this paper introduces a basic strategy by combinating linear-nonlinear adaptive current mirror and local cross-pair to solve the mutual restraint between AC and DC characteristics of the circuit. In order to simplify the multi-mode complicated circuit design, an analytical model for the new OTA is proposed, which is consistent with SPICE simulation results. Under the limitation of the static current consumption, the maximum limit of the circuit performance can be predicted by the proposed model. Under the condition of 29 μA quiescent current and 30 pF load capacitance, a chip is implemented in 0.18 μm CMOS technology, and the test results show that the DC gain, GBW and slew rate achieve 73 dB, 6 MHz and 14 V/μS, respectively, and the optimal performance of DC, AC and transient can be obtained almost simultaneously.\n",
      "\n",
      "5. id: 5390a25820f70186a0e5eb77   score: 0.8830344   abstract: A new operational transconductance amplifier (OTA) builds with CMOS inverters only is proposed in this paper. Simulations with typical BSIM3V3 parameters of a 0.35 μm CMOS process have shown a 3.56 GHz gain-bandwidth product under 2.5 V supply voltage. The corresponding total harmonic distortion is equal to 0.46% for 2 V peak---peak differential output voltage. At the same supply voltage, the circuit can provided at each output a voltage swing of 2.25 V peak---peak. From VDD = 2 V to VDD = 2.5 V the differential transconductance varies from 72 to 108.4 μ驴驴1. The corresponding common mode rejection ratio and the total power consumption are always lower than 驴31 dBc and 800 μW, respectively. Typical application of a biquad filter is proposed to illustrate the circuit capabilities.\n",
      "\n",
      "6. id: 53908bcc20f70186a0dc50bd   score: 0.8702668   abstract: Two SC common mode feedbacks are separately used to control the output dc level of a fully differential OTA. Both circuits are compared in order to put forward their drawbacks and advantages. Simulations and measurements are presentedfor both circuits. They show that the design of these SC networks has to be optimized in order to reduce their sensitivity to the layout. Finally the performance of the amplifier are presented, it achieves 70 dB of dc gain, 50 Mhz of gain-bandwidth and a phase margin greater than 80 degrees.\n",
      "\n",
      "7. id: 5390a74f20f70186a0e8c856   score: 0.8692713   abstract: Feed-forward techniques are explored for the design of high-frequency Operational Transconductance Amplifiers (OTAs). For single-stage amplifiers, a recycling folded-cascode OTA presents twice the GBW (197.2 MHz versus 106.3 MHz) and more than twice the slew rate (231.1 V/µs versus 99.3 V/µs) as a conventional folded cascode OTA for the same load, power consumption, and transistor dimensions. It is demonstrated that the efficiency of the recycling folded-cascode is equivalent to that of a telescopic OTA. As for multistage amplifiers, a No-Capacitor Feed-Forward (NCFF) compensation scheme which uses a high-frequency pole-zero doublet to obtain greater than 90 dB DC gain, GBW of 325 MHz and better than 70° phase margin is discussed. The settling-time-of the NCFF topology can be faster than that of OTAs with Miller compensation. Experimental results for the recycling folded-cascode OTA fabr\n",
      "\n",
      "8. id: 5390995d20f70186a0e15a19   score: 0.8288003   abstract: High-performance operational transconductance amplifier (OTA) is designed for switched-capacitor applications. Without using a cascoded output stage, which limits the voltage swing, the output resistance is significantly increased for high DC gain by accurately controlling the output current. Also, the output stage has class-AB operation, so the overall power efficiency is improved. With significantly low quiescent current, the presented new OTA achieves higher DC gain than conventional OTAs. Theoretical analysis and HSPICE simulations prove the performance of the new OTA.\n",
      "\n",
      "9. id: 53909e8b20f70186a0e2e316   score: 0.8067004   abstract: In this paper, improved CMRR, high gain CMOS current-mode operational amplifier (COA) is presented. A new class AB input stage is used in order to obtain very low input resistance. The proposed COA is operated under ±1.5 V voltage supplies and designed with 0.35-μm CMOS process. Results of simulation indicate a 107 dB DC gain, 123 Ω input resistance, common mode rejection ratio exceeding 110dB and a gain-bandwidth product at about 100 MHz.\n",
      "\n",
      "10. id: 5390a1f820f70186a0e5da52   score: 0.7572562   abstract: A frequency compensation technique for three-stage amplifiers is introduced. The proposed solution exploits only one Miller capacitor and a resistor in the compensation network. The straightness of the technique is used to design, using a standard CMOS 0.35-µm process, a 1.5-V OTA driving a 150-pF load capacitor. The dc consumption is about 14µA at DC and a 1.8-MHz gain–bandwidth product is obtained, providing significant improvement in both (MHzpF)-mA and ((V-µs)pF)-mA performance parameters. Copyright © 2007 John Wiley & Sons, Ltd.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1748238\n",
      "index                                        554e3f0f0cf22ca2c80f9d7d\n",
      "title               Robust closed-form time-of-arrival source loca...\n",
      "authors                 Chee-Hyun Park, Soojeong Lee, Joon-Hyuk Chang\n",
      "year                                                           2015.0\n",
      "venue                                               Signal Processing\n",
      "references          558fd2e5612c29c89cd7b561;558b432e612c41e6b9d4796f\n",
      "abstract            In this paper, we propose an NLOS source local...\n",
      "id                                                            1748238\n",
      "clustered_labels                                                    2\n",
      "Name: 1748238, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac5720f70186a0eb618f   score: 0.96267307   abstract: We present a source localization approach using resampling within a sparse representation framework. In particular, the amplitude and phase information of the sparse solution is considered holistically to estimate the direction-of-arrival (DOA), where a resampling technique is developed to determine which information will give a more precise estimation. The simulation results confirm the efficacy of our proposed method.\n",
      "\n",
      "2. id: 5390a79f20f70186a0e9275a   score: 0.9576754   abstract: Non-Line-Of-Sight (NLOS) signal propagation is the major source of error in conventional Time-Of-Arrival (TOA) based wireless location estimation algorithms. Previous research has mainly sought to address this problem in two ways: NLOS identification and NLOS mitigation. This paper focuses on the latter issue. It deals with the problem that even when NLOS measurements can be identified, among all the measurements obtained, there may still be not enough Line-Of-Sight (LOS) measurements for accurate location estimation using traditional TOA-based algorithms. With the assumptions that the total number of the measurements is greater than the minimum required and the NLOS measurements are identifiable, this paper proposes an enhanced TOA-based localization algorithm. It contains two parts: a combination stage and a Maximum Likelihood (ML) estimator. The proposed algorithm has an advantage tha\n",
      "\n",
      "3. id: 5390b9d520f70186a0f30f7f   score: 0.9478657   abstract: Source localization can be achieved by making use of the time-of-arrival (TOA) measurements, but it is not a trivial task because the TOAs have nonlinear relationships with the source coordinates. This paper exploits a neural network technique, namely, Lagrange programming neural networks, for TOA-based localization. We also investigate the local stability of our formulation. Simulation results demonstrate that the performance of the proposed location estimator approaches the optimality benchmark of Cram${\\rm\\acute{e}}$r-Rao lower bound.\n",
      "\n",
      "4. id: 5390a37f20f70186a0e6d8d3   score: 0.9462003   abstract: Mitigation of non-line-of-sight (NLoS) errors in the geolocation problem has received much attention. It is well-known that these errors degrade the robustness and accuracy of localization systems, whereby localization is performed using time-of-arrival (ToA) measurements. In this paper, we propose a robust approach to mitigate the NLoS effect in location estimation. The approach is based on modeling NLoS errors as contaminated Gaussian noise, which masks the unknown ToA. We then suggest a non-parametric estimate of the noise density, obtained from observations. It provides robustness against the outliers caused by the NLoS errors common in urban areas. Simulation results show an improvement over conventional approaches.\n",
      "\n",
      "5. id: 558b432e612c41e6b9d4796f   score: 0.94122344   abstract: We consider robust geolocation in mixed line-of-sight (LOS)/non-LOS (NLOS) environments in cellular radio networks. Instead of assuming known propagation channel states (LOS or NLOS), we model the measurement error with a general two-mode mixture distribution although it deviates from the underlying error statistics. To avoid offline calibration, we propose to jointly estimate the geographical coordinates and the mixture model parameters. Two iterative algorithms are developed based on the well-known expectation-maximization (EM) criterion and joint maximum a posteriori-maximum likelihood (JMAP-ML) criterion to approximate the ideal maximum-likelihood estimator (MLE) of the unknown parameters with low computational complexity. Along with concrete examples, we elaborate the convergence analysis and the complexity analysis of the proposed algorithms. Moreover, we numerically compute the Cr\n",
      "\n",
      "6. id: 5390b68720f70186a0f1c799   score: 0.92788786   abstract: Recent advances in wireless sensor networks have led to renewed interests in the problem of source localization. Source localization has broad range of applications such as emergency rescue, asset inventory, and resource management. Among various measurement models, one important and practical source signal measurement is the received signal time of arrival (TOA) at a group of collaborative wireless sensors. Without time-stamp at the transmitter, in traditional approaches, these received TOA measurements are subtracted pairwise to form time-difference of arrival (TDOA) data for source localization, thereby leading to a 3-dB loss in signal-to-noise ratio (SNR). We take a different approach by directly applying the original measurement model without the subtraction preprocessing. We present two new methods that utilize semidefinite programming (SDP) relaxation for direct source localizatio\n",
      "\n",
      "7. id: 53909a9320f70186a0e22ae1   score: 0.9219218   abstract: With few exceptions, high-resolution source localization algorithms require an exact characterization of the array, including knowledge of the sensor positions, sensor gain/phase response, mutual coupling, and receiver equipment effects. In practice, all such information is inevitably subject to errors. Recently, several different methods have been proposed for alleviating the inherent sensitivity of parametric methods to such modeling errors. The technique proposed herein is related to the class of so-called auto-calibration procedures, but it is assumed that certain prior knowledge of the array response errors is available. The optimal maximum a posteriori (MAP) estimator for the problem at hand is formulated, and a more computationally attractive large-sample approximation is derived. In addition, the performance advantage of the algorithm is illustrated by an example involving a line\n",
      "\n",
      "8. id: 558fcb5c612c29c89cd7b229   score: 0.9186263   abstract: This correspondence studies the received signal strength-based localization problem when the transmit power or path-loss exponent is unknown. The corresponding maximum-likelihood estimator (MLE) poses a difficult nonconvex optimization problem. To avoid the difficulty in solving the MLE, we use suitable approximations and formulate the localization problem as a general trust region subproblem, which can be solved exactly under mild conditions. Simulation results show a promising performance for the proposed methods, which also have reasonable complexities compared to existing approaches.\n",
      "\n",
      "9. id: 5390b52620f70186a0f04144   score: 0.90414387   abstract: Localization of a wireless device using the time-of-arrivals (TOAs) from different base stations has been studied extensively in the literature. Numerous localization algorithms with different accuracies, computational complexities, a-priori knowledge requirements, and different levels of robustness against non-line-of-sight (NLOS) bias effects also have been reported. However, to our best knowledge, a detailed unified survey of different localization and NLOS mitigation algorithms is not available in the literature. This paper aims to give a comprehensive review of these different TOA-based localization algorithms and their technical challenges, and to point out possible future research directions. Firstly, fundamental lower bounds and some practical estimators that achieve close to these bounds are summarized for line-of-sight (LOS) scenarios. Then, after giving the fundamental lower b\n",
      "\n",
      "10. id: 5390a55520f70186a0e7b378   score: 0.9003547   abstract: The range-based localization problem often arises in TOA or RSSI based position estimation schemes. It is well-known that such a localization problem can be formulated as a nonlinear least-squares (NLS) estimation problem. In this paper, we formulate the problem as a constrained optimization problem, which is equivalent to the general NLS problem. By using a greedy optimization strategy, we derive a simple iterative algorithm with closed-form expressions for the NLS localization, which can be implemented in a distributed way. Simulation results show that the localization performance of the proposed localization algorithm is very close to the Cramer-Rao lower bound.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696431\n",
      "index                                        559121400cf232eb904fae50\n",
      "title               Sperner's colorings, hypergraph labeling probl...\n",
      "authors                                Maryam Mirzakhani, Jan Vondrák\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558ce22c0cf2b0acc6503466;5390981d20f70186a0e05...\n",
      "abstract            We prove three results about colorings of the ...\n",
      "id                                                            1696431\n",
      "clustered_labels                                                    2\n",
      "Name: 1696431, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b3044612c41e6b9d4627d   score: 0.77713084   abstract: We prove improved inapproximability results for hypergraph coloring using the low-degree polynomial code (aka, the\\\"short code\\\" of Barak et. al. [FOCS 2012]) and the techniques proposed by Dinur and Guruswami [FOCS 2013] to incorporate this code for inapproximability results. In particular, we prove quasi-NP-hardness of the following problems on n-vertex hypergraphs: •Coloring a 2-colorable 8-uniform hypergraph with [EQUATION] colors. •Coloring a 4-colorable 4-uniform hypergraph with [EQUATION] colors. •Coloring a 3-colorable 3-uniform hypergraph with [EQUATION] colors. In each of these cases, the hardness results obtained are (at least) superpolynomially stronger (if not exponentially stronger as in the third case) than what was previously known for the respective cases. In fact, prior to this result, (log n)O(1) colors was the strongest quantitative bound on the number of colors ruled\n",
      "\n",
      "2. id: 5390990f20f70186a0e10a11   score: 0.6834195   abstract: The UNIQUE GAMES problem is the following: we are given a graph G = (V, E), with each edge e = (u, v) having a weight we and a permutation πuv on [k]. The objective is to find a labeling of each vertex u with a label fu ∈ [k] to minimize the weight of unsatisfied edges---where an edge (u, v) is satisfied if fv = πuv(fu).The Unique Games Conjecture of Khot [8] essentially says that for each ε 0, there is a k such that it is NP-hard to distinguish instances of Unique games with (1-ε) satisfiable edges from those with only ε satisfiable edges. Several hardness results have recently been proved based on this assumption, including optimal ones for Max-Cut, Vertex-Cover and other problems, making it an important challenge to prove or refute the conjecture.In this paper, we give an O(log n)-approximation algorithm for the problem of minimizing the number of unsatisfied edges in any Unique game.\n",
      "\n",
      "3. id: 5390962020f70186a0df50d8   score: 0.61201984   abstract: Certain NP-hard problems like Clique and MAX-3SAT have resisted all attempts to find non-trivial approximation. Is there any inherent reason for the apparent inapproximability of these problems? The discovery of PCP Theorem and subsequent research have shown that for Clique and MAX-3SAT, any non-trivial approximation is as hard as finding the exact solution! In this work, we continue this line of research and show inapproximability of many fundamental NP-hard problems. These include (Hyper-)Graph Coloring, Shortest Vector Problem (SVP) in lattices, Hypergraph Vertex Cover, Clique and Chromatic Number of graphs, and some results based on the Unique Games Conjecture (UGC) that we propose. Specifically, we show that: (Hyper-)Graph Coloring: It is hard to color (i) k-colorable graphs with kΩ(log k ) colors, (ii) 3-colorable 3-uniform hypergraphs with (log log n)Ω(1) colors, and (iii) k-color\n",
      "\n",
      "4. id: 53908d6520f70186a0dd090b   score: 0.5283357   abstract: We consider an algorithmic problem of coloring r-uniform hypergraphs. The problem of finding the exact value of the chromatic number of a hypergraph is known to be NP$hard, so we discuss approximate solutions to it. Using a simple construction and known results on hardness of graph coloring, we show that for any r\\\n",
      "\n",
      "5. id: 5390b3ae20f70186a0ef321f   score: 0.50123787   abstract: We prove almost tight hardness results under randomized reductions for finding independent sets in bounded degree graphs and hypergraphs that admit a good coloring. Our specific results include the following (where Δ, a constant, is a bound on the degree, and n is the number of vertices): • NP-hardness of finding an independent set of size larger than O (n[EQUATION]) in a 2-colorable r-uniform hypergraph for each fixed r ≥ 4. A simple algorithm is known to find independent sets of size Ω ([EQUATION]) in any r-uniform hypergraph of maximum degree Δ. Under a combinatorial conjecture on hypergraphs, the (log Δ)1/(r−1) factor in our result is necessary. • Conditional hardness of finding an independent set with more than O ([EQUATION]) vertices in a k-colorable (with k ≥ 7) graph for some absolute constant c ≤ 4, under Khot's 2-to-1 Conjecture. This suggests the near-optimality of Karger, Mot\n",
      "\n",
      "6. id: 539095ba20f70186a0df0eb7   score: 0.4851271   abstract: We consider an algorithmic problem of coloring r-uniform hypergraphs. The problem of finding the exact value of the chromatic number of a hypergraph is known to be NP-hard, so we discuss approximate solutions to it. Using a simple construction and known results on hardness of graph coloring, we show that for any r ≥ 3 it is impossible to approximate in polynomial time the chromatic number of r-uniform hypergraphs on n vertices within a factor n1-ε for any ε 0, unless NP ⊆ ZPP. On the positive side, improving a result of Hofmeister and Lefmann, we present an approximation algorithm for coloring r-uniform hypergraphs on n vertices, whose performance ratio is O(n(log logn)2/(logn)2).\n",
      "\n",
      "7. id: 53908b1820f70186a0db4fa2   score: 0.46468902   abstract: We consider an algorithmic problem of coloring r-uniform hypergraphs. The problem of finding the exact value of the chromatic number of a hypergraph is known to be NP-hard, so we discuss approximate solutions to it. Using a simple construction and known results on hardness of graph coloring, we show that for any r ≥ 3 it is impossible to approximate in polynomial time the chromatic number of r-uniform hypergraphs on n vertices within a factor n1-Ɛ for any Ɛ 0, unless NP ⊆ ZPP. On the positive side, we present an approximation algorithm for coloring r-uniform hypergraphs on n vertices, whose performance ratio is O(n(log log n)2=(log n)2). We also describe an algorithm for coloring 3-uniform 2-colorable hypergraphs on n vertices in Õ (n9/41) colors, thus improving previous results of Chen and Frieze and of Kelsen, Mahajan and Ramesh.\n",
      "\n",
      "8. id: 5390a63c20f70186a0e82b1b   score: 0.45181018   abstract: We study the maximization version of the fundamental graph coloring problem. Here the goal is to color the vertices of a k -colorable graph with k colors so that a maximum fraction of edges are properly colored (i.e. their endpoints receive different colors). A random k -coloring properly colors an expected fraction $1-\\frac{1}{k}$ of edges. We prove that given a graph promised to be k -colorable, it is NP-hard to find a k -coloring that properly colors more than a fraction of edges. Previously, only a hardness factor of $1- O\\bigl(\\frac{1}{k^2}\\bigr)$ was known. Our result pins down the correct asymptotic dependence of the approximation factor on k . Along the way, we prove that approximating the Maximum 3-colorable subgraph problem within a factor greater than $\\frac{32}{33}$ is NP-hard. Using semidefinite programming, it is known that one can do better than a random coloring and prope\n",
      "\n",
      "9. id: 53908b4920f70186a0dbb068   score: 0.4500875   abstract: A mixed hypergraph is a pair H = (V, E ∪ A), where V is the vertex set and E (A) the edge (the co-edge) set of H. A legal colouring of H gives the same (different) colour(s) to at least two vertices of any co-edge (of any edge). The upper chromatic number of H is the maximum number X(H) of colours that can be used in a legal colouring. After giving a general upper bound to X(H), we here consider the co-hypergraph J = (X,φ ∪ I), where I is a (v3,b2)- configuration I over X. We prove that computing X(J) is NP-hard, but there exists a polynomial-time algorithm returning a colouring with ≥ 5/6 X(J) colours. We also provide an example showing that this approximation factor is tight.\n",
      "\n",
      "10. id: 558ce188612c5d355d05256a   score: 0.43314204   abstract: We consider vertex colorings of hypergraphs in which lower and upper bounds are prescribed for the largest cardinality of a monochromatic subset and/or of a polychromatic subset in each edge. One of the results states that for any integers s=2 and a=2 there exists an integer f(s,a) with the following property. If an interval hypergraph admits some coloring such that in each edge E\\\"i at least a prescribed number s\\\"i@?s of colors occur and also each E\\\"i contains a monochromatic subset with a prescribed number a\\\"i@?a of vertices, then a coloring with these properties exists with at most f(s,a) colors. Further results deal with estimates on the minimum and maximum possible numbers of colors and the time complexity of determining those numbers or testing colorability, for various combinations of the four color bounds prescribed. Many interesting problems remain open.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1690332\n",
      "index                                        5592525c0cf28b1a968ffb39\n",
      "title               A High-Order Discontinuous Galerkin Discretiza...\n",
      "authors             Nils Gerhard, Francesca Iacono, Georg May, Sie...\n",
      "year                                                           2015.0\n",
      "venue                                 Journal of Scientific Computing\n",
      "references          539087ae20f70186a0d4e5d9;53908a4020f70186a0d9d...\n",
      "abstract            Multiresolution-based mesh adaptivity using bi...\n",
      "id                                                            1690332\n",
      "clustered_labels                                                    1\n",
      "Name: 1690332, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390975920f70186a0dfc9ff   score: 0.89143354   abstract: In this paper we present the main conceptual ingredients and the current state of development of the new solver QUADFLOW for large scale simulations of compressible fluid flow and fluid-structure interaction. In order to keep the size of the discrete problems at every stage as small as possible, we employ a multiresolution adaptation strategy that will be described in the first part of the paper. In the second part we outline a new mesh generation concept that is to support the adaptive concepts as well as possible. A key idea is to understand meshes as parametric mappings determined by possibly few control points as opposed to store each mesh cell separately. Finally, we present a finite volume discretization which again is to support the adaptation concepts. We conclude with numerical examples of realistic applications demonstrating different features of the solver.\n",
      "\n",
      "2. id: 53909f2d20f70186a0e39258   score: 0.7316342   abstract: This paper presents a mesh adaptation method for higher-order (p1) discontinuous Galerkin (DG) discretizations of the two-dimensional, compressible Navier-Stokes equations. A key feature of this method is a cut-cell meshing technique, in which the triangles are not required to conform to the boundary. This approach permits anisotropic adaptation without the difficulty of constructing meshes that conform to potentially complex geometries. A quadrature technique is proposed for accurately integrating on general cut cells. In addition, an output-based error estimator and adaptive method are presented, appropriately accounting for high-order solution spaces in optimizing local mesh anisotropy. Accuracy on cut-cell meshes is demonstrated by comparing solutions to those on standard, boundary-conforming meshes. Robustness of the cut-cell and adaptation technique is successfully tested for highl\n",
      "\n",
      "3. id: 53908bcc20f70186a0dc5fdc   score: 0.6649755   abstract: In this paper, a new method for multiresolution volume representation is presented. The method is based on wavelets and it can be used for representing volumetric data defined on non structured grids. The basic contribution is the extension of wavelets to volumetric domains of arbitrary topological type. This extension is made by constructing a wavelet basis defined on any tetrahedrized volume. This basis construction is achieved using multiresolution analysis and lifting scheme.\n",
      "\n",
      "4. id: 5390aca820f70186a0eb812f   score: 0.6102212   abstract: We present a space-time adaptive solver for single- and multi-phase compressible flows that couples average interpolating wavelets with high-order finite volume schemes. The solver introduces the concept of wavelet blocks, handles large jumps in resolution and employs local time-stepping for efficient time integration. We demonstrate that the inherently sequential wavelet-based adaptivity can be implemented efficiently in multicore computer architectures using task-based parallelism and introducing the concept of wavelet blocks. We validate our computational method on a number of benchmark problems and we present simulations of shock-bubble interaction at different Mach numbers, demonstrating the accuracy and computational performance of the method.\n",
      "\n",
      "5. id: 53909f6a20f70186a0e3b89b   score: 0.5265254   abstract: In this work, we consider second order elliptic problems arising in the modeling of single phase flows in porous media in 2D and in the analysis of transverse electromagnetic modes in wave guides using a discontinuous Galerkin (DG) method, the so-called Local Discontinuous Galerkin (LDG) method. We designed and developed an object oriented framework for performing DG computations on unstructured meshes that allows the use of arbitrary degree in the polynomial approximations and non conformal meshes with an arbitrary number of hanging nodes per edge. We present numerical studies of an automatic mesh adaptation technique and a semi-algebraic multilevel preconditioner for the LDG method. DG methods may be viewed as high-order extensions of the classical finite volume method. Since no inter-element continuity is imposed, they can be defined on very general meshes, including non-conforming me\n",
      "\n",
      "6. id: 5390aefb20f70186a0ecc40d   score: 0.51508635   abstract: Shock waves and contact discontinuities usually appear in compressible flows, requiring a fine mesh in order to achieve an acceptable accuracy of the numerical solution. The usage of a mesh adaptation strategy is convenient as uniform refinement of the whole mesh becomes prohibitive in three-dimensional (3D) problems. An unsteady h-adaptive strategy for unstructured finite element meshes is introduced. Non-conformity of the refined mesh and a bounded decrease in the geometrical quality of the elements are some features of the refinement algorithm. A 3D extension of the well-known refinement constraint for 2D meshes is used to enforce a smooth size transition among neighbour elements with different levels of refinement. A density-based gradient indicator is used to track discontinuities. The solution procedure is partially parallelised, i.e. the inviscid flow equations are solved in paral\n",
      "\n",
      "7. id: 53909fca20f70186a0e45a24   score: 0.4941409   abstract: In this paper, high-resolution finite volume schemes are combined with an adaptive mesh technique inspired by multiresolution analysis to improve the computational efficiency for two-dimensional hyperbolic conservation laws. The method is conservative. Moreover, it is stable which is proven numerically in this paper. The computational grid is dynamically adapted so that higher spatial resolution is automatically allocated to regions where strong gradients are observed. Using this proposed scheme, we compute several two-dimensional model problems and a compressive rate ranging from about 5-10 is observed in all simulations.\n",
      "\n",
      "8. id: 5390a4cc20f70186a0e73ef2   score: 0.41851866   abstract: Adaptive strategies in space and time allow considerable speed-up of finite volume schemes for conservation laws, while controlling the accuracy of the discretization. In this paper, a multiresolution technique for finite volume schemes with explicit time discretization is presented. An adaptive grid is introduced by suitable thresholding of the wavelet coefficients, which maintains the accuracy of the finite volume scheme of the regular grid. Further speed-up is obtained by local scale-dependent time stepping, i.e., on large scales larger time steps can be used without violating the stability condition of the explicit scheme. Furthermore, an estimation of the truncation error in time, using embedded Runge-Kutta type schemes, guarantees a control of the time step for a given precision. The accuracy and efficiency of the fully adaptive method is illustrated with applications for compressi\n",
      "\n",
      "9. id: 5390a05a20f70186a0e4b196   score: 0.39676917   abstract: An adaptive ghost fluid finite volume method is developed for one- and two-dimensional compressible multi-medium flows in this work. It couples the real ghost fluid method (GFM) [C.W. Wang, T.G. Liu, B.C. Khoo, A real-ghost fluid method for the simulation of multi-medium compressible flow, SIAM J. Sci. Comput. 28 (2006) 278-302] and the adaptive moving mesh method [H.Z. Tang, T. Tang. Moving mesh methods for one- and two-dimensional hyperbolic conservation laws, SIAM J. Numer. Anal. 41 (2003) 487-515; H.Z. Tang, T. Tang, P.W. Zhang, An adaptive mesh redistribution method for non-linear Hamilton-Jacobi equations in two- and three-dimensions, J. Comput. Phys. 188 (2003) 543-572], and thus combines their advantages. This work shows that the local mesh clustering in the vicinity of the material interface can effectively reduce both numerical and conservative errors caused by the GFM around t\n",
      "\n",
      "10. id: 5390af8920f70186a0ecfd34   score: 0.38421875   abstract: We use a multiwavelet basis with the Discontinuous Galerkin (DG) method to produce a multi-scale DG method. We apply this Multiwavelet DG method to convection and convection-diffusion problems in multiple dimensions. Merging the DG method with multiwavelets allows the adaptivity in the DG method to be resolved through manipulation of multiwavelet coefficients rather than grid manipulation. Additionally, the Multiwavelet DG method is tested on non-linear equations in one dimension and on the cubed sphere.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1695025\n",
      "index                                        559249940cf28b1a968ff6fe\n",
      "title               On team formation with expertise query in coll...\n",
      "authors                       Cheng-Te Li, Man-Kwan Shan, Shou-De Lin\n",
      "year                                                           2015.0\n",
      "venue                               Knowledge and Information Systems\n",
      "references          5390aca820f70186a0eb809b;5390ad0620f70186a0eba...\n",
      "abstract            Given a collaborative social network and a tas...\n",
      "id                                                            1695025\n",
      "clustered_labels                                                    0\n",
      "Name: 1695025, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ada620f70186a0ec2288   score: 0.9925103   abstract: Given an expertise social network and a task consisting of a set of required skills, the team formation problem aims at finding a team of experts who not only satisfy the requirements of the given task but also communicate to one another in an effective manner. To solve this problem, Lappas et al. has proposed the Enhance Steiner algorithm. In this work, we generalize this problem by associating each required skill with a specific number of experts. We propose three approaches to form an effective team for the generalized task. First, we extend the Enhanced-Steiner algorithm to a generalized version for generalized tasks. Second, we devise a density-based measure to improve the effectiveness of the team. Third, we present a novel grouping-based method that condenses the expertise information to a group graph according to required skills. This group graph not only drastically reduces the \n",
      "\n",
      "2. id: 5390b20120f70186a0ee3f65   score: 0.98282325   abstract: We study the problem of discovering a team of experts from a social network. Given a project whose completion requires a set of skills, our goal is to find a set of experts that together have all of the required skills and also have the minimal communication cost among them. We propose two communication cost functions designed for two types of communication structures. We show that the problem of finding the team of experts that minimizes one of the proposed cost functions is NP-hard. Thus, an approximation algorithm with an approximation ratio of two is designed. We introduce the problem of finding a team of experts with a leader. The leader is responsible for monitoring and coordinating the project, and thus a different communication cost function is used in this problem. To solve this problem, an exact polynomial algorithm is proposed. We show that the total number of teams may be exp\n",
      "\n",
      "3. id: 5390a4cc20f70186a0e75017   score: 0.9798227   abstract: Given a task T, a pool of individuals X with different skills, and a social network G that captures the compatibility among these individuals, we study the problem of finding X, a subset of X, to perform the task. We call this the TEAM FORMATION problem. We require that members of X' not only meet the skill requirements of the task, but can also work effectively together as a team. We measure effectiveness using the communication cost incurred by the subgraph in G that only involves X'. We study two variants of the problem for two different communication-cost functions, and show that both variants are NP-hard. We explore their connections with existing combinatorial problems and give novel algorithms for their solution. To the best of our knowledge, this is the first work to consider the TEAM FORMATION problem in the presence of a social network of individuals. Experiments on the DBLP da\n",
      "\n",
      "4. id: 5390baa120f70186a0f39c5f   score: 0.9781772   abstract: A lot of research in social networks has been devoted in the recommendation of individuals. Most of the work has focused on finding appropriate people one in question, regarding the input social graph and their attributes. However, for many applications one is interested in finding a team of experts for a given query. For a given social graph and a task including a set of required skills, we study the problem of finding a team of experts that their expertises match the given task and the relationships among them represent how well team members work together. Our proposed framework, named Team Finder, first provide a grouping method to aggregate the set of experts that are strongly correlated based on their skills as well as the best connection among them. By considering the groups, search space is significantly reduced and moreover it causes to prevent from the growth of redundant commun\n",
      "\n",
      "5. id: 558bd3050cf23f2dfc593c8f   score: 0.97389406   abstract: A tool for efficient team formation based on students' proficiency and their communication in social networks is demonstrated. An experiment was conducted using two separate group discovering mechanisms with a pool of 68 students in two subjects. The proposed tool was used to discover groups in the experimental group with 31 students who followed the subject Object Oriented Analysis and Design and the grouping mechanism available in Moodle was used in the control group of 37 students who followed the subject Project Management during four weeks in the Spring semester of the 2013/ 2014 academic year. The study was carried out in accordance with a quasi-experimental research with a pretest and a post test design. The result indicates that small grouping based on the algorithm has better outcome.\n",
      "\n",
      "6. id: 5390b2fc20f70186a0eefee7   score: 0.9721515   abstract: A system for efficient team formation in social networks is demonstrated. Given a project whose completion requires a set of skills, our system finds a set of experts that together have all of the required skills and also have the minimal communication cost. The system finds the best teams with or without a leader using two types of communication structures. After discovering the teams of experts, our system can display the relationships among the experts in a team by showing how the experts are connected in the social network. Since the total number of teams might be exponential with respect to the number of required skills, procedures that produce top-k teams of experts with or without a leader in polynomial delay are implemented. The system is demonstrated using the well-known DBLP dataset.\n",
      "\n",
      "7. id: 5390bb1d20f70186a0f3d399   score: 0.966028   abstract: Given a task T, a set of experts V with multiple skills and a social network G(V, W) reflecting the compatibility among the experts, team formation is the problem of identifying a team C ? V that is both competent in performing the task T and compatible in working together. Existing methods for this problem make too restrictive assumptions and thus cannot model practical scenarios. The goal of this paper is to consider the team formation problem in a realistic setting and present a novel formulation based on densest subgraphs. Our formulation allows modeling of many natural requirements such as (i) inclusion of a designated team leader and/or a group of given experts, (ii) restriction of the size or more generally cost of the team (iii) enforcing locality of the team, e.g., in a geographical sense or social sense, etc. The proposed formulation leads to a generalized version of the classi\n",
      "\n",
      "8. id: 5390b2fc20f70186a0eefe5c   score: 0.96028674   abstract: In this paper, a novel approach for forming an effective expert team in social network is proposed by using of skill grading and minimum communication cost. In this work, first a framework is recommended to determine skill level of experts based on their skill abilities and their collaboration among expert neighbors. Second, the existing diameter algorithm is extended to its more generalized version. Third, the communication cost is customized by presenting the cost metric containing both distance and skill grade of individuals. The experimental results show the advantages of this method which using DBLP co-authorship graph through an effective and efficient expert team in practice.\n",
      "\n",
      "9. id: 558b275d612c41e6b9d450e5   score: 0.9570375   abstract: Solving today's problems demands more than the effort of an individual, however, brilliant mind. Collaboration and team work are fundamental skills for tackling such problems. The ability of team members to work together and communicate with one another thus becomes an uppermost concern. In this context, to assemble an effective team requires an approach that goes beyond the analysis of individual skills. This paper proposes and examines the problem that takes into account different skill attributes and social ties to build an interconnected team. Our proposed solution is evaluated by means of building one team to defeat an opposite team defined in the same social network. Our experimental results show that our algorithms produces meaningful socially collaborative skilled teams.\n",
      "\n",
      "10. id: 558af3cb612c41e6b9d3e69b   score: 0.95589906   abstract: In this paper, we study the problem of finding teams of experts from an expert network while optimizing three objectives. Given a project, the objective is to find teams of experts that cover all the required skills and also optimize the communication cost as well as the personnel cost and the expertise level of the team members. The expert network is modeled as a graph, where nodes represent experts and edges between nodes specify the communication costs between the experts. In this paper, we are interested in finding a Pareto front of teams that not only cover the required skills but are also not dominated by other feasible teams with respect to the three criteria. Since the problem is NP-hard, we propose algorithms to use with a two-phase method to find an approximation of the Pareto front for the three criteria team formation problem. In the first phase, an initial population which i\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696212\n",
      "index                                        559120250cf232eb904faddb\n",
      "title               When is Too Few Too Bad: How Many Participants...\n",
      "authors                                                Khai N. Truong\n",
      "year                                                           2015.0\n",
      "venue                  GetMobile: Mobile Computing and Communications\n",
      "references          5390ad0620f70186a0eba066;5390bed320f70186a0f4e...\n",
      "abstract            Evaluating a system with users is an important...\n",
      "id                                                            1696212\n",
      "clustered_labels                                                    3\n",
      "Name: 1696212, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b56a20f70186a0f06c57   score: 0.98282325   abstract: System evaluators face several challenges in designing evaluation methods, including measurement and relevance, context, establishing common ground with users, and eliciting users' tacit knowledge. To address these challenges, we propose applying participatory design theory to designing evaluation methods by increasing user involvement and by integrating this process into the overall process of system design.\n",
      "\n",
      "2. id: 5390a30b20f70186a0e694d7   score: 0.93593454   abstract: The importance of Interaction Design (IxD) has been on the increase for the last couple of years. The success of Ipod and failure of Jukebox justify the significance of the discipline. In this paper we present a generic methodology to evaluate any interactive system. Eight factors form the basis of the evaluation process. These factors include Learnability, Usage, Error and Feedback, Comfort, Collaboration, Affect, Guidance and Support and Accessibility. Each factor is measured independently and the resulting numbers contribute to the overall evaluation of the interactive system under observation. A novel case study to evaluate the use of digital pen and paper in a real life scenario is also presented in the paper.\n",
      "\n",
      "3. id: 5390baa120f70186a0f38d39   score: 0.9281487   abstract: Evaluation has been a dominant theme in HCI for decades, but it is far from being a solved problem. As interactive systems and their uses change, the nature of evaluation must change as well. In this paper, we outline the challenges our community needs to address to develop adequate methods for evaluating systems in modern (and future) use contexts. We begin by tracing how evaluation efforts have been shaped by a continuous adaptation to technological and cultural changes and conclude by discussing important research directions that will shape evaluation's future.\n",
      "\n",
      "4. id: 5390b29820f70186a0eea4b0   score: 0.90632194   abstract: This paper presents an evaluation method, along with the underlying theory, for assessing interactive systems and specifying their quality in terms of universal access. The method is an adaptation of traditional walkthroughs and is aimed to incorporate user diversity, for example in terms of individual abilities, skills, background, levels of expertise, equipment used, etc., as key input to evaluation. The method aims at addressing as many as possible of the qualities of a system that might affect diverse users throughout their usage of the system and which, ultimately, have an impact on the system’s wide acceptance. The proposed method, described here, extends the cognitive walkthrough method by introducing a simulation of the users’ reasoned action process in order to assess whether users can, and will be, in favour of accessing, exploring, utilising and, ultimately, adopting a system.\n",
      "\n",
      "5. id: 5390b60d20f70186a0f12525   score: 0.840259   abstract: Despite the emphasis on the importance of evaluation and the plethora of recommended methods, evaluation continues to be anad hoc rather than a systematic practice. Recommendations for evaluation methodologies have not accomodated the evolving perceptions of the nature and the value of information systems. In this paper we argue that current professional evaluation practices are particularly inadequate for systems that have strategic issues at stake and approach to determine the value of proposed or implemented information systems by means of informed group consultation and negotiation processes. We then propose two possible ways to implement the suggested approach.\n",
      "\n",
      "6. id: 5390b0ca20f70186a0edaf6e   score: 0.8296301   abstract: In this paper, we propose a method for the analysis and the interpretation of the interactions between the user and the interactive system to evaluate. The proposed approach is based on the comparison between the user interaction sequences and the sequences average. This confrontation concerns the task execution duration, the realized tasks number and the action sequence used in a defined experimental scenario.\n",
      "\n",
      "7. id: 53909f8220f70186a0e3c947   score: 0.7970834   abstract: Collaborative systems evaluation is still an open issue for researchers working on the fields of HCI and CSCW. In this paper we present the challenges experienced by evaluators in a real system evaluation. These challenges and the discussion about them contribute to the research in the field, as well as to the planning of evaluations involving users for collaborative systems.\n",
      "\n",
      "8. id: 5390a05920f70186a0e499f7   score: 0.78431904   abstract: There is a wealth of user-centred evaluation methods (UEMs) to support the analyst in assessing interactive systems. Many of these support detailed aspects of use-for example: is the feedback helpful? Are labels appropriate? Is the task structure optimal? Few UEMs encourage the analyst to step back and consider how well a system supports users' conceptual understandings and system utility. In this paper, we present CASSM, a method, which focuses on the quality of 'fit' between users and an interactive system. We describe the methodology of conducting a CASSM analysis and illustrate the approach with three contrasting worked examples (a robotic arm, a digital library system and a drawing tool) that demonstrate different depths of analysis. We show how CASSM can help identify re-design possibilities to improve system utility. CASSM complements established evaluation methods by focusing on \n",
      "\n",
      "9. id: 5390a74f20f70186a0e8c78c   score: 0.75437254   abstract: Discussed are observations on the usage of an interactive computing system in a research environment. Empirical data on user behavior are discussed that concern the duration and frequency of terminal sessions, the use of language processors, user response ...\n",
      "\n",
      "10. id: 5390a63c20f70186a0e817ef   score: 0.7527403   abstract: The inclusion of participants that are representative of the diverse populations of users is essential for meaningful and useful evaluations of usability and accessibility on the web. This paper proposes the requirements and architecture for an automated tool suite to help manage the design and deployment of evaluations to these participants. A prototype implementation of this architecture that is being prepared is also discussed.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691332\n",
      "index                                        559255960cf2aff368683bf9\n",
      "title               NestDE: generic parameters tuning for automati...\n",
      "authors                   Wei Feng, Xuefei Yin, Yifeng Zhang, Lei Xie\n",
      "year                                                           2015.0\n",
      "venue               Soft Computing - A Fusion of Foundations, Meth...\n",
      "references          5390b56a20f70186a0f0658b;5390b56a20f70186a0f06...\n",
      "abstract            Parameters tuning is a crucial task in automat...\n",
      "id                                                            1691332\n",
      "clustered_labels                                                    2\n",
      "Name: 1691332, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b68720f70186a0f1d1d0   score: 0.86726075   abstract: We propose Laplacian Eigenmaps (LE)-based approaches to automatic story segmentation on speech recognition transcripts of broadcast news. We reinforce story boundaries by applying LE analysis to sentence connective strength matrix and reveal the intrinsic geometric structure of stories. Specifically, we construct a Euclidean space in which each sentence is mapped to a vector. As a result, the original inter-sentence connective strength is reflected by the Euclidean distances between the corresponding vectors and cohesive relations between sentences become geometrically evident. Taking advantage of LE, we present three story segmentation approaches: LE-TextTiling, spectral clustering and LE-DP. In LE-DP, we formalize story segmentation as a straightforward criterion minimization problem and give a fast dynamic programming solution to it. Extensive story segmentation experiments on three c\n",
      "\n",
      "2. id: 53908b4920f70186a0dbbd72   score: 0.83318895   abstract: This chapter presents the system used by the Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts for its participation in four of the five TDT tasks: tracking, detection, first story detection, and story link detection. For each task, we discuss the parameter setting approach that we used and the results of our system on the test data.For the task of link detection, we look more carefully at score normalization across different languages and media types. We find that we can improve results noticeably though not substantially by normalizing scores differently depending upon the source language. We also consider smoothing the vocabulary in stories using a \"query expansion\" technique from Information Retrieval to add additional words from the corpus to each story. This results in substantial improvements.In addition, we use TDT evaluation approaches to sho\n",
      "\n",
      "3. id: 5390a63c20f70186a0e82c76   score: 0.6808786   abstract: This paper describes our on-going work toward the improvement of Broadcast News story segmentation module. We have tried to improve our baseline algorithm by further exploring the typical structure of a broadcast news show, first by training a CART and then by integrating it in a 2-stage algorithm that is able to deal with shows with double anchors. In order to deal with shows with a thematic anchor, a more complex approach is adopted including a topic classification stage. The automatic segmentation is currently being compared with the manual segmentation done by a professional media watch company. The results are very promising so far, specially taking into account that no video information is used.\n",
      "\n",
      "4. id: 5390962020f70186a0df5588   score: 0.6371437   abstract: This paper describes the design and application of time-enhanced, finite state models of discourse cues to the automated segmentation of broadcast news, We describe our analysis of a broadcast news corpus, the design of a discourse cue based story segmentor that builds upon information extraction techniques, and finally its computational implementation and evaluation in the Broadcast News Navigator (BNN) to support video news browsing, retrieval, and summarization.\n",
      "\n",
      "5. id: 53909f2d20f70186a0e38bfb   score: 0.6006569   abstract: The paper describes a maximum entropy based story segmentation system for Arabic, Chinese and English. In experiments with broadcast news data from TDT-3, TDT-4, and corpora collected in the DARPA GALE project we obtain a substantial performance gain using multiple overlapping windows for text-based features.\n",
      "\n",
      "6. id: 5539291a0cf26c551af542fb   score: 0.56487876   abstract: The paper proposes a modified version of Differential Evolution (DE) algorithm and optimization criterion function for extractive text summarization applications. Cosine Similarity measure has been used to cluster similar sentences based on a proposed criterion function designed for the text summarization problem, and important sentences from each cluster are selected to generate a summary of the document. The modified Differential Evolution model ensures integer state values and hence expedites the optimization as compared to conventional DE approach. Experiments showed a 95.5% improvement in time in the Discrete DE approach over the conventional DE approach, while the precision and recall of extracted summaries remained comparable in all cases.\n",
      "\n",
      "7. id: 5390a55520f70186a0e79ff4   score: 0.5575442   abstract: In this paper, we describe an approach to segmenting news video based on the perceived shift in content using features spanning multiple modalities.We investigate a number of multimedia features, which serve as potential indicators of a change in story in order to determine which are the most effective. The efficacy of our approach is demonstrated by the performance of our prototype, where a number of feature combinations demonstrate an up to 18% improvement in Window Diff score above that of other state of the art story segmenters. In our investigation, there was no, one, clearly superior feature, rather the best segmentation results occurred when there was synergy between multiple features.\n",
      "\n",
      "8. id: 53908b4920f70186a0dbbd6d   score: 0.54386276   abstract: This chapter reports on CMU's work in all the five TDT-1999 tasks, including segmentation (story boundary identification), topic tracking, topic detection, first story detection, and story-link detection. We have addressed these tasks as supervised or unsupervised classification problems, and applied a variety of statistical learning algorithms to each problem for comparison. For segmentation we used exponential language models and decision trees; for topic tracking we used primarily k-nearest-neighbors classification (also language models, decision trees and a variant of the Rocchio approach); for topic detection we used a combination of incremental clustering and agglomerative hierarchical clustering, and for first story detection and story link detection we used a cosine-similarity based measure. We also studied the effect of combining the output of alternative methods for producing j\n",
      "\n",
      "9. id: 53908b6c20f70186a0dbf6a2   score: 0.53731453   abstract: Based on a simple temporal structural model of news program, this paper presents a practical solution to automatic news story segmentation by integrating syntactic and semantic methods. First, a syntactic segmentation method is used to detect the shot boundaries in order to partition video frames into video shots. Then a semantic segmentation method based on the graph-theoretical cluster analysis is developed to classify the video shots into anchorpersonshots and news footage shots. Finally, a structural model of news video is used to complete the news-story segmentation. The proposed method obtains a precision of 90.45% and a recall of 95.83%in the segmentation experiment of 168 news stories from two Hong Kong news stations.\n",
      "\n",
      "10. id: 5390a63c20f70186a0e81661   score: 0.51182336   abstract: In this paper we examine topic segmentation of narrative documents, which are characterized by long passages of text with few headings. We first present results suggesting that previous topic segmentation approaches are not appropriate for narrative text. We then present a feature-based method that combines features from diverse sources as well as learned features. Applied to narrative books and encyclopedia articles, our method shows results that are significantly better than previous segmentation approaches. An analysis of individual features is also provided and the benefit of generalization using outside resources is shown.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1724571\n",
      "index                                        55323d0f45cec66b6f9dd6e0\n",
      "title               A novel method for vegetation encroachment mon...\n",
      "authors             Junaid Ahmad, Aamir Saeed Malik, Mohd Faris Ab...\n",
      "year                                                           2015.0\n",
      "venue                                 Pattern Analysis & Applications\n",
      "references                                   558ffec5612c29c89cd7cca1\n",
      "abstract            The dangerous, overgrown vegetation/trees unde...\n",
      "id                                                            1724571\n",
      "clustered_labels                                                    2\n",
      "Name: 1724571, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a8b120f70186a0e9b3ec   score: 0.96028674   abstract: The traditional monitoring mode can't meet existing security requirements, so an effective monitoring method is needed to ensure that important equipments and peripheral situation in distribution network are visualization and control ability. This paper introduced a new system based on embedded system, video compression and 3G network, which can transport video image to remote monitoring center and PDA mobile phone users anytime to complete video capture and browsing in real-time. The prestored and real-time video can be transmitted mobile terminal to mobile terminal and mobile terminal to Internet.\n",
      "\n",
      "2. id: 5390b60d20f70186a0f11876   score: 0.93108755   abstract: In monitoring remote unmanned sites using a visual system, it is generally adequate to transmit low-resolution pictures. Also, it is not necessary to transmit every picture frame captured by the video camera. By reducing the bandwidth requirements of the image transmission, we can use the existing telephone network to transmit a frame within a time period which is acceptable for monitoring purposes.\n",
      "\n",
      "3. id: 5390a63c20f70186a0e8245b   score: 0.848847   abstract: Wireless networks have become an effective solution for remote surveillance and data gathering without an existing communication infrastructure. The poster presents a system for monitoring the high-voltage transmission lines. This system implemented a two-tier wireless network to gather the data for the high voltage transmission lines and transmit these data to the control center based on WiFi technology and Zigbee technology.\n",
      "\n",
      "4. id: 5390b19020f70186a0edf668   score: 0.7744129   abstract: Many devices are still unconnected. We apply mobile imaging to log measurements from personal health devices. Such devices sometimes offer wired and wireless links, but they suffer from many problems (setup, breaking connections, non-mobility). We propose to use instead ubiquitous mobile phone cameras to capture the measurements and store them for further viewing and follow-up. In this paper we discuss the principle, the prototype, the user study and initial conclusions of this approach.\n",
      "\n",
      "5. id: 5390a72220f70186a0e89025   score: 0.7439129   abstract: We will demonstrate a wireless sensor network system for the surveillance of critical areas and properties -- e.g. borders. The system consists of up to 10 sensor nodes that monitor a small border area. The protocols we show focus on detecting trespassers across a predefined area and reporting the detection to a gateway node securely. There, the trespasser's path will be graphically diplayed on a border map. The demonstration features secure protocols for the detection of trespassers, node failure and network partitioning, along with a duty cycle protocol to ensure network longevity. All information pertaining to relevant events in the network or border area will be graphically displayed on a gateway computer.\n",
      "\n",
      "6. id: 55323d0045cec66b6f9dd52a   score: 0.72894126   abstract: Highlights¿ An indoor virtual field server monitored Xanthoceras sorbifolia tree seedling growth. ¿ A camera model and reconstruction formula were developed for the virtual field server. ¿ Images from two cameras were used to calculate tree crown widths. The application of field servers is proving to be increasingly crucial to the process of remote monitoring. These devices are built to continuously obtain large amounts of environmental and meteorological data and, at the same time, transmit back a vast quantity of in situ imagery. The question of how to more effectively utilize these data must be answered. This paper discusses the reconstruction of spatial information, as well as the collection of this information through technical methods. These actions are performed using computer vision based on field server imagery. In order to test and verify the technical approaches involved, such\n",
      "\n",
      "7. id: 5390ba3820f70186a0f3753e   score: 0.71432924   abstract: First responder applications often require safety surveillance using wireless camera networks~\\cite{breadcrums}. To ensure visual sensing coverage, it is crucial to place optical sensor nodes at proper locations. Under the scenario of energy constrained wireless camera deployment, the issue of communication cost should also be considered. Previous camera deployment research (e.g Art Gallery Problem) mainly concerned sensing coverage. One well-known solution for the art gallery problem is to triangulate the objective polygon and then select vertices to ensure full coverage. However, deploying cameras only in the vertices of polygon may induce inefficiency both in number of necessary cameras and overall communication cost. To reduce the cost, we propose two deployment algorithms: 1) connected visibility region planning algorithm for static deployment given the floor plan is known, and 2) c\n",
      "\n",
      "8. id: 53908b2120f70186a0db5c4d   score: 0.6782204   abstract: This paper describes an ongoing collaborative research program between the Computer Science and the Forestry and Wildlife Management Departments at the University of Massachusetts to develop cost-effective methodologies for monitoring biomass and other environmental parameters over large areas. The data acquisition system consists of a differential GPS system, a 3-axis solid state inertial reference system, a small format (70mm) aerial survey camera, two video cameras, a laser profiling altimeter, and a PC based data recording system. Two aerial survey techniques for determining biomass are discussed. One primarily based on video and the other relying additionally on the 3D terrain models generated from the aerial photographs. In the first technique, transects are flown at 1,000 feet with dual-camera wide angle and zoom video, and a profiling laser operating at 238 Hz. The video coverage\n",
      "\n",
      "9. id: 539088b820f70186a0d8fc46   score: 0.6517983   abstract: From the Publisher:This edited book is directed primarily to the discussion of the most recent developments and on-going research related to all areas pertaining to plant surveillance and diagnosis. The secondary aim of this book is to identify the successful applications of already well-settled methodological tools in the field. It will highlight advantages of intelligent systems, AI techniques, and integration of soft computing tools and traditional tools, for a better service in all aspects related to power plant surveillance and diagnostics. It also reports recent research results and provides a state of the art on AI in power plant surveillance and diagnostics. The book especially focuses on theoretical and analytical solutions to the problems of real interest in AI techniques, possibly combined with other traditional computing tools.\n",
      "\n",
      "10. id: 5390ad8920f70186a0ec0d89   score: 0.648355   abstract: This paper briefly describes the characteristics of wireless sensor networks(WSN) and introduces it into the power grid. WSN can dynamically monitor malfunctions on power grid cables. This is the significance of the article. We make some improvements on the DV_Hop algorithm in WSN and propose a feasible method of failure localization which can be applied on power grid cables.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696453\n",
      "index                                        558e40bc0cf2af9ee80eb1b9\n",
      "title               Lopsidependency in the Moser-Tardos framework:...\n",
      "authors                                               David G. Harris\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558b0f27612c41e6b9d41c93;558b3d1d612c41e6b9d47...\n",
      "abstract            The Lopsided Lovász Local Lemma (LLLL) is a po...\n",
      "id                                                            1696453\n",
      "clustered_labels                                                    2\n",
      "Name: 1696453, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bed320f70186a0f4f431   score: 0.88602656   abstract: The resampling algorithm of Moser & Tardos is a powerful approach to develop versions of the Lovasz Local Lemma. We develop a partial resampling approach motivated by this methodology: when a bad event holds, we resample an appropriately-random subset of the set of variables that define this event, rather than the entire set as in Moser & Tardos. This leads to several improved algorithmic applications in scheduling, graph transversals, packet routing etc. For instance, we improve the approximation ratio of a generalized D-dimensional scheduling problem studied by Azar & Epstein from O(D) to O(log D / log log D), and settle a conjecture of Szabo & Tardos on graph transversals asymptotically.\n",
      "\n",
      "2. id: 558b0f27612c41e6b9d41c93   score: 0.88483775   abstract: While there has been significant progress on algorithmic aspects of the Lovász Local Lemma (LLL) in recent years, a noteworthy exception is when the LLL is used in the context of random permutations: the \\\"lopsided\\\" version of the LLL is usually at play here, and we do not yet have subexponential-time algorithms. We resolve this by developing a randomized polynomial-time algorithm for such applications. A noteworthy application is for Latin Transversals: the best-known general result here (Bissacot et al., improving on Erd&odblac;s and Spencer), states that any n x n matrix in which each entry appears at most (27/256)n times, has a Latin transversal. We present the first polynomial-time algorithm to construct such a transversal. Our approach also yields RNC algorithms: for Latin transversals, as well as the first efficient ones for the strong chromatic number and (special cases of) acyc\n",
      "\n",
      "3. id: 5390a55520f70186a0e79bae   score: 0.8818186   abstract: Let *** be a uniformly distributed random k -SAT formula with n variables and m clauses. We present a polynomial time algorithm that finds a satisfying assignment of *** with high probability for constraint densities $m/n , where *** k ***0. Previously no efficient algorithm was known to find solutions with non-vanishing probability beyond m /n = 1.817·2 k /k [Frieze and Suen, Journal of Algorithms 1996].\n",
      "\n",
      "4. id: 53908a4020f70186a0d9df17   score: 0.84606963   abstract: In his seminal result, Beck gave the first algorithmic version of the Lovász Local Lemma by giving polynomial time algorithms for 2-coloring and partitioning uniform hypergraphs. His work was later generalized by Alon, and Molloy and Reed. Recently, Czumaj and Scheideler gave an efficient algorithm for 2-coloring non-uniform hypergraphs. But the partitioning algorithm obtained based on their second paper only applies to a more limited range of hypergraphs, so much so that their work doesn't imply the result of Beck for the uniform case. Here we give an algorithmic version of the general form of the Local Lemma which captures (almost) all applications of the results of Beck and Czumaj and Scheideler, with an overall simpler proof. In particular, if H is a non-uniform hypergraph in which every edge ei intersects at most &verbar;ei&verbar;2ak other edges of size at most k, for some small co\n",
      "\n",
      "5. id: 5390980720f70186a0e02dcd   score: 0.84117436   abstract: In his seminal result, Beck gave the first algorithmic version of the Lovász Local Lemma by giving polynomial time algorithms for 2-coloring and partitioning uniform hypergraphs. His work was later generalized by Alon, and Molloy and Reed. Recently, Czumaj and Scheideler gave an efficient algorithm for 2-coloring nonuniform hypergraphs. But the partitioning algorithm obtained based on their second paper only applies to a more limited range of hypergraphs, so much so that their work doesn't imply the result of Beck for the uniform case. Here we give an algorithmic version of the general form of the Local Lemma which captures (almost) all applications of the results of Beck and Czumaj and Scheideler, with an overall simpler proof. In particular, if H is a nonuniform hypergraph in which every edge ei intersects at most |ei|2αk other edges of size at most k, for some small constant α, then w\n",
      "\n",
      "6. id: 5390a77d20f70186a0e8dc12   score: 0.7994423   abstract: The Lovász Local Lemma discovered by Erd&odblac;s and Lovász in 1975 is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. In 1991, József Beck was the first to demonstrate that a constructive variant can be given under certain more restrictive conditions, starting a whole line of research aimed at improving his algorithm's performance and relaxing its restrictions. In the present article, we improve upon recent findings so as to provide a method for making almost all known applications of the general Local Lemma algorithmic.\n",
      "\n",
      "7. id: 5390aefc20f70186a0ecd605   score: 0.7929457   abstract: Let $\\boldsymbol{\\Phi}$ be a uniformly distributed random $k$-SAT formula with $n$ variables and $m$ clauses. We present a polynomial time algorithm that finds a satisfying assignment of $\\boldsymbol{\\Phi}$ with high probability for constraint densities $m/nJ. Algorithms, 20 (1996), pp. 312-355].\n",
      "\n",
      "8. id: 5390893e20f70186a0d9356c   score: 0.7871142   abstract: The Lovász local lemma (LLL) is a powerful tool that is increasingly playing a valuable role in computer science. The original lemma was nonconstructive; a breakthrough of Beck and its generalizations (due to Alon and Molloy and Reed) have led to constructive versions. However, these methods do not capture some classes of applications of the LLL. We make progress on this by providing algorithmic approaches to two families of applications of the LLL. The first provides constructive versions of certain applications of an extension of the LLL (modeling, e.g., hypergraph-partitioning and low-congestion routing problems); the second provides new algorithmic results on constructing disjoint paths in graphs. Our results can also be seen as constructive upper bounds on the integrality gap of certain packing problems. One common theme of our work is a \"gradual rounding\" approach.\n",
      "\n",
      "9. id: 5390981d20f70186a0e066d7   score: 0.7831604   abstract: We apply techniques from the theory of approximation algorithms to the problem of deciding whether a random k-SAT formula is satisfiable. Let Form n,k,m denote a random k-SAT instance with n variables and m clauses. Using known approximation algorithms for MAX CUT or MIN BISECTION, we show how to certify that Formn,4,m is unsatisfiable efficiently, provided that m ≥ Cn2 for a sufficiently large constant C 0. In addition, we present an algorithm based on the Lovász υ' function that decides within polynomial expected time whether Formn,k,m is satisfiable, provided that k is even and m ≥ C ċ 4knk/2. Finally, we present an algorithm that approximates random MAX 2-SAT on input Formn,2,m within a factor of 1 - O(n/m)1/2 in expected polynomial time. for m ≥ Cn.\n",
      "\n",
      "10. id: 5390b1d220f70186a0ee0fa7   score: 0.78249633   abstract: The Lovász Local Lemma (LLL) is a powerful tool that gives sufficient conditions for avoiding all of a given set of “bad” events, with positive probability. A series of results have provided algorithms to efficiently construct structures whose existence is non-constructively guaranteed by the LLL, culminating in the recent breakthrough of Moser and Tardos [2010] for the full asymmetric LLL. We show that the output distribution of the Moser-Tardos algorithm well-approximates the conditional LLL-distribution, the distribution obtained by conditioning on all bad events being avoided. We show how a known bound on the probabilities of events in this distribution can be used for further probabilistic analysis and give new constructive and nonconstructive results. We also show that when a LLL application provides a small amount of slack, the number of resamplings of the Moser-Tardos algorithm i\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1700222\n",
      "index                                        55922f6f0cf2c3a0875c9f04\n",
      "title               Solving Hierarchical Stochastic Programs: Appl...\n",
      "authors                                                           NaN\n",
      "year                                                           2015.0\n",
      "venue                                    INFORMS Journal on Computing\n",
      "references          5390aa7620f70186a0eab3be;539089bb20f70186a0d97...\n",
      "abstract            <P>This paper presents a solution scheme for a...\n",
      "id                                                            1700222\n",
      "clustered_labels                                                    1\n",
      "Name: 1700222, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55323be045cec66b6f9dac07   score: 0.99909246   abstract: <P>This paper presents a solution scheme for a class of multistage stochastic programs possibly mixed-integer at all stages in which a hierarchy of decisions emerges. A special structure, common to many strategic problems affected by uncertainty, allows decomposing the problem into a master problem and many independent linear programming subproblems, facilitating the isolation and reduction of the complicating mixed-integer component of the problem. Specialized possibly heuristic procedures can be used for solving the master problem while subproblems can be efficiently solved to optimality. We adapt and test the decomposition scheme for a case of the maritime fleet renewal problem, whose real life instances cannot be solved by means of commercial off-the-shelf solvers.</P>\n",
      "\n",
      "2. id: 55323be045cec66b6f9dac1c   score: 0.99908185   abstract: <P>This paper presents a solution scheme for a class of multistage stochastic programs possibly mixed-integer at all stages in which a hierarchy of decisions emerges. A special structure, common to many strategic problems affected by uncertainty, allows decomposing the problem into a master problem and many independent linear programming subproblems, facilitating the isolation and reduction of the complicating mixed-integer component of the problem. Specialized possibly heuristic procedures can be used for solving the master problem while subproblems can be efficiently solved to optimality. We adapt and test the decomposition scheme for a case of the maritime fleet renewal problem, whose real life instances cannot be solved by means of commercial off-the-shelf solvers.</P>\n",
      "\n",
      "3. id: 558b2aee612c41e6b9d456e3   score: 0.9986223   abstract: <P>This paper presents a solution scheme for a class of multistage stochastic programs (possibly mixed-integer at all stages) in which a hierarchy of decisions emerges. A special structure, common to many strategic problems affected by uncertainty, allows decomposing the problem into a master problem and many independent linear programming subproblems, facilitating the isolation and reduction of the complicating mixed-integer component of the problem. Specialized (possibly heuristic) procedures can be used for solving the master problem while subproblems can be efficiently solved to optimality. We adapt and test the decomposition scheme for a case of the maritime fleet renewal problem, whose real life instances cannot be solved by means of commercial off-the-shelf solvers.</P>\n",
      "\n",
      "4. id: 55922f970cf2c3a0875c9f26   score: 0.9986223   abstract: <P>This paper presents a solution scheme for a class of multistage stochastic programs (possibly mixed-integer at all stages) in which a hierarchy of decisions emerges. A special structure, common to many strategic problems affected by uncertainty, allows decomposing the problem into a master problem and many independent linear programming subproblems, facilitating the isolation and reduction of the complicating mixed-integer component of the problem. Specialized (possibly heuristic) procedures can be used for solving the master problem while subproblems can be efficiently solved to optimality. We adapt and test the decomposition scheme for a case of the maritime fleet renewal problem, whose real life instances cannot be solved by means of commercial off-the-shelf solvers.</P>\n",
      "\n",
      "5. id: 55924e3d0cf26384af04a3aa   score: 0.9986223   abstract: <P>This paper presents a solution scheme for a class of multistage stochastic programs (possibly mixed-integer at all stages) in which a hierarchy of decisions emerges. A special structure, common to many strategic problems affected by uncertainty, allows decomposing the problem into a master problem and many independent linear programming subproblems, facilitating the isolation and reduction of the complicating mixed-integer component of the problem. Specialized (possibly heuristic) procedures can be used for solving the master problem while subproblems can be efficiently solved to optimality. We adapt and test the decomposition scheme for a case of the maritime fleet renewal problem, whose real life instances cannot be solved by means of commercial off-the-shelf solvers.</P>\n",
      "\n",
      "6. id: 5390c04520f70186a0f57d49   score: 0.9408984   abstract: Consideration was given to the a priori formulation of the multistage problem of stochastic programming with a quantile criterion which is reducible to the two-stage problem. Equivalence of the two-stage problems with the quantile criterion in the a priori and a posteriori formulations was proved for the general case. The a posteriori formulation of the two-stage problem was in turn reduced to the equivalent problem of mixed integer linear programming. An example was considered.\n",
      "\n",
      "7. id: 555a1d550cf2b21909ba3460   score: 0.91715443   abstract: Although stochastic programming is probably the most effective framework for handling decision problems that involve uncertain variables, it is always a costly task to formulate the stochastic model that accurately embodies our knowledge of these variables. In practice, this might require one to collect a large amount of observations, to consult with experts of the specialized field of practice, or to make simplifying assumptions about the underlying system. When none of these options seem feasible, a common heuristic has been to simply seek the solution of a version of the problem where each uncertain variable takes on its expected value otherwise known as the solution of the mean value problem. In this paper, we show that when 1 the stochastic program takes the form of a two-stage mixed-integer stochastic linear programs, and 2 the uncertainty is limited to the objective function, the \n",
      "\n",
      "8. id: 5546564b0cf2bf4efefa5cc4   score: 0.91715443   abstract: Although stochastic programming is probably the most effective framework for handling decision problems that involve uncertain variables, it is always a costly task to formulate the stochastic model that accurately embodies our knowledge of these variables. In practice, this might require one to collect a large amount of observations, to consult with experts of the specialized field of practice, or to make simplifying assumptions about the underlying system. When none of these options seem feasible, a common heuristic has been to simply seek the solution of a version of the problem where each uncertain variable takes on its expected value otherwise known as the solution of the mean value problem. In this paper, we show that when 1 the stochastic program takes the form of a two-stage mixed-integer stochastic linear programs, and 2 the uncertainty is limited to the objective function, the \n",
      "\n",
      "9. id: 539099b320f70186a0e19853   score: 0.88778967   abstract: We begin this paper by identifying a class of stochastic mixed-integer programs that have column-oriented formulations suitable for solution by a branch-and-price algorithm (B&P). We then survey a number of examples, and use a stochastic facility-location problem (SFLP) for a detailed demonstration of the relevant modeling and solution techniques. Computational results with a scenario representation of uncertain costs, demands and capacities show that B&P can be orders of magnitude faster than solving the standard formulation by branch and bound. We also demonstrate how B&P can solve SFLP exactly – as exactly as a deterministic mixed-integer program – when demands and other parameters can be represented as certain types of independent, random variables, e.g., independent, normal random variables with integer means and variances.\n",
      "\n",
      "10. id: 554e3f120cf22ca2c80f9dc9   score: 0.88523525   abstract: Multistage stochastic linear programs model problems in financial planning, dynamic traffic assignment, economic policy analysis, and many other applications. Equivalent representations of such problems as deterministic linear programs are, however, excessively large. This paper develops decomposition and partitioning methods for solving these problems and reports on computational results on a set of practical test problems.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707564\n",
      "index                                        559154450cf232eb904fbc63\n",
      "title               Vi-Bros: Tactile Feedback for Indoor Navigatio...\n",
      "authors             Hyunchul Lim, YoonKyong Cho, Wonjong Rhee, Bon...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390be6620f70186a0f4c4a6;5390b19020f70186a0edf...\n",
      "abstract            Vi-Bros is a new interface that simultaneously...\n",
      "id                                                            1707564\n",
      "clustered_labels                                                    0\n",
      "Name: 1707564, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4c4a6   score: 0.971779   abstract: The emergence of smart devices (e.g., smart watches and smart eyewear) is redefining mobile interaction from the solo performance of a smart phone, to a symphony of multiple devices. In this paper, we present Duet -- an interactive system that explores a design space of interactions between a smart phone and a smart watch. Based on the devices' spatial configurations, Duet coordinates their motion and touch input, and extends their visual and tactile output to one another. This transforms the watch into an active element that enhances a wide range of phone-based interactive tasks, and enables a new class of multi-device gestures and sensing techniques. A technical evaluation shows the accuracy of these gestures and sensing techniques, and a subjective study on Duet provides insights, observations, and guidance for future work.\n",
      "\n",
      "2. id: 559150a60cf232eb904fbb86   score: 0.91505283   abstract: Display devices on and around the body such as smartwatches, head-mounted displays or tablets enable users to interact on the go. However, diverging input and output fidelities of these devices can lead to interaction seams that can inhibit efficient mobile interaction, when users employ multiple devices at once. We present MultiFi, an interactive system that combines the strengths of multiple displays and overcomes the seams of mobile interaction with widgets distributed over multiple devices. A comparative user study indicates that combined head-mounted display and smartwatch interfaces can outperform interaction with single wearable devices.\n",
      "\n",
      "3. id: 559255740cf28b1a968ffc6a   score: 0.9124362   abstract: Cross-device interaction between multiple mobile devices is a popular field of research in HCI. However, the appropriate design of this interaction is still an open question, with competing approaches such as spatially-aware vs. spatially-agnostic techniques. In this paper, we present the results of a two-phase user study that explores this design space: In phase 1, we elicited gestures for typical mobile cross-device tasks from 4 focus groups (N=17). The results show that 71% of the elicited gestures were spatially-aware and that participants strongly associated cross-device tasks with interacting and thinking in space. In phase 2, we implemented one spatially-agnostic and two spatially-aware techniques from phase 1 and compared them in a controlled experiment (N=12). The results indicate that spatially-aware techniques are preferred by users and can decrease mental demand, effort, and \n",
      "\n",
      "4. id: 55902d680cf28fa91031790a   score: 0.9064877   abstract: Smartwatches now allow information to be conveniently accessed directly from the user's wrist. However, the smartwatches currently available in the market offer a limited number of applications. In this paper, we propose a new interaction technique named Harmonious Haptics, which provides users with enhanced tactile sensations by utilizing smartwatches as additional tactile displays for smartphones. When combined with typical mobile devices, our technique enables the design of a wide variety of tactile stimuli. To illustrate the potential of our approach, we developed a set of example applications that provide users with rich tactile feedback such as feeling textures in a graphical user interface, transferring a file between the tablet and the smartwatch device, and controlling UI components.\n",
      "\n",
      "5. id: 5590371f0cf28fa9103179db   score: 0.7823301   abstract: In this video, we present a new interaction technique, called Touch+, which expands touch input vocabulary by using both mobile devices and wrist-worn devices. This technique enables to recognize twisting, rolling, and lifting of a finger while it is touching the device. Moreover, our system differentiates between a fast touch (release) and a normal touch (release). This is achieved by calculating the relative differences in a movement speed and an angle between the smartphone and the smartwatch. To illustrate the potential of our approach, we developed a set of possible applications. We believe that Touch+ will open a large area for designing input interactions of mobile devices by combining wrist-worn devices.\n",
      "\n",
      "6. id: 5390995c20f70186a0e1413d   score: 0.70466846   abstract: Mobile interaction can potentially be enhanced with well-designed haptic control and display. However, advances have been limited by a vicious cycle whereby inadequate haptic technology obstructs inception of vitalizing applications. We present the first stages of a systematic design effort to break that cycle, beginning with specific usage scenarios and a new handheld display platform based on lateral skin stretch. Results of a perceptual device characterization inform mappings between device capabilities and specific roles in mobile interaction, and the next step of hardware re-engineering.\n",
      "\n",
      "7. id: 5390a63c20f70186a0e82a47   score: 0.5905431   abstract: We discuss the benefits of using a mobile device to expand and improve the interactions on a large touch-sensitive surface. The mobile device's denser arrangement of pixels and touch-sensor elements, and its rich set of mechanical on-board input controls, can be leveraged for increased expressiveness, visual feedback and more precise direct-manipulation. We also show how these devices can support unique input from multiple simultaneous users in collaborative scenarios. Handheld mobile devices and large interactive surfaces can be mutually beneficial in numerous ways, while their complementary nature allows them to preserve the behavior of the original user interface.\n",
      "\n",
      "8. id: 558ae891612c41e6b9d3d151   score: 0.58794314   abstract: While a great deal of work has been done exploring non-visual navigation interfaces using audio and haptic cues, little is known about the combination of the two. We investigate combining different state-of-the-art interfaces for communicating direction and distance information using vibrotactile and audio music cues, limiting ourselves to interfaces that are possible with current off-the-shelf smartphones. We use experimental logs, subjective task load questionnaires, and user comments to see how users' perceived performance, objective performance, and acceptance of the system varied for different combinations. Users' perceived performance did not differ much between the unimodal and multimodal interfaces, but a few users commented that the multimodal interfaces added some cognitive load. Objective performance showed that some multimodal combinations resulted in significantly less direc\n",
      "\n",
      "9. id: 5390a63c20f70186a0e822e6   score: 0.57170683   abstract: An approach to providing tangible feedback to users of a mobile device in both highly visual touchscreen-based and eyes-free interaction scenarios and the transition between the two is presented. A rotational dynamical systems metaphor for the provision of feedback is proposed, which provides users with physically based feedback via the audio, tactile and visual senses. By using a consistent metaphor in this way it is possible to support the seamless movement between highly visual touch-based interaction and eyes-free gestural interaction.\n",
      "\n",
      "10. id: 5390b3ae20f70186a0ef4975   score: 0.5493989   abstract: While mobile technologies, such as smart phones offer the benefits of portability and ubiquity, the small visual display can introduce a variety of challenges for users. Tactile feedback provides one solution to reduce the burden on the visual channel. However, the vibro-tactile signals presented by existing commercial mobile devices are relatively simple in nature and limited in number and hence cannot communicate broad set of semantic meanings or commands used in mobile applications and technologies. The research proposed in this paper, focuses on investigating how tactile feedback can be designed specifically for mobile interfaces, with the aim of improving and enriching user interactions when the visual channel is blocked or restricted (e.g. for blind or situationally-impaired users). More specifically, the aim is to investigate ways to widen the tactile bandwidth, through the manipu\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1651095\n",
      "index                                        55915af80cf232eb904fbe3d\n",
      "title                Flock: Hybrid Crowd-Machine Learning Classifiers\n",
      "authors                            Justin Cheng, Michael S. Bernstein\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390b71120f70186a0f1e2bb;5390bda020f70186a0f47...\n",
      "abstract            We present hybrid crowd-machine learning class...\n",
      "id                                                            1651095\n",
      "clustered_labels                                                    3\n",
      "Name: 1651095, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909ee020f70186a0e321fe   score: 0.3733045   abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments). We find that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves l\n",
      "\n",
      "2. id: 5390b2d620f70186a0eebb81   score: 0.36795136   abstract: The present paper discusses the issue of enhancing classification performance by means other than improving the ability of certain Machine Learning algorithms to construct a precise classification model. On the contrary, we approach this significant problem from the scope of an extended coding of training data. More specifically, our method attempts to generate more features in order to reveal the hidden aspects of the domain, modeled by the available training examples. We propose a novel feature construction algorithm, based on the ability of Bayesian networks to represent the conditional independence assumptions of a set of features, thus projecting relational attributes which are not always obvious to a classifier when presented in their original format. The augmented set of features results in a significant increase in terms of classification performance, a fact that is depicted to a\n",
      "\n",
      "3. id: 5554d6960cf2edeeee33f5bb   score: 0.3579041   abstract: According to standard procedure, building a classifier using machine learning is a fully automated process that follows the preparation of training data by a domain expert. In contrast, interactive machine learning engages users in actually generating the classifier themselves. This offers a natural way of integrating background knowledge into the modelling stage¿as long as interactive tools can be designed that support efficient and effective communication. This paper shows that appropriate techniques can empower users to create models that compete with classifiers built by state-of-the-art learning algorithms. It demonstrates that users¿even users who are not domain experts¿can often construct good classifiers, without any help from a learning algorithm, using a simple two-dimensional visual interface. Experiments on real data demonstrate that, not surprisingly, success hinges on the d\n",
      "\n",
      "4. id: 5390bd1520f70186a0f43d01   score: 0.19806418   abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. \"I think this is a forest, what do you think?\". If the learner is wrong, the supervisor provides an explanation e.g. \"No, this is too open to be a forest\". With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvem\n",
      "\n",
      "5. id: 53909f2d20f70186a0e38ba7   score: 0.17245041   abstract: Standard machine learning techniques typically require ample training data in the form of labeled instances. In many situations it may be too tedious or costly to obtain sufficient labeled data for adequate classifier performance. However, in text classification, humans can easily guess the relevance of features, that is, words that are indicative of a topic, thereby enabling the classifier to focus its feature weights more appropriately in the absence of sufficient labeled data. We will describe an algorithm for tandem learning that begins with a couple of labeled instances, and then at each iteration recommends features and instances for a human to label. Tandem learning using an \"oracle\" results in much better performance than learning on only features or only instances. We find that humans can emulate the oracle to an extent that results in performance (accuracy) comparable to that o\n",
      "\n",
      "6. id: 5390bb1d20f70186a0f3dd37   score: 0.1668111   abstract: Labeled data is a prerequisite for successfully applying machine learning techniques to a wide range of problems. Recently, crowd-sourcing has shown to provide effective solutions to many labeling tasks. However, tasks in specialist domains are difficult to map to Human Intelligence Tasks (or HITs) that can be solved adequately by \"the crowd\". The question addressed in this paper is whether these specialist tasks can be cast in such a way, that accurate results can still be obtained through crowd-sourcing. We study a case where the goal is to identify fish species in images extracted from videos taken by underwater cameras, a task that typically requires profound domain knowledge in marine biology and hence would be difficult, if not impossible, for the crowd. We show that by carefully converting the recognition task to a visual similarity comparison task, the crowd achieves agreement wi\n",
      "\n",
      "7. id: 5390be6620f70186a0f4ad91   score: 0.16171701   abstract: Active learning and crowd sourcing are becoming increasingly popular in the machine learning community for fast and cost effective generation of labels for large volumes of data. However, such labels may be noisy. So, it becomes important to ignore the noisy labels for building of a good classifier. We propose a framework for finding the best possible augmentation of a classifier for the character recognition problem using minimum number of crowd labeled samples. The approach inherently rejects the noisy data and tries to accept a subset of correctly labeled data to maximize the classifier performance.\n",
      "\n",
      "8. id: 5390bf1320f70186a0f502b9   score: 0.12765263   abstract: The goal of interactive machine learning is to help scientists and engineers exploit more specialized data from within their deployed environment in less time, with greater accuracy and fewer costs. A basic introduction to the main components is provided here, untangling the many ideas that must be combined to produce practical interactive learning systems. This article also describes recent developments in machine learning that have significantly advanced the theoretical and practical foundations for the next generation of interactive tools.\n",
      "\n",
      "9. id: 5390b71120f70186a0f1dd3e   score: 0.09842541   abstract: Machine learning algorithms have been successfully applied to learning classifiers in many domains such as computer vision, fraud detection, and brain image analysis. Typically, classifiers are trained to predict a class value given a set of labeled training data that includes all possible class values, and sometimes additional unlabeled training data.Little research has been performed where the possible values for the class variable include values that have been omitted from the training examples. This is an important problem setting, especially in domains where the class value can take on many values, and the cost of obtaining labeled examples for all values is high.We show that the key to addressing this problem is not predicting the held-out classes directly, but rather by recognizing the semantic properties of the classes such as their physical or functional attributes. We formalize\n",
      "\n",
      "10. id: 5390bd1520f70186a0f451fc   score: 0.07613248   abstract: The mission of machine learning is to empower computers to make generalizations from available data: labeled and unlabeled. The more labeled data we have the better predictions we’ll make, but labeled data usually comes at a cost and should be used sparingly. In some cases, the nature of prediction problem can be changed by using a different sensor modality or by obtaining a different kind of annotation. In this dissertation we first present methods to enhance predictive ability by improving the use of existing data: by constructing feature spaces for human activity recognition and by developing semi-supervised methods for object recognition. We then develop methods for collecting, storing and visualizing information about activity in an indoor office environment. By using a dense array of simple motion sensors, we can track people in the office space, while preserving reasonable expecta\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691426\n",
      "index                                        559255510cf2aff368683bcb\n",
      "title               Aerial service vehicles for industrial inspect...\n",
      "authors             Jonathan Cacace, Alberto Finzi, Vincenzo Lippi...\n",
      "year                                                           2015.0\n",
      "venue                                            Applied Intelligence\n",
      "references          5390a1e620f70186a0e59a07;53909fbd20f70186a0e42...\n",
      "abstract            This work proposes a high-level control system...\n",
      "id                                                            1691426\n",
      "clustered_labels                                                    0\n",
      "Name: 1691426, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a88c20f70186a0e9a492   score: 0.97543687   abstract: This paper presents the autonomous control for UAV. The autonomous control concept and Autonomous Control Level (ACL) metrics that can measure autonomy of UAVs are introduced. Compared with manned aircraft control task in battlefield, the functions of autonomous UAV system are organized. According to the laws of Increasing Precision with Decreasing Intelligent (IPDI), the architecture of autonomous control for UAV is given. The architecture can be divided into three levels: Execution Level, Coordination Level and Organization Level. The constraint conditions and realizations of Coordination Level and Organization Level are studied comprehensively. The key hardware and software technologies for multi-tasks are modularized depending on the requirements of mission. The software technologies are distributed to stages of flying particularly.\n",
      "\n",
      "2. id: 5390b8d720f70186a0f2bc85   score: 0.93686515   abstract: The paper presents work that has been done by three different research institutions. The aim was to realize an autonomous team of heterogeneous unmanned ground and aerial vehicles performing certain reconnaissance and surveillance tasks, where the tasks were set by an operator at a team level instead of controlling each vehicle seperately. To overcome the lack of a common middleware, the interfaces between vehicles and graphical user interface have been defined using Robot Operating System (ROS) and Battle Management Language (BML). We present approaches for autonomous control of the vehicles, focussing on the unmanned ground vehicle. Moreover, we conducted some large field experiments and present the results.\n",
      "\n",
      "3. id: 5390adfd20f70186a0ec60ee   score: 0.86511016   abstract: In the past decade Unmanned Aerial Vehicles (UAVs) have rapidly grown into a major field of robotics in both industry and academia. Many well established platforms have been developed, and the demand continues to grow. However, the UAVs utilized in industry are predominately remotely piloted aircraft offering very limited levels of autonomy. In contrast, fully autonomous flight has been achieved in research, and the degree of autonomy continues to grow, with research now focusing on advanced tasks such as navigating cluttered terrain and formation flying. The gap between academia and industry is the robustness of control algorithms. Academic research often focuses on proof of concept demonstrations with little or no consideration to real world concerns such as adverse weather or sensor integration. One of the goals of this thesis is to integrate real world issues into the design process.\n",
      "\n",
      "4. id: 5390a79f20f70186a0e92500   score: 0.86083776   abstract: The work contains the analysis of the design methodology of an unmanned aerial vehicle (UAV), which is able to carry out environmental monitoring, define the location of various objects and targets with high accuracy, map the seats of fire and areas of environmental contamination, fulfil patrolling functions to solve the tasks of the National Armed Forces and police, carry out meteorological research, etc. The original UAV design is provided with special compartments to carry useful load (engine, batteries, surveillance camera, control elements, 80 etc). The UAV has a combined control system intended for the control in an automatic mode including the application of GPS system.\n",
      "\n",
      "5. id: 539098b820f70186a0e0ab41   score: 0.77696157   abstract: This paper reports experiences and outcomes of designing and developing an agent-based, autonomous mission control system for an unmanned aerial vehicle (UAV). Most UAVs are not truly autonomous or even unmanned but are more correctly termed 'uninhabited' or 'remotely piloted'. This paper explores two quite different approaches for adding autonomous control to an existing UAV. Both designs were implemented using an agent-based language. The first takes a fairly standard approach, adding a layer over the flight control system to control the mission. The second takes the human metaphor of agency more seriously and implements an autonomous controller based on a model of human decision making widely referenced in the military command and control literature. Implementing these two designs allowed a comparison of their relative strengths and weaknesses. Preliminary findings indicate both the f\n",
      "\n",
      "6. id: 539098dd20f70186a0e0e98a   score: 0.75868946   abstract: The UAV (Unmanned Aerial Vehicle) performs various kinds of missions while performing autonomous flight navigation. In order to realize all functionalities of the UAV, the software part becomes a very complex hard real-time system because some hard real-time tasks, soft real-time tasks and non-real-time tasks are concurrently working together under a RTOS. In this paper, a hierarchical architecture for Unmanned Autonomous Helicopter System is proposed that guarantees the real-time performance of hard real-time tasks and the re-configurability of soft real-time or non-real-time tasks under the RT-Linux. This software architecture has four layers: hardware, execution, service agent and remote user interface layer according to the reactiveness level for external events. In addition, the layered separation of concurrent tasks makes different kinds of mission reconfiguration possible in the s\n",
      "\n",
      "7. id: 5390ac1820f70186a0eb4456   score: 0.7226249   abstract: This paper focuses on a critical component of the situational awareness (SA), the control of autonomous vertical flight for an unmanned aerial vehicle (UAV). Autonomous vertical flight is a challenging but important task for tactical UAVs to achieve high level of autonomy under adverse conditions. With the SA strategy, we proposed a two stage flight control procedure using two autonomous control subsystems to address the dynamics variation and performance requirement difference in initial and final stages of flight trajectory for a nontrivial nonlinear three-rotor mini-aircraft model. This control strategy for chosen mini-aircraft model has been verified by simulation of hovering maneuvers using software package Simulink and demonstrated good performance for fast SA in real-time search-andrescue operations.\n",
      "\n",
      "8. id: 5390adfd20f70186a0ec5695   score: 0.65367985   abstract: Unmanned aerial vehicles (UAVs) present in a wide range of scales, airframe types and possible systems configurations. Assessing how these different systems perform, therefore, should be an essential part of their design. This task, however, is particularly difficult due to the complex, dynamic interactions these vehicles are capable of, and the often complex operational conditions. In this paper we describe and apply an evaluation framework based on spatial cost-to-go (SCTG) maps. These maps describe the spatial distributions of optimal state and cost-to-go for a given geographical environment, and are computed using a finite-state approximation of the vehicle dynamics. The SCTG maps embed the interaction effects between vehicle dynamics and environment, and thus provide a rigorous basis for the evaluation of the airframe, various system and environment factors. The paper describes the \n",
      "\n",
      "9. id: 5390a28120f70186a0e63267   score: 0.6441132   abstract: Unmanned Aerial Vehicle (UAV) has typically been used in civil and military field all over the world. Based on the UAV mission of autonomous flight, the ground control station (GCS) including hardware and software become important equipment to be developed. This paper emphasizes three parts to build a well functioning ground control station: a portable ground station hardware system, a virtual instrument panel for the attitude information and flight path showing, all kinds of error alert. Through the whole test on the ground and in the sky, GCS can show the remote sensing information precisely and send the control command in time. The system can be also used to assist in the function of autonomous cruise task for UAV.\n",
      "\n",
      "10. id: 5390b71120f70186a0f1dd3d   score: 0.6288634   abstract: Currently deployed unmanned rotorcraft rely on preplanned missions or teleoperation and do not actively incorporate information about obstacles, landing sites, wind, position uncertainty, and other aerial vehicles during online motion planning. Prior work has successfully addressed some tasks such as obstacle avoidance at slow speeds, or landing at known to be good locations. However, to enable autonomous missions in cluttered environments, the vehicle has to react quickly to previously unknown obstacles, respond to changing environmental conditions, and find unknown landing sites. We consider the problem of enabling autonomous operation at low-altitude with contributions to four problems. First we address the problem of fast obstacle avoidance for a small aerial vehicle and present results from over a 1000 rims at speeds up to 10 m/s. Fast response is achieved through a reactive algorit\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674109\n",
      "index                                        559257690cf205530abc97dd\n",
      "title               Minimizing consistency-control overhead with r...\n",
      "authors             Hyun Ku Lee, Junghoon Kim, Dong Hyun Kang, You...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 9th International Conferenc...\n",
      "references          5390a25820f70186a0e5f772;5390b2fc20f70186a0eef...\n",
      "abstract            To guarantee system reliability, consistency-c...\n",
      "id                                                            1674109\n",
      "clustered_labels                                                    3\n",
      "Name: 1674109, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b5c620f70186a0f088d5   score: 0.9168571   abstract: This dissertation improves the consistency and durability guarantees that file systems can efficiently provide, both by allowing each application to choose appropriate trade-offs between consistency and performance and by dramatically lowering the overheads of durability and consistency using new hardware and careful file system design. We first describe a new abstraction, the patch, which represents a write to persistent storage and its ordering requirements. This abstraction allows file system modules to specify ordering guarantees without simultaneously requesting more expensive, immediate durability. Algorithmic and data structure optimizations make this abstraction practical, and our patch-based file system implementation is performance-competitive with similarly-reliable Linux ext2 and ext3 configurations. To illustrate the benefits of patchgroups, an application-accessible version\n",
      "\n",
      "2. id: 5390be6620f70186a0f4ce81   score: 0.86703575   abstract: Journaling techniques are widely used in modern file systems as they provide high reliability and fast recovery from system failures. However, it reduces the performance benefit of buffer caching as journaling accounts for a bulk of the storage writes in real system environments. To relieve this problem, we present a novel buffer cache architecture that subsumes the functionality of caching and journaling by making use of nonvolatile memory such as PCM or STT-MRAM. Specifically, our buffer cache supports what we call the in-place commit scheme. This scheme avoids logging, but still provides the same journaling effect by simply altering the state of the cached block to frozen. As a frozen block still provides the functionality of a cache block, we show that in-place commit does not degrade cache performance. We implement our scheme on Linux 2.6.38 and measure the throughput and execution \n",
      "\n",
      "3. id: 558da1360cf2af9ee80e9d40   score: 0.8247444   abstract: Database systems use the journaling techniques in order to guarantee database consistency even at system crashes. Since the journaling needs duplicated write operations, it exhausts the lifetime of NAND flash-based storage device. In this paper, we propose a novel file system-level block remapping technique, which changes only the file system metadata at database checkpointing without any data write operations. Experiments show that the proposed scheme can reduce the write traffic on storage device by 17% on average.\n",
      "\n",
      "4. id: 5390bfa220f70186a0f53a05   score: 0.7858021   abstract: Journaling techniques are widely used in modern file systems as they provide high reliability and fast recovery from system failures. However, it reduces the performance benefit of buffer caching as journaling accounts for a bulk of the storage writes in real system environments. In this paper, we present a novel buffer cache architecture that subsumes the functionality of caching and journaling by making use of non-volatile memory such as PCM or STT-MRAM. Specifically, our buffer cache supports what we call the in-place commit scheme. This scheme avoids logging, but still provides the same journaling effect by simply altering the state of the cached block to frozen. As a frozen block still performs the function of caching, we show that in-place commit does not degrade cache performance. We implement our scheme on Linux 2.6.38 and measure the throughput and execution time of the scheme w\n",
      "\n",
      "5. id: 55924397612cfc7d8b214687   score: 0.6907677   abstract: The file system durability is provided by flushing dirty pages periodically into the non-volatile storage. Since the traditional storage devices such as hard disk and flash memory can be written in the unit of block, the file system writes a whole block even when only a small number of bytes are modified. To resolve such a wasting write traffic problem, we propose a two-level logging scheme by exploiting non-volatile and byte-addressable memories (NVMs). Whereas the previous approach which exploits the NVM device is targeted for EXT4 file system, our scheme uses log-structured file systems in order to guarantee the file system reliability even for sudden system crashes. While the NVM is used for fine-grained logging, the flash memory is used for coarse-grained logging. Experiments with a real NVM device show that the proposed scheme reduces the write traffic on storage by up to 78% and i\n",
      "\n",
      "6. id: 5390b20120f70186a0ee3e90   score: 0.6893056   abstract: This paper considers the problem of how to implement a file system on Storage Class Memory (SCM), that is directly connected to the memory bus, byte addressable and is also non-volatile. In this paper, we propose a new file system, called SCMFS, which is implemented on the virtual address space. In SCMFS, we utilize the existing memory management module in the operating system to do the block management and keep the space always contiguous for each file. The simplicity of SCMFS not only makes it easy to implement, but also improves the performance. We have implemented a prototype in Linux and evaluated its performance through multiple benchmarks.\n",
      "\n",
      "7. id: 5390be6620f70186a0f4c856   score: 0.67608553   abstract: The emerging Storage Class Memory, which offers characteristics of byte-addressability, persistence, and low power consumption, will be expected to replace memory/storages. A new file system is required in this environment. In the design of file systems, data consistency is one of the most important issues that should be taken into account. To do this, most file systems exploit journaling or shadow paging for the consistency. Shadow paging employs copy-on-write for the consistency. However, it incurs many copy overheads. In order to relieve these problems, BPFS proposed a short-circuit shadow paging. But, in our experiments, we showed that it incurs many copy-on-write blocks as ever. In this paper, we propose a last block logging mechanism for improving the performance and the lifetime of SCM-based file system, by reducing copy-on-write blocks considerably. Our approach is to store the o\n",
      "\n",
      "8. id: 5390bfa220f70186a0f53a2d   score: 0.58296597   abstract: Lightweight databases and key-value stores manage the consistency and reliability of their own data, often through rollback-recovery journaling or write-ahead logging. They further rely on file system journaling to protect the file system structure and metadata. Such journaling of journal appears to violate the classic end-to-end argument for optimal database design. In practice, we observe a significant cost (up to 73% slowdown) by adding the Ext4 file system journaling to the SQLite database on a Google Nexus 7 tablet running a Ubuntu Linux installation. The cost of file system journaling is up to 58% on a conventional machine with an Intel 311 SSD. In this paper, we argue that such cost is largely due to implementation limitations of the existing system. We apply two simple techniques--ensuring a single I/O operation on the synchronous commit path, and adaptively allowing each file to\n",
      "\n",
      "9. id: 53908e0020f70186a0dd4274   score: 0.52553624   abstract: With the recent trend to use storage area networks in distributedand cluster systems there is a need to improve theintegrity and consistency guarantees of stored data in thepresence of node or network failures. Currently, the mainmethod for preserving data integrity and consistency is bylogging techniques, e.g. journaling. This paper presentsa new, general method for preserving data consistency byAtomic multi-block Writes. The Atomic Writes methodguarantees that either all the blocks in a write operationare written or no blocks are written at all. Its main advantageis that it does not require a recovery phase after afailure. The Atomic Writes method should be implementedin both the operating system and the storage system levels.It is easy to use and to implement. We present the method,its implementation and an example of its use for handlingmeta-data consistency in the Global File System\n",
      "\n",
      "10. id: 5390bb1d20f70186a0f3ed67   score: 0.5004902   abstract: In this paper, we aim to improve the reliability of a central part of the operating system storage stack: the page cache. We consider two reliability threats: memory errors, where bits in DRAM are flipped due to cosmic rays, and software bugs, where programming errors may ultimately result in data corruption and crashes. We argue that by making use of checksums, we can significantly reduce the probability that either threat results in any application-visible effects. In particular, we can use checksums to detect memory corruption as well as validate the integrity of the cache's internal state for recovery after a crash. We show that in many cases, we can avoid the overhead of computing checksums especially for these purposes. We implement our ideas in the Loris storage stack. Our analysis and evaluation show that our approach improves the overall reliability of the cache at relatively li\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1705637\n",
      "index                                        55323b6045cec66b6f9d9cdd\n",
      "title               Routing Protocol Based on CDSE Virtual Topolog...\n",
      "authors             Ali Kies, Zoulikha Mekkakia Maaza, Redouane Be...\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references          5390a4cc20f70186a0e74923;539087c720f70186a0d56...\n",
      "abstract            This paper describes the Connected Dominating ...\n",
      "id                                                            1705637\n",
      "clustered_labels                                                    1\n",
      "Name: 1705637, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908b4920f70186a0dbca82   score: 0.9793935   abstract: An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any fixed infrastructure. Indeed, an important task of an ad hoc network is to determine an appropriate topology over which high-level routing protocols are implemented. Furthermore, since the underlying topology may change with time, we need to design routing algorithms that effectively react to dynamically changing network conditions.The aim of this paper is to explore the limits of communication in wireless mobile networks, concentrating on local-control algorithms for topology control and routing. We analyze the performance of the algorithms under three measures: throughput, which is the rate at which packets can be delivered, space overhead, i.e. the space necessary to buffer packets, and the total energy consumed due to packet transmissions. Energy consumption is an important p\n",
      "\n",
      "2. id: 5390a1e620f70186a0e5b730   score: 0.9700703   abstract: Construction of CDS (connected dominating set) is the foundation to build virtual backbone subnet or deploy layered routing in ad hoc or wireless sensor networks where packet flooding is extensively used. Current algorithm generally falls into two types: (1) cluster-based and (2) direct algorithm. This paper proposes a direct algorithm with linear time and message complexity. Our algorithm RCDS partitions the network nodes into dominators and dominatees. RCDS is a greedy algorithm which always chooses the farther and steadier node as future dominators. The experiment result shows that RCDS can construct smaller CDS than other algorithms.\n",
      "\n",
      "3. id: 5390ad5620f70186a0ebe8d8   score: 0.9673486   abstract: The connected dominating set (CDS) is widely used as a virtual backbone in mobile ad hoc networks. Although many distributed algorithms for constructing the CDS have been proposed, nearly all of them require two or more separated phases, which may cause problems such as long delay in the later phases when the network size is large. This paper proposes a Distributed Single-Phase algorithm for constructing a connected dominating set, DSP-CDS, in ad hoc networks. The DSP-CDS is an asynchronous distributed algorithm and converges quickly in a single phase. Each node uses one-hop neighborhood information and makes a local decision on whether to join the dominating set. Each node bases its decision on a key variable, strength, which guarantees that the dominating set is connected when the algorithm converges. The rules for computing strength can be changed to accommodate different application \n",
      "\n",
      "4. id: 5390afc920f70186a0ed1b43   score: 0.9574373   abstract: Connected dominating set (CDS) algorithm based on a virtual backbone technology has an important role for wireless ad hoc network, route optimization, energy conservation and allocation of resources. In this paper, on the base of several classic distributed CDS approximation algorithm and connected dominating set, a new distributed CDS algorithm based on weight is presented, the correctness and efficiency of the algorithm is proved from theory. The simulation results show that the algorithm performs better than classical algorithms.\n",
      "\n",
      "5. id: 5390994d20f70186a0e139a7   score: 0.9552357   abstract: Recent research points out that the flooding mechanism for topology update or route request in existing ad hoc routing protocols greatly degrades the network capacity. If we restrict the broadcast of control packets within a small subset of hosts in the network, the protocol overhead can be substantially reduced. This motivates our research of constructing a virtual backbone by computing a connected dominating set (CDS) in unit-disk graphs. In this paper, we propose two distributed algorithms to approximate a minimum CDS. These algorithms take linear time. Their performance is verified by a complete theoretical analysis. Copyright © 2006 John Wiley & Sons, Ltd.\n",
      "\n",
      "6. id: 5390a28120f70186a0e633f0   score: 0.9462997   abstract: The connected dominating set (CDS) has been generally used for routing and broadcasting in mobile ad hoc networks (MANETs). To reduce the cost of routing table maintenance, it is preferred that the size of CDS to be as small as possible. A number of protocols have been proposed to construct CDS with competitive size, however only few are capable of maintaining CDS under topology changes. In this research, we propose a novel extended mobility handling algorithm which will not only shorten the recovery time of CDS mobility handling but also keep a competitive size of CDS. Our simulation results validate that the algorithm successfully achieves its design goals. In addition, we will introduce an analytical model for the convergence time and the number of messages required by the CDS construction.\n",
      "\n",
      "7. id: 5390adfc20f70186a0ec4ca6   score: 0.9452985   abstract: In mobile ad hoc networks, the nodes are powered by batteries. So one of the most important merits when evaluating a given protocol is its ability to decrease node energy consumption and prolong network lifetime. Concerning this problem, an improved routing algorithm based on extreme prediction is presented for a mobile ad hoc network. It is an energy-saving and stable routing algorithm in which the stable zone is divided by the distance between two nodes and the link availability is calculated according to the area where the node is. The availability of a route is the minimum link availability which consist the whole route and the most stable path is chosen to send information data. And the node adjusts its transmitting power referring to the location of the next hop. Simulation results show that the proposed algorithm decreases network energy consumption effectively compared with basic\n",
      "\n",
      "8. id: 539095ba20f70186a0df104b   score: 0.9349913   abstract: A dynamic ad-hoc network consists of a collection of mobile hosts with frequently changing network topology. We propose a distributed algorithm that adapts to the topology by utilizing spanning trees in the regions where the topology is stable, and resorting to an intelligent flooding-like approach in highly dynamic regions of the network. Routing is performed using the spanning trees based a hold-and-forward or shuttling mechanisms. We introduce the notion of connectivity-through-time and the parameter holding-time as new fundamental concepts that can be used by ad-hoc routing algorithms. For various network connectivity scenarios we evaluate the impact of these concepts on the performance of ad-hoc routing algorithms. Using simulation, we study the throughput, reachability and message-reachability ratio of the proposed schemes under various connection/disconnection rates and holding ti\n",
      "\n",
      "9. id: 5390a2e920f70186a0e66cb8   score: 0.93045825   abstract: It is proved that multipoint relaying (MPR) is an efficient strategy for on-the-fly broadcasting in mobile ad hoc networks [8]. Upon receiving a flooding packet, each relaying node designated by the packet selects nodes from its neighbors for next packet relay. All the selected relaying nodes finally form a connected dominating set (CDS) for the network. However, computing a minimum size CDS is NP hard. Several existing algorithms use the idea of MPR to reduce the size of the CDS. In this paper, we observed two drawbacks in the existing algorithms and proposed a new greedy algorithm based on node connectivity. Also, a connectivity based rule based on the new greedy algorithm is proposed to reduce the CDS size of Wu’s EMPR [11]. Both theoretical analysis and experiment results show that the new greedy algorithm outperforms the existing broadcasting algorithms in the terms of number of rel\n",
      "\n",
      "10. id: 539099b320f70186a0e1a04e   score: 0.9203613   abstract: A novel routing protocol for wireless, mobile ad hoc networks is presented. This protocol incorporates features that enhance routing reliability, defined as the ability to provide almost 100% packet delivery rate. The protocol is based on a virtual structure, unrelated to the physical network topology, where mobile nodes are connected by virtual links and are responsible for keeping physical routes to their neighbors in the virtual structure. Routes between pairs of mobiles are set up by using information to translate virtual paths discovered in the virtual structure. Route discovery and maintenance phases of the protocol are based on unicast messages travelling across virtual paths, with sporadic use of flooding protocol. Most flooding is executed in the background using low priority messages. The routing protocol has been evaluated and compared with the Dynamic Source Routing protocol \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1662619\n",
      "index                                        559166cb0cf2e89307ca993e\n",
      "title               A Framework for Psychophysiological Classifica...\n",
      "authors             Alexander J. Karran, Stephen H. Fairclough, Ki...\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Computer-Human Interaction...\n",
      "references          539089ab20f70186a0d94fd2;539087cf20f70186a0d5c...\n",
      "abstract            This article presents a psychophysiological co...\n",
      "id                                                            1662619\n",
      "clustered_labels                                                    0\n",
      "Name: 1662619, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390baa120f70186a0f38d06   score: 0.9328214   abstract: The contemporary heritage institution visitor model is built around passive receivership where content is consumed but not influenced by the visitor. This paper presents work in progress towards an adaptive interface designed to respond to the level of interest of the visitor, in order to deliver a personalised experience within cultural heritage institutions. A subject-dependent experimental approach was taken to record and classify physiological signals using mobile physiological sensors and a machine learning algorithm. The results show a high classification rate using this approach, informing future work for the development of a real-time physiological computing component for use within an adaptive cultural heritage experience.\n",
      "\n",
      "2. id: 5390baa120f70186a0f39bba   score: 0.87557435   abstract: Emotions are a great source of information in communication and interaction among people. There is a continuous interaction between emotions, thoughts and behavior, in such a way that they constantly influence each other. in this paper, we propose an emotion classification system that can classify four emotions (happiness, sadness, fear and anger). Participants隆娄 physiological signals are acquired by electrocardiogram (ECG), galvanic skin responses (GSR), blood volume pulse (BVP), and pulse. We adopt sequential floating forward selection (SFFS) and F-score feature selection methods to get discriminative features that influence emotion. the selected features are used to train the support vector machine (SVM) classifier. Experiment results show that the proposed method achieves 89.6%.\n",
      "\n",
      "3. id: 5390bfa220f70186a0f52cc9   score: 0.8177201   abstract: The paper presents an audio-based emotion recognition system that is able to classify emotions as anger, fear, happy, neutral, sadness or disgust in real time. We use the virtual coach as an application example of how emotion recognition can be used to modulate intelligent systems' behavior. A novel minimum-error feature removal mechanism to reduce bandwidth and increase accuracy of our emotion recognition system has been introduced. A two-stage hierarchical classification approach along with a One-Against-All (OAA) framework are used. We obtained an average accuracy of 82.07% using the OAA approach, and 87.70% with a two-stage hierarchical approach, by pruning the feature set and using Support Vector Machines (SVMs) for classification.\n",
      "\n",
      "4. id: 5390a1d420f70186a0e56c69   score: 0.7711549   abstract: This paper describes an experiment on emotion measurement and classification based on different physiological parameters, which was conducted in the context of a European project on ambient intelligent mobile devices. Emotion induction material consisted of five four-minute video films that induced two positive and three negative emotions. The experimental design gave consideration to both, the basic and the dimensional model of the structure of emotion. Statistical analyses were conducted for films and for self-assessed emotional state and in addition, supervised machine learning technique was utilized. Recognition rates reached up to 72% for a specific emotion (one out of five) and up to 82% for an underlying dimension (one out of two).\n",
      "\n",
      "5. id: 5390c04520f70186a0f565ca   score: 0.7602949   abstract: Since emotion technology has been applied into numerous applications, the role of recognizing human emotion has become more important. In this paper, two autonomic nervous signals such as SKT and PPG were analyzed in order to extract 2D emotional feature vector (PPI and SKT amplitude) for classification between happy and sad emotions. A support vector machine was adopted for non-linear classification between happiness and sadness. We collected SKT and PPG signals from 5 undergraduates who respectively watched two different kinds of video inducing happiness and sadness. At result, the classification accuracy of 92.41% was obtained by combining two features through using support vector machine which was even more increased result compared with the results using single feature such as SKT (89.29%) and PPG (63.67%).\n",
      "\n",
      "6. id: 5390a7f520f70186a0e93f2c   score: 0.7559974   abstract: Reliable classification of an individual's affective state through processing of physiological response requires the use of appropriate machine learning techniques, and the analysis of how experimental factors influence the data recorded. While many studies have been conducted in this field, the effect of many of these factors is yet to be properly investigated and understood. This study investigates the relative effects of number of subjects, number of recording sessions, sampling rate and a variety of different classification approaches. Results of this study demonstrate accurate classification is possible in isolated sessions and that variation between sessions and subjects has a significant effect on classifier success. The effect of sampling rate is also shown to impact on classifier success. The results also indicate that affective space is likely to be continuous and that developi\n",
      "\n",
      "7. id: 5390a5dc20f70186a0e7f5f6   score: 0.742048   abstract: A two-phase procedure, based on biosignal recordings, is applied in an attempt to classify the emotion valence content in human-agent interactions. In the first phase, participants are exposed to a sample of pictures with known valence values (taken from IAPS dataset) and classifiers are trained on selected features of the biosignals recorded. During the second phase, biosignals are recorded for each participant while watching video clips with interactions with a female and male ECAs. The classifiers trained in the first phase are applied and a comparison between the two interfaces is carried on based on the classifications of the emotional response from the video clips. The results obtained are promising and are discussed in the paper together with the problems encountered, and the suggestions for possible future improvement.\n",
      "\n",
      "8. id: 53909fbc20f70186a0e41a33   score: 0.7210562   abstract: A real-time user-independent emotion detection system using physiological signals has been developed. The system has the ability to classify affective states into 2-dimensions using valence and arousal. Each dimension ranges from 1 to 5 giving a total of 25 possible affective regions. Physiological signals were measured using 3 biometric sensors for Blood Volume Pulse (BVP), Skin Conductance (SC) and Respiration (RESP). Two emotion inducing experiments were conducted to acquire physiological data from 13 subjects. The data from 10 of these subjects were used to train the system, while the remaining 3 datasets were used to test the performance of the system. A recognition rate of 62% for valence and 67% for arousal was achieved within +/- 1 units of the valence and arousal rating.\n",
      "\n",
      "9. id: 5390a1bc20f70186a0e56108   score: 0.6997676   abstract: A physiological signal based emotion recognition method, for the assessment of three emotional classes: happiness, disgustand fear, is presented. Our approach consists of four steps: (i) biosignal acquisition, (ii) biosignal preprocessing and feature extraction, (iii) feature selection and (iv) classification. The input signals are facial electromyograms, the electrocardiogram, the respiration and the electrodermal skin response. We have constructed a dataset which consists of 9 healthy subjects. Moreover we present preliminary results which indicate on average, accuracy rates of 0.48,0.68 and 0.69 for recognition of happiness, disgust and fear emotions, respectively.\n",
      "\n",
      "10. id: 5390ac1820f70186a0eb33ae   score: 0.68637   abstract: In this work we describe the processing and classifying of EEG-data that was acquired under emotional conditions. In the context of assistive environment technology it is one of the most important challenges to get information about a persons emotional state. To get this information, psychophysiological data was recorded while stimulating subjects with emotional pictures. Afterwards a classifier was trained to differentiate between physiological patterns of negative, positive and neutral conditions. The classification results show an accuracy of about 72%.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1724710\n",
      "index                                        55323d0e45cec66b6f9dd6cb\n",
      "title               Write once, get 50% free: saving SSD erase cos...\n",
      "authors                    Gala Yadgar, Eitan Yaakobi, Assaf Schuster\n",
      "year                                                           2015.0\n",
      "venue               FAST'15 Proceedings of the 13th USENIX Confere...\n",
      "references          558bd1a90cf23f2dfc593aa9;558bd98e0cf25dbdbb04e...\n",
      "abstract            NAND flash, used in modern SSDs, is a write-on...\n",
      "id                                                            1724710\n",
      "clustered_labels                                                    3\n",
      "Name: 1724710, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b48420f70186a0efaa2f   score: 0.9142907   abstract: NAND flash-based SSDs suffer from limited lifetime due to the fact that NAND flash can only be programmed or erased for limited times. Among various approaches to address this problem, we propose to reduce the number of writes to the flash via exploiting the content locality between the write data and its corresponding old version in the flash. This content locality means, the new version, i.e., the content of a new write request, shares some extent of similarity with its old version. The information redundancy existing in the difference (delta) between the new and old data leads to a small compression ratio. The key idea of our approach, named Delta-FTL (Delta Flash Translation Layer), is to store this compressed delta in the SSD, instead of the original new data, in order to reduce the number of writes committed to the flash. This write reduction further extends the lifetime of SSDs du\n",
      "\n",
      "2. id: 553a799a0cf288c3e22746c8   score: 0.8494724   abstract: As consumer pressure for more bits per dollar and higher density-per-solid-state disk (SSD) forces manufacturers to squeeze more than one bit per flash cell and feature sizes downwards, wear-out is again becoming an increasing concern. Specifically, while single-level cell flash at larger feature sizes used to boast over 100,000 program/erase (P/E) cycles, modern triple-level cell flash can only sustain a measly 3,000 P/E cycles before it can no longer be reliably used. However, one lesser known facet of NAND flash design is that there is no material difference between cells that store one, two, or three bits per cell - it is merely a logical interpretation of the cells contents. Therefore, in this work we leverage this interesting property to explore how resurrecting dead flash cells to create \\\"Zombie-NAND\\\" flash can improve an SSD's lifetime, and what, if any, impact on latency resul\n",
      "\n",
      "3. id: 5390bae520f70186a0f3ae59   score: 0.8067004   abstract: Flash-based SSDs are becoming prevalent in the entire storage stack, but their in-born limitations still remain there, which prevents users' complete confidence in them, despite of their very attractive performance advantages. One of the critical concerns is their limited lifespan and the associated degrading reliability as the capacity is consumed. A variety of techniques based on caching optimization and redundancy elimination have been proposed to overcome the problem. The main principle behind those techniques is to attempt to reduce the effective write traffic to SSDs. In this paper, we present Flash Saver, which is coupled with the ext2/3 file system and aims to significantly reduce the write traffic to SSDs. Flash Saver deploys deduplication and delta-encoding to reduce the write traffic. Specifically, Flash Saver applies deduplication to file system data blocks and delta-encoding\n",
      "\n",
      "4. id: 5390b52620f70186a0f0368a   score: 0.6888872   abstract: In NAND flash memory, once a page program or block erase (P/E) command is issued to a NAND flash chip, the subsequent read requests have to wait until the time-consuming P/E operation to complete. Preliminary results show that the lengthy P/E operations may increase the read latency by 2x on average. As NAND flash-based SSDs enter the enterprise server storage, this increased read latency caused by the contention may significantly degrade the overall system performance. Inspired by the internal mechanism of NAND flash P/E algorithms, we propose in this paper a low-overhead P/E suspension scheme, which suspends the on-going P/E to service pending reads and resumes the suspended P/E afterwards. In our experiments, we simulate a realistic SSD model that adopts multi-chip/channel and evaluate both SLC and MLC NAND flash as storage materials of diverse performance. Our experimental results sh\n",
      "\n",
      "5. id: 5390b52620f70186a0f0368c   score: 0.6583077   abstract: Over the last decade we have witnessed the relentless technological improvement in flash-based solid-state drives (SSDs) and they have many advantages over hard disk drives (HDDs) as a secondary storage such as performance and power consumption. However, the random write performance in SSDs still remains as a concern. Even in modern SSDs, the disparity between random and sequential write bandwidth is more than tenfold. Moreover, random writes can shorten the limited lifespan of SSDs because they incur more NAND block erases per write. In order to overcome these problems due to random writes, in this paper, we propose a new file system for SSDs, SFS. First, SFS exploits the maximum write bandwidth of SSD by taking a log-structured approach. SFS transforms all random writes at file system level to sequential ones at SSD level. Second, SFS takes a new data grouping strategy on writing, inst\n",
      "\n",
      "6. id: 5390b60d20f70186a0f11eaf   score: 0.60130095   abstract: The popularity of NAND flash memory has been growing rapidly in recent years, but the SSD (Solid-State Disk) has shown limited success in its battle against the hard disk. Besides the high price, SSD suffers performance degradation under random write requests, due to the intrinsic weak points of NAND flash: erase-before-write, asymmetric read/write access time, and limited program/erase cycles. In order to overcome these drawbacks, many buffer replacement algorithms have been proposed. However, considering the cost of write operations, it would be beneficial to have dirty pages updated before being flushed to flash memory. In this paper, we propose a new buffer management scheme to retain write-intensive pages in the buffer, and we confirm its effectiveness by applying it to one of the existing buffer management schemes. The simulation results indicate that the proposed scheme reduces up\n",
      "\n",
      "7. id: 5390b60d20f70186a0f11cc0   score: 0.5786863   abstract: The use of NAND flash memory for building permanent storage has been increasing in many embedded systems due to idiosyncrasies such as non-volatility and low energy consumption. The persistent requirements for high storage capacity have given rise to the increase of bit density per cell as in multi-level cells but this has come at the expense of performance and has resulted in degradation of durability. In this paper, we introduce a complementary approach to boost the performance and durability of MLC-based storage systems by employing a non-volatile buffer that temporarily holds the data heading to MLCs. We also propose algorithms to efficiently eliminate unnecessary write and erase operations in MLCs by performing a pre-merge in the buffer. Our experiments show that the proposed approach can decrease average response time by up to 4 times and increase durability by 4 times by adding on\n",
      "\n",
      "8. id: 5390bfa220f70186a0f53db8   score: 0.5670977   abstract: Since NAND flash cannot be updated in place, SSDs must perform all writes in pre-erased pages. Consequently, pages containing superseded data must be invalidated and garbage collected. This garbage collection adds significant cost in terms of the extra writes necessary to relocate valid pages from erasure candidates to clean blocks, causing the well-known write amplification problem. SSDs reserve a certain amount of flash space which is invisible to users, called over-provisioning space, to alleviate the write amplification problem. However, NAND blocks can support only a limited number of program/erase cycles. As blocks are retired due to exceeding the limit, the reduced size of the over-provisioning pool leads to degraded SSD performance. In this work, we propose a novel system design that we call the Smart Retirement FTL (SR-FTL) to reuse the flash blocks which have been cycled to the\n",
      "\n",
      "9. id: 5390ada620f70186a0ec1fb9   score: 0.51656497   abstract: Flash memory SSDs pose a well-known challenge, that is, the erase-before-write problem. Researchers try to solve this inherent problem from two different angles by either designing sophisticated Flash Translation Layer (FTL) schemes to postpone and minimize erasures or designing flash-aware buffer management algorithms to absorb unnecessary erasures. Our experimental results show that buffer management inside SSD is necessary and indispensable. Further, based on our observation that TPC and some server workloads have strong temporal locality, this paper proposes a new flash-aware write-buffer management algorithm, called PUD-aware LRU algorithm (PUD-LRU), based on the Predicted average Update Distance (PUD) as the key block replacement criterion on top of log-block FTL schemes. The main idea of PUD-LRU is to differentiate blocks and judiciously destage blocks based on their frequency and\n",
      "\n",
      "10. id: 5390bded20f70186a0f48368   score: 0.5161534   abstract: Solid-state drives (SSDs) are quickly becoming the default storage medium as the cost of NAND flash memory continues to drop. However, flash memory introduces new challenges, as data cannot be eciently updated in-place. To overcome the technology's limitations, SSDs incorporate a software Flash Translation Layer (FTL) that implements out-of-place updates, typically by storing data in a log-structured fashion. Despite a large number of existing FTL algorithms, SSD performance, predictability, and lifetime remain an issue, especially for the write-intensive workloads specific to database applications. In this paper, we show how to design FTLs that are more efficient by using the I/O write skew to guide data placement on flash memory. We model the relationship between data placement and write performance for basic I/O write patterns and detail the most important concepts of writing to flash\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691442\n",
      "index                                        5592553e0cf2aff368683bc1\n",
      "title               A framework for generalising the Newton method...\n",
      "authors                                            Jonathan H. Manton\n",
      "year                                                           2015.0\n",
      "venue                                           Numerische Mathematik\n",
      "references          5390a4cc20f70186a0e751e7;5390893e20f70186a0d93...\n",
      "abstract            The Newton iteration is a popular method for m...\n",
      "id                                                            1691442\n",
      "clustered_labels                                                    2\n",
      "Name: 1691442, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909f2d20f70186a0e3850a   score: 0.92121595   abstract: An improvement of the local order of convergence is presented to increase the efficiency of the iterative method with an appropriate number of evaluations of the function and its derivative. The third and fourth order of known two-step like Newton methods have been improved and the efficiency has also been increased.\n",
      "\n",
      "2. id: 559247ed0cf28b1a968ff5ff   score: 0.8989441   abstract: We discuss the question of which features and/or properties make a method for solving a given problem belong to the \\\"Newtonian class.\\\" Is it the strategy of linearization (or perhaps, second-order approximation) of the problem data (maybe only part of the problem data)? Or is it fast local convergence of the method under natural assumptions and at a reasonable computational cost of its iteration? We consider both points of view, and also how they relate to each other. In particular, we discuss abstract Newtonian frameworks for generalized equations, and how a number of important algorithms for constrained optimization can be related to them by introducing structured perturbations to the basic Newton iteration. This gives useful tools for local convergence and rate-of-convergence analysis of various algorithms from unified perspectives, often yielding sharper results than provided by ot\n",
      "\n",
      "3. id: 53908b4920f70186a0dbc2a2   score: 0.83815056   abstract: This paper proposes a new Newton-like method which defines new iterates using a linear system with the same coefficient matrix in each iterate. while the correction is performed on the right-hand-side vector of the Newton system. In this way a method is obtained which is less costly than the Newton method and faster than the fixed Newton method. Local convergence is proved for nonsingular systems. The influence of the relaxation parameter is analyzed and explicit formulae for the selection of an optimal parameter are presented. Relevant numerical examples are used to demonstrate the advantages of the proposed method.\n",
      "\n",
      "4. id: 539089ab20f70186a0d958f3   score: 0.8174288   abstract: An important class of optimization problems involve minimizing a cost function on a Lie group. In the case where the Lie group is non-compact there is no natural choice of a Riemannian metric and it is not possible to apply recent results on the optimization of functions on Riemannian manifolds. In this paper the invariant structure of a Lie group is exploited to provide a strong interpretation of a Newton iteration on a general Lie group. The paper unifies several previous algorithms proposed in the literature in a single theoretical framework. Local asymptotic quadratic convergence is proved for the algorithms considered.\n",
      "\n",
      "5. id: 5390a0b720f70186a0e4fd1c   score: 0.81375694   abstract: A finer optimal convergence condition for the local convergence of Newton's method is obtained using more precise error estimates. A condition for finer computational complexity is also obtained.\n",
      "\n",
      "6. id: 5390975920f70186a0dfdb1c   score: 0.79755676   abstract: Recently, a modification of the Newton method for finding a zero of a univariate function with local cubic convergence has been introduced. Here, we extend this modification to the multi-dimensional case, i.e., we introduce a modified Newton method for vector functions that converges locally cubically, without the need to compute higher derivatives. The case of multiple roots is not treated. Per iteration the method requires one evaluation of the function vector and solving two linear systems with the Jacobian as coefficient matrix, where the Jacobian has to be evaluated twice. Since the additional computational effort is nearly that of an additional Newton step, the proposed method is useful especially in difficult cases where the number of iterations can be reduced by a factor of two in comparison to the Newton method. This much better convergence is indeed possible as shown by a numer\n",
      "\n",
      "7. id: 539096cb20f70186a0df7b31   score: 0.7897206   abstract: This paper studies convergence properties of regularized Newton methods for minimizing a convex function whose Hessian matrix may be singular everywhere. We show that if the objective function is LC2, then the methods possess local quadratic convergence under a local error bound condition without the requirement of isolated nonsingular solutions. By using a backtracking line search, we globalize an inexact regularized Newton method. We show that the unit stepsize is accepted eventually. Limited numerical experiments are presented, which show the practical advantage of the method.\n",
      "\n",
      "8. id: 539087dd20f70186a0d62f3a   score: 0.783823   abstract: This note presents a simplified algorithm for achieving acceleration of convergence in Newton's method.\n",
      "\n",
      "9. id: 539098dc20f70186a0e0d94b   score: 0.7574357   abstract: This paper studies Newton-type methods for minimization of partly smooth convex functions. Sequential Newton methods are provided using local parameterizations obtained from **-Lagrangian theory and from Riemannian geometry. The Hessian based on the **-Lagrangian depends on the selection of a dual parameter g; by revealing the connection to Riemannian geometry, a natural choice of g emerges for which the two Newton directions coincide. This choice of g is also shown to be related to the least-squares multiplier estimate from a sequential quadratic programming (SQP) approach, and with this multiplier, SQP gives the same search direction as the Newton methods.\n",
      "\n",
      "10. id: 5390a2e820f70186a0e66a40   score: 0.7538293   abstract: In this paper , we provide a new method through modifying the iterative matrix of Newton method . For this modification, we prove general local convergence results .The new method improves the condition number of the Hessian . The numeric results shows that the new method avoid the singular phenomenon.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674132\n",
      "index                                        55911fd70cf232eb904fadc7\n",
      "title               Evaluating the importance of personal informat...\n",
      "authors             Ake Osothongs, Vorapong Suppakitpaisan, Noboru...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 9th International Conferenc...\n",
      "references          558af143612c41e6b9d3e114;5390a8dc20f70186a0e9e...\n",
      "abstract            Personal information is used as a medium for s...\n",
      "id                                                            1674132\n",
      "clustered_labels                                                    3\n",
      "Name: 1674132, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a6d920f70186a0e86a29   score: 0.23775287   abstract: Personal data is a valuable asset for service providers. To collect such data, free services are offered to users, for whom the risk of loosing privacy by subscribing to a service is often not clear. Although the services are free in terms of money, the user does not know how much he or she actually pays for a given service when allowing his or her data to be collected, unaware of taking a significant privacy risk by doing so. In practice, this risk is even not taken into account when deciding how long the data will be retained; the service provider simply wants to optimize the total worth of the stored data by retaining the data as long as possible. In this paper, we express the privacy risk for the user in terms of such a retention period; the user wants to optimize its privacy by allowing the data to be retained as short as possible. Now, in stead of only considering the interests of \n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3d34d   score: 0.1260303   abstract: Most online service providers offer free services to users and in part, these services collect and monetize personally identifiable information (PII), primarily via targeted advertisements. Against this backdrop of economic exploitation of PII, it is vital to understand the value that users put to their own PII. Although studies have tried to discover how users value their privacy, little is known about how users value their PII while browsing, or the exploitation of their PII. Extracting valuations of PII from users is non-trivial - surveys cannot be relied on as they do not gather information of the context where PII is being released, thus reducing validity of answers. In this work, we rely on refined Experience Sampling - a data collection method that probes users to valuate their PII at the time and place where it was generated in order to minimize retrospective recall and hence inc\n",
      "\n",
      "3. id: 5390a45620f70186a0e72144   score: 0.08647765   abstract: The market for consumer information is already a lively market, where consumer information and consumer profile data are often among the most valuable assets owned by online retailers. The value of such commodity derives from the ability of firms to identify consumers and charge them personalized prices [1]. We argue that if consumers’ identity and personal information is such a valuable asset, should not consumers benefit from their asset as well? In this paper, we propose a negotiation process between an online consumer agent and an online seller. The online consumer agent acts on behalf of consumers to maximize their social welfare. In our model, the agent derives a quantified privacy risk for each private data and uses it to determine a cost premium value to make the bargaining process manageable. We also provide a computational example to evaluate the model.\n",
      "\n",
      "4. id: 5390bfa220f70186a0f54b12   score: 0.0433658   abstract: Currently personal data gathering in online markets is done on a far larger scale and much cheaper and faster than ever before. Within this scenario, a number of highly relevant companies for whom personal data is the key factor of production have emerged. However, up to now, the corresponding economic analysis has been restricted primarily to a qualitative perspective linked to privacy issues. Precisely, this paper seeks to shed light on the quantitative perspective, approximating the value of personal information for those companies that base their business model on this new type of asset. In the absence of any systematic research or methodology on the subject, an ad hoc procedure is developed in this paper. It starts with the examination of the accounts of a number of key players in online markets. This inspection first aims to determine whether the value of personal information datab\n",
      "\n",
      "5. id: 5390ada620f70186a0ec373f   score: 0.015072629   abstract: Collection and analysis of personal information is among the most far-reaching developments in online retail practices. While the potential value of harnessing data about people is expected to improve the online service offerings, it raises reasonable concerns about privacy. Rather than cutting off opportunities to make personal data available for enhancing online services, we introduce a model where people can opt out to share personal information for a payoff value that balances out the costs of privacy. Our model is based on intelligent agents working on behalf of consumers to maximize their benefit. The analysis of the model and a proof of concept implementation are presented in this paper.\n",
      "\n",
      "6. id: 5390b71120f70186a0f1ec48   score: 0.012479699   abstract: We present an empirical study of personal information revealed in public profiles of people who use multiple Online Social Networks (OSNs). This study aims to examine how users reveal their personal information across multiple OSNs. We first consider the number of publicly available attributes in public profiles, based on various demographics and show a correlation between the amount of information revealed in OSN profiles and specific occupations and the use of pseudonyms. Then, we measure the complementarity of information across OSNs and contrast it with our observations about users who share a larger amount of information. We also measure the consistency of information revelation patterns across OSNs, finding that users have preferred patterns when revealing information across OSNs. To evaluate the quality of aggregated profiles we introduce a consistency measure for attribute values\n",
      "\n",
      "7. id: 5390bded20f70186a0f49a62   score: 0.009196552   abstract: Along with the dramatic growth in the use of the Internet, online services such as social networking sites have become more and more popular and sophisticated. However, at the same time, the collection of life-log data, and in particular the use of this collected personal information, has widely raised public concern regarding privacy protection. In this study, we divide personal information into three categories and find that people are more sensitive about their personal identifiers than other types of information such as demographic information or preferences. We also attempt to measure the value compatibility with online services by using survey questions that ask online users how much they are willing to pay to protect (limited disclosure or complete non-disclosure) their personal information. The responses revealed that people are willing to shoulder the cost to keep using online s\n",
      "\n",
      "8. id: 5390a5dc20f70186a0e7f665   score: 0.0077555366   abstract: Consider an \"information market\" where private and potentially sensitive data are collected, treated as commodity and processed into aggregated information with commercial value. Access and processing privileges of such data can be specified by enforceable \"service contracts\" and different contract rules can be associated with different data fields.Clearly the sources of such data, which may include companies, organizations and individuals, must be protected against loss of privacy and confidentiality. However, mechanisms for ensuring privacy per data source or data field do not scale well due to state information that needs to be maintained. We propose a scalable approach to this problem which assures data sources that the information will only be revealed as an aggregate or as part of a large set (akin of k-anonymity constraints).In particular, this work presents a model and protocols \n",
      "\n",
      "9. id: 5390b7fe20f70186a0f2628b   score: 0.007665875   abstract: A lot of information has recently been collected and the need to put it to secondary use is expanding. This is because a lot of useful knowledge is contained in it. There are always privacy concerns with the secondary use of personal information. k-anonymization is a tool that enables us to release personal information in a manner that is privacy-protected. In classical k-anonymization, side information, which is termed generalization hierarchies, is always needed. In addition, the quality of k-anonymized data has always been a central problem in the area because information loss is an inherent feature of anonymization. This paper proposes a new scheme in which generalization hierarchies are automatically constructed by input information. This scheme not only contributes to reducing the cost of operations for preparing side information, but also to increasing the quality of k-anonymizati\n",
      "\n",
      "10. id: 5390ae2e20f70186a0ec777c   score: 0.0069576087   abstract: The increasing use of fast and efficient data mining algorithms in huge collections of personal data, facilitated through the exponential growth of technology, in particular in the field of electronic data storage media and processing power, has raised serious ethical, philosophical and legal issues related to privacy protection. To cope with these concerns, several privacy preserving methodologies have been proposed, classified in two categories, methodologies that aim at protecting the sensitive data and those that aim at protecting the mining results. In our work, we focus on sensitive data protection and compare existing techniques according to their anonymity degree achieved, the information loss suffered and their performance characteristics. The $\\ell$-diversity principle is combined with k-anonymity concepts, so that background information can not be exploited to successfully att\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1715097\n",
      "index                                        55323c4345cec66b6f9dba63\n",
      "title               Multi-parameter Analysis for Local Graph Parti...\n",
      "authors             Édouard Bonnet, Bruno Escoffier, Vangelis Th. ...\n",
      "year                                                           2015.0\n",
      "venue                                                    Algorithmica\n",
      "references                                   558b29e2612c41e6b9d45479\n",
      "abstract            We study the parameterized complexity of a bro...\n",
      "id                                                            1715097\n",
      "clustered_labels                                                    2\n",
      "Name: 1715097, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bed320f70186a0f4e1f7   score: 0.966347   abstract: The Maximum Common Induced Subgraph problem (MCIS) takes a pair of graphs as input and asks for a graph of maximum order that is isomorphic to an induced subgraph of each of the input graphs. The problem is NP-hard in general, and remains so on many graph classes including graphs of bounded treewidth. In the framework of parameterized complexity, the latter assertion means that MCIS is W[1]-hard when parameterized by the treewidths of input graphs. A classical graph parameter that has been used recently in many parameterization problems is Vertex Cover. In this paper we prove constructively that MCIS is fixed-parameter tractable when parameterized by the vertex cover numbers of the input graphs. Our algorithm is also an improved exact algorithm for the problem on instances where the minimum vertex cover is small compared to the order of input graphs.\n",
      "\n",
      "2. id: 5390893e20f70186a0d93940   score: 0.95846003   abstract: In this paper, we consider a large class of vertex partitioning problems and apply to them the theory of algorithm design for problems restricted to partial k-trees. We carefully describe the details of algorithms and analyze their complexity in an attempt to make the algorithms feasible as solutions for practical applications.We give a precise characterization of vertex partitioning problems, which include domination, coloring and packing problems, and their variants. Several new graph parameters are introduced as generalizations of classical parameters. This characterization provides a basis for a taxonomy of a large class of problems, facilitating their common algorithmic treatment and allowing their uniform complexity classification.We present a design methodology of practical solution algorithms for generally $\\NP$-hard problems when restricted to partial k-trees (graphs with treewi\n",
      "\n",
      "3. id: 5390b8d720f70186a0f2c52b   score: 0.94825053   abstract: We investigate the computational complexity of the Densest-k-Subgraph (DkS) problem, where the input is an undirected graph G=(V,E) and one wants to find a subgraph on exactly k vertices with a maximum number of edges. We extend previous work on DkS by studying its parameterized complexity. On the positive side, we show that, when fixing some constant minimum density μ of the sought subgraph, DkS becomes fixed-parameter tractable with respect to either of the parameters maximum degree and h-index of G. Furthermore, we obtain a fixed-parameter algorithm for DkS with respect to the combined parameter \"degeneracy of G and |V|−k\". On the negative side, we find that DkS is W[1]-hard with respect to the combined parameter \"solution size k and degeneracy of G\". We furthermore strengthen a previous hardness result for DkS [Cai, Comput. J., 2008] by showing that for every fixed μ, 0μG contains a \n",
      "\n",
      "4. id: 5390af8920f70186a0ed0352   score: 0.93257624   abstract: We compare the fixed parameter complexity of various variants of coloring problems (including List Coloring, Precoloring Extension, Equitable Coloring, L(p,1)-Labeling and Channel Assignment) when parameterized by treewidth and by vertex cover number. In most (but not all) cases we conclude that parametrization by the vertex cover number provides a significant drop in the complexity of the problems.\n",
      "\n",
      "5. id: 5390b00c20f70186a0ed5252   score: 0.93171144   abstract: We study the parameterized complexity of the problems of determining whether a graph contains a k-edge subgraph (k-vertex induced subgraph) that is a @P-graph for @P-graphs being one of the following four classes of graphs: Eulerian graphs, even graphs, odd graphs, and connected odd graphs. We also consider the parameterized complexity of their parametric dual problems. For these sixteen problems, we show that eight of them are fixed parameter tractable and four are W[1]-hard. Our main techniques are the color-coding method of Alon, Yuster and Zwick, and the random separation method of Cai, Chan and Chan.\n",
      "\n",
      "6. id: 5390a4cc20f70186a0e75b3a   score: 0.9253649   abstract: We compare the fixed parameter complexity of various variants of coloring problems (including List Coloring, Precoloring Extension, Equitable Coloring, L (p ,1)-Labeling and Channel Assignment ) when parameterized by treewidth and by vertex cover number. In most (but not all) cases we conclude that parametrization by the vertex cover number provides a significant drop in the complexity of the problems.\n",
      "\n",
      "7. id: 5390b48420f70186a0efc170   score: 0.9186263   abstract: In this paper, we investigate the parameterized complexity of the problem of finding k edges (vertices) in a graph G to form a subgraph (respectively, induced subgraph) H such that H belongs to one the following four classes of graphs: even graphs, Eulerian graphs, odd graphs, and connected odd graphs. We also study the parameterized complexity of their parametric dual problems. Among these sixteen problems, we show that eight of them are fixed parameter tractable and four are W[1]-hard. Our main techniques are the color-coding method of Alon, Yuster and Zwick, and the random separation method of Cai, Chan and Chan.\n",
      "\n",
      "8. id: 5390b2fc20f70186a0eed949   score: 0.8893368   abstract: In this article we study the parameterized complexity of problems consisting in finding degree-constrained subgraphs, taking as the parameter the number of vertices of the desired subgraph. Namely, given two positive integers d and k, we study the problem of finding a d-regular (induced or not) subgraph with at most k vertices and the problem of finding a subgraph with at most k vertices and of minimum degree at least d. The latter problem is a natural parameterization of the d-girth of a graph (the minimum order of an induced subgraph of minimum degree at least d). We first show that both problems are fixed-parameter intractable in general graphs. More precisely, we prove that the first problem is W[1]-hard using a reduction from Multi-Color Clique. The hardness of the second problem (for the non-induced case) follows from an easy extension of an already known result. We then provide ex\n",
      "\n",
      "9. id: 5390bb7b20f70186a0f3fa68   score: 0.87914675   abstract: We present a method for reducing the treewidth of a graph while preserving all of its minimal s-t separators up to a certain fixed size k. This technique allows us to solve s-t Cut and Multicut problems with various additional restrictions (e.g., the vertices being removed from the graph form an independent set or induce a connected graph) in linear time for every fixed number k of removed vertices. Our results have applications for problems that are not directly defined by separators, but the known solution methods depend on some variant of separation. For example, we can solve similarly restricted generalizations of Bipartization (delete at most k vertices from G to make it bipartite) in almost linear time for every fixed number k of removed vertices. These results answer a number of open questions in the area of parameterized complexity. Furthermore, our technique turns out to be rele\n",
      "\n",
      "10. id: 53908bde20f70186a0dc92b1   score: 0.86130506   abstract: For a family F of graphs and a nonnegative integer k, F + ke and F - ke, respectively, denote the families of graphs that can be obtained from F graphs by adding and deleting at most k edges, and F + kv denotes the family of graphs that can be made into F graphs by deleting at most k vertices.This paper is mainly concerned with the parameterized complexity of the vertex colouring problem on F + ke, F - ke and F - kv for various families F of graphs. In particular, it is shown that the vertex colouring problem is fixed-parameter tractable (linear time for each fixed k) for split + ke graphs and split - ke graphs, solvable in polynomial time for each fixed k but W[1]-hard for split + kv graphs. Furthermore, the problem is solvable in linear time for bipartite + 1v graphs and bipartite + 2e graphs but, surprisingly, NP-complete for bipartite + 2v graphs and bipartite + 3e graphs.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672418\n",
      "index                                        55915b890cf232eb904fbe64\n",
      "title               Can't You Hear Me Knocking: Identification of ...\n",
      "authors             Mauro Conti, Luigi V. Mancini, Riccardo Spolao...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 5th ACM Conference on Data ...\n",
      "references          558b787c612c6b62e5e89499;5390995d20f70186a0e16...\n",
      "abstract            While smartphone usage become more and more pe...\n",
      "id                                                            1672418\n",
      "clustered_labels                                                    3\n",
      "Name: 1672418, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b4f2e612c41e6b9d48d5e   score: 0.717507   abstract: In this paper, we highlight a potential privacy threat in the current smartphone platforms, which allows any third party to collect a snapshot of installed applications without the user's consent. This can be exploited by third parties to infer various user attributes similar to what is done through tracking. We show that using only installed apps, user's gender, a demographic attribute that is frequently used in targeted advertising, can be instantly predicted with an accuracy around 70%, by training a classifier using established supervised learning techniques.\n",
      "\n",
      "2. id: 55912a210cf232eb904fb0e9   score: 0.717507   abstract: In this paper, we highlight a potential privacy threat in the current smartphone platforms, which allows any third party to collect a snapshot of installed applications without the user's consent. This can be exploited by third parties to infer various user attributes similar to what is done through tracking. We show that using only installed apps, user's gender, a demographic attribute that is frequently used in targeted advertising, can be instantly predicted with an accuracy around 70%, by training a classifier using established supervised learning techniques.\n",
      "\n",
      "3. id: 5390a05920f70186a0e498a4   score: 0.52354217   abstract: We analyze three new consumer electronic gadgets in order to gauge the privacy and security trends in mass-market UbiComp devices. Our study of the Slingbox Pro uncovers a new information leakage vector for encrypted streaming multimedia. By exploiting properties of variable bitrate encoding schemes, we show that a passive adversary can determine with high probability the movie that a user is watching via her Slingbox, even when the Slingbox uses encryption. We experimentally evaluated our method against a database of over 100 hours of network traces for 26 distinct movies. Despite an opportunity to provide significantly more location privacy than existing devices, like RFIDs, we find that an attacker can trivially exploit the Nike+iPod Sport Kit's design to track users; we demonstrate this with a GoogleMaps-based distributed surveillance system. We also uncover security issues with the \n",
      "\n",
      "4. id: 558b3088612c41e6b9d4632d   score: 0.3465413   abstract: Instant messaging services are quickly becoming the most dominant form of communication among consumers around the world. Apple iMessage, for example, handles over 2 billion messages each day, while WhatsApp claims 16 billion messages from 400 million international users. To protect user privacy, many of these services typically implement end-to-end and transport layer encryption, which are meant to make eavesdropping infeasible even for the service providers themselves. In this paper, however, we show that it is possible for an eavesdropper to learn information about user actions, the language of messages, and even the length of those messages with greater than 96% accuracy despite the use of state-of-the-art encryption technologies simply by observing the sizes of encrypted packets. While our evaluation focuses on Apple iMessage, the attacks are completely generic and we show how they \n",
      "\n",
      "5. id: 5390bae620f70186a0f3bf91   score: 0.31079894   abstract: Recent years have seen an explosive growth in the number of mobile devices such as smart phones and tablets. This has resulted in a growing need of the operators to understand the usage patterns of the mobile apps used on these devices. Previous studies in this area have relied on volunteers using instrumented devices or using fields in the HTTP traffic such as User-Agent to identify the apps in network traces. However, the results of the former approach are difficult to be extrapolated to real-world scenario while the latter approach is not applicable to platforms like Android where developers generally use generic strings, that can not be used to identify the apps, in the User-Agent field. In this paper, we present a novel way of identifying Android apps in network traces using mobile in-app advertisements. Our preliminary experiments with real world traces show that this technique is \n",
      "\n",
      "6. id: 5390bfa220f70186a0f53da3   score: 0.29380956   abstract: Smartphone ownership and usage has seen massive growth in the past years. As a result, their users have attracted unwanted attention from malicious entities and face many security challenges, including malware and privacy issues. This paper concentrates on IDS carefully designed to cater to the security needs of modern mobile platforms. Two main research issues are tackled: (a) the definition of an architecture which can be used towards implementing and deploying such a system in a dual-mode (host/cloud) manner and irrespectively of the underlying platform, and (b) the evaluation of a proof-of-concept anomaly-based IDS implementation that incorporates dissimilar detection features, with the aim to assess its performance qualities when running on state-of-the-art mobile hardware on the host device and on the cloud. This approach allows us to argue in favor of a hybrid host/cloud IDS arran\n",
      "\n",
      "7. id: 558b0b56612c41e6b9d414c8   score: 0.25590113   abstract: In recent years, not only has the number of wireless devices significantly increased, but also their level of integration into daily life. Devices ranging from laptops and cell phones to cameras and TVs are now connected to networks. As the ability to secure these devices advances, public and private organizations are adopting and establishing both public and private wireless networks. Wireless networks ease this integration, but not without cost. The nature of this medium presents challenges. This work aims to demonstrate and codify a mechanism by which we can increase our ability to verify and validate the identity of the device through encrypted data observation. This paper focuses on device identification. Multiple supervised learning techniques were vetted and a reference implementation was constructed and executed using real traffic. Incremental learning methods were identified as \n",
      "\n",
      "8. id: 558acff6612c41e6b9d3aa6e   score: 0.24472383   abstract: We propose a new approach for authenticating users of mobile devices that is based on analyzing the user's touch interaction with common user interface (UI) elements, e.g., buttons, checkboxes and sliders. Unlike one-off authentication techniques such as passwords or gestures, our technique works continuously in the background while the user uses the mobile device. To evaluate our approach's effectiveness, we conducted a lab study with 20 participants, where we recorded their interaction traces on a mobile phone and a tablet (e.g., touch pressure, locations), while they filled out electronic forms populated with UI widgets. Using classification methods based on SVM and Random Forests, we achieved an average of 97.9% accuracy with a mobile phone and 96.79% accuracy with a tablet for single user classification, demonstrating that our technique has strong potential for real-world use. We be\n",
      "\n",
      "9. id: 558ce811e4b0b692772c9ba8   score: 0.24328269   abstract: We develop and discuss automated and self-adaptive systems for detecting and classifying botnets based on machine learning techniques and integration of human expertise. The proposed concept is purely passive and is based on analyzing information collected at three levels: (i) the payload of single packets received, (ii) observed access patterns to a darknet at the level of network traffic, and (iii) observed contents of TCP/IP traffic at the protocol level. We illustrate experiments based on real-life data collected with a darknet set up for this purpose to show the potential of the proposed concept for Levels (i) and (ii). As darknets cannot capture TCP/IP traffic data, we use a small spamtrap in our experiments at Level (iii). Strictly speaking, this approach for Level (iii) is not purely passive. However, traffic moving through a network could potentially be analyzed in a similar way\n",
      "\n",
      "10. id: 5390baa120f70186a0f37ad7   score: 0.22918998   abstract: The overall network traffic patterns generated by today's smartphones result from the typically large and diverse set of installed applications. In addition to the traffic generated by the user, most applications generate characteristic traffic from their background activities, such as periodic update requests or server synchronisation. Although the encryption of transmitted data in 3G networks prevents an eavesdropper from analysing the content, periodic traffic patterns leak side-channel information like timing and data volume. In this work, we extract such side-channel features from network traffic generated from the most popular applications, such as Facebook, WhatsApp, Skype, Dropbox, and others, and evaluate whether they can be used to reliably identify a smartphone. By computing fingerprints from approx,6,hours of background traffic, we show that 15 minutes of monitored traffic su\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674992\n",
      "index                                        5591439c0cf232eb904fb834\n",
      "title               IDSense: A Human Object Interaction Detection ...\n",
      "authors                        Hanchuan Li, Can Ye, Alanson P. Sample\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390a6b120f70186a0e83dce;5390aefc20f70186a0ece...\n",
      "abstract            In order to enable unobtrusive human object in...\n",
      "id                                                            1674992\n",
      "clustered_labels                                                    0\n",
      "Name: 1674992, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b44620f70186a0ef8a03   score: 0.46971792   abstract: We propose a gesture recognition technique based on RFID: cheap and unintrusive passive RFID tags can be easily attached to or interweaved into user clothes, which are then read by RFID antennas. These readings can be used to recognize hand gestures, which enable interaction with applications in an RFID-enabled environment. For instance, it allows people to interact with large displays in public collaboration spaces without the need to carry a dedicated device. We propose the use of multiple hypothesis tracking and the use of subtag count information to track the motion patterns of passive RFID tags. To the best of our knowledge, this work is the first on motion pattern tracking using passive RFID tags. Despite the reading uncertainties inherent in passive RFID technology, our experiments show that the proposed gesture recognition technique has an accuracy of up to 93%.\n",
      "\n",
      "2. id: 5390990f20f70186a0e0fba9   score: 0.21816926   abstract: Recent research has explored ways to obtain and use knowledge of person-object interactions. We present a novel pair of wearables, a glove and a bracelet, that detect when users interact with unobtrusively tagged objects. The glove can also report whether the grasp was with the palm or the fingertips. Both devices have been built and deployed. We present the requirements, design and early experiences.\n",
      "\n",
      "3. id: 5390aa0f20f70186a0ea89dd   score: 0.16559312   abstract: We present a system which allows to request information on physical objects by taking a picture of them. This way, using a mobile phone with integrated camera, users can interact with objects or \"things\" in a very simple manner. A further advantage is that the objects themselves don't have to be tagged with any kind of markers. At the core of our system lies an object recognition method, which identifies an object from a query image through multiple recognition stages, including local visual features, global geometry, and optionally also metadata such as GPS location. We present two applications for our system, namely a slide tagging application for presentation screens in smart meeting rooms and a cityguide on a mobile phone. Both systems are fully functional, including an application on the mobile phone, which allows simplest point-and-shoot interaction with objects. Experiments evalua\n",
      "\n",
      "4. id: 5390a6b120f70186a0e83dce   score: 0.12863456   abstract: We explore a dense sensing approach that uses RFID sensor network technology to recognize human activities. In our setting, everyday objects are instrumented with UHF RFID tags called WISPs that are equipped with accelerometers. RFID readers detect when the objects are used by examining this sensor data, and daily activities are then inferred from the traces of object use via a Hidden Markov Model. In a study of 10 participants performing 14 activities in a model apartment, our approach yielded recognition rates with precision and recall both in the 90% range. This compares well to recognition with a more intrusive short-range RFID bracelet that detects objects in the proximity of the user; this approach saw roughly 95% precision and 60% recall in the same study. We conclude that RFID sensor networks are a promising approach for indoor activity monitoring.\n",
      "\n",
      "5. id: 5390a2be20f70186a0e65741   score: 0.109134786   abstract: We propose a novel method to recognize a user’s activities of daily living with accelerometers and rfid sensor. Two wireless accelerometers are used for the classification of 5 human body states using decision tree, and detection of RFID tagged objects with hand movement provides additional object related hand motion information. To do this, we used Bluetooth based wireless triaxial accelerometers and iGrabber which is a glove type RFID reader. Our experiments show that our method can be applicable to a real environment with strong confidence.\n",
      "\n",
      "6. id: 5390a96f20f70186a0ea44a4   score: 0.059210252   abstract: We study activity recognition using 104 hours of annotated data collected from a person living in an instrumented home. The home contained over 900 sensor inputs, including wired reed switches, current and water flow inputs, object and person motion detectors, and RFID tags. Our aim was to compare different sensor modalities on data that approached \"real world\" conditions, where the subject and annotator were unaffiliated with the authors. We found that 10 infra-red motion detectors outperformed the other sensors on many of the activities studied, especially those that were typically performed in the same location. However, several activities, in particular \"eating\" and \"reading\" were difficult to detect, and we lacked data to study many fine-grained activities. We characterize a number of issues important for designing activity detection systems that may not have been as evident in prio\n",
      "\n",
      "7. id: 5390a06e20f70186a0e4ccec   score: 0.0586686   abstract: Over the course of a day a human interacts with tens or hundreds of individual objects. Many of these articles are nomadic, relying on human memory to manually index, inventory, organize, search, and locate them. However, Radio Frequency Identification (RFID) tags hold great promise for automating these tasks. While originally envisioned for managing supply chains and store inventories, RFID tags support the properties necessary for helping humans to manage their objects. This paper presents Sherlock, a system that leverages RFID tags for human-object interaction. Sherlock combines concepts from sensors, radar technology, and computer graphics to implement a novel localization and visualization system for everyday objects. At the heart of Sherlock is a new RFID localization technique that uses steerable antennas to sweep a room, discovering, localizing and indexing tagged objects. In res\n",
      "\n",
      "8. id: 5390b2fc20f70186a0eee982   score: 0.056756895   abstract: This paper introduces a novel approach to enhancing safety through RFID technology, location tracking, and monitoring person-object interaction. We design and develop RFID-based wearable devices for (1)tracking people’s locations, (2)monitoring person-object interactions, and (3)tracking objects’ locations. An intelligent object reminder and safety alert system is proposed to relief the common safety-related worries many of us face in our everyday lives – “Where did I leave my keys?”, “Did I turn off the stove?”, or “Did I close all the windows in my house?” etc. Experimental results on the precision of object identification and location tracking are also presented.\n",
      "\n",
      "9. id: 5390979920f70186a0e00fb6   score: 0.041539967   abstract: The human-object interaction signatures approach to object recognition proposes to find and classify objects in a scene by referring solely to related human actions. This method specifically addresses the problems and opportunities encountered in the typical smart-home monitoring system: wide-angle views of cluttered scenes with frequent, repeated human activity. Traditional shape-based object recognition tends to fail under these conditions owing to the unconstrained variety of object shapes, target objects' low resolution, and the partial occlusion of target objects by other scene objects. In this new approach, the system labels objects using evidence accumulated over time and multiple instances of human-object interactions. Furthermore, it uses partial occlusions of the person by an object to refine the object label's position. Preliminary experiments with this approach have investiga\n",
      "\n",
      "10. id: 53908bcc20f70186a0dc54b0   score: 0.041152887   abstract: Passive RFID technology and unobtrusive Bluetooth-enabled active tags are means to augment products and everyday objects with information technology invisible to human users. This paper analyzes general interaction patterns in such pervasive computing settings where information about the user's context is derived by a combination of active and passive tags present in the user's environment. The concept of invisible preselection of interaction partnersbased on the user's context is introduced. It enables unobtrusive interaction with smart objects in that it combines different forms of association, e.g. implicit and user initiated association, by transferring interaction stubs to mobile devices based on the user's current situation. Invisible preselection can also be used for remote interaction. By assigning phone numbers to smart objects, we propose making this remote user interaction wit\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672799\n",
      "index                                        559150420cf232eb904fbb65\n",
      "title                    The Internet of Things Has a Gateway Problem\n",
      "authors             Thomas Zachariah, Noah Klugman, Bradford Campb...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 16th International Workshop...\n",
      "references          5390893e20f70186a0d9272e;5390a96e20f70186a0ea3...\n",
      "abstract            The vision of an Internet of Things (IoT) has ...\n",
      "id                                                            1672799\n",
      "clustered_labels                                                    3\n",
      "Name: 1672799, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b44620f70186a0ef89fb   score: 0.9223426   abstract: The Internet of Things (IoT) concept is being widely presented as the next revolution toward massively distributed information, where any real-world object can automatically participate in the Internet and thus be globally discovered and queried. Despite the consensus on the great potential of the concept and the significant progress in a number of enabling technologies, there is a general lack of an integrated vision on how to realize it. This paper examines the technologies that will be fundamental for realizing the IoT and proposes an architecture that integrates them into a single platform. The architecture introduces the use of the Smart Object framework to encapsulate radio-frequency identification (RFID), sensor technologies, embedded object logic, object ad-hoc networking, and Internet-based information infrastructure. We evaluate the architecture against a number of energy-based\n",
      "\n",
      "2. id: 5390aefb20f70186a0ecc2f3   score: 0.7893961   abstract: In this article, we present the current status of the Internet of Things, and discuss how the current situation of many \"Intranets\" of Things should evolve into a much more integrated and heterogeneous system. We also summarize what in our opinion are the main wireless- and mobility-related technical challenges that lie ahead, and outline some initial ideas on how such challenges can be addressed in order to facilitate the IoT's development and acceptance in the next few years. We also describe a case study on the IoT protocol architecture.\n",
      "\n",
      "3. id: 5390bded20f70186a0f4a56d   score: 0.75904685   abstract: As the Internet, in due course, developed, more and more computing devices became connected, in particular mobile devices. The internet-of-things (IoT) is emerging and well underway, and the majority of internet traffic will be dominated by things rather than by computers built to fit the needs of humans. Connectivity is a prerequisite to machine-to-machine (M2M) communications featuring many-to-many, low-power, quality-of-service (QoS), fail over, multi-radio, multi-transport and orders of magnitude more nodes than the conventional counterpart, which makes management become extremely difficult. Development, adoption, and implementation of connectivity middleware are necessitated to facilitate the growth of M2M applications, and bring us closer to the IoT vision. The contribution of the paper is two-fold. First, it offers the methodological lessons learned for addressing the connectivity\n",
      "\n",
      "4. id: 5390b71120f70186a0f1e69c   score: 0.4639603   abstract: Recent activity in the field of Internet-of-Things experimentation has focused on the federation of discrete testbeds, thus placing less effort in the integration of other related technologies, such as smartphones; also, while it is gradually moving to more application-oriented paths, such as urban settings, it has not dealt in large with applications having social networking features. We argue here that current IoT infrastructure, testbeds and related software technologies should be used in such a context, capturing real-world human mobility and social networking interactions, for use in evaluating and fine-tuning realistic mobility models and designing human-centric applications. We discuss a system for producing traces for a new generation of human-centric applications, utilizing technologies such as Bluetooth and focusing on human interactions. We describe the architecture for this s\n",
      "\n",
      "5. id: 5390b00c20f70186a0ed5199   score: 0.42238548   abstract: The \"Web of Things\" is emerging as an exciting vision for seamlessly integrating everyday objects like home appliances, digital picture frames, health monitoring devices and energy meters into the Internet using the Web's well-known standards and blueprints. The key idea is to represent resources on these devices as URIs and use HTTP verbs (GET, PUT, POST, DELETE) as the uniform interface to manipulate them. Unfortunately, practical considerations such as bandwidth or energy constraints, firewalls/NATs and mobility pose interesting challenges in the realization of this ideal vision. This paper describes these challenges, identifies some potential solutions and presents the design and implementation of a gateway-based network architecture to address these concerns. To the best of our knowledge, it represents the first attempt within the Web of Things community to tackle these issues in a \n",
      "\n",
      "6. id: 559133990cf232eb904fb385   score: 0.36037648   abstract: The Internet of Things (IoT) is a paradigm in which smart objects actively collaborate with other physical and virtual resources available in the Internet. IoT environments are characterized by a high degree of heterogeneity, encompassing devices with different capabilities, functionalities, and network protocols. To address such a heterogeneity, some platforms have been proposed aiming at abstracting away the specificities of such devices and promoting interoperability among them. Nevertheless, the lack of standardization in IoT makes these platforms to often not properly address several important requirements in this context. In this context, reference architectures can define an initial set of building blocks for IoT environments and to provide a solid foundation for leveraging its wide adoption. In this paper, we introduce two recent reference architectures for IoT, namely the IoT Ar\n",
      "\n",
      "7. id: 558b06e9612c41e6b9d40afa   score: 0.35443306   abstract: Internet of Things (IoT) is a trending topic. Market analysts predict a large growth of the entire IoT market with an explosion of new products. In this paper, we argue how the data of these devices can be connected in an efficient and extensible way. For this purpose, we present glue.things a mashup platform for wiring data of Web-enabled IoT devices and Web services. The work carried out in this paper addresses concepts and technologies that are referred as \\\"web-friendly IoT technologies\\\". We elaborate aspects of device integration, real-time communication and data stream mashups. These aspects are discussed on features and technologies of the prototype implementation glue.things.\n",
      "\n",
      "8. id: 558af3ef612c41e6b9d3e71b   score: 0.34886694   abstract: For the last fifteen years, research explored the hardware, software, sensing, communication abstractions, languages, and protocols that could make networks of small, embedded devices---motes---sample and report data for long periods of time unattended. Today, the application and technological landscapes have shifted, introducing new requirements and new capabilities. Hardware has evolved past 8 and 16 bit microcontrollers: there are now 32 bit processors with lower energy budgets and greater computing capability. New wireless link layers have emerged, creating protocols that support rapid and efficient setup and teardown but introduce novel limitations that systems must consider. The time has come to look beyond optimizing networks of motes. We look towards new technologies such as Bluetooth Low Energy, Cortex M processors, and capable energy harvesting, with new application spaces such\n",
      "\n",
      "9. id: 5390b29820f70186a0ee8bd2   score: 0.3270232   abstract: The use of Internet Protocol (IP) technology in Wireless Sensor Network (WSN) is a key prerequisite for the realization of the Internet of Things (IoT) vision. The IPv6 over Low power Wireless Personal Area Networks (6LoWPAN) standard enables the use of IPv6 in networks of constrained devices. 6LowPAN enables the use of Service Oriented Architectures (SOAs) in WSN. The Internet Engineering Task Force (IETF) has defined the Constrained Application Protocol (CoAP), a web transfer protocol which provides several Hypertext Transfer Protocol (HTTP) functionalities, re-designed for constrained embedded devices. CoAP allows WSN applications to be built on top of Representational State Transfer (REST) architectures. This considerably eases the IoT application development and facilitates the integration of constrained devices with the Web. This work describes the prototype design and development \n",
      "\n",
      "10. id: 5390b29820f70186a0eea356   score: 0.32412842   abstract: Internet of Things (IoT) is considered as a future paradigm whose main challenge is to give an IP based transparent access to the huge number of services available as IoT resources. Due to the large number of resource-constrained devices and the dynamic nature of IoT environments, this integration problem becomes more intricate. The current state-of-the-art is mostly focused on the integration of IP enabled smart objects on the basis of Service Oriented Architecture (SOA) and the Representational State Transfer (REST) architectural style. However, beyond these approaches, we intend to address the flexible and adaptive composition of services in Very Large Scale (VLS) IoT systems by exploiting the concepts of service orchestration and choreography. In particular, we present an architectural model that enables efficient integration of services by locally orchestrating distributed web-enabl\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710582\n",
      "index                                        55323bdb45cec66b6f9dabd4\n",
      "title                        Privacy-preserving LOF outlier detection\n",
      "authors             Lu Li, Liusheng Huang, Wei Yang, Xiaohui Yao, ...\n",
      "year                                                           2015.0\n",
      "venue                               Knowledge and Information Systems\n",
      "references          558fed09612c29c89cd7c497;558d7f480cf222bc17bbf...\n",
      "abstract            LOF is a well-known approach for density-based...\n",
      "id                                                            1710582\n",
      "clustered_labels                                                    0\n",
      "Name: 1710582, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ab8820f70186a0eb06fe   score: 0.9755304   abstract: Outlier detection can find its tremendous applications in areas such as intrusion detection, fraud detection, and image processing. Among many outlier detection algorithms, LOF is a very important density-based algorithm in which one critical step is to find the k-distance neighbors. In some privacy preserving circumstances, the cooperation between data holders is necessary while the privacy of the participators should be guaranteed. In this paper, we focus on privacy preserving LOF. We propose a novel algorithm for privacy preserving k-distance neighbors search. Combining it with other secure multiparty computation techniques, we detect outliers by LOF in a privacy preserving way.\n",
      "\n",
      "2. id: 5390b2fc20f70186a0eefef2   score: 0.9623913   abstract: In this paper, we give approximate algorithms for privacy preserving distance based outlier detection for both horizontal and vertical distributions, which scale well to large datasets of high dimensionality in comparison with the existing techniques. In order to achieve efficient private algorithms, we introduce an approximate outlier detection scheme for the centralized setting which is based on the idea of Locality Sensitive Hashing. We also give theoretical and empirical bounds on the level of approximation of the proposed algorithms.\n",
      "\n",
      "3. id: 5390a1e620f70186a0e5a369   score: 0.953188   abstract: We give efficient protocols for secure and private k-nearest neighbor (k-NN) search, when the data is distributed between two parties who want to cooperatively compute the answers without revealing to each other their private data. Our protocol for the single-step k-NN search is provably secure and has linear computation and communication complexity. Previous work on this problem had a quadratic complexity, and also leaked information about the parties' inputs. We adapt our techniquesto also solve the general multi-step k-NN search, and describe a specific embodiment of it for the case of sequence data. The protocols and correctness proofs can be extended to suit other privacy-preserving data mining tasks, such as classification and outlier detection.\n",
      "\n",
      "4. id: 5390b2d620f70186a0eebae1   score: 0.9434519   abstract: We consider scenarios in which two parties, each in possession of a graph, wish to compute some algorithm on their joint graph in a privacy-preserving manner, that is, without leaking any information about their inputs except that revealed by the algorithm’s output. Working in the standard secure multi-party computation paradigm, we present new algorithms for privacy-preserving computation of APSD (all pairs shortest distance) and SSSD (single source shortest distance), as well as two new algorithms for privacy-preserving set union. Our algorithms are significantly more efficient than generic constructions. As in previous work on privacy-preserving data mining, we prove that our algorithms are secure provided the participants are “honest, but curious.”\n",
      "\n",
      "5. id: 53909fca20f70186a0e43e1d   score: 0.9105454   abstract: It is not surprising that there is strong interest in k- NN queries to enable clustering, classification and outlier- detection tasks. However, previous approaches to privacy- preserving k-NN are costly and can only be realistically ap- plied to small data sets. We provide efficient solutions for k-NN queries queries for vertically partitioned data. We pro- vide the first solution for the L (or Chessboard) metric as well as detailed privacy-preserving computation of all other Minkowski metrics. We enable privacy-preserving L by providing a solution to the Yao's Millionaire Problem with more than two parties. This is based on a new and practi- cal solution to Yao's Millionaire with shares. We also provide privacy-preserving algorithms for combinations of local met- rics into a global that handles the large dimensionality and diversity of attributes common in vertically partitioned data.\n",
      "\n",
      "6. id: 5390ada620f70186a0ec29b7   score: 0.8795612   abstract: Data objects which do not comply with the general behavior or model of the data are called Outliers. Outlier Detection in databases has numerous applications such as fraud detection, customized marketing, and the search for terrorism. However, the use of Outlier Detection for various purposes has raised concerns about the violation of individual privacy. Therefore, Privacy Preserving Outlier Detection must ensure that privacy concerns are addressed and balanced, so that the data analyst can get the benefits of outlier detection without being thwarted by legal counter-measures by privacy advocates. In this paper, we propose a technique for detecting outliers while preserving privacy, using hierarchical clustering methods. We analyze our technique to quantify the privacy preserved by this method and also prove that reverse engineering the perturbed data is extremely difficult.\n",
      "\n",
      "7. id: 5390b20120f70186a0ee4e8d   score: 0.84606963   abstract: In this paper, we study some parties - each has a private data set - want to conduct the outlier detection on their joint data set, but none of them want to disclose its private data to the other parties. We propose a linear transformation technique to design protocols of secure multivariate outlier detection in both horizontally and vertically distributed data models. While different from the most of previous techniques in a privacy preserving fashion for distance-based outliers detection, our focus is the technique in statistics for detecting outliers.\n",
      "\n",
      "8. id: 5390a63c20f70186a0e82877   score: 0.73411953   abstract: k -Nearest Neighbor (k -NN) mining aims to retrieve the k most similar objects to the query objects. It can be incorporated into many data mining algorithms, such as outlier detection, clustering, and k -NN classification. Privacy-preserving distributedk -NN is developed to address the issue while preserving the participants' privacy. Several two-party privacy-preserving k -NN mining protocols on horizontally partitioned data had been proposed, but they fail to deal with the privacy issue when the number of the participating parties is greater than two. This paper proposes a set of protocols that can address the privacy issue when there are more than two participants. The protocols are devised with the probabilistic public-key cryptosystem and the communicative cryptosystem as the core privacy-preserving infrastructure. The protocols' security is proved based on the Secure Multi-party Co\n",
      "\n",
      "9. id: 53909ee020f70186a0e32925   score: 0.61103386   abstract: We propose a multiparty protocol for various computations using comparator networks such as sorting and searching. By repeating the execution of a comparator, the proposed protocol can efficiently detect outlier values, without revealing them. In our scenario, all input values to a comparator network and the intermediate output from each comparator are kept secret assuming the presence of an honest majority. Possible application areas for the proposed protocol include statistical analysis while preserving the privacy of respondents.\n",
      "\n",
      "10. id: 5390b95420f70186a0f2d60a   score: 0.6109178   abstract: Many techniques have been proposed to protect the privacy of data outsourced for analysis by external parties. However, most of these techniques distort the underlying data properties, and therefore, hinder data mining algorithms from discovering patterns. The aim of Privacy-Preserving Data Mining (PPDM) is to generate a data-friendly transformation that maintains both the privacy and the utility of the data. We have proposed a novel privacy-preserving framework based on non-linear dimensionality reduction (i.e. non-metric multidimensional scaling) to perturb the original data. The perturbed data exhibited good utility in terms of distance-preservation between objects. This was tested on a clustering task with good results. In this paper, we test our novel PPDM approach on a classification task using a k-Nearest Neighbour (k-NN) classification algorithm. We compare the classification res\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1690429\n",
      "index                                        559124260cf232eb904faf46\n",
      "title               Conductive rubber electrodes for earphone-base...\n",
      "authors                 Hiroyuki Manabe, Masaaki Fukumoto, Tohru Yagi\n",
      "year                                                           2015.0\n",
      "venue                               Personal and Ubiquitous Computing\n",
      "references          5390b44620f70186a0ef83c2;53908a4020f70186a0d9d...\n",
      "abstract            An eartip made of conductive rubber that also ...\n",
      "id                                                            1690429\n",
      "clustered_labels                                                    0\n",
      "Name: 1690429, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bb1d20f70186a0f3e47d   score: 0.99927634   abstract: An eartip made of conductive rubber that also realizes bio-potential electrodes is proposed for a daily-use earphone-based eye gesture input interface. Several prototypes, each with three electrodes to capture Electrooculogram (EOG), are implemented on earphones and examined. Experiments with one subject over a 10 day period reveal that all prototypes capture EOG similarly but they differ as regards stability of the baseline and the presence of motion artifacts. Another experiment conducted on a simple eye-controlled application with six subjects shows that the proposed prototype minimizes motion artifacts and offers good performance. We conclude that conductive rubber with mixed Ag filler is the most suitable setup for daily-use.\n",
      "\n",
      "2. id: 5390baa120f70186a0f38cca   score: 0.2641685   abstract: In this work-in-progress paper, we make a case for leveraging the unique affordances of the human ear for eyes-free, mobile interaction. We present EarPut, a novel interface concept, which instruments the ear as an interactive surface for touch-based interactions and its prototypical hardware implementation. The central idea behind EarPut is to go beyond prior work by unobtrusively augmenting a variety of accessories that are worn behind the ear, such as headsets or glasses. Results from a controlled experiment with 27 participants provide empirical evidence that people are able to target salient regions on their ear effectively and precisely. Moreover, we contribute a first, systematically derived interaction design space for ear-based interaction and a set of exemplary applications.\n",
      "\n",
      "3. id: 5390a25820f70186a0e5ef43   score: 0.20882356   abstract: The aim of this work was to model electro-oculogram (EOG) to find optimal electrode positions for wearable human-computer interface system. The system is a head cap developed in our institute and with it we can measure EOG and facial electromyography (fEMG) and those can be used to control the computer interface: gaze direction moves the cursor and muscle activations correspond to clicking. In this work a very accurate 3D model of the human head was developed and it was used in the modeling of EOG. The optimal positions of four electrodes on the forehead measuring the vertical and horizontal eye movements were defined. More tests and optimization is still needed to verify the positions. Modeling of EOG and the head model were successful.\n",
      "\n",
      "4. id: 558b277f612c41e6b9d45145   score: 0.16954333   abstract: Electroencephalogram (EEG) has been one of the important means to study brain functions and diseases. We fabricated an innovative MEMS elastic-based dry electrode using photolithography and electroforming process. The pitch of elastic-based dry electrode tip is 100 μm. We adopted polydimethylsiloxane as an elastic layer which provides the flexibility when the conductive layer and the electrode tip contact with the skin. This kind of dry electrode array does not need conductive gel during testing procedure. Compared with the traditional Ag/AgCl wet electrode, it greatly reduced the preparation time for EEG measurement and it could make the examinee more comfortable.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f51552   score: 0.15662019   abstract: We introduce eyeglasses that present haptic feedback when using gaze gestures for input. The glasses utilize vibrotactile actuators to provide gentle stimulation to three locations on the user's head. We describe two initial user studies that were conducted to evaluate the easiness of recognizing feedback locations and participants' preferences for combining the feedback with gaze gestures. The results showed that feedback from a single actuator was the easiest to recognize and also preferred when used with gaze gestures. We conclude by presenting future use scenarios that could benefit from gaze gestures and haptic feedback.\n",
      "\n",
      "6. id: 53909a0320f70186a0e20931   score: 0.09534947   abstract: A communication support interface controlled by eye movements and voluntary eye blink has been developed for disabled individuals with motor paralysis who cannot speak. Horizontal and vertical electro-oculograms were measured using two surface electrodes attached above and beside the dominant eye and referring to an earlobe electrode and amplified with AC-coupling in order to reduce the unnecessary drift. Four directional cursor movements ---up, down, right, and left--- and one selected operation were realized by logically combining the two detected channel signals based on threshold settings specific to the individual. Letter input experiments were conducted on a virtual screen keyboard. The method's usability was enhanced by minimizing the number of electrodes and applying training to both the subject and the device. As a result, an accuracy of 90.1 ± 3.6% and a processing speed of 7.7\n",
      "\n",
      "7. id: 5390b63320f70186a0f17a6e   score: 0.06941547   abstract: In this paper the techniques for developing an efficient method of eye gesture tracking are explored. The goal is to verify the possibility of building an effective eye gesture input method relying on built-in hardware of the smart phone platform. The achieved results will be used as a basis for researching the technologies for gaze tracking on a mobile device. Effectiveness is measured according to detection accuracy, robustness, and processing performance. The results show limited success in achieving the stated goals, especially with respect to processing performance and provide a solid foundation for future work.\n",
      "\n",
      "8. id: 55922d330cf2c3a0875c9d57   score: 0.06255973   abstract: vice class with a lot of possibilities for user interac- tion design and unobtrusive activity tracking. In this paper we show applications using an early prototype of J!NS MEME, smart glasses with integrated electrodes to detect eye movements (Electrooculography, EOG) and motion sensors (accelerometer and gyroscope) to monitor head motions. We present several demonstrations: We show a simple eye movement visualization, detecting left/right eye motion and blink. Additionally, users can play a game, \\\"Blinky Bird\\\". They need to help a bird avoid obstacles using eye movements. We implemented online detection of reading and talking behavior using a combination of blink, eye movement and head motion. We can give people a long term view of their reading, talking, and also walking activity over the day.\n",
      "\n",
      "9. id: 5539291a0cf26c551af54347   score: 0.05561761   abstract: Electrooculography signal is a bio-signal potential instrument obtained by an EOG acquisition device. The signals were divided into vertical and horizontal signals according to the channel and the placement of electrodes respectively. This paper is to investigate the differences in absolute mean values of the EOG signals before and after using an eye massage device based on four EOG tests designed. The first test conducted is relaxing and blinking which is useful in distinguishing between the actual EOG signals and signal artifacts. Secondly, smooth tracking allows us to to track slow and distant moving object. Thirdly, eye movement detection is controlled via reflexes which linked to the detecting vestibular system. Lastly, saccades occurs when the subject reading a paragraph. Based on the results, for relaxing and blinking, smooth tracking, eye movement detection and saccades, the valu\n",
      "\n",
      "10. id: 5390b60d20f70186a0f11d27   score: 0.050611433   abstract: Based on the fact that there are many challenging cases of infirm persons, who are able to control only their eye muscles, a low-cost mobile device for electronic eye gesture recognition has been designed as a human-machine interface, which enables the control of different applications and home appliances by user's eye gestures by the IR and Bluetooth wireless technology. In this paper the embedded system design of the device is presented in detail, including hardware and software design, modes of operation, and used methods for the eye gesture recognition. Beside these, measurements of the used differential amplifier and the achieved eye gesture recognition efficiency are presented within the test results. Furthermore, a newly designed adjustable head mounted EOG acquisition device with the permanent surface electrodes is proposed.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1712251\n",
      "index                                        55323c1345cec66b6f9db1f0\n",
      "title               Immune-inspired extreme learning machine: Perf...\n",
      "authors                                            Pablo A. D. Castro\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Hybrid Intelligent Sy...\n",
      "references          558ceb940cf2b0acc65036a6;5390bfa220f70186a0f54...\n",
      "abstract            Recently, it was proposed a novel hybrid appro...\n",
      "id                                                            1712251\n",
      "clustered_labels                                                    1\n",
      "Name: 1712251, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a8dc20f70186a0e9efeb   score: 0.9152046   abstract: Extreme learning machine (ELM) [G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: a new learning scheme of feedforward neural networks, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25-29 July 2004], a novel learning algorithm much faster than the traditional gradient-based learning algorithms, was proposed recently for single-hidden-layer feedforward neural networks (SLFNs). However, ELM may need higher number of hidden neurons due to the random determination of the input weights and hidden biases. In this paper, a hybrid learning algorithm is proposed which uses the differential evolutionary algorithm to select the input weights and Moore-Penrose (MP) generalized inverse to analytically determine the output weights. Experimental results show that this approach is able to achieve good generalization performance with \n",
      "\n",
      "2. id: 5390aeba20f70186a0ec9945   score: 0.9014011   abstract: Extreme Learning Machine (ELM) is an approach recently proposed in the literature for Neural Network (NN) training. It has been shown to be much faster than the traditional gradient-based learning algorithms, with many variants, extensions and applications also investigated in the last few years. Among them, the ELM paradigm has been applied to train Time-Variant Neural Networks (TV-NN), through which the training time can be greatly reduced w.r.t. common Back-Propagation (BP). However, this approach may require more hidden nodes and the right type of basis function (through which time-dependence is introduced in TV-NN weight parameters) to attain good generalization. In this paper, we propose a hybrid learning algorithm, which applies differential evolutionary (to determine related input weights) and group selection method (to determine the type of basis function). Experimental results \n",
      "\n",
      "3. id: 5390a54620f70186a0e77de0   score: 0.8523219   abstract: There are two problems preventing the further development of extreme learning machine (ELM). First, the ill-conditioning of hidden layer output matrix reduces the stability of ELM. Second, the complexity of singular value decomposition (SVD) for computing Moore-Penrose generalized inverse limits the learning speed of ELM. For these two problems, this paper proposes the partial Lanczos ELM (PL-ELM) which employs the hybrid of partial Lanczos bidiagonalization and SVD to compute output weights. Experimental results indicate that, compared with ELM, PL-ELM not only effectively improves the stability and generalization performance but also raises the learning speed.\n",
      "\n",
      "4. id: 5390ab8820f70186a0eb0092   score: 0.8143483   abstract: The artificial immune system (AIS) community has been vibrant and active for a number of years now, producing a prolific amount of research ranging from modeling the natural immune system, to tackling real world applications, using an equally diverse set of immune inspired algorithms. We review the current immune applications of the AIS approach, and propose a number of suggestions to the AIS community that can be undertaken to help move the area forward. Despite many successes of AIS techniques, there remain some open issues which have to be addressed in order to make the AIS a real-world problem solving technique.\n",
      "\n",
      "5. id: 558e0a740cf2c779a647697f   score: 0.7318259   abstract: Artificial immune systems (AIS) can be defined as computational systems inspired by theoretical immunology, observed immune functions, principles and mechanisms in order to solve problems. Their development and application domains follow those of soft computing paradigms such as artificial neural networks (ANN), evolutionary algorithms (EA) and fuzzy systems (FS). Despite some isolated efforts, the field of AIS still lacks an adequate framework for design, interpretation and application. This paper proposes one such framework, discusses the suitability of AIS as a novel soft computing paradigm and reviews those works from the literature that integrate AIS with other approaches, focusing ANN, EA and FS. Similarities and differences between AIS and each of the other approaches are outlined. New trends on how to create hybrids of these paradigms and what could be the benefits of this hybrid\n",
      "\n",
      "6. id: 53909a0220f70186a0e1eb5d   score: 0.7108291   abstract: Methods inspired by immune systems have recently shown their efficiency among other machine learning algorithms. This paper presents a new algorithm from the group of Artificial Immune Systems (AIS) basing on a natural phenomenon taking place in human organism. The purpose of this work is to introduce to the negative selection approach for multi-class problems classification and investigate its efficiency.\n",
      "\n",
      "7. id: 5390bf1320f70186a0f50a98   score: 0.70831347   abstract: In this paper, a novel 1-norm extreme learning machine (ELM) for regression and multiclass classification is proposed as a linear programming problem whose solution is obtained by solving its dual exterior penalty problem as an unconstrained minimization problem using a fast Newton method. The algorithm converges from any starting point and can be easily implemented in MATLAB. The main advantage of the proposed approach is that it leads to a sparse model representation meaning that many components of the optimal solution vector will become zero and therefore the decision function can be determined using much less number of hidden nodes in comparison to ELM. Numerical experiments were performed on a number of interesting real-world benchmark datasets and their results are compared with ELM using additive and radial basis function (RBF) hidden nodes, optimally pruned ELM (OP-ELM) and suppo\n",
      "\n",
      "8. id: 559031f50cf235154271937e   score: 0.6880493   abstract: Extreme learning machine (ELM) is a new learning algorithm for the single hidden layer feedforward neural networks. Compared with the conventional neural network learning algorithm it overcomes the slow training speed and over-fitting problems. ELM is based on empirical risk minimization theory and its learning process needs only a single iteration. The algorithm avoids multiple iterations and local minimization. It has been used in various fields and applications because of better generalization ability, robustness, and controllability and fast learning rate. In this paper, we make a review of ELM latest research progress about the algorithms, theory and applications. It first analyzes the theory and the algorithm ideas of ELM, then tracking describes the latest progress of ELM in recent years, including the model and specific applications of ELM, finally points out the research and dev\n",
      "\n",
      "9. id: 5390aaf920f70186a0eae30f   score: 0.68162084   abstract: Recently Extreme Learning Machine (ELM) has been attracting attentions for its simple and fast training algorithm, which randomly selects input weights. Given sufficient hidden neurons, ELM has a comparable performance for a wide range of regression and classification problems. However, in this paper we argue that random input weight selection may lead to an ill-conditioned problem, for which solutions will be numerically unstable. In order to improve the conditioning of ELM, we propose an input weight selection algorithm for an ELM with linear hidden neurons. Experiment results show that by applying the proposed algorithm accuracy is maintained while condition is perfectly stable.\n",
      "\n",
      "10. id: 5390ad5620f70186a0ebda6e   score: 0.6765131   abstract: Extreme Learning Machine (ELM) is a novel learning algorithm for Neural Networks (NN) much faster than the traditional gradient-based learning techniques, and many variants, extensions and applications in the NN field have been appeared in the recent literature. Among them, an ELM approach has been applied to training Time-Variant Neural Networks (TV-NN), with the main objective to reduce the training time. Moreover, interesting approaches have been proposed to automatically determine the number of hidden nodes, which represents one of the limitations of original ELM algorithm for NN. In this paper, we extend the Error Minimized Extreme Learning Machine (EMELM) algorithm along with other two incremental based ELM methods to the time-variant case study, which is actually missing in the related literature. Comparative simulation results show the the proposed EMELM-TV is efficient to optima\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696612\n",
      "index                                        559169c70cf2e89307ca9a67\n",
      "title               Algorithmic regularity for polynomials and app...\n",
      "authors             Arnab Bhattacharyya, Pooya Hatami, Madhur Tuls...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558ce73e0cf23fdd601e1085;539087c320f70186a0d54...\n",
      "abstract            In analogy with the regularity lemma of Szemer...\n",
      "id                                                            1696612\n",
      "clustered_labels                                                    2\n",
      "Name: 1696612, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b95520f70186a0f2ea29   score: 0.8543993   abstract: We prove a structural result for degree-$d$ polynomials. In particular, we show that any degree-$d$ polynomial, $p$ can be approximated by another polynomial, $p_0$, which can be decomposed as some function of polynomials $q_1, \\ldots, q_m$ with $q_i$ normalized and $m=O_d(1)$, so that if $X$ is a Gaussian random variable, the probability distribution on $(q_1(X), \\ldots, q_m(X))$ does not have too much mass in any small box. Using this result, we prove improved versions of a number of results about polynomial threshold functions, including producing better pseudorandom generators, obtaining a better invariance principle, and proving improved bounds on noise sensitivity.\n",
      "\n",
      "2. id: 53909e7c20f70186a0e2c0fc   score: 0.8352149   abstract: Recent work of Gowers [T. Gowers, A new proof of Szemerédi's theorem, Geom. Funct. Anal. 11 (2001) 465-588] and Nagle, Rödl, Schacht, and Skokan [B. Nagle, V. Rödl, M. Schacht, The counting lemma for regular k-uniform hypergraphs, Random Structures Algorithms, in press; V. Rödl, J. Skokan, Regularity lemma for k-uniform hypergraphs, Random Structures Algorithms, in press; V. Rödl, J. Skokan, Applications of the regularity lemma for uniform hypergraphs, preprint] has established a hypergraph removal lemma, which in turn implies some results of Szemerédi [E. Szemerédi, On sets of integers containing no k elements in arithmetic progression, Acta Arith. 27 (1975) 299-345], and Furstenberg and Katznelson [H. Furstenberg, Y. Katznelson, An ergodic Szemerédi theorem for commuting transformations, J. Anal. Math. 34 (1978) 275-291] concerning one-dimensional and multidimensional arithmetic progre\n",
      "\n",
      "3. id: 5390981d20f70186a0e05973   score: 0.8333246   abstract: We investigate constructions of pseudorandom generators that fool polynomial tests of degree d in m variables over finite fields F. Our main construction gives a generator with seed length O(d4 log m (1 + log(d ⁄ ε) ⁄ log log m) + log |F|) bits that achieves arbitrarily small bias ε and works whenever |F| is at least polynomial in d, log m, and 1⁄ε. We also present an alternate construction that uses a seed that can be described by O(c2d8m6⁄(c-2) log(d⁄ε) + log |F|) bits (more precisely, O(c2d8m6⁄(c-2)) field elements, each chosen from a set of size poly(cd⁄ε), plus two field elements ranging over all of F), works whenever |F| is at least polynomial in c, d, and 1⁄ε, and has the property that every element of the output is a function of at most c field elements in the input. Both generators are computable by small arithmetic circuits. The main tool used in the construction is a reduction\n",
      "\n",
      "4. id: 5390a5dc20f70186a0e80207   score: 0.83196384   abstract: We present an efficient randomized algorithm to test if a given function f : &Fopf; pn → &Fopf;p (where p is a prime) is a low-degree polynomial. This gives a local test for Generalized Reed-Muller codes over prime fields. For a given integer t and a given real ε 0, the algorithm queries f at O($ O({{1}\\over{\\epsilon}}+t.p^{{2t \\over p-1}+1}) $) points to determine whether f can be described by a polynomial of degree at most t. If f is indeed a polynomial of degree at most t, our algorithm always accepts, and if f has a relative distance at least ε from every degree t polynomial, then our algorithm rejects f with probability at least $ {1\\over 2} $. Our result is almost optimal since any such algorithm must query f on at least $ \\Omega ( {1 \\over \\epsilon} + p^ {t+1 \\over p-1})$ points. © 2009 Wiley Periodicals, Inc. Random Struct. Alg., 2009 A preliminary version of this paper appeared \n",
      "\n",
      "5. id: 558b882c612c6b62e5e8ad3e   score: 0.82375413   abstract: We give a length-efficient puncturing of Reed-Muller codes which preserves its distance properties. Formally, for the Reed-Muller code encoding n-variate degree-d polynomials over F_q with q gtrsim d/delta, we present an explicit (multi)-set S subseteq F_q^n of size N=mathrm {poly}(n^d/delta) such that every nonzero polynomial vanishes on at most delta N points in S. Equivalently, we give an explicit hitting set generator (HSG) for degree-d polynomials of seed length log N = O(d log n + log (1/delta)) with \\\"density\\\" 1-delta (meaning every nonzero polynomial is nonzero with probability at least 1-delta on the output of the HSG). The seed length is optimal up to constant factors, as is the required field size Omega(d/delta). Plugging our HSG into a construction of Bogdanov (STOC'05) gives explicit pseudorandom generators for n-variate degree-d polynomials with error eps and seed length O\n",
      "\n",
      "6. id: 53909fbd20f70186a0e436ce   score: 0.81316423   abstract: We present a new approach to constructing pseudorandom generators that fool low-degree polynomials over finite fields, based on the Gowers norm. Using this approach, we obtain the following main constructions of explicitly computable generators G : \\mathbb{F}^s\\to \\mathbb{F}^n that fool polynomials over a prime field \\mathbb{F} : 1. a generator that fools degree-2 (i.e., quadratic) polynomials to within error 1/n, with seed length s = O(log n), 2. a generator that fools degree-3 (i.e., cubic) polynomials to within error \\in, with seed length {\\text{s=O(log}}\\left| \\mathbb{F} \\right|n) + f( \\in ,\\mathbb{F}) where f depends only on\\in and \\mathbb{F} (not on n), 3. assuming the \"Gowers inverse conjecture,\" for every d a generator that fools degree-d polynomials to within error \\in, with seed length {\\text{s=O(d\\cdotlog}}\\left| \\mathbb{F} \\right|n) + f{\\text{(d,}} \\in ,\\mathbb{F}) where f de\n",
      "\n",
      "7. id: 5390b3ae20f70186a0ef320e   score: 0.8119744   abstract: We consider the following computational problem. Let F be a field. Given two n-variate polynomials f(x1,.., xn) and g(x1,.., xn) over the field F, is there an invertible linear transformation of the variables which sends f to g? In other words, can we substitute a linear combination of the xi's for each xj appearing in f and obtain the polynomial g? This problem is known to be at least as difficult as the graph isomorphism problem even for homogeneous degree three polynomials. There is even a cryptographic authentication scheme (Patarin, 1996) based on the presumed average-case hardness of this problem. Here we show that at least in certain (interesting) special cases there is a polynomial-time randomized algorithm for determining this equivalence, if it exists. Somewhat surprisingly, the algorithms that we present are efficient even if the input polynomials are given as arithmetic circu\n",
      "\n",
      "8. id: 53908b9320f70186a0dbfa2a   score: 0.79613405   abstract: We give a simple and new randomized primality testing algorithm by reducing primality testing for number n to testing if a specific univariate identity over Zn holds.We also give new randomized algorithms for testing if a multivariate polynomial, over a finite field or over rationals, is identically zero. The first of these algorithms also works over Zn for any n. The running time of the algorithms is polynomial in the size of arithmetic circuit representing the input polynomial and the error parameter. These algorithms use fewer random bits and work for a larger class of polynomials than all the previously known methods, for example, the Schwartz--Zippel test [Schwartz 1980; Zippel 1979], Chen--Kao and Lewin--Vadhan tests [Chen and Kao 1997; Lewin and Vadhan 1998].\n",
      "\n",
      "9. id: 539087aa20f70186a0d4b0cd   score: 0.7856377   abstract: In this paper we give the first self-testers and checkers for polynomials over rational and integer domains. We also show significantly stronger bounds on the efficiency of a simple modification of the algorithm for self-testing polynomials over finite fields given in [8].\n",
      "\n",
      "10. id: 53908b6c20f70186a0dbd228   score: 0.78464925   abstract: Let F(x, u) be a given bivariate polynomial and ε be a small positive number. We consider the approximate factorization of F: find polynomials G, H and ΔF such that F = GH + ΔF and &Verbar;ΔF&Verbar; / &Verbar;F&Verbar;= ε, where &Verbar;P&Verbar; denotes 2-norm of polynomial P. At first, we introduce a relation between the irreducibility of F and the singular value of a certain matrix. By this relation and an upper bound of variations of the power-series roots of a bivariate polynomial, we give an algorithm for an absolute irreducibility test of a polynomial whose coefficients are perturbed within a given tolerance. In addition, we give a lower bound for a tolerance of the approximate factorization of a given bivariate polynomial. The lower bound is the necessary magnitude of perturbations which make a given polynomial reducible.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1731295\n",
      "index                                        5534d8bb45cedae85c379624\n",
      "title               Memristor based computation-in-memory architec...\n",
      "authors             Said Hamdioui, Lei Xie, Hoang Anh Du Nguyen, M...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references                                   558febc3612c29c89cd7c3d6\n",
      "abstract            One of the most critical challenges for today'...\n",
      "id                                                            1731295\n",
      "clustered_labels                                                    0\n",
      "Name: 1731295, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bd1520f70186a0f44f8e   score: 0.68151486   abstract: Rapid data growth nowadays makes it more critical to reduce search time to improve the performance of search-intensive applications. However, huge data size makes it more difficult to efficiently perform search operations. Representative conventional approaches to reduce search time, such as CAM and in-memory databases, are no longer efficient because of the data explosion: CMOS-based CAM has low capacity which cannot be increased through CMOS scaling, and in-memory databases have performance degradation as data size increases. As a result, we have to exploit emerging nanotechnologies to accelerate search. Among emerging nanotechnologies, memristors have become promising candidates to build storage structures because of high capacity, short switching time and low power consumption. However, the benefit we can obtain from these storage structures is limited by low endurance of memristors.\n",
      "\n",
      "2. id: 5390bae620f70186a0f3c0f5   score: 0.44143218   abstract: Memristors offer many potential advantages over more traditional memory-cell technologies, including the potential for extreme densities, and fast read times. Current devices, however, are plagued by problems of yield, and durability. We present a limit study of an aggressive neural network application that has a high update rate and a strict latency requirement, analog neural branch predictor. Of course, traditional analog neural network (ANN) implementations of branch predictors are not built with the idea that the underlying bits are likely to fail due to both manufacturing and wear-out issues. Without some careful precautions, a direct one-to-one replacement will result in poor behavior. We propose a hybrid system that uses SRAM front-end cache, and a distributed-sum scheme to overcome memristors' limitations. Our design can leverage devices with even modest durability (surviving onl\n",
      "\n",
      "3. id: 5390baa120f70186a0f37ed0   score: 0.42333883   abstract: Memristive devices with a simple structure are not only very small but also very versatile, which makes them an ideal candidate used for the next generation computing system in the post-Si era. The working mechanism of the devices and a family of nanodevices built based on this working mechanism are introduced first followed by some proposed applications of these novel devices. The promises and challenges of these devices are then discussed, together with the significant progresses made recently in dealing with these challenges.\n",
      "\n",
      "4. id: 5390b4c320f70186a0efd311   score: 0.3863582   abstract: In this article, we propose a data storage system with the emerging nonvolatile memory technologies used for the implantable electrocardiography (ECG) recorder. The proposed storage system can record the digitalized real-time ECG waveforms continuously inside the implantable device and export the stored data to external reader periodically to obtain a long-term backup. Spin transfer torque random access memory (STT-RAM) and spintronic memristor are selected as the storage elements for their nonvolatility, high density, high reliability, low power consumption, good scalability, and CMOS technology compatibility. The new read and write schemes of STT-RAM and spintronic memristors are presented and optimized to fit the specific application scenario. The tradeoffs among data accuracy, chip area, and read/write energy for the different technologies are thoroughly analyzed and compared. Our si\n",
      "\n",
      "5. id: 5390ac1820f70186a0eb389f   score: 0.3336118   abstract: Recently, the emerging memristor device technology has attracted significant research interests due to its distinctive hysteresis characteristic, which potentially can enable novel circuit designs for future VLSI circuits. In particular, characteristics such as non-volatility, non-linearity, low power consumption, and good scalability make memristor one of the most promising emerging memory technologies. Some important design parameter, however, such as speed, energy consumption, and distingushiablility, are mainly determined by the memristor's physical characteristics. In this paper, a key observation of memristor's asymmetric energy consumption is made by the detailed analysis of the transient power consumption. Based on this observation, we propose a dual-element memory structure in which each memory cell consists of two memristors. By constantly writing complement bits into the two e\n",
      "\n",
      "6. id: 5390ad0620f70186a0eba78f   score: 0.27708158   abstract: In this paper, we present a compact model of the spintronic memristor based on the magnetic-domain-wall motion mechanism for circuit design. Our model also takes into account the variations of material parameters and fabrication process, which significantly affects the actual electrical characteristics of a memristor in nano-scale technologies. Our proposed model can be easily implemented by Verilog-A languages and compatible to SPICE-based simulation. Based on our model, we also show some potential applications of memristor in computing system, including the detailed analysis and optimizations based on our proposed model.\n",
      "\n",
      "7. id: 5390b5c620f70186a0f08a77   score: 0.21403356   abstract: The cost of running a data center is increasingly dominated by energy consumption, contributed by power provisioning, cooling and server components such as processors, memories and disk drives. Meanwhile, emerging classes of complex data center workloads place a heavier burden on processing and storage hardware, involving accesses to huge datasets for each operation. Fortunately, emerging technologies promise better performance and efficiency. Non-volatile (NV) memories for applications such as disk caches [7, 4, 2, 8] are proven ways to save energy, and in recent developments, byte-addressable persistent storage such as phase-change memory (PCM) or Memristors can serve as both main memory and permanent storage, reducing data transfers between layers of hierarchy. Further, 3D die-stacking provides a low-energy high-bandwidth means of connecting storage with computation hardware. The chal\n",
      "\n",
      "8. id: 554505640cf21e970c06b285   score: 0.18713269   abstract: To further extend the scaling trend of traditional CMOS technology, many hybrid architectures integrating emerging device technologies have been proposed recently. Among them, memristor based cross-point memory (MBCPM) demonstrates great potential in data storage and computation. However, accessing a cross-point memory inevitably induces currents flowing through those unselected cells, called as sneak paths. The existence of sneak paths severely limits the capacity growth of MBCPM. In this work, we proposes a pseudo-weight sensing scheme. The design minimizes the impact of sneak paths in read operations by forcing all the unselected rows and columns to the same potential. Moreover, we use an op-amp to sum up the weighted currents through multiple cells and retrieve their information at a time. Thus, the efficiency of read peripheral circuit is significantly enhanced.\n",
      "\n",
      "9. id: 5390a8dc20f70186a0e9f3ff   score: 0.18082882   abstract: A memristor is a passive electronic device that was proposed and described by Leon Chua in 1971. The first practical implementation has been realized by Stan Williams’ group at HP Labs in 2008. The goal of this paper is to give the reader a brief introduction to the possibilities of logic design using memristors. It paper is intended as a tutorial on how to use memristor crossbars for logic design and is a consolidation of various recent publications.\n",
      "\n",
      "10. id: 558b35b3612c41e6b9d46b7a   score: 0.16532344   abstract: Memristor, the fourth passive circuit element, hasattracted increased attention since it was rediscovered by HPLab in 2008. Its distinctive characteristic to record the historicprofile of the voltage/current creates a great potential for futureneuromorphic computing system design. However, at the nanoscale, process variation control in the manufacturing of memristordevices is very difficult. The impact of process variations on amemristive system that relies on the continuous (analog) statesof the memristors could be significant. In addition, the stochasticswitching behaviors have been widely observed. To facilitate theinvestigation on memristor-based hardware implementation, wecompare and summarize different memristor modeling methodologies, from the simple static model, to statistical analysis bytaking the impact of process variations into consideration, andthe stochastic behavior model\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707395\n",
      "index                                        559166830cf2e89307ca9929\n",
      "title               AtmoSPHERE: Representing Space and Movement Us...\n",
      "authors             Ruofei Du, Kent R. Wills, Max Potasznik, Jon E...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558af557612c41e6b9d3ea24;5390bb7b20f70186a0f40...\n",
      "abstract            A Zen garden, also known as Japanese rock gard...\n",
      "id                                                            1707395\n",
      "clustered_labels                                                    0\n",
      "Name: 1707395, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909ed120f70186a0e30dee   score: 0.6398486   abstract: Zen gardens exhibit sophisticated visual designs achieved with minimal compositions and engender a calm, contemplative atmosphere. Here we use perceptual models to study Zen garden design with the aim of discovering guidelines for the creation of calm visual spaces.\n",
      "\n",
      "2. id: 539087b320f70186a0d4f0e1   score: 0.123365626   abstract: The traditional style of Japanese gardens, which has been elaborated over centuries, can be an inspiration for a highly refined style of human-computer interaction (HCI). It offers an example of an \"interface\" that provides an elegant way of combining heterogeneous factors.\n",
      "\n",
      "3. id: 5390880720f70186a0d7a758   score: 0.018725162   abstract: This paper describes a project proposal for a vision-based multimedia installation, bringing together computer vision, computer graphics and other modalities to generate a virtual Japanese garden. The installation provides a platform for exploring human-computer interaction through physical objects.\n",
      "\n",
      "4. id: 5390b3da20f70186a0ef6401   score: 0.0018243829   abstract: This paper presents a novel interactive installation \"Jing Hua\", which augments physical garden with virtual flowers. Visitors influence the growth of virtual flowers by manipulating a tangible interface: a white oversized bowl, on which the flowers are projected. The installation sought to explore how the physical environment and digital projection can be naturally merged to evoke subjects' mixed experience of aesthetics.\n",
      "\n",
      "5. id: 5390881720f70186a0d80a22   score: 0.0014721896   abstract: This paper describes X-Rooms&153;, a PC-based immersive visualization environment with integrated spatial audio, video and graphics. The concept is similar to the original CAVE© [1], but it uses a novel web-based visualization system running on top of a PC-cluster with Windows XP installed. Compared to other common installations based on expensive high-end unix graphics systems, costs are reduced by up to one magnitude (depending on the installation setup) using of the shelf PC and projector equipment. Using the developed external API X-Rooms systems can be deployed in the same application domain as conventional CAVE systems; in addition several new areas are open for less expensive systems. Processing VRML the X-Rooms system is a high end visualization platform for interaction with online 3D web content. Furthermore, the X-Rooms content can be driven by the web.\n",
      "\n",
      "6. id: 5390b44620f70186a0ef8bf6   score: 0.0010202174   abstract: The recent widespread of RGBD cameras such as Kinect® device from Microsoft® opens many new interaction metaphors available to the general public. In this poster we introduce a tabletop interaction metaphor using the depth map from the Kinect: each element on the table is included as a set of physical constraints in a virtual environment. We briefly discuss visualization methods and present a gaming genre adapted to this framework: action-construction games.\n",
      "\n",
      "7. id: 558adb8a612c41e6b9d3ba72   score: 0.00081038737   abstract: Third places are social places like coffee shop and bars, where people come together to catch up with friends and meet new people. Our research explores how Ubiquitous Computing experiences in public space, particularly interactive public displays, can be leveraged to encourage interaction between strangers. We present a multi-display application based on the metaphor of a table-top community garden. This application is built using our Really Easy Displays (RED) framework, a set of web based technologies that allow the rapid development of applications that span multiple displays, sensors and actuators. The prototype makes use of a situated large screen, a projected surface and an Arduino microcontroller to support collaborative interaction, by allowing groups of people to collectively nurture a table-top garden by interacting with furniture, touch-enabled projections and mobile phones.\n",
      "\n",
      "8. id: 53909fca20f70186a0e4610e   score: 0.0008072305   abstract: We present a novel, tangible interface demonstrated by means of the artwork, GranulatSynthese, an installation for the intuitive, tangible creation of ambient, meditative audio-visuals. The interface uses granules distributed over a tabletop surface and combines them with rear-projected visuals and dynamically selected sound samples. The haptic landscape can be explored with the hands, shaped into both hills and open space and composed intuitively. Form, position, and size of cleared table areas control parameters of the computer generated audio-visuals. GranulatSynthese is a meditative application, which invites to either play or step back, watching the visuals and sounds evolve. The installation has proven very accessible. It is inviting and absorbing for a long time for many visitors to the installation.\n",
      "\n",
      "9. id: 53909f6a20f70186a0e3b231   score: 0.0008072305   abstract: We have developed a set of small interactive throw pillows containing intelligent touch-sensing surfaces, in order to explore new ways to model the environment, participants, artefacts, and their interactions, in the context of expressive non-verbal interaction. We present the overall architecture of the environment, describing a model of the user, the interface (the interactive pillows and the devices it can interact with) and the context engine. We describe the representation and process modules of the context engine and demonstrate how they support real-time adaptation. We present an evaluation of the current prototype and conclude with plans for future work.\n",
      "\n",
      "10. id: 53909f2d20f70186a0e38e70   score: 0.0006487308   abstract: We present a new style of tangible interaction, which provides an improved user experience by seamlessly combining real objects and graphical models. In the tangible user interaction zone of our system, physical objects and virtual models are displayed in the same technical illustration style. Regions outside of the interaction zone, and also the user's hands, are shown unaltered in order to maintain an unmodified visual feedback in these areas. Our example application is a tangible urban planning environment, in which the placement of both real and virtual building models affects the flow of wind and the casting of shadows.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691624\n",
      "index                                        559254860cf2aff368683b57\n",
      "title               The blind men and the elephant: on meeting the...\n",
      "authors                                  Arthur Zimek, Jilles Vreeken\n",
      "year                                                           2015.0\n",
      "venue                                                Machine Learning\n",
      "references          5390b29820f70186a0ee97e0;539099b320f70186a0e1a...\n",
      "abstract            In this position paper, we discuss how differe...\n",
      "id                                                            1691624\n",
      "clustered_labels                                                    2\n",
      "Name: 1691624, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b0ca20f70186a0eda7b6   score: 0.93651766   abstract: Clustering is one of the most basic mental activities used by humans to handle the huge amount of information they receive every day. As such, clustering has been extensively studied in different disciplines including: statistics, pattern recognition, machine learning and data mining. Nevertheless, the body of knowledge concerning clustering has focused on objects represented as feature vectors stored in a single dataset. Clustering in this setting aims at grouping objects of a single type in a single table into clusters using the feature vectors. On the other hand, modern real-world applications are composed of multiple, large interrelated datasets comprising distinct attribute sets and containing objects from many domains; typically such data is stored in an information network. The types of patterns and knowledge desired in these applications goes far beyond grouping similar homogenou\n",
      "\n",
      "2. id: 539087fe20f70186a0d7519f   score: 0.92093205   abstract: Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overviewof pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also \n",
      "\n",
      "3. id: 5390b64020f70186a0f1900a   score: 0.91102153   abstract: Subspace clustering refers to the task of identifying clusters of similar objects or data records (vectors) where the similarity is defined with respect to a subset of the attributes (i.e., a subspace of the data space). The subspace is not necessarily (and actually is usually not) the same for different clusters within one clustering solution. In this article, the problems motivating subspace clustering are sketched, different definitions and usages of subspaces for clustering are described, and exemplary algorithmic solutions are discussed. Finally, we sketch current research directions. © 2012 Wiley Periodicals, Inc. © 2012 Wiley Periodicals, Inc.\n",
      "\n",
      "4. id: 5390a2e920f70186a0e676c1   score: 0.90448195   abstract: As a prolific research area in data mining, subspace clustering and related problems induced a vast quantity of proposed solutions. However, many publications compare a new proposition—if at all—with one or two competitors, or even with a so-called “naïve” ad hoc solution, but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this survey, we try to clarify: (i) the different problem definitions related to subspace clustering in general; (ii) the specific difficulties encountered in this field of research; (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches; a\n",
      "\n",
      "5. id: 5390a1f820f70186a0e5d66e   score: 0.8951567   abstract: As a prolific research area in data mining, subspace clustering and related problems induced a vast amount of proposed solutions. However, many publications compare a new proposition -- if at all -- with one or two competitors or even with a so called \"naïve\" ad hoc solution but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this tutorial, we try to clarify (i) the different problem definitions related to subspace clustering in general, (ii) the specific difficulties encountered in this field of research, (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches\n",
      "\n",
      "6. id: 5390b0ca20f70186a0edb38c   score: 0.8940517   abstract: In today's applications we face the challenge of analyzing databases with many attributes per object. For these high dimensional data it is known that traditional clustering algorithms fail to detect meaningful patterns: mining the full-space is futile. As a solution subspace clustering techniques were introduced. They analyze arbitrary subspace projections of the data to detect clustering structures. Recently, public available mining software integrates subspace clustering as a novel mining paradigm and sets the stage for its wide applicability. Though, a common standard to describe, exchange and process the subspace clustering results is still missing, which hinders the application in practice. In this work, we propose an extension of the PMML standard to describe mining models resulting from subspace clustering methods. Thus, we bridge the gap between the different tools and realize a\n",
      "\n",
      "7. id: 5390b9d520f70186a0f317c4   score: 0.83815056   abstract: A considerable amount of work has been done in data clustering research during the last four decades, and a myriad of methods has been proposed focusing on different data types, proximity functions, cluster representation models, and cluster presentation. However, clustering remains a challenging problem due to its ill-posed nature: it is well known that off-the-shelf clustering methods may discover different patterns in a given set of data, mainly because every clustering algorithm has its own bias resulting from the optimization of different criteria. This bias becomes even more important as in almost all real-world applications, data is inherently high-dimensional and multiple clustering solutions might be available for the same data collection. In this respect, the problems of projective clustering and clustering ensembles have been recently defined to deal with the high dimensionali\n",
      "\n",
      "8. id: 5390bb7b20f70186a0f3fd24   score: 0.83128005   abstract: Cluster detection is a very traditional data analysis task with several decades of research. However, it also includes a large variety of different subtopics investigated by different communities such as data mining, machine learning, statistics, and database systems. \"Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering\" names several challenges around clustering: making sense or even making use of many, possibly redundant clustering results, of different representations and properties of data, of different sources of knowledge. Approaches such as ensemble clustering, semi-supervised clustering, subspace clustering meet around these problems. Yet they tackle these problems with different backgrounds, focus on different details, and include ideas from different research communities. This diversity is a major potential for this emerging field and should be h\n",
      "\n",
      "9. id: 5390a7f620f70186a0e9483e   score: 0.80332816   abstract: We view association of concepts as a complex network and present a heuristic for clustering concepts by taking into account the underlying network structure of their associations. Clusters generated from our approach are qualitatively better than clusters generated from the conventional spectral clustering mechanism used for graph partitioning.\n",
      "\n",
      "10. id: 5390b86b20f70186a0f29be7   score: 0.80084777   abstract: Clustering is an important research problem for knowledge discovery from databases. It focuses on finding hidden structures embedded in datasets. It is non-trivial to arrive at a clustering in a dataset such that each pair of data points within the same cluster is similar to each other, and each pair in different clusters is distinct from each other. This is due to the multiplicity of meanings of similarity between data points and also from criteria determining the number, shape, and boundaries of clusters. Despite a large body of published research, new clustering problems keep arising requiring novel solutions. Such a situation is evolving in the field of biomedical research which is generating a large number of interrelated and interdependent datasets, and also in many other domains of science and business. We have developed three novel methodologies for clustering to meet these newly\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1616313\n",
      "index                                        5592566c0cf205530abc974f\n",
      "title                   A Software Scheme for Multithreading on CGRAs\n",
      "authors              Jared Pager, Reiley Jeyapaul, Aviral Shrivastava\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Embedded Computing Systems...\n",
      "references          5390a96e20f70186a0ea2e4e;5390a30b20f70186a0e6b...\n",
      "abstract            Recent industry trends show a drastic rise in ...\n",
      "id                                                            1616313\n",
      "clustered_labels                                                    3\n",
      "Name: 1616313, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908b6c20f70186a0dbdf4a   score: 0.78727776   abstract: Abstract: Recent trends in the cost and performance of application-specific hardware relative to conventional processors discourage investing much time and energy in special-purpose architectures except for niche applications. These trends, however, may be reversed by the increasing complexity of computer architectures and the advent of configurable computing. Configurable computers have attracted considerable attention recently because they promise to deliver the performance of application-specific hardware along with the flexibility of general-purpose computers. In this paper, we discuss some of the forces driving configurable computing, and we argue that new configurable architectures are needed to realize the enormous potential of configurable computing. In particular, we believe that the commercial FPGAs currently used to construct configurable computers are too fine-grained to achi\n",
      "\n",
      "2. id: 5390a1d420f70186a0e56cf6   score: 0.45211253   abstract: As embedded applications are getting more complex, they are also demanding highly diverse computational capabilities. The majority of all previously proposed reconfigurable architectures targets static data stream oriented applications, optimizing very specific computational kernels, corresponding to the typical embedded systems characteristics in the past. Modern embedded devices, however, impose totally new requirements. They are expected to support a wide variety of programs on a single platform. Besides getting more heterogeneous, these applications have very distinct behaviors. In this paper we explore this trend in more detail. First, we present a study about the behavioral difference of embedded applications based on the Mibench benchmark suite. Thereafter, we analyze the potential optimizations and constraints for two different run-time dynamic reconfigurable architectures with d\n",
      "\n",
      "3. id: 5390b68720f70186a0f1c588   score: 0.44939268   abstract: Coarse-grained reconfigurable arrays (CGRAs) are a very promising platform, providing both up to 10–100 MOps/mW of power efficiency and software programmability. However, this promise of CGRAs critically hinges on the effectiveness of application mapping onto CGRA platforms. While previous solutions have greatly improved the computation speed, they have largely ignored the impact of the local memory architecture on the achievable power and performance. This paper motivates the need for memory-aware application mapping for CGRAs, and proposes an effective solution for application mapping that considers the effects of various memory architecture parameters including the number of banks, local memory size, and the communication bandwidth between the local memory and the external main memory. Further we propose efficient methods to handle dependent data on a double-buffering local memory, wh\n",
      "\n",
      "4. id: 5390ab8820f70186a0eb01d9   score: 0.41786525   abstract: Most of the coarse-grained reconfigurable architectures (CGRAs) are composed of reconfigurable ALU arrays and configuration cache (or context memory) to achieve high performance and flexibility. Specially, configuration cache is the main component in CGRA that provides distinct feature for dynamic reconfiguration in every cycle. However, frequent memory-read operations for dynamic reconfiguration cause much power consumption. Thus, reducing power in configuration cache has become critical for CGRA to be more competitive and reliable for its use in embedded systems. In this paper, we propose dynamically compressible context architecture for power saving in configuration cache. This power-efficient design of context architecture works without degrading the performance and flexibility of CGRA. Experimental results show that the proposed approach saves up to 39.72% power in configuration cac\n",
      "\n",
      "5. id: 5390b48420f70186a0efbd9b   score: 0.40433103   abstract: Coarse-Grained Reconfigurable Arrays (CGRAs) are a very promising platform, providing both, up to 10-100 MOps/mW of power efficiency and are software programmable. However, this cardinal promise of CGRAs critically hinges on the effectiveness of application mapping onto CGRA platforms. While previous solutions have greatly improved the computation speed, they have largely ignored the impact of the local memory architecture on the achievable power and performance. This paper motivates the need for memory-aware application mapping for CGRAs, and proposes an effective solution for application mapping that considers the effects of various memory architecture parameters including the number of banks, local memory size, and the communication bandwidth between the local memory and the external main memory. Our proposed solution achieves 62% reduction in the energy-delay product, which factors i\n",
      "\n",
      "6. id: 5390b00c20f70186a0ed4fc8   score: 0.37765545   abstract: Modern portable embedded devices require processors that can provide sufficient performance for demanding multimedia and wireless applications. At the same time they have to be flexible to support a wide range of products and extremely energy efficient to provide a long battery life. Coarse Grained Reconfigurable Architectures (CGRAs) potentially meet these constraints by providing a mix of flexible computational resources and large amounts of programmable interconnect. The vast design space of CGRAs complicates the development of optimized processors. Most effort has been spent on improving the performance. However, the energy cost of the programmable interconnect is becoming more expensive and this cost can no longer be neglected. In this work we present an energy-and performance-aware exploration for the interconnect of a CGRA and show that important tradeoffs can be made for those me\n",
      "\n",
      "7. id: 5390bed320f70186a0f4e692   score: 0.30871102   abstract: Power-efficiency has been a key issue for today's application and system design, ranging from embedded systems to data centers. While application-specific designs and optimizations may improve the power efficiency, it requires significant efforts to co-design the hardware and software, which are difficult to re-use. On the hardware front, the trend of heterogeneous computing enables custom designs for specific applications by integrating different types of processors and reconfigurable hardware to handle compute-intensive tasks. However, what is still missing is an elegant application framework, i.e., a programming environment and a runtime system, to develop portable applications which can offload tasks or be reconfigured dynamically to run on a variety of systems efficiently. Our ongoing work, MobileFBP, provides an application framework which aims to support heterogeneous and reconfig\n",
      "\n",
      "8. id: 5390b20120f70186a0ee4a65   score: 0.30290636   abstract: Coarse-Grained Reconfigurable Arrays or CGRAs are programmable fabrics that promise both high performance and high power efficiency. Traditionally, CGRAs were used to accelerate extremely-embedded systems, and were typically manually programmed. However, as CGRAs are conceived to be used as more general-purpose accelerators, there is a need to develop software tools and capabilities. Much work has been done on developing compiler techniques for CGRAs, making programming them easier, however, there is no support for multithreading. As an accelerator to a multithreaded processor, CGRAs now are restricted to accelerating only one kernel of one thread running on the processor at any point in time. Supporting multithreading is difficult, since the start times and end times of threads are dynamic in nature, while CGRAs are statically scheduled. In this paper, we propose a strategy to do multit\n",
      "\n",
      "9. id: 558c301f0cf2e30013db60d1   score: 0.27192786   abstract: Coarse-grained reconfigurable arrays (CGRAs) are a promising class of architectures conjugating flexibility and efficiency. Devising effective methodologies to map applications onto CGRAs is a challenging task, due to their parallel execution paradigm and constrained hardware resources. In order to handle complex applications, it is important to devise efficient strategies to partition a kernel into pieces that obey resource constraint and methodologies to schedule them on the underlying hardware. In this paper, we tackle these problems by proposing algorithms to address partitioning based on recursive searches over abstract trees. A novel scheduling strategy is also described that, leveraging differences in delays of various operations, is able to efficiently map operations on CGRA architectures. Experimental evidence on kernels derived from a diverse set of data flow graphs and EEMBC b\n",
      "\n",
      "10. id: 5390a93b20f70186a0ea03d5   score: 0.2594502   abstract: Application-specific optimization of embedded systems becomes inevitable to satisfy the market demand for designers to meet tighter constraints on cost, performance and power. On the other hand, the flexibility of a system is also important to accommodate the short time-to-market requirements for embedded systems. To compromise these incompatible demands, coarse-grained reconfigurable architecture (CGRA) has emerged as a suitable solution. A typical CGRA requires many processing elements (PEs) and a configuration cache for reconfiguration of its PE array. However, such a structure consumes significant area and power. Therefore, designing cost-effective CGRA has been a serious concern for reliability of CGRA-based embedded systems. As an effort to provide such cost-effective design, the first half of this work focuses on reducing power in the configuration cache. For power saving in the c\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696396\n",
      "index                                        55913b240cf232eb904fb5df\n",
      "title               Distributed computation of large-scale graph p...\n",
      "authors             Hartmut Klauck, Danupon Nanongkai, Gopal Pandu...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twenty-Sixth Annual ACM-SIA...\n",
      "references          558bd17b0cf25dbdbb04dce3;558b15b8612c41e6b9d42...\n",
      "abstract            Motivated by the increasing need for fast dist...\n",
      "id                                                            1696396\n",
      "clustered_labels                                                    2\n",
      "Name: 1696396, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a72220f70186a0e8a306   score: 0.98646784   abstract: Traditionally, the performance of distributed algorithms has been measured in terms of time and message complexity. Message complexity concerns the number of messages transmitted over all the edges during the course of the algorithm. However, in energy-constrained ad hoc wireless networks (e.g., sensor networks), energy is a critical factor in measuring the efficiency of a distributed algorithm. Transmitting a message between two nodes has an associated cost (energy) and moreover this cost can depend on the two nodes (e.g., the distance between them among other things). Thus in addition to the time and message complexity, it is important to consider energy complexity that accounts for the total energy associated with the messages exchanged among the nodes in a distributed algorithm. This paper addresses the minimum spanning tree (MST) problem, a fundamental problem in distributed computi\n",
      "\n",
      "2. id: 539090c420f70186a0dde5b5   score: 0.98486924   abstract: This paper presents a lower bound of [(W)\\tilde](D+On) on the time required for the distributed construction of a minimum-weight spanning tree (MST) in weighted n-vertex networks of diameter D = W(logn), in the bounded message model. This establishes the asymptotic near-optimality of existing time-efficient distributed algorithms for the problem, whose complexity is O(D + On log* n).\n",
      "\n",
      "3. id: 53908b9320f70186a0dc0769   score: 0.98421544   abstract: This paper presents a lower bound of \\math\\math on the time required for the distributed construction of a minimum-weight spanning tree (MST) in n-vertex networks of diameter \\math, in the bounded message model. This establishes the asymptotic near-optimality of existing time-efficient distributed algorithms for the problem, whose complexity is \\math.\n",
      "\n",
      "4. id: 5390893e20f70186a0d935b4   score: 0.9837225   abstract: This paper presents a lower bound of $\\Omega(D+\\sqrt n/\\log n)$ on the time required for the distributed construction of a minimum-weight spanning tree (MST) in weighted n-vertex networks of diameter $D=\\Omega(\\log n)$, in the bounded message model. This establishes the asymptotic near-optimality of existing time-efficient distributed algorithms for the problem, whose complexity is $O(D + \\sqrt n \\log^* n)$.\n",
      "\n",
      "5. id: 5390881720f70186a0d7fe2e   score: 0.9759006   abstract: This paper considers the problem of distributively constructing a minimum-weight spanning tree (MST) for graphs of constant diameter in the bounded-messages model, where each message can contain at most B bits for some parameter B. It is shown that the time required to compute an MST for graphs of diameter 4 or 3 can be as high as &OHgr;(3√n/B) and &OHgr;(4√n/2√B), respectively. The lower bound holds even if the algorithm is allowed to be randomized. On the other hand, it is shown that O(log n) time units suffice to compute an MST deterministically for graphs with diameter 2, when B = O(log n). These results complement a previously known lower bound of &OHgr;(2√n/B) for graphs of diameter &OHgr;(log n).\n",
      "\n",
      "6. id: 5390962020f70186a0df5c47   score: 0.97529614   abstract: This paper studies the problem of constructing a minimum-weight spanning tree (MST) in a distributed network. This is one of the most important problems in the area of distributed computing. There is a long line of gradually improving protocols for this problem, and the state of the art today is a protocol with running time O(∧(G) + √n log* n) due to Kutten and Peleg [KP95], where ∧(G) denotes the diameter of the graph G. Peleg and Rubinovich [PR99] have shown that (√n) time is required for constructing MST even on graphs of small diameter, and claimed that their result \"establishes the asymptotic near-optimality\" of the protocol of [KP95].In this paper we refine this claim, and devise a protocol that constructs the MST in Õ(μ(G = w + √n) rounds, where μ(G,μ) is the MST-radius of the graph. The ratio between the diameter and the MST-radius may be as large as Θ(n), and, consequently, on s\n",
      "\n",
      "7. id: 5390a06e20f70186a0e4db75   score: 0.97339284   abstract: This paper deals with distributed graph algorithms. Processors reside in the vertices of a graph G and communicate only with their neighbors. The system is synchronous and reliable, there is no limit on message lengths and local computation is instantaneous. The results: A maximal independent set in an n-cycle cannot be found faster than Ω(log* n) and this is optimal by [CV]. The d-regular tree of radius r cannot be colored with fewer than √d colors in time 2r / 3. If Δ is the largest degree in G which has order n, then in time O(log*n) it can be colored with O(Δ2) colors.\n",
      "\n",
      "8. id: 55323b9045cec66b6f9da289   score: 0.97246706   abstract: This paper studies fundamental problems for distributed graph simulation. Given a pattern query Q and a graph G that is fragmented and distributed, a graph simulation algorithm A is to compute the matches Q(G) of Q in G. We say that A is parallel scalable in (a) response time if its parallel computational cost is determined by the largest fragment Fm of G and the size |Q| of query Q, and (b) data shipment if its total amount of data shipped is determined by |Q| and the number of fragments of G, independent of the size of graph G. (1) We prove an impossibility theorem: there exists no distributed graph simulation algorithm that is parallel scalable in either response time or data shipment. (2) However, we show that distributed graph simulation is partition bounded, i.e., its response time depends only on |Q|, |Fm| and the number |Vf| of nodes in G with edges across different fragments; an\n",
      "\n",
      "9. id: 5390b3ae20f70186a0ef3a8f   score: 0.97074324   abstract: We give a distributed algorithm that constructs a O(logn)- approximate minimum spanning tree (MST) in arbitrary networks. Our algorithm runs in time $\\tilde{O}(D(G) + L(G,w))$ where L(G,w) is a parameter called the local shortest path diameter and D(G) is the (unweighted) diameter of the graph. Our algorithm is existentially optimal (up to polylogarithmic factors), i.e., there exists graphs which need Ω(D(G)+ L(G,w)) time to compute an H-approximation to the MST for any H ∈[1, Θ(logn)]. Our result also shows that there can be a significant time gap between exact and approximate MST computation: there exists graphs in which the running time of our approximation algorithm is exponentially faster than the time-optimal distributed algorithm that computes the MST. Finally, we show that our algorithm can be used to find an approximate MST in wireless networks and in random weighted networks in\n",
      "\n",
      "10. id: 5390ad5620f70186a0ebe0d4   score: 0.97023994   abstract: Since in general it is NP-hard to solve the minimum dominating set problem even approximatively, a lot of work has been dedicated to central and distributed approximation algorithms on restricted graph classes. In this paper, we compromise between generality and efficiency by considering the problem on graphs of small arboricity a. These family includes, but is not limited to, graphs excluding fixed minors, such as planar graphs, graphs of (locally) bounded treewidth, or bounded genus. We give two viable distributed algorithms. Our first algorithm employs a forest decomposition, achieving a factor O(a2) approximation in randomized time O(log n). This algorithm can be transformed into a deterministic central routine computing a linear-time constant approximation on a graph of bounded arboricity, without a priori knowledge on a. The second algorithm exhibits an approximation ratio of O(alo\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691280\n",
      "index                                        558f851f0cf2e0e37c4f390c\n",
      "title               An improved semidefinite programming hierarchi...\n",
      "authors                           Chenchen Wu, Donglei Du, Dachuan Xu\n",
      "year                                                           2015.0\n",
      "venue                           Journal of Combinatorial Optimization\n",
      "references          558c06910cf23f2dfc596c14;5390b29820f70186a0eea...\n",
      "abstract            We present a unified semidefinite programming ...\n",
      "id                                                            1691280\n",
      "clustered_labels                                                    1\n",
      "Name: 1691280, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390882420f70186a0d894a1   score: 0.98840266   abstract: We present an improved semidefinite programming based approximation algorithm for the MAX CUT problem in graphs of maximum degree at most 3. The approximation ratio of the new algorithm is at least 0.9326. This improves, and also somewhat simplifies, a result of Feige, Karpinski and Langberg. We also observe that results of Hopkins and Staton and of Bondy and Locke yield a simple combinatorial 4/5-approximation algorithm for the problem. Slightly improved results would appear in the full version of the paper.\n",
      "\n",
      "2. id: 539089ab20f70186a0d97150   score: 0.9843364   abstract: We obtain improved semidefinite programming based approximation algorithms for all the natural maximum bisection problems of graphs. Among the problems considered are: MAX n/2-BISECTION--partition the vertices of the graph into two sets of equal size such that the total weight of edges connecting vertices from different sides is maximized; MAX n/2-VERTEX-COVER--find a set containing half of the vertices such that the total weight of edges touching this set is maximized; MAX n/2-DENSE-SUBGRAPH--find a set containing half of the vertices such that the total weight of edges connecting two vertices from this set is maximized; and MAX n/2-UNCUT partition the vertices into two sets of equal size such that the total weight of edges that do not cross the cut is maximized. We also consider the directed versions of these problems, such as MAX n/2-DIRECTED-BISECTION and MAX n/2-DIRECTED-UNCUT. Thes\n",
      "\n",
      "3. id: 53908a5820f70186a0da19b6   score: 0.9840936   abstract: We obtain improved semidefinite programming based approximation algorithms for all the natural maximum bisection problems of graphs. Among the problems considered are: MAX n/2 -BISECTION - partition the vertices of a graph into two sets of equal size such that the total weight of edges connecting vertices from different sides is maximized; MAX n/2 -VERTEX-COVER - find a set containing half of the vertices such that the total weight of edges touching this set is maximized; MAX n/2 -DENSE-SUBGRAPH - find a set containing half of the vertices such that the total weight of edges connecting two vertices from this set is maximized; and MAX n/2 -UnCUT - partition the vertices into two sets of equal size such that the total weight of edges that do not cross the cut is maximized. We also consider the directed versions of these problems, MAX n/2 -DIRECTED-BISECTION and MAX n/2 -DIRECTED-UnCUT. The\n",
      "\n",
      "4. id: 53909e8b20f70186a0e2dd8e   score: 0.967101   abstract: The max-bisection problem is to find a partition of the vertices of a graph into two equal size subsets that maximizes the number of edges with endpoints in both subsets. We obtain new improved approximation ratios for the max-bisection problem on the low degree k-regular graphs for 3≤k≤8, by deriving some improved transformations from a maximum cut into a maximum bisection. In the case of three regular graphs we obtain an approximation ratio of 0.854, and in the case of four and five regular graphs, approximation ratios of 0.805, and 0.812, respectively.\n",
      "\n",
      "5. id: 5390b78a20f70186a0f24888   score: 0.96697646   abstract: The max-bisection problem is to find a partition of the vertices of a graph into two equal size subsets that maximizes the number of edges with endpoints in both subsets. We obtain new improved approximation ratios for the max-bisection problem on the low degree k-regular graphs for 3≤k≤8, by deriving some improved transformations from a maximum cut into a maximum bisection. In the case of three regular graphs we obtain an approximation ratio of 0.854, and in the case of four and five regular graphs, approximation ratios of 0.805, and 0.812, respectively.\n",
      "\n",
      "6. id: 5390b44620f70186a0ef87a4   score: 0.93593454   abstract: In this paper we improve the analysis of approximation algorithms based on semidefinite programming for the maximum graph partitioning problems MAX-k-CUT, MAX- k -UNCUT, MAX- k -DIRECTED-CUT, MAX -k-DIRECTED-UNCUT, MAX- k -DENSE-SUBGRAPH, and MAX-k-VERTEX-COVER.It was observed by Han, Ye, Zhang (2002) and Halperin, Zwick (2002) that a parameter-driven random hyperplane can lead to better approximation factors than obtained by Goemans and Williamson (1994). Halperin and Zwick could describe the approximation factors by a mathematical optimization problem for the above problems for $k=\\frac{n}{2}$ and found a choice of parameters in a heuristic way. The innovation of this paper is twofold. First, we generalize the algorithm of Halperin and Zwick to cover all cases of k, adding some algorithmic features. The hard work is to show that this leads to a mathematical optimization problem for an \n",
      "\n",
      "7. id: 5390893e20f70186a0d93f0d   score: 0.9293122   abstract: Let α ≃ 0.87856 denote the best approximation ratio currently known for the Max-Cut problem on general graphs. We consider a semidefinite relaxation of the Max-Cut problem, round it using the random hyperplane rounding technique of Goemans and Williamson [J. ACM 42 (1995) 1115-1145], and then add a local improvement step. We show that for graphs of degree at most Δ, our algorithm achieves an approximation ratio of at least α + ε, where ε 0 is a constant that depends only on Δ. Using computer assisted analysis, we show that for graphs of maximal degree 3 our algorithm obtains an approximation ratio of at least 0.921, and for 3-regular graphs the approximation ratio is at least 0.924. We note that for the semidefinite relaxation of Max-Cut used by Goemans and Williamson the integrality gap is at least 1/0.885, even for 2-regular graphs.\n",
      "\n",
      "8. id: 558e0c710cf2c779a6476b8e   score: 0.92563426   abstract: <P>An exact solution method for the graph bisection problem is presented. We describe a branch-and-bound algorithm which is based on a cutting plane approach combining semidefinite programming and polyhedral relaxations. We report on extensive numerical experiments which were performed for various classes of graphs. The results indicate that the present approach solves general problem instances with 80—90 vertices exactly in reasonable time and provides tight approximations for larger instances. Our approach is particularly well suited for special classes of graphs as planar graphs and graphs based on grid structures.</P>\n",
      "\n",
      "9. id: 5390980720f70186a0e026a9   score: 0.9092645   abstract: We present an improved semidefinite programming based approximation algorithm for the MAX CUT problem in graphs of maximum degree at most 3. The approximation ratio of the new algorithm is at least 0.9326. This improves, and also somewhat simplifies, a result of Feige, Karpinski and Langberg. We also observe that results of Hopkins and Staton and of Bondy and Locke yield a simple combinatorial 4/5-approximation algorithm for the problem. Finally, we present a combinatorial 22/27-approximation algorithm for the MAX CUT problem for regular cubic graphs.\n",
      "\n",
      "10. id: 5390bf1320f70186a0f519db   score: 0.90515476   abstract: We consider the max-bisection problem and the disjoint 2-catalog segmentation problem, two well-known NP-hard combinatorial optimization problems. For the first problem, we apply the semidefinite programming (SDP) relaxation and the RPR2 technique of Feige and Langberg (J. Algorithms 60:1---23, 2006) to obtain a performance curve as a function of the ratio of the optimal SDP value over the total weight through finer analysis under the assumption of convexity of the RPR2 function. This ratio is shown to be in the range of [0.5,1]. The performance curve implies better approximation performance when this ratio is away from 0.92, corresponding to the lowest point on this curve with the currently best approximation ratio of 0.7031 due to Feige and Langberg (J. Algorithms 60:1---23, 2006). For the second problem, similar technique results in an approximation ratio of 0.7469, improving the prev\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1705826\n",
      "index                                        55323b6245cec66b6f9d9d1a\n",
      "title               Determining Optimal Parameters for Expediting ...\n",
      "authors                                                           NaN\n",
      "year                                                           2015.0\n",
      "venue                   Manufacturing & Service Operations Management\n",
      "references          5390a0b720f70186a0e506be;5390a88c20f70186a0e99...\n",
      "abstract            <P>We consider an inventory policy that expedi...\n",
      "id                                                            1705826\n",
      "clustered_labels                                                    0\n",
      "Name: 1705826, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 554e3f120cf22ca2c80f9de1   score: 0.936285   abstract: Consider a single-item, periodic review, stationary inventory model with stochastic demands, proportional ordering costs, and convex holding and shortage costs, where shortages are backordered and Veinott's well known terminal condition holds. Orders can be scheduled for any period, but the actual inventory level is determined every T periods through an audit. This leads to a dynamic programming model where stage n contains periods n-1T + 1 through nT. For both discounted and averaging criteria, a simple rule optimally describes the orders for the T periods of a stage as a function of the state beginning inventory level and the cumulative T-period order. The latter is optimally determined by a base stock policy with two base stock levels: one for the final stage, another for the rest. The horizon may be finite or infinite. Methods are presented for computing optimal policies, together wi\n",
      "\n",
      "2. id: 539095bb20f70186a0df2e3c   score: 0.92944044   abstract: This note is concerned with the inventory models with finite horizon and cost changes. We presented more specific results on the optimal inventory policy than those in previous papers and showed that some of the results in Lev and Weiss' (1990) paper are not necessarily optimal.\n",
      "\n",
      "3. id: 53908b4920f70186a0dbb416   score: 0.9281487   abstract: We consider a stochastic, capacitated production-inventory model in which the customer provides information about the expected timing of future orders to the supplier. We allow for randomness in customer order arrivals as well as the quantity demanded, but work under the assumption that the customer is making every effort to follow the schedule provided. We term this as a target reverting policy. This gives rise to an interesting nonstationary inventory control model at the supplier. After characterizing the optimal policy, we develop solution procedures to compute the optimal parameters. An extensive computational study provides insights into the behavior of this model at optimality. Further, comparing the cost of the optimal policy to the cost of simple policies that either ignore the customer's information or the capacity constraint, we are able to provide insights as to when these si\n",
      "\n",
      "4. id: 55323d4845cec66b6f9de01c   score: 0.8329173   abstract: <P>Minimum average cost ordering policies for continuously reviewed two product inventory systems with joint setup costs are sought. Disappointly, the optimal policy, even in a simple symmetric case, is not always simple: For some values of cost and demand parameters, a policy that would be difficult to implement is optimal. Markov Renewal Programming is used to find the region in parameter space where a given policy is optimal.</P>\n",
      "\n",
      "5. id: 555a1d560cf2b21909ba3514   score: 0.82893884   abstract: This paper studies the numerical computation of the two parameters the reorder level s and the order up to level S of inventory policies for discrete time shortage cost systems. Our goal is to obtain approximately optimal policies with little computational effort. The paper introduces three new methods that are designed to achieve this goal. Two of the methods are shortcuts based on the method of Freeland and Porteus and one is a heuristic that makes several modifications to a standard continuous review approximation. The paper provides a fairly detailed survey of other methods for easily computing approximately optimal inventory policies. It then numerically compares all these methods on a reasonably broad range of problems. One of the shortcuts and the new heuristic method performed very well: the percentage error of their average costs was approximately 1%. Some commonly cited competi\n",
      "\n",
      "6. id: 5390880220f70186a0d764f1   score: 0.8000679   abstract: A two-stage inventory system is considered where Poisson demand occurs at Stage 1, and Stage 1 replenishes its inventory from Stage 2, which in turn orders from an outside supplier with unlimited stock. Each shipment, either to Stage 2 or to Stage 1, incurs a fixed setup cost. Under the assumption that the supply leadtime at Stage 2 is zero, we characterize a simple heuristic policy whose long-run average cost is guaranteed to be within 6% of optimality, i.e., a 94%-effective policy. The paper also provides heuristic policies for more general inventory systems and reports computational results.\n",
      "\n",
      "7. id: 554cedc50cf21c5c67b8ff05   score: 0.7900447   abstract: The past decade has given rise to a considerable amount of research in the fields of inventory control and production planning. One of the most useful mathematical models originated with the paper of K. D. Arrow, T. E. Harris, and J. Marschak, \\\"Optimal Inventory Policy,\\\" Econometrica 19, 250-272 1951. This model was further improved in Richard Bellman's book, Dynamic Programming, pp. 152-182, Princeton University Press, Princeton, New Jersey, 1957. In the present paper, another formulation of the same problem is exhibited. The attentive reader will observe many similarities to the dynamic programming formulation.\n",
      "\n",
      "8. id: 5390995d20f70186a0e14a74   score: 0.77864933   abstract: We study a single-product periodic-review inventory model in which the ordering quantity is either zero or at least a minimum order size. The ordering cost is a linear function of the ordering quantity, and the demand in different time periods are independent random variables. The objective is to characterize the inventory policies that minimize the total discounted ordering, holding, and backorder penalty costs over a finite time horizon. We introduce the concept of an M-increasing function. These functions allow us to characterize the optimal inventory policies everywhere in the state space outside of an interval for each time period. Furthermore, we identify easily computable upper bounds and asymptotic lower bounds for these intervals. Finally, examples are given to demonstrate the complex structure of the optimal inventory policies.\n",
      "\n",
      "9. id: 5390bb7b20f70186a0f40728   score: 0.70232594   abstract: We study a single-item, single-site, periodic-review inventory system with negligible fixed ordering costs. The supplier to this system is not entirely reliable, such that each order is a Bernoulli trial, meaning that, with a given probability, the supplier delivers the current order and any accumulated backorders at the end of the current period, resulting in a Geometric distribution for the actual resupply lead time. We develop a recursive expression for the steady-state probability vector of a discrete-time Markov process (DTMP) model of this imperfect-supply inventory system. We use this recursive expression to prove the convexity of the inventory system objective function, and also to prove the optimality of our computational procedure for finding the optimal base-stock level. We present a service-constrained version of the problem and show how the computation of the optimal base-st\n",
      "\n",
      "10. id: 5390ad0620f70186a0eba6d9   score: 0.6900372   abstract: We consider a single-item, periodic-review inventory control problem in which discrete stochastic demand must be satisfied. When shortages occur, the unmet demand must be filled by some form of expediting; we allow a very general form for the cost structure of expediting. We explicitly consider the case where expedited production is allowed to produce up to a positive inventory level. We also consider the case where expedited production beyond the deficit is not permitted; an alternate application for this model is an inventory system with general lost sales costs. For the infinite-horizon discounted problem, we characterize the structure of the optimal stationary expediting policy and show that an (s, S) policy is optimal for regular production. For the special cases where the expediting cost function is concave or consists of a fixed and linear per-unit cost, we show that the optimal s\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1650918\n",
      "index                                        559162c40cf2e89307ca97d5\n",
      "title               Accessible Crowdwork?: Understanding the Value...\n",
      "authors             Kathryn Zyskowski, Meredith Ringel Morris, Jef...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390aca920f70186a0eb93da;5390a37f20f70186a0e6c...\n",
      "abstract            We present the first formal study of crowdwork...\n",
      "id                                                            1650918\n",
      "clustered_labels                                                    3\n",
      "Name: 1650918, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390af8920f70186a0ed00d6   score: 0.7329743   abstract: In this paper, we describe three tools that facilitate 'crowdsourcing' open source development to help overcome accessibility, usability and productivity issues identified by disabled students.\n",
      "\n",
      "2. id: 558b277c612c41e6b9d4513c   score: 0.45634922   abstract: Crowd work web sites such as Amazon Mechanical Turk enable individuals to work from home, which may be useful for people with disabilities. However, the web sites for finding and performing crowd work tasks must be accessible if people with disabilities are to use them. We performed a heuristic analysis of one crowd work site, Amazon's Mechanical Turk, using the Web Content Accessibility Guidelines 2.0. This paper presents the accessibility problems identified in our analysis and offers suggestions for making crowd work platforms more accessible\n",
      "\n",
      "3. id: 559157540cf232eb904fbd42   score: 0.43278244   abstract: Participation levels of people with disabilities in the SIGCHI community reflect a general inadequacy in how they are supported, and their interests promoted, within the ACM, the wider computing industry and academia itself. In response, we propose a manifesto for overhauling existing SIGCHI practices to increase the opportunities for including a wide range of disabled people in our research community through dissemination venues such as CHI. We set out the moral case for change, before providing a summary of UK disability discrimination law which we use identify sources of direct and indirect discrimination. Our goal has been to go beyond just accessibility: instead we emphasize disability inclusion in a much broader sense, and articulate a range of steps that can be conducted in order to meet this.\n",
      "\n",
      "4. id: 5390b95420f70186a0f2d8fb   score: 0.33906123   abstract: Crowdsourcing involves outsourcing some job to a distributed group of people online, typically by breaking the job down into microtasks. Online markets offer human users payment for completing small tasks, or users can participate in nonpaid platforms such as games and volunteer sites. These platforms' general availability has enabled researchers to recruit large numbers of participants for user studies, generate third-party content and assessments, or even build novel user experiences. This special issue provides a snapshot of the most recent crowdsourcing research.\n",
      "\n",
      "5. id: 558b1e5f612c41e6b9d43e15   score: 0.2649285   abstract: Crowd sourcing is changing the way people work and solve problems from \\\"in-house working\\\" to \\\"public out-sourcing\\\". Most online crowd sourcing platforms perform two main functions: (i) allowing users to advertise their tasks and (ii) helping them find candidate workers. However, they do not support crowd sourcing of complex work consisting of interdependent tasks. Those tasks require not only having simple task/worker pairs, but also coordinating multiple workers together for completion of the crowd work. In this paper, we propose a conceptual framework to bring the coordination support into current online crowd sourcing platforms. In our framework, each crowd worker is modeled as a service that can be self-described, dynamically discovered and assembled into the complex crowd work, meanwhile, we define a workflow-based schema to structure and represent the complex crowd work. Then t\n",
      "\n",
      "6. id: 5390b04120f70186a0ed8759   score: 0.22135067   abstract: Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best prac\n",
      "\n",
      "7. id: 5390ba0a20f70186a0f33839   score: 0.2155158   abstract: Paid crowd work offers remarkable opportunities for improving productivity, social mobility, and the global economy by engaging a geographically distributed workforce to complete complex tasks on demand and at scale. But it is also possible that crowd work will fail to achieve its potential, focusing on assembly-line piecework. Can we foresee a future crowd workplace in which we would want our children to participate? This paper frames the major challenges that stand in the way of this goal. Drawing on theory from organizational behavior and distributed computing, as well as direct feedback from workers, we outline a framework that will enable crowd work that is complex, collaborative, and sustainable. The framework lays out research challenges in twelve major areas: workflow, task assignment, hierarchy, real-time response, synchronous collaboration, quality control, crowds guiding AIs, \n",
      "\n",
      "8. id: 5390bda020f70186a0f4640b   score: 0.21403356   abstract: We investigate characteristics of the technology platform for different types of crowdsourcing initatives, as characterized by their task type--specifically we classify crowdsourcing applications by task structure, task interdependence, and task commitment. The method employed is to examine best practices of well-known crowdsourcing applications, investigating their user interface features, and characteristics that make them successful examples of crowdsourcing. Among the best practices uncovered were the following: easy searching for information; adaptive user interfaces that learned from the crowd; easy-to-use mobile interfaces; the ability to vote ideas up or down; credentialing; and creating sticky user interfaces that engaged the user. Finally, we consider issues for further study and investigation.\n",
      "\n",
      "9. id: 5390a17720f70186a0e53a02   score: 0.20657375   abstract: A number of positive changes have taken place since Glinert and York's 1992 call-to-arms. Progress reviewed in this article includes evolving considerations of universal design in the marketplace, ubiquitous computing with accessibility features, increasing computing research and conference venues that address needs of users with disabilities, and attention to the importance of user empowerment in development.\n",
      "\n",
      "10. id: 55914e6c0cf232eb904fbb07   score: 0.20545557   abstract: Crowdsourcing for social goals (e.g., supporting public libraries or people with disabilities) is a promising area. However, little is known about how to develop active worker communities for such goals. First, we need reliable metrics for the workers' motivation. Second, the characteristics of senior crowd workers have rarely been studied, even though they often play a primary role in social-purpose work. This work introduces a four-quadrant worker motivation model for social-purpose crowdsourcing and describes a system based on that model. Then we investigate the outcomes from the system's operations for six months, which involved both young and senior workers, seeking better ways to build an active community of crowd workers. We analyzed the workers' activities based on the system logs, conducted a survey, assessed the correlations between the subjective values and actual behaviors, a\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691447\n",
      "index                                        559255880cf2aff368683bf0\n",
      "title               Assessing methodologies for intelligent bankru...\n",
      "authors                                             Efstathios Kirkos\n",
      "year                                                           2015.0\n",
      "venue                                  Artificial Intelligence Review\n",
      "references          5590d0de0cf237666fc28f0a;5390a2e920f70186a0e68...\n",
      "abstract            Bankruptcy prediction is one of the most impor...\n",
      "id                                                            1691447\n",
      "clustered_labels                                                    2\n",
      "Name: 1691447, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a74f20f70186a0e8d05d   score: 0.97675806   abstract: Bankruptcy prediction is one of the major business classification problems. In this paper, we use four different techniques (1) logit model, (2) quadratic interval logit model, (3) backpropagation multi-layer perceptron (i.e., MLP), and (4) radial basis function network (i.e., RBFN) to predict bankrupt and non-bankrupt firms in England. The average hit ratio of four methods range from 91.15% to 77.05%. The original classification accuracy and the validation test results indicate that RBFN outperforms the other models.\n",
      "\n",
      "2. id: 5390a1bc20f70186a0e55341   score: 0.9231778   abstract: Evaluation of the reasons for business failures has been a major preoccupation of researchers and practitioners for many years. A large number of methods including multiple regression analysis, discriminant analysis, logit analysis, recursive partitioning algorithm, and several other techniques have been used for the prediction of business failures. This study has followed a different approach using the Rough Set theory to investigate the reasons of bankruptcies in the Turkish banking system. For this purpose, financial ratios of 29-41 commercial banks were examined for the 1995-2007 period. The results showed that this model is a promising alternative to the conventional methods for bankruptcy prediction.\n",
      "\n",
      "3. id: 558ceb0c0cf2cffe760cdfaa   score: 0.87589324   abstract: For financial institutions, the ability to predict or forecast business failures is crucial, as incorrect decisions can have direct financial consequences. Bankruptcy prediction and credit scoring are the two major research problems in the accounting and finance domain. In the literature, a number of models have been developed to predict whether borrowers are in danger of bankruptcy and whether they should be considered a good or bad credit risk. Since the 1990s, machine-learning techniques, such as neural networks and decision trees, have been studied extensively as tools for bankruptcy prediction and credit score modeling. This paper reviews 130 related journal papers from the period between 1995 and 2010, focusing on the development of state-of-the-art machine-learning techniques, including hybrid and ensemble classifiers. Related studies are compared in terms of classifier design, da\n",
      "\n",
      "4. id: 55922cf80cf2c3a0875c9d3d   score: 0.86385167   abstract: Over the last years, it has been observed an increasing interest of the finance and business communities in any application tool related to the prediction of credit and bankruptcy risk, probably due to the need of more robust decision-making systems capable of managing and analyzing complex data. As a result, plentiful techniques have been developed with the aim of producing accurate prediction models that are able to tackle these issues. However, the design of experiments to assess and compare these models has attracted little attention so far, even though it plays an important role in validating and supporting the theoretical evidence of performance. The experimental design should be done carefully for the results to hold significance; otherwise, it might be a potential source of misleading and contradictory conclusions about the benefits of using a particular prediction system. In thi\n",
      "\n",
      "5. id: 5390ae2e20f70186a0ec726a   score: 0.86188734   abstract: Bankruptcy prediction has been a topic of active research for business and corporate organizations since past few decades. The problem has been tackled using various models viz., Statistical, Market Based and Computational Intelligence in the past. Among Computational Intelligence models, Artificial Neural Network has become dominant modeling paradigm. In this Paper, we use a novel Soft Computing tool viz., Fuzzy Support Vector Machine (FSVM) to solve bankruptcy prediction problem. Support Vector Machine is a powerful statistical classification technique based on the idea of Structural Risk Minimization. Fuzzy Sets are capable of handling uncertainty and impreciseness in corporate data. Thus, using the advantage of Machine Learning and Fuzzy Sets prediction accuracy of whole model is enhanced. FSVM is implemented for analyzing predictors as financial ratios. A method of adapting it to de\n",
      "\n",
      "6. id: 5390ba3820f70186a0f371a7   score: 0.8518296   abstract: Predicting business failure is an important and challenging issue that has served as an impetus for many academic studies over the past three decades. This study aims at developing CBR-based hybrid models of predicting business failure. The need to supplement CBR (Case-Based Reasoning) with other classification and diagnosis techniques is triggered by the fact that accuracy and effectiveness tend to get reduced when CBR alone is applied to handle too many attributes. To enhance the accuracy of bankruptcy prediction, the hybrid models developed by this study include: RST-CBR (combining Rough Set Theory with CBR), RST-GRA-CBR (integrating RST, Grey Relational Analysis, and CBR), and CART-CBR (combining Classification and Regression Tree with CBR). In order to verify the ability of the proposed models to achieve optimal accuracy rate, this study further compares the predictive ability of CB\n",
      "\n",
      "7. id: 53909ed120f70186a0e2f833   score: 0.8498466   abstract: We compare several accounting-based models for bankruptcy prediction. The models are developed and tested on large data sets containing annual financial statements for Norwegian limited liability firms. Out-of-sample and out-of-time validation shows that generalized additive models significantly outperform popular models like linear discriminant analysis, generalized linear models and neural networks at all levels of risk. Further, important issues like default horizon and performance depreciation are examined. We clearly see a performance depreciation as the default horizon is increased and as time goes by. Finally a multi-year model, developed on all available data from three consecutive years, is compared with a one-year model, developed on data from the most recent year only. The multi-year model exhibits a desirable robustness to yearly fluctuations that is not present in the one-ye\n",
      "\n",
      "8. id: 5390aa7620f70186a0eaa164   score: 0.82417905   abstract: Bankruptcy prediction is a hot topic. Traditional methods consist of univariate model and multivariate model such as neural network. However, the NNs can not extract effective rules. Thus, a novel approach was proposed in this paper to extract rules. First, t-test method was used to select 5 features from 55 original features. Second, the rule encoding was constructed. Third, the ant colony algorithm was utilized to find the optimal rule. Experiments on 200 corporate demonstrate that this proposed algorithm is effective and rapid.\n",
      "\n",
      "9. id: 5390bda020f70186a0f476d0   score: 0.8213309   abstract: Due to the economic significance of bankruptcy prediction of companies for financial institutions, investors and governments, many quantitative methods have been used to develop effective prediction models. Support vector machine SVM, a powerful classification method, has been used for this task; however, the performance of SVM is sensitive to model form, parameter setting and features selection. In this study, a new approach based on direct search and features ranking technology is proposed to optimise features selection and parameter setting for 1-norm and least-squares SVM models for bankruptcy prediction. This approach is also compared to the SVM models with parameter optimisation and features selection by the popular genetic algorithm technique. The experimental results on a data set with 2010 instances show that the proposed models are good alternatives for bankruptcy prediction.\n",
      "\n",
      "10. id: 53908bfb20f70186a0dca4eb   score: 0.7905303   abstract: This study uses the feature selection algorithm proposed by Setiono and Liu to select the most relevant features for the bankruptcy prediction problem. The method uses a feedforward neural network with one hidden layer to decide which features to be removed. Our data consists of 139 matched pair of bankrupt and non-bankrupt U.S. firms for the period 1983-1994. The results of this study indicate that the final neural network obtained with reduced number of inputs gives significantly better prediction results than the one that uses all initial features.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1637348\n",
      "index                                        559166210cf2e89307ca98fc\n",
      "title                 An Evaluation-Driven Decision Procedure for G3i\n",
      "authors              Mauro Ferrari, Camillo Fiorentini, Guido Fiorino\n",
      "year                                                           2015.0\n",
      "venue                  ACM Transactions on Computational Logic (TOCL)\n",
      "references          5390bda020f70186a0f45ba7;5390b2d620f70186a0eeb...\n",
      "abstract            It is well known that G3i, the sequent calculu...\n",
      "id                                                            1637348\n",
      "clustered_labels                                                    3\n",
      "Name: 1637348, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909a0320f70186a0e209fa   score: 0.888372   abstract: It is well-known that proof search, in general, does not terminate. In some decidable logics (e.g. intuitionistic propositional logic) it is possible to give a terminating sequent calculus, i.e. one in which a naive backward proof search will always terminate. However, such calculi are not always available, even for decidable logics. In this paper we investigate the incorporation of a loop detection mechanism into an inference system for propositional affine logic (i.e. propositional linear logic with arbitrary weakening). This logic is decidable, but no terminating sequent calculus for it is known. We adapt the history techniques used for intuitionistic and modal logics, but in this case we cannot assume that the context will always be non-decreasing. We show how to overcome this problem, and hence to provide a loop detection mechanism for propositional affine logic.\n",
      "\n",
      "2. id: 5390a80f20f70186a0e96590   score: 0.6978149   abstract: Many proof search strategies can be expressed as restrictions on the order of application of the rules of the sequent calculus. Properties of these strategies are then shown by permutation arguments, such as showing that a given strategy is complete by transforming an arbitrary proof into one which obeys the strategy. Such analyses involve some very tedious manipulations of proofs, and are potentially overwhelming for humans. In this paper we investigate the development of systematic techniques for the analysis of sequent calculi. We show how a particular specification of inference rules leads to a detailed analysis of permutation properties for these rules, and we also investigate how to detect redundancies in proofs resulting from these rules.\n",
      "\n",
      "3. id: 5390a1bc20f70186a0e565a3   score: 0.6808786   abstract: The paper introduces a free variable, labelled proof system for intuitionistic propositional logic with variable splitting. In this system proofs can be found without backtracking over rules by generating a single, uniform derivation. We prove soundness, introduce a construction that extracts finite countermodels from unprovable sequents, and formulate a branchwise termination condition. This is the first proof system for intuitionistic propositional logic that admits goal-directed search procedures without compromising proof lengths, compared to corresponding tableau calculi.\n",
      "\n",
      "4. id: 5390b2d720f70186a0eec54b   score: 0.554772   abstract: We give an inductive method for proving weak innermost termination of rule-based programs, from which we automatically infer, for each successful proof, a finite strategy for data evaluation. We first present the proof principle, using an explicit induction on the termination property, to prove that any input data has at least one finite evaluation. For that, we observe proof trees built from the rewrite system, schematizing the innermost rewriting tree of any ground term, and generated with two mechanisms: abstraction, schematizing normalization of subterms, and narrowing, schematizing rewriting steps. Then, we show how, for any ground term, a normalizing rewriting strategy can be extracted from the proof trees, even if the ground term admits infinite rewriting derivations.\n",
      "\n",
      "5. id: 5390b2d620f70186a0eebbf7   score: 0.51107603   abstract: LJQ is a focused sequent calculus for intuitionistic logic, with a simple restriction on the first premisss of the usual left introduction rule for implication. We discuss its history (going back to about 1950, or beyond), present the underlying theory and its applications both to terminating proof-search calculi and to call-by-value reduction in lambda calculus.\n",
      "\n",
      "6. id: 5390bb7b20f70186a0f405eb   score: 0.44417298   abstract: We describe how the Davis-Putnam-Logemann-Loveland procedure DPLL is bisimilar to the goal-directed proof-search mechanism described by a standard but carefully chosen sequent calculus. We thus relate a procedure described as a transition system on states to the gradual completion of incomplete proof-trees. For this we use a focused sequent calculus for polarised classical logic, for which we allow analytic cuts. The focusing mechanisms, together with an appropriate management of polarities, then allows the bisimulation to hold: The class of sequent calculus proofs that are the images of the DPLL runs finishing on UNSAT, is identified with a simple criterion involving polarities. We actually provide those results for a version DPLL(T) of the procedure that is parameterised by a background theory T for which we can decide whether conjunctions of literals are consistent. This procedure is \n",
      "\n",
      "7. id: 539099b320f70186a0e195e0   score: 0.44360042   abstract: We consider the following decision problems:ProofNet: Is a given multiplicative linear logic (MLL) proof structure a proof net?EssNet: Is a given essential net (of an intuitionistic MLL sequent) correct?In this article we show how to obtain linear-time algorithms for EssNet. As a corollary, by showing that ProofNet is linear-time reducible to EssNet (by the Trip Translation), we obtain a linear-time algorithm for ProofNet.We show further that it is possible to optimize the verification so that each node of the input structure is visited at most once. Finally, we present linear-time algorithms for sequentializing proof nets and essential nets, that is, for finding derivations of the underlying sequents.\n",
      "\n",
      "8. id: 53908e0020f70186a0dd42dd   score: 0.4378235   abstract: Abstract: A methodology is introduced for the automatic generation of theorem provers from sets of proof examples. As an example, this methodology was used to generate a theorem prover for intuitionistic propositional calculus which proves any theorem for this logic found D. van Dalen's book \"Logic and structure\" (Springer-Verlag, 1994), using a depth-first search strategy without loop detection.\n",
      "\n",
      "9. id: 539095bb20f70186a0df22f5   score: 0.4154916   abstract: We introduce a method to associate calculi of proof terms and rewrite rules with cut elimination procedures for logical deduction systems (i.e., Gentzen-style sequent calculi) in the case of intuitionistic logic. We illustrate this method using two different versions of the cut rule for a variant of the intuitionistic fragment of Kleene's logical deduction system G3.Our systems are in fact calculi of explicit substitution, where the cut rule introduces an explicit substitution and the left-→ rule introduces a binding of the result of a function application. Cut propagation steps of cut elimination correspond to propagation of explicit substitutions, and propagation of weakening (to eliminate it) corresponds to propagation of index-updating operations. We prove various subject reduction, termination, and confluence properties for our calculi.Our calculi improve on some earlier calculi for\n",
      "\n",
      "10. id: 53908b6c20f70186a0dbed0d   score: 0.40927982   abstract: We present a bottom-up decision procedure for propositional modal logic K based on the inverse method. The procedure is based on the 驴inverted驴 version of a sequent calculus. To restrict the search space, we prove a number of redundancy criteria for derivations in the sequent calculus. We introduce a new technique of proving redundancy criteria, based on the analysis of tableau-based derivations in K. Moreover, another new technique is used to prove completeness of proof-search with a strong notion of subsumption. This technique is based on so-called traces. A new formalization of the inverse method in the form of a path calculus considerably simplifies all proofs as compared to the previously published presentations of the inverse method. Experimental results reported elsewhere demonstrate that our method is competitive with many state-of-the-art implementations of K.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1700401\n",
      "index                                        55923776612c4fa28ff79fb0\n",
      "title               On the causes of subject-specific citation rat...\n",
      "authors                                    Werner Marx, Lutz Bornmann\n",
      "year                                                           2015.0\n",
      "venue                                                  Scientometrics\n",
      "references          5390a2be20f70186a0e6467a;5390aa7620f70186a0eab...\n",
      "abstract            It is well known in bibliometrics that the ave...\n",
      "id                                                            1700401\n",
      "clustered_labels                                                    2\n",
      "Name: 1700401, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390990f20f70186a0e10373   score: 0.94427973   abstract: We analyze citation frequencies for two main database conferences (SIGMOD, VLDB) and three database journals (TODS, VLDB Journal, Sigmod Record) over 10 years. The citation data is obtained by integrating and cleaning data from DBLP and Google Scholar. Our analysis considers different comparative metrics per publication venue, in particular the total and average number of citations as well as the impact factor which has so far only been considered for journals. We also determine the most cited papers, authors, author institutions and their countries.\n",
      "\n",
      "2. id: 5390be6620f70186a0f4cf64   score: 0.81640637   abstract: Bibliometric indicators can be determined by comparing specific citation records with the percentiles of a reference set. However, there exists an ambiguity in the computation of percentiles because usually a significant number of papers with the same citation count are found at the border between percentile rank classes. The present case study of the citations to the journal Europhysics Letters (EPL) in comparison with all physics papers from the Web of Science shows the deviations which occur due to the different ways of treating the tied papers in the evaluation of the percentage of highly cited publications. A strong bias can occur, if the papers tied at the threshold number of citations are all considered as highly cited or all considered as not highly cited.\n",
      "\n",
      "3. id: 5390bf1320f70186a0f51fb9   score: 0.75815266   abstract: The number of references per paper, perhaps the best single index of a journal's scholarliness, has been studied in different disciplines and periods. In this paper we present a four decade study of eight engineering journals. A data set of over 70,000 references was generated after automatic data gathering and manual inspection for errors. Results show a significant increase in the number of references per paper, the average rises from 8 in 1972 to 25 in 2013. This growth presents an acceleration around the year 2000, consistent with a much easier access to search engines and documents produced by the generalization of the Internet.\n",
      "\n",
      "4. id: 53908bad20f70186a0dc3a4a   score: 0.753648   abstract: The most frequently cited journal articles in the Arts & Humanities Citation Index (A&HCI) are analyzed in terms of their disciplinary classification. Four years (1976-79) of the A&HCI data base yielded 144 journal articles which were cited 10 or more times during the period. These articles are predominantly from the disciplines of Language and Linguistics (31%), Philosophy (23%), History (13%), Religion (8%), and Archeology (6%).However, most citations in the A&HCI are to books rather than journal articles (96% versus 4% among those items cited 10 or more times). The discipline of Literature (or Literary Criticism) is predominant in the list of highly cited books.These statistics suggest systematic variation in the resources used by the different disciplines. Various other aspects of the A&HCI data base are explored, and some specific examples are discussed in depth.\n",
      "\n",
      "5. id: 5390b19020f70186a0ee0727   score: 0.7140302   abstract: Employing a citation analysis, this study explored and compared the bibliometric characteristics and the subject relationship with other disciplines of and among the three leading information science journals, Journal of the American Society for Information Science and Technology (JASIST), Information Processing and Management and Journal of Documentation. The citation data were drawn from references of each article of the three journals during 1998 and 2008. The Ulrich's Periodical Directory, Library of Congress Subject Heading, retrieved from the WorldCat, and LISA database were used to identify the main class, subclass and subject of cited journals and books. Quantitative results on the number of JASIST, IPM and JOD literature references, average number of references cited per paper, document type of cited literature and the journal self-citation rate are reported. Moreover, the highl\n",
      "\n",
      "6. id: 53908b4920f70186a0dbbe06   score: 0.6689884   abstract: In each of 41 research journals in the physical, life, and social sciences there is a linear relationship between the average number of references and the normalized paper lengths. For most of the journals in a given field, the relationship is the same within statistical errors. For papers of average lengths in different sciences the average number of references is the same within ±17%. Because papers of average lengths in various sciences have the same number of references, we conclude that the citation counts to them can be intercompared within that accuracy. However, review journals are different: after scanning 18 review journals we found that those papers average twice the number of references as research papers of the same lengths.\n",
      "\n",
      "7. id: 5390b04120f70186a0ed7401   score: 0.49872017   abstract: This paper studies evidence from Thomson Scientific (TS) about the citation process of 3.7 million articles published in the period 1998---2002 in 219 Web of Science (WoS) categories, or sub-fields. Reference and citation distributions have very different characteristics across sub-fields. However, when analyzed with the Characteristic Scores and Scales (CSS) technique, which is replication and scale invariant, the shape of these distributions over three broad categories of articles appears strikingly similar. Reference distributions are mildly skewed, but citation distributions with a 5-year citation window are highly skewed: the mean is 20 points above the median, while 9---10% of all articles in the upper tail account for about 44% of all citations. The aggregation of sub-fields into disciplines and fields according to several aggregation schemes preserve this feature of citation dist\n",
      "\n",
      "8. id: 5390a17720f70186a0e529af   score: 0.479915   abstract: This study examines the differences between Scopus and Web of Science in the citation counting, citation ranking, and h-index of 22 top human-computer interaction (HCI) researchers from EQUATOR—a large British Interdisciplinary Research Collaboration project. Results indicate that Scopus provides significantly more coverage of HCI literature than Web of Science, primarily due to coverage of relevant ACM and IEEE peer-reviewed conference proceedings. No significant differences exist between the two databases if citations in journals only are compared. Although broader coverage of the literature does not significantly alter the relative citation ranking of individual researchers, Scopus helps distinguish between the researchers in a more nuanced fashion than Web of Science in both citation counting and h-index. Scopus also generates significantly different maps of citation networks of indi\n",
      "\n",
      "9. id: 558bd3070cf2e30013db1846   score: 0.441673   abstract: This study puts an emphasis on the disciplinary differences observed for the behaviour of citations and downloads. This was exemplified by studying citations over the last 10 years in four selected fields, namely, arts and humanities, computer science, economics, econometrics, and finance, and oncology. Differences in obsolescence characteristics were studied using synchronic as well as diachronic counts. Furthermore, differences between document types were taken into consideration and correlations between journal impact and journal usage measures were calculated. The number of downloads per document remains almost constant for all four observed areas within the last four years, varying from approximately 180 (oncology) to 300 (economics). The percentage of downloaded documents is higher than 90 % for all areas. The number of citations per document ranges from one (arts and humanities) t\n",
      "\n",
      "10. id: 53909ed120f70186a0e30bfc   score: 0.357119   abstract: We use a new data gathering method, “Web&sol;URL citation,” Web&sol;URL and Google Scholar to compare traditional and Web-based citation patterns across multiple disciplines (biology, chemistry, physics, computing, sociology, economics, psychology, and education) based upon a sample of 1,650 articles from 108 open access (OA) journals published in 2001. A Web&sol;URL citation of an online journal article is a Web mention of its title, URL, or both. For each discipline, except psychology, we found significant correlations between Thomson Scientific (formerly Thomson ISI, here: ISI) citations and both Google Scholar and Google Web&sol;URL citations. Google Scholar citations correlated more highly with ISI citations than did Google Web&sol;URL citations, indicating that the Web&sol;URL method measures a broader type of citation phenomenon. Google Scholar citations were more numerous than IS\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1673006\n",
      "index                                        55916b0d0cf2e89307ca9ae8\n",
      "title                           Statistical Debugging for Simulations\n",
      "authors             Ross Gore, Paul F. Reynolds Jr., David Kamensk...\n",
      "year                                                           2015.0\n",
      "venue               ACM Transactions on Modeling and Computer Simu...\n",
      "references          5390b0ca20f70186a0edb7ef;53908a4020f70186a0d9d...\n",
      "abstract            Predictions from simulations have entered the ...\n",
      "id                                                            1673006\n",
      "clustered_labels                                                    3\n",
      "Name: 1673006, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b9d520f70186a0f313d6   score: 0.9129032   abstract: Predictions from simulations with inherent uncertainty have entered the mainstream of public policy decision-making practices. Unfortunately, methods for gaining insight into unexpected simulation outcomes have not kept pace. Subject matter experts (SMEs) need to understand if the unexpected outcomes reflect a fault in the simulation or new knowledge. Recent work has adapted statistical debuggers, used in software engineering, to automatically identify simulation faults via extensive profiling of executions. The adapted debuggers have been shown to be effective, but have only been applied to simulations with large test suites and known faults. Here we employ these debuggers in a different manner. We investigate how they facilitate a SME exploring an unexpected outcome that reflects new knowledge. We also evaluate the debuggers in the face of smaller test suites and sparse execution profi\n",
      "\n",
      "2. id: 5390b04120f70186a0ed69fc   score: 0.5563996   abstract: Debugging is notoriously difficult and extremely time consuming. Researchers have therefore invested a considerable amount of effort in developing automated techniques and tools for supporting various debugging tasks. Although potentially useful, most of these techniques have yet to demonstrate their practical effectiveness. One common limitation of existing approaches, for instance, is their reliance on a set of strong assumptions on how developers behave when debugging (e.g., the fact that examining a faulty statement in isolation is enough for a developer to understand and fix the corresponding bug). In more general terms, most existing techniques just focus on selecting subsets of potentially faulty statements and ranking them according to some criterion. By doing so, they ignore the fact that understanding the root cause of a failure typically involves complex activities, such as na\n",
      "\n",
      "3. id: 5390b71120f70186a0f1dae6   score: 0.4076281   abstract: Statistical debuggers use data collected during test case execution to automatically identify the location of faults within software. Recent work has applied causal inference to eliminate or reduce control and data flow dependence confounding bias in statement-level statistical debuggers. The result is improved effectiveness. This is encouraging but motivates two novel questions: (1) how can causal inference be applied in predicate-level statistical debuggers and (2) what other biases can be eliminated or reduced. Here we address both questions by providing a model that eliminates or reduces control flow dependence and failure flow confounding bias within predicate-level statistical debuggers. We present empirical results demonstrating that our model significantly improves the effectiveness of a variety of predicate-level statistical debuggers, including those that eliminate or reduce on\n",
      "\n",
      "4. id: 5390b04120f70186a0ed7768   score: 0.34257153   abstract: Software debugging is a notoriously difficult, extremely time consuming, and human-intensive activity. For this reason, researchers have invested a great deal of effort in developing automated techniques and tools for supporting various debugging tasks. Although potentially useful, most of these techniques have yet to fully demonstrate their practical effectiveness. Moreover, many current debugging approaches suffer from some common limitations. In particular, many techniques rely on a set of strong assumptions on how developers behave when debugging, fail to leverage the rich source of information represented by the user population, and focus mainly on trying to reduce the number of statements to examine, mostly ignoring the importance of identifying relevant inputs. This talk will provide an overview of the state of the art in the area of software debugging, discuss strengths and weakn\n",
      "\n",
      "5. id: 539088b820f70186a0d8f362   score: 0.24908175   abstract: Decision-making is a complex and important task in software engineering. The current state-of-the-practice is rather non-systematic as it typically relies upon personal experience without using explicit models. Empirical studies can help but are to some extent context dependent and costly to conduct. Typically it is not efficient or even possible to conduct empirical studies for a large number of context parameter variations. We propose to build on a set of systematic empirical studies to fill gaps in context variable space with simulation: (a) Simulation can use the empirical results from different contexts and apply them to a planning situation as appropriate. (b) The analysis of simulation results can point out situations and factors for which conducting empirical studies would be most worthwhile. This paper presents a general decision model, a simulation framework, and examples for d\n",
      "\n",
      "6. id: 5390979920f70186a0e00301   score: 0.2180027   abstract: A large part of software engineering research suffers from a major problem-there are insufficient data to test software hypotheses, or to estimate parameters in models. To obtain statistically significant results, a large set of programs is needed, each set comprising many programs built to the same specification. We have gained access to such a large body of programs (written in C, C++, Java or Pascal) and in this paper we present the results of an exploratory analysis of around 29,000 C programs written to a common specification. The objectives of this study were to characterise the types of fault that are present in these programs; to characterise how programs are debugged during development; and to assess the effectiveness of diverse programming. The findings are discussed, together with the potential limitations on the realism of the findings.\n",
      "\n",
      "7. id: 5390b4c420f70186a0eff11d   score: 0.18271701   abstract: Traditional debugging and fault localization methods have addressed localization of sources of software failures. While these methods are effective in general, they are not tailored to an important class of software, including simulations and computational models, which employ floating-point computations and continuous stochastic distributions to represent, or support evaluation of, an underlying model. To address this shortcoming, we introduce elastic predicates, a novel approach to predicate-based statistical debugging. Elastic predicates introduce profiling of values assigned to variables within a failing program. These elastic predicates are better predictors of software failure than the static and uniform predicates used in existing techniques such as Cooperative Bug Isolation (CBI). We present experimental results for established fault localization benchmarks and widely used simula\n",
      "\n",
      "8. id: 53909ee020f70186a0e32280   score: 0.16357902   abstract: Manual debugging is tedious, as well as costly. The high cost has motivated the development of fault localization techniques, which help developers search for fault locations. In this paper, we propose a new statistical method, called Sober, which automatically localizes software faults without any prior knowledge of the program semantics. Unlike existing statistical approaches that select predicates correlated with program failures, Sober models the predicate evaluation in both correct and incorrect executions and regards a predicate as fault-relevant if its evaluation pattern in incorrect executions significantly diverges from that in correct ones. Featuring a rationale similar to that of hypothesis testing, Sober quantifies the fault relevance of each predicate in a principled way. We systematically evaluate Sober under the same setting as previous studies. The result clearly demonstr\n",
      "\n",
      "9. id: 5390b9d520f70186a0f3198f   score: 0.16277891   abstract: This paper describes and explores applications of several new methods for explaining unexpected behavior in Monte Carlo simulations: (1) the use of fuzzy logic to represent the extent to which a program behaves as expected, (2) the analysis of variable value density distributions, and (3) the geometric treatment of predicate lists as vectors when comparing simulation runs with normal and unexpected outputs. These methods build on previous attempts to localize faults in computer programs. They address weaknesses of existing techniques in cases where programs contain real-valued random variables. The new methods were able to locate a source of error in a Monte Carlo simulation and find faults in benchmarks used by the fault localization community.\n",
      "\n",
      "10. id: 539099a220f70186a0e18458   score: 0.13206616   abstract: We describe a statistical approach to software debugging in the presence of multiple bugs. Due to sparse sampling issues and complex interaction between program predicates, many generic off-the-shelf algorithms fail to select useful bug predictors. Taking inspiration from bi-clustering algorithms, we propose an iterative collective voting scheme for the program runs and predicates. We demonstrate successful debugging results on several real world programs and a large debugging benchmark suite.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1650945\n",
      "index                                        5591732b0cf2e89307ca9e3c\n",
      "title               (Re)defining Land Change Science through Synth...\n",
      "authors                             Alyson L. Young, Wayne G. Lutters\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390b3da20f70186a0ef5a77;539095ba20f70186a0df0...\n",
      "abstract            This paper investigates the co-evolution of sc...\n",
      "id                                                            1650945\n",
      "clustered_labels                                                    3\n",
      "Name: 1650945, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4c352   score: 0.94427973   abstract: This article presents preliminary findings from an ongoing study of collaborative knowledge creation in the land change science (LCS) research community. Using observational data from two international workshops on LCS meta-study practice, we document the challenges to current approaches highlighting the need for direct interaction with case study authors. Results from the study are being used to enhance the meta-study process through GLOBE, new scientific cyberinfrastructure for users to share, compare, and synthesize local and regional data.\n",
      "\n",
      "2. id: 5390be6620f70186a0f4c444   score: 0.94122344   abstract: My project is a field study of scientific practice in land change science (LCS). Through observations, interviews and documents, my research investigates the impact and role of the meta-study as a tool of scientific knowledge creation. Data from my study is being used to inform the design and refinement of collaborative cyber-infrastructure for case study researchers to share, compare and synthesize local and regional studies at global scales to be used in meta-analysis.\n",
      "\n",
      "3. id: 5390b86b20f70186a0f2893e   score: 0.6981237   abstract: Supporting scientific practice has been a longstanding goal of CSCW research. This paper explores how we might design for social science research practices and collaboration. Drawing on sixteen interviews with fieldwork-based social scientists we document the importance of small-scale long-term collaborative arrangements for research and intellectual work - pairs of researchers who work together in-depth over their careers, developing a common yet distinctive view of their research field. This contrasts with the large-scale short-lived collaborations that have classically been the target of cyber-infrastructure work. We describe technology practices among social scientists and how these can inform technology design for fieldwork practices.\n",
      "\n",
      "4. id: 5390990f20f70186a0e10df1   score: 0.6321622   abstract: A balanced cyberinfrastructure is necessary to meet growing data-intensive scientific needs.\n",
      "\n",
      "5. id: 5390ad0720f70186a0ebae2d   score: 0.6151456   abstract: e-Research and Cyberinfrastructure programmes actively promote the development of new forms of scientific practice and collaboration through the implementation of tools and technologies that support distributed collaborative work across geographically dispersed research institutes and laboratories. Whilst originating in scientific domains, we have more recently seen a turn to the design of systems that support research practices in the social sciences and the arts and humanities. Attempts to embed large-scale infrastructures into research settings has brought to the fore the necessity of understanding the knowledge, skills and practices of researchers within a variety of disciplines that might use these technologies. In this paper, we consider an approach to gathering requirements through the introduction of various technical interventions for relatively short term periods so that we may\n",
      "\n",
      "6. id: 5390b3ae20f70186a0ef2ead   score: 0.56799656   abstract: Scientific information infrastructures, or cyberinfrastructures, are expected to operate over long time scales, but this creates challenges for the design of those infrastructures. This paper reports on a qualitative study of cyberinfrastructure development in the emerging field of metagenomics to illustrate some of the issues that can arise when cyberinfrastructures are faced with new scientific communities, practices, and research questions. New science inevitably brings new forms of data, new analysis tools, and the need to recontextualize existing data. Cyberinfrastructures must be prepared to adapt to the new scientific context. In this study, developers employed three strategies for addressing new scientific requirements: work-arounds, extensions, and from-scratch development. These strategies are informed by the tension between fitting the CI to the needs of a specific community a\n",
      "\n",
      "7. id: 5390baa120f70186a0f38ce3   score: 0.55588734   abstract: In this work-in-progress paper, we present GLOBE, a system that enables the quantitative comparison and synthesis of local case study data to support meta-analyses of global environmental change. Using data from a workshop on the state-of-the-art of meta-study in the land change science research community, we highlight the limitations of current approaches and illustrate how our system can be designed to enhance data accuracy and produce globally relevant results.\n",
      "\n",
      "8. id: 5390ad0720f70186a0ebc1dd   score: 0.5086051   abstract: Cyberinfrastructure is a rapidly growing area of global research and funding with a history of emphasizing the role technology will play in changing scientific work practices. This paper proposes a practice-theoretic perspective that is informative to cyberinfrastructure research and design. To illustrate the relevancy of a practice-theoretic perspective to cyberinfrastructure, this paper presents a critical review of 160 cyberinfrastructure research papers and reports published in the last decade through a perspective of embodied practice. After relating common cyberinfrastructure research themes through a focus on embodied practice, we propose a series of early implications for design aimed at incorporating the lessons of embodied practice into the design and development of future cyberinfrastructure applications.\n",
      "\n",
      "9. id: 5390a1f820f70186a0e5c7a0   score: 0.42560524   abstract: A community cyberinfrastructure would enable a new era of multidisciplinary research and collaboration in science and engineering. With such an infrastructure, researchers could share knowledge and results along with computing cycles, storage, and bandwidth. A generic, transparent cyberinfrastructure would also foster more meaningful analyses of data and visualization, modeling, and simulation of real-world phenomena.\n",
      "\n",
      "10. id: 558aee2b612c41e6b9d3da67   score: 0.3829488   abstract: Cyberinfrastructures--scientific research environments that span multiple institutions--require careful crafting and collaboration among domain specialists and software engineers, but several factors impede this collaboration. Recognizing five rules of thumb can help facilitate successful cyberinfrastructure creation and accelerate science in the disciplines they support.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1701416\n",
      "index                                        55323ad445cec66b6f9d8e23\n",
      "title                  Approximating Spanning Trees with Few Branches\n",
      "authors                             Markus Chimani, Joachim Spoerhase\n",
      "year                                                           2015.0\n",
      "venue                                     Theory of Computing Systems\n",
      "references          539087f820f70186a0d71b5e;5390bf1320f70186a0f51...\n",
      "abstract            Given an undirected, connected graph, the aim ...\n",
      "id                                                            1701416\n",
      "clustered_labels                                                    2\n",
      "Name: 1701416, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a1bc20f70186a0e5608a   score: 0.9519527   abstract: We consider a natural generalization of the classical minimum spanning tree problem called Minimum Spanning Tree with Neighborhoods (MSTN)which seeks a tree of minimum length to span a set of 2D regions called neighborhoods. Each neighborhood contributes exact one node to the tree, and the MSTN has the minimum total length among all possible trees spanning the set of nodes. We prove the NP-hardness of this problem for the case in which the neighborhoods are a set of disjoint discs and rectangles. When the regions considered are a set of disjoint 2D unit discs, we present the following approximation results: (1) A simple algorithm that achieves an approximation ratio of 7.4; (2) Lower bounds and two 3-approximation algorithms; (3) A PTAS for this problem. Our algorithms can be easily generalized to higher dimensions.\n",
      "\n",
      "2. id: 5390b44620f70186a0ef8f60   score: 0.94024336   abstract: We consider the problem of finding a spanning tree with maximum number of leaves. A 2-approximation algorithm is known for this problem, and a 3/2-approximation algorithm when restricted to graphs where every vertex has degree 3 (cubic graphs). The problem is known to be APX-hard in general, and NP-hard for cubic graphs. We show that it is also APX-hard for cubic graphs. The APX-hardness of the related problem Minimum Connected Dominating Set for cubic graphs follows.\n",
      "\n",
      "3. id: 5390a55520f70186a0e7950d   score: 0.9374402   abstract: We review the literature on minimum spanning tree problems with two or more objective functions (MOST) each of which is of the sum or bottleneck type. Theoretical aspects of different types of this problem are summarized and available algorithms are categorized and explained. The paper includes a concise tabular presentation of all the reviewed papers.\n",
      "\n",
      "4. id: 53909f6920f70186a0e39c03   score: 0.90747637   abstract: Given a metric graph G, we are concerned with finding a spanning tree of G where the maximum weighted degree of its vertices is minimum. In a metric graph (or its spanning tree), the weighted degree of a vertex is defined as the sum of the weights of its incident edges. In this paper, we propose a 4.5-approximation algorithm for this problem. We also prove it is NP-hard to approximate this problem within a 2-ε factor.\n",
      "\n",
      "5. id: 5390a40520f70186a0e6fa9c   score: 0.90414387   abstract: Let G = (V ,E ) be a graph which models a set of wireless devices (nodes V ) that can communicate by means of multiple radio interfaces, according to proximity and common interfaces (edges E ). In general, every node holds a subset of all the possible k interfaces. Such networks are known as multi-interface networks. In this setting, we study a basic problem called Connectivity , corresponding to the well-known Minimum Spanning Tree problem in graph theory. In practice, we need to cover a subgraph of G of minimum cost which contains a spanning tree of G . A connection is covered (activated) when the endpoints of the corresponding edge share at least one active interface. The connectivity problem turns out to be APX -hard in general and for many restricted graph classes, however it is possible to provide approximation algorithms: 2-approximation in general and $(2-\\frac 1 k)$-approximatio\n",
      "\n",
      "6. id: 53908adf20f70186a0dac0f4   score: 0.89982784   abstract: Let G = (V, E) be a requirements graph. Let d = (dij)i,j=1n be a length metric. For a tree T denote by dT (i,j) the distance between i and j in T (the length according to d of the unique i - j path in T). The restricted diameter of T, DT, is the maximum distance in T between pair of vertices with requirement between them. The minimum restricted diameter spanning tree problem is to find a spanning tree T such that the minimum restricted diameter is minimized. We prove that the minimum restricted diameter spanning tree problem is NP- hard and that unless P = NP there is no polynomial time algorithm with performance guarantee of less than 2. In the case that G contains isolated vertices and the length matrix is defined by distances over a tree we prove that there exist a tree over the non-isolated vertices such that its restricted diameter is at most 4 times the minimum restricted diameter \n",
      "\n",
      "7. id: 539096cb20f70186a0df6fe1   score: 0.8978745   abstract: Let G = (V,E) be a requirement graph. Let d = (dij)ni,j=1 be a length metric. For a tree T denote by dT(i,j) the distance between i and j in T (the length according to d of the unique i - j path in T). The restricted diameter of T, DT, is the maximum distance in T between pair of vertices with requirement between them. The minimum restricted diameter spanning tree problem is to find a spanning tree T such that the restricted diameter is minimized. We prove that the minimum restricted diameter spanning tree problem is NP-hard and that unless P = NP there is no polynomial time algorithm with performance guarantee of less than 2. In the case that G contains isolated vertices and the length matrix is defined by distances over a tree we prove that there exists a tree over the non-isolated vertices such that its restricted diameter is at most 4 times the minimum restricted diameter and that th\n",
      "\n",
      "8. id: 5390a7f520f70186a0e94527   score: 0.8978745   abstract: Consider the NP-hard problem of, given a simple graph G, to find a series-parallel subgraph of G with the maximum number of edges. The algorithm that, given a connected graph G, outputs a spanning tree of G, is a $\\frac12$-approximation. Indeed, if n is the number of vertices in G, any spanning tree in G has n驴1 edges and any series-parallel graph on n vertices has at most 2n驴3 edges. We present a $\\frac{7}{12}$-approximation for this problem and results showing the limits of our approach.\n",
      "\n",
      "9. id: 5390b86b20f70186a0f292be   score: 0.89733595   abstract: We consider the MaximumInternalSpanningTree problem which is to find a spanning tree of a given graph with a maximum number of non-leaf nodes. From an optimization point of view, this problem is equivalent to the MinimumLeafSpanningTree problem, and is NP-hard as being a generalization of the HamiltonianPath problem. Although there is no constant factor approximation for the MinimumLeafSpanningTree problem [1], MaximumInternalSpanningTree can be approximated within a factor of 2 [2]. In this paper we improve this factor by giving a 7/4 -approximation algorithm. We also investigate the node-weighted case, when the weighted sum of the internal nodes is to be maximized. For this problem, we give a (2Δ - 3)-approximation for general graphs, and a 2-approximation for claw-free graphs. All our algorithms are based on local improvement steps.\n",
      "\n",
      "10. id: 53908ae020f70186a0dae1cb   score: 0.8969756   abstract: We consider the following problem: given a connected graph G = (V, ƐE) and an additional edge set E, find a minimum size subset of edges F ⊆ E suchth at (V, Ɛ ∪ F) is 2-edge connected. This problem is NP-hard. For a long time, 2 was the best approximation ratio known. Recently, Nagamochi reported a (1.875 + Ɛ)-approximation algorithm. We give a new algorithm with a better approximation ratio of 3/2 and a practical running time.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707115\n",
      "index                                        55323b8a45cec66b6f9da1ad\n",
      "title               An Efficient Curve-Scanline Intersection Locat...\n",
      "authors                                  Yun-Nan Chang, Ting-Chi Tong\n",
      "year                                                           2015.0\n",
      "venue                            Journal of Signal Processing Systems\n",
      "references          5390a93b20f70186a0ea1562;5390a9a520f70186a0ea5...\n",
      "abstract            This paper presents an efficient intersection ...\n",
      "id                                                            1707115\n",
      "clustered_labels                                                    1\n",
      "Name: 1707115, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a1e620f70186a0e599c8   score: 0.9790758   abstract: The class of scan-line processing algorithms introduced here can be used for efficient image generation and volume integral calculations and is shown to outperform previous methods.\n",
      "\n",
      "2. id: 53909f8220f70186a0e3d042   score: 0.97246706   abstract: Innovative concepts in hardware architecture and data structure enable graphics processing at a speed about five times that of conventional raster-scan devices.\n",
      "\n",
      "3. id: 539090c420f70186a0dde3be   score: 0.94355595   abstract: Given a collection of polygons (or a collection of sets of polygons) with vertex points specified to some fixed-point precision, a technique is presented for accurately representing the intersection points formed by elements in the collection. In particular, we recognize that in 2D, the result of a series of intersections between polygons must be composed of line segments that are part of the line segments making up the original polygons. While our techniques greatly improve the accuracy of floating point representations for intersection points, we also provide algorithms based on rational arithmetic that compute these intersection points exactly. The storage required to represent an intersection vertex is slightly greater than four times the number of bits in the input resolution. Furthermore, no intermediate quantity requires resolution slightly greater than four times the resolution r\n",
      "\n",
      "4. id: 53909f8220f70186a0e3d09c   score: 0.9405718   abstract: Low-cost PCs and workstations with bit-mapped graphics possess limited processing power. As a result, efficient algorithms are needed to draw curves interactively.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f50340   score: 0.9078038   abstract: Because the B-spline surface intersection is a fundamental operation in geometric design software, it is important to make the surface intersection operation robust and efficient. As is well known, affine arithmetic is robust for calculating the surface intersection because it is able to not only find every branch of the intersection, but also deal with some singular cases, such as surface tangency. However, the classical affine arithmetic is defined only for the globally supported polynomials, and its computation is very time consuming, thus hampering its usefulness in practical applications, especially in geometric design. In this paper, we extend affine arithmetic to calculate the range of recursively and locally defined B-spline basis functions, and we accelerate the affine arithmetic-based surface intersection algorithm by using a GPU. Moreover, we develop efficient methods to thin \n",
      "\n",
      "6. id: 5390879220f70186a0d3c87d   score: 0.7829945   abstract: With widespread use of raster scan displays and the ever-increasing desire for faster interactivity, higher image complexity, and higher resolution in displayed images, several techniques have been proposed for rasterizing primitive graphical objects. This paper characterizes the performance of these techniques and shows how they evolve for more complex images on higher resolution displays. This characterization will not only show the strengths and deficiencies of existing rasterization techniques, but will also reveal new architectures for future raster graphics systems.\n",
      "\n",
      "7. id: 5390b9d520f70186a0f30bc1   score: 0.7324005   abstract: The 3D devices have been widely applied in people's life, for example, smart phone, internet games, high-definition video, geography navigation, etc. The large scale graphics rendering depends on the computing power of the hardware greatly, the calculation of model rasterization operations need amounts of data. It has become the bottleneck of system performance. This paper proposed the method which can accelerate graphics rasterization procedure, based on the idea of tile to meet the needs of graphical applications on embedded platform. The rules and procedures of algorithm are introduced. By using of the XUP-LX110T, the experiments are carried out. It is verified that the method should been compensated the features for the lack of resources and poor computing power of embedded platforms. It can apply smaller chip area to achieve graphics acceleration with fewer resources.\n",
      "\n",
      "8. id: 539089bb20f70186a0d980af   score: 0.72710437   abstract: Many measurement application fields need to calculate cross bar intersection locations of horizontal and vertical bars. The system we developed and that we present in this paper is an embedded system that measures cross bar positions with sub-pixel accuracy on 1024 × 1024 pixel images delivered by a camera at a 50 MHz data rate in real time. This is done using an algorithm that looks for intersection areas and then locally calculates two lines representing horizontal and vertical bars. The two line intersection is considered to be the bar intersection. To achieve real time, we developed a hybrid architecture in which low level processes are implemented into FPGAs and others into DSPs. As a result, at the end of the camera scanning (20 ms), all calculations are completed and the results are available.\n",
      "\n",
      "9. id: 53908f5b20f70186a0dd9d83   score: 0.717408   abstract: The problem of computing the intersection of parametric and algebraic curves arises in many applications of computer graphics, geometric and solid modeling. Previous algorithms are based on techniques from Elimination theory or subdivision and iteration. The former is however, restricted to low degree curves. This is mainly due to issues of efficiency and numerical stability. In this paper we use Elimination theory and express the resultant of the equations of intersection as a matrix determinant. The matrix itself rather than its symbolic determinant, a polynomial, is used as representation. The algorithm for intersection corresponds to substituting the other equation to construct an equivalent matrix such that the intersection points can be extracted from the eigenvalues and eigenvectors of the latter. Moreover, the algebraic and geometric multiplicities of eigenvalues give us informat\n",
      "\n",
      "10. id: 5390a7f620f70186a0e94d78   score: 0.6786465   abstract: The rasterization stage in a graphics processing unit (GPU), which consists of triangle setup, rasterization, and parameter interpolation with plane equations, always requires huge operations and is usually the bottleneck of the performance. For real-time applications, a Universal Rasterizer (UR) with edge equations and a tile-scan triangle traversal algorithm are proposed for low cost graphics rendering. In UR, the basic functions for parameter interpolation and rasterization can be executed with a universal shared hardware to reduce the cost. The result shows that it can minimize the processing time of triangle traversal and guarantee no reiteration when traverse. With the hardware sharing and architecture design techniques of pipelining and scheduling, it can achieve the real-time requirements for graphics applications with reasonable hard-ware cost.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717673\n",
      "index                                        559136680cf232eb904fb474\n",
      "title                         Robust vision-based indoor localization\n",
      "authors                    Ronald Clark, Niki Trigoni, Andrew Markham\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 14th International Conferen...\n",
      "references          53909eef20f70186a0e35924;5390a1e620f70186a0e5b...\n",
      "abstract            Vision-based positioning has proven to be high...\n",
      "id                                                            1717673\n",
      "clustered_labels                                                    3\n",
      "Name: 1717673, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908ae020f70186a0dae0a2   score: 0.9073123   abstract: We describe a vision-based indoor mobile robot localization algorithm that does not require historical position estimates. The method assumes the presence of an a priori map and a reference omnidirectional view of the workspace. The current omnidirectional image of the environment is captured whenever the robot needs to relocalise. A modified hue profile is generated for each of the incoming images and compared with that of the reference image to find the correspondence. The current position of the robot can then be determined using triangulation as both the reference position and the map of the workspace are available. The method was tested by mounting the camera system at a number of random positions positions in a 11.0m × 8.5 m room. The average localization error was 0.45 m. No mismatch of features between the reference and incoming image was found amongst the testing cases.\n",
      "\n",
      "2. id: 5390a1bc20f70186a0e56631   score: 0.89218736   abstract: In this paper, we propose a robust method for the process of localization of a mobile robotthrough a vision system. The mobile robot is a compact system consisting of an embedded board and a fish-eye camera. The fish-eye camera looks upward to capture ceiling images. The camera provides a sequence of images for the process of detection and tracking ceiling features. These features are used like natural landmarks to detect the state of translation and rotation of the robot. Our method requires less computational power and resources for a robot, and thus can be used at home. The results produced in this study showed the advantages of our method in terms of both speed and accuracy.\n",
      "\n",
      "3. id: 5390bed320f70186a0f4fb4e   score: 0.883236   abstract: The localization of mobile robots in indoor environments finds lots of problems such as accumulated errors and the constant changes that occur at these places. A technique called global vision intends to localize robots using images acquired by cameras placed in such a way that covers the place where the robots movement takes place. Localization is obtained by marks put on top of the robot. This work proposes a mobile robot localization system at indoor environments using a technique called global vision to estimate the position of a robot in real time. Since the method relies only on information extracted from the current image, accurate position estimates can be calculated.\n",
      "\n",
      "4. id: 5390a72220f70186a0e8913c   score: 0.85693204   abstract: This paper presents a vision-based approach for mobile robot localization. The model of the environment is topological. The new approach characterizes a place using a signature. This signature consists of a constellation of descriptors computed over different types of local affine covariant regions extracted from an omnidirectional image acquired rotating a standard camera with a pan-tilt unit. This type of representation permits a reliable and distinctive environment modelling. Our objectives were to validate the proposed method in indoor environments and, also, to find out if the combination of complementary local feature region detectors improves the localization versus using a single region detector. Our experimental results show that if false matches are effectively rejected, the combination of different covariant affine region detectors increases notably the performance of the appr\n",
      "\n",
      "5. id: 5390a93b20f70186a0ea182f   score: 0.84247476   abstract: While commercial solutions for precise indoor positioning exist, they are costly and require installation of additional infrastructure, which limits opportunities for widespread adoption. Inspired by robotics techniques of Simultaneous Localization and Mapping (SLAM) and computer vision approaches using structured light patterns, we propose a self-contained solution to precise indoor positioning that requires no additional environmental infrastructure. Evaluation of our prototype, called TrackSense, indicates that such a system can deliver up to 4 cm accuracy with 3 cm precision in rooms up to five meters squared, as well as 2 degree accuracy and 1 degree precision on orientation. We explain the design and performance characteristics of our prototype and demonstrate a feasible miniaturization that supports applications that require a single device localizing itself in a space. We also di\n",
      "\n",
      "6. id: 5390a77d20f70186a0e8f20f   score: 0.8106289   abstract: This paper presents an implementation and comparison between odometry and probabilistic algorithms for the mobile robot localization problem in indoor environments.The hardware and software tools used for the experiments are briefly described. Also, a software architecture is proposed to make easier the development of computer applications including the tested algorithms which are used to get the results to compare.\n",
      "\n",
      "7. id: 5390b72e20f70186a0f21e73   score: 0.77304506   abstract: This paper examines the use of vision-based localization techniques for indoor environments in outdoor environments. A new method is presented for robust data association and finding camera trajectory; based on these, a simple augmented reality game is implemented.\n",
      "\n",
      "8. id: 5390bded20f70186a0f49eb4   score: 0.73753643   abstract: Nowadays with the dispersion of wireless networks, smartphones and diverse related services, different localization techniques have been developed. Global Positioning System (GPS) has a high rate of accuracy for outdoor localization but the signal is not available inside of buildings. Also other existing methods for indoor localization have low accuracy. In addition, they use fixed infrastructure support. In this paper, we present a novel system for indoor localization, which also works well outside. We have developed a mathematical model for estimating location (distance and direction) of a mobile device using wireless technology. Our experimental results on Smartphones (Android and iOS) show good accuracy (an error less than 2.5 meters). We have also used our developed system in asset tracking and complex activity recognition.\n",
      "\n",
      "9. id: 5390b72e20f70186a0f20be4   score: 0.6689884   abstract: This paper demonstrates a method for global localization of autonomous mobile robots based on the creation of visual memory maps, through detection and description of reference points from captured images, associated to odometer data in a specific environment. The proposed procedure, coupled with specific knowledge of the environment, allows for localization to be achieved through the pairing of these memorized features with the scene being observed in real time. Experiments are conducted to show the effectiveness of the proposed method for the localization of mobile robots in indoor environments. The results are analyzed and navigation alternatives and possible future refinements are discussed.\n",
      "\n",
      "10. id: 5390b9d520f70186a0f30477   score: 0.6415346   abstract: Indoor localization of mobile agents using wireless technologies is becoming very important in military and civil applications. This paper introduces an approach for the indoor localization of a mini UAV based on Ultra-WideBand technology, low cost IMU and vision based sensors. In this work an Extended Kalman Filter (EKF) is introduced as a possible technique to improve the localization. The proposed approach allows to use a low-cost Inertial Measurement Unit (IMU) in the prediction step and the integration of vision-odometry for the detection of markers nearness the touchdown area. The ranging measurements allow to reduce the errors of inertial sensors due to the limited performance of accelerometers and gyros. The obtained results show that an accuracy of 10 cm can be achieved.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691635\n",
      "index                                        5592563c0cf2aff368683c52\n",
      "title               Hardware Specialization in Low-power Sensing A...\n",
      "authors                         Zhuo Wang, Kyong Ho Lee, Naveen Verma\n",
      "year                                                           2015.0\n",
      "venue                            Journal of Signal Processing Systems\n",
      "references          558fdcbb612c29c89cd7b9f8;539099a220f70186a0e18...\n",
      "abstract            This paper explores implications of introducin...\n",
      "id                                                            1691635\n",
      "clustered_labels                                                    1\n",
      "Name: 1691635, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539089d220f70186a0d99fd5   score: 0.8804891   abstract: The authors propose a learning-hardware approach as a generalization of evolvable hardware. A massively parallel, reconfigurable processor speeds up logic operators performed in learning hardware. The approach uses combinatorial synthesis methods developed within the framework of the logic synthesis in digital-circuit-design automation.\n",
      "\n",
      "2. id: 5390b56a20f70186a0f06096   score: 0.83601975   abstract: A broad range of embedded networked sensing (ENS) applications have appeared for large-scale systems, introducing new requirements leading to new embedded architectures, associated algorithms, and supporting software systems. These new requirements include the need for diverse and complex sensor systems that present demands for energy and computational resources, as well as for broadband communication. To satisfy application demands while maintaining critical support for low-energy operation, a new multiprocessor node hardware and software architecture, Low Power Energy Aware Processing (LEAP), has been developed. In this article, we described the LEAP design approach, in which the system is able to adaptively select the most energy-efficient hardware components matching an application’s needs. The LEAP platform supports highly dynamic requirements in sensing fidelity, computational load\n",
      "\n",
      "3. id: 559158530cf232eb904fbd7a   score: 0.7673414   abstract: To meet the demanding requirements in the growing area of wireless sensing applications, some sensing platforms have included low-power application-specific hardware to process the sensor data for compression and pre-classification of the relevant information. While this additional hardware can reduce the overall power consumption of the system, a unique hardware solution is required for each application. To diminish this burden, we will demonstrate a reconfigurable analog/mixed-signal sensing platform. At the hardware-level, this platform consists of a reconfigurable integrated circuit containing many commonly used circuit components that can be connected in any configuration to perform sensor interfacing and ultra-low-power signal processing. At the software level, this platform provides a framework for abstracting the underlying hardware. We will demonstrate how our platform allows a \n",
      "\n",
      "4. id: 5390a88c20f70186a0e9a2b5   score: 0.715424   abstract: In this paper, we discuss a low power embedded sensor node architecture we are developing for distributed sensor network systems deployed in a natural environment. In particular, we examine the sensor node for energy efficient processing-at-the-sensor. We analyze the following modes of operation; event detection, data acquisition, and data processing using low power, high performance embedded technology such as specialized embedded DSP processors and low power FPGAs at the sensing node. We use compute intensive sensor node applications: an acoustic vehicle classifier (frequency domain analysis) and a video license plate identification application (learning algorithm). We report performance and energy for these applications and discuss the system architecture design trade offs.\n",
      "\n",
      "5. id: 5590a4060cf2ce4b6f39e72e   score: 0.6504673   abstract: There is a growing concern about the increasing vulnerability of future computing systems to errors in the underlying hardware. Traditional redundancy techniques are expensive for designing energy-efficient systems that are resilient to high error rates. We present ${\\\\bf \\\\underline{E}rror~\\\\underline{R}esilient~\\\\underline{S}ystem~\\\\underline{A}rchitecture}$ (ERSA), a robust system architecture which targets emerging killer applications such as recognition, mining, and synthesis (RMS) with inherent error resilience, and ensures high degrees of resilience at low cost. Using the concept of configurable reliability, ERSA may also be adapted for general-purpose applications that are less resilient to errors (but at higher costs). While resilience of RMS applications to errors in low-order bits of data is well-known, execution of such applications on error-prone hardware significantly degra\n",
      "\n",
      "6. id: 5390a6b120f70186a0e85bda   score: 0.647241   abstract: The central theme of our work is the probabilistic and approximate design of embedded computing systems. This novel approach consists of two distinguishing aspects: (i) the design and implementation of embedded systems, using components which are susceptible to perturbations from various sources and (ii) a design methodology which consists of an exploration of a design space which characterizes the trade-off between quality of output and cost, to implement high performance and low energy embedded systems. In contrast with other work, our design methodology does not attempt to correct the errors introduced by components which are susceptible to perturbations, instead we design \"good enough\" systems. Our work has the potential to address challenges and impediments to Moore's law arising from material properties and manufacturing difficulties, which dictate that we shift from the current-da\n",
      "\n",
      "7. id: 5390bda020f70186a0f46176   score: 0.5456186   abstract: In recent years we have seen the emergence of context-aware mobile sensing apps which employ machine learning algorithms on real-time sensor data to infer user behaviors and contexts. These apps are typically optimized for power and performance on the app processors of mobile platforms. However, modern mobile platforms are sophisticated system on chips (SoCs) where the main app processors are complemented by multiple co-processors. Recently chip vendors have undertaken nascent efforts to make these previously hidden co-processors such as the digital signal processors (DSPs) programmable. In this paper, we explore the energy and performance implications of off-loading the computation associated with machine learning algorithms in context-aware apps to DSPs embedded in mobile SoCs. Our results show a 17% reduction in a TI OMAP4 based mobile platform's energy usage from off-loading context \n",
      "\n",
      "8. id: 5390bd1520f70186a0f44b8e   score: 0.540742   abstract: Recently, a broad range of ENS applications have appeared for large-scale systems, introducing new requirements leading to new embedded architectures, associated algorithms, and supporting software systems. These new requirements include the need for diverse and complex sensor systems that present demands for energy and computational resources as well as for broadband communication. To satisfy application demands while maintaining critical support for low energy operation, a new multiprocessor node hardware and software architecture, Low Power Energy Aware Processing (LEAP), has been developed. In this thesis we described the LEAP design approach, in which the system is able to adaptively select the most energy efficient hardware components matching an application's needs. The LEAP approach supports highly dynamic requirements in sensing fidelity, computational load, storage media, and n\n",
      "\n",
      "9. id: 559128660cf232eb904fb07d   score: 0.51447654   abstract: The requirements of many wireless sensing applications approach, or even exceed, the limited hardware capabilities of energy-constrained sensing platforms. To achieve such demanding requirements, some sensing platforms have included low-power application-specific hardware---at the expense of generality---to pre-process the sensor data for reduction to only the relevant information. While this additional hardware can save power by reducing the activity of the microcontroller and radio, a unique hardware solution is required for each application, which presents an unrealistic burden in terms of design time, cost, and ease of integration. To diminish these burdens, we present a reconfigurable analog/mixed-signal sensing platform in this work. At the hardware-level, this platform consists of a reconfigurable integrated circuit containing many commonly used signal-processing blocks and circui\n",
      "\n",
      "10. id: 5390bed320f70186a0f4e24b   score: 0.44939268   abstract: System integration density increased tremendously in recent years, resulting in various problems for designers. First, a variety of dependability issues were a direct consequence from high clock frequencies and system-on-chip complexity, such as thermal, power, and stability challenges. Furthermore, deep sub-micron semiconductor processes were increasingly prone to single-event-upsets and multiple-event-upsets caused by logic degradation and environmental sources. Besides these reliability issues, the intentional introduction of faults into the system by adversaries, is of increasing concern to system developers of smart-cards. Therefore, there is a strong need for hardware-accelerated evaluation techniques during the design phase to identify weaknesses in cryptographic software implementations. To map power and fault models to such FPGA-based evaluation systems, characterization and ben\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674968\n",
      "index                                        55913b900cf232eb904fb601\n",
      "title                        Evaluation of Interfaces for 3d Pointing\n",
      "authors                        Daniel A. Lazewatsky, William D. Smart\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Tenth Annual ACM/IEEE Inter...\n",
      "references          53908bad20f70186a0dc2d7c;5390a01420f70186a0e46...\n",
      "abstract            A variety of tasks with robots require directi...\n",
      "id                                                            1674968\n",
      "clustered_labels                                                    2\n",
      "Name: 1674968, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390972920f70186a0dfa810   score: 0.5613351   abstract: Pointing has been conceptualized and implemented so far as the act of selecting pixels in bitmap displays. We show that the current technique, which we call bitmap pointing (BMP), is often sub-optimal as it requires continuous information from the mouse while the system often just needs the discrete specification of objects. The paper introduces object pointing (OP), a novel interaction technique based on a special screen cursor that skips empty spaces, thus drastically reducing the waste of input information. We report data from 1D and 2D Fitts' law experiments showing that OP outperforms BMP and that the performance facilitation increases with the task's index of difficulty. We discuss the implementation of OP in current interfaces.\n",
      "\n",
      "2. id: 5390b86b20f70186a0f29bb3   score: 0.50016975   abstract: We establish that two-part models of pointing performance (Welford’s model) describe pointing on a computer display significantly better than traditional one-part models (Fitts’s Law). We explore the space of pointing models and describe how independent contributions of movement amplitude and target width to pointing time can be captured in a parameter k. Through a reanalysis of data from related work we demonstrate that one-part formulations are fragile in describing pointing performance, and that this fragility is present for various devices and techniques. We show that this same data can be significantly better described using two-part models. Finally, we demonstrate through further analysis of previous work and new experimental data that k increases linearly with gain. Our primary contribution is the demonstration that Fitts’s Law is more limited in applicability than previously appr\n",
      "\n",
      "3. id: 5390bf1320f70186a0f51be1   score: 0.4940951   abstract: In this paper we address an important issue in human-robot interaction, that of accurately deriving pointing information from a corresponding gesture. Based on the fact that in most applications it is the pointed object rather than the actual pointing direction which is important, we formulate a novel approach which takes into account prior information about the location of possible pointed targets. To decide about the pointed object, the proposed approach uses the Dempster-Shafer theory of evidence to fuse information from two different input streams: head pose, estimated by visually tracking the off-plane rotations of the face, and hand pointing orientation. Detailed experimental results are presented that validate the effectiveness of the method in realistic application setups.\n",
      "\n",
      "4. id: 5390b9d520f70186a0f307d9   score: 0.47583362   abstract: We present two studies to comparatively evaluate the performance of gesture-based 2D and 3D pointing tasks. In both of them, a Wiimote controller and a standard mouse were used by six participants. For the 3D experiments we introduce a novel configuration analogous to the ISO 9241-9 standard methodology. We examine the pointing devices' conformance to Fitts' law and we measure eight extra parameters that describe more accurately the cursor movement trajectory. For the 2D tasks using Wiimote, Throughput is 41,2% lower than using the mouse, target re-entry is almost the same, and missed clicks count is three times higher. For the 3D tasks using Wiimote, Throughput is 56,1% lower than using the mouse, target re-entry is increased by almost 50%, and missed clicks count is sixteen times higher. Fitts' law, 3D pointing, Gesture User Interface, Wiimote\n",
      "\n",
      "5. id: 539089d320f70186a0d9b7f4   score: 0.4242927   abstract: An experiment is described comparing the performance of an eye tracker and a mouse in a simple pointing task. Subjects had to make rapid and accurate horizontal movements to targets that were vertical ribbons located at various distances from the cursor's starting position. The dwell-time protocol was used for the eye tracker to make selections. Movement times were shorter for the mouse than for the eye tracker. Fitts' Law model was shown to predict movement times using both interaction techniques equally well. The model is thus seen to be a potential contributor to design of modern multimodal human-computer interfaces.\n",
      "\n",
      "6. id: 5390b2d720f70186a0eed09a   score: 0.41851866   abstract: We introduce FlowMouse, a computer vision-based pointing device and gesture input system. FlowMouse uses optical flow techniques to model the motion of the hand and a capacitive touch sensor to enable and disable interaction. By using optical flow rather than a more traditional tracking based method, FlowMouse is exceptionally robust, simple in design, and offers opportunities for fluid gesture-based interaction that go well beyond merely emulating pointing devices such as the mouse. We present a Fitts law study examining pointing performance, and discuss applications of the optical flow field for gesture input.\n",
      "\n",
      "7. id: 5390bda020f70186a0f471ff   score: 0.3680649   abstract: We present a study to comparatively evaluate the performance of computer-based 2D and 3D pointing tasks. In our experiments, based on the ISO 9241-9 standard methodology, a Microsoft Kinect device and a mouse were used by seven participants. For the 3D experiments we introduced a novel experiment layout, supplementing the ISO. We examine the pointing devices' conformance to Fitts' law and we measure a number of extra parameters that describe more accurately the cursor movement trajectories. Throughput, measured in bits per second is the most important performance measure. For the 2D tasks using Microsoft Kinect, Throughput is almost 39% lower than using the mouse, Target Re-Entry is 10 times up and Missed Clicks count is almost 50% higher. However, for the 3D tasks the mouse has a 9% lower Throughput than the Kinect, while Target Re-Entry and Missed Clicks are almost identical. Our resul\n",
      "\n",
      "8. id: 5390a01420f70186a0e46d80   score: 0.3511998   abstract: We present a novel interface for human-robot interaction that enables a human to intuitively and unambiguously select a 3D location in the world and communicate it to a mobile robot. The human points at a location of interest and illuminates it (``clicks it'') with an unaltered, off-the-shelf, green laser pointer. The robot detects the resulting laser spot with an omnidirectional, catadioptric camera with a narrow-band green filter. After detection, the robot moves its stereo pan/tilt camera to look at this location and estimates the location's 3D position with respect to the robot's frame of reference. Unlike previous approaches, this interface for gesture-based pointing requires no instrumentation of the environment, makes use of a non-instrumented everyday pointing device, has low spatial error out to 3 meters, is fully mobile, and is robust enough for use in real-world applications. \n",
      "\n",
      "9. id: 5390975920f70186a0dfe469   score: 0.3024941   abstract: Fitts' law is a tool for evaluating pointing devices, which has been accepted and applied widespread in the human computer interaction field. However, there are still some problems embarrassing the researchers in this field about its validity. One problem is derived from the request on inputs hits' distribution (i.e. spatial constraint) by the origin of Fitts' law. Therefore, a new model based on temporal distribution was developed to alter the traditional models. We carried out an experiment including five tasks with different requirement on speed and accuracy tot est the effect of the new model. The new model and the traditional models are compared with both the mixed data and the individual task data of the experiment using AIC (Akaike's Information Criterion), a criterion for statistical model selection. All results show that the new model is better than the traditional ones in perfo\n",
      "\n",
      "10. id: 5390bae620f70186a0f3c27e   score: 0.28199843   abstract: Pointing at something refers to orienting the hand, the arm, the head or the body in the direction of an object or an event. This skill constitutes a basic communicative ability for cognitive agents like, e.g. humanoid robots. The goal of this study is to show that approximate and, in particular, precise pointing can be learned as a direct mapping from the object's pixel coordinates in the visual field to hand positions or to joint angles. This highly nonlinear mapping defines the pose and orientation of a robot's arm. The study underlines that this is possible without calculating the object's depth and 3D position explicitly since only the direction is required. To this aim, three state-of-the-art neural network paradigms (multilayer perceptron, extreme learning machine and reservoir computing) are evaluated on real world data gathered from the humanoid robot iCub. Training data are int\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1690334\n",
      "index                                        559252290cf28b1a968ffb26\n",
      "title               A Unified Mortar Condition for Nonconforming F...\n",
      "authors                      Chunmei Wang, Shangyou Zhang, Jinru Chen\n",
      "year                                                           2015.0\n",
      "venue                                 Journal of Scientific Computing\n",
      "references          539087c720f70186a0d568c7;53909e8b20f70186a0e2e...\n",
      "abstract            A continuously interpolated mortar condition i...\n",
      "id                                                            1690334\n",
      "clustered_labels                                                    1\n",
      "Name: 1690334, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908f5b20f70186a0dd8ccd   score: 0.9451974   abstract: Mortar finite elements are nonconforming finite elements that allow for a geometrically nonconforming decomposition of the computational domain and, at the same time, for the optimal coupling of different variational approximations in different subregions. Poincare and Friedrichs inequalities for mortar finite elements are derived. Using these inequalities, it is shown that the condition number for self-adjoint elliptic problems discretized using mortars is comparable to that of the conforming finite element case. Geometrically non-conforming mortars of the second generation are considered, i.e. no continuity conditions are imposed at the vertices of the subregions.\n",
      "\n",
      "2. id: 5390c04520f70186a0f57571   score: 0.86647177   abstract: A mortar method for second-order elliptic problems is considered using the nonconforming finite element proposed by Douglas et al. [J. Douglas, J.E. Santos, D. Sheen, X. Ye, Nonconforming Galerkin methods based on quadrilateral elements for second order elliptic problems, M2AN Math. Model. Numer. Anal. 33 (1999) 747-770]. The error estimate of optimal order is proved in the broken energy norm. Numerical experiments which confirm the convergence analysis are provided.\n",
      "\n",
      "3. id: 5390a01420f70186a0e47004   score: 0.85195273   abstract: A new approximate mortar condition is proposed for the lowest order Crouzeix-Raviart finite element on nonmatching grids, which uses only the nodal values on the interface for the calculation of the mortar projection. This approach allows for improved and more flexible algorithms compared to those for the standard mortar condition where nodal values in the interior of a sub-domain, those closest to a mortar side of the subdomain, are also required in the calculation.\n",
      "\n",
      "4. id: 5390a93b20f70186a0ea0505   score: 0.8365546   abstract: The purpose of this paper is to study the effect of the numerical quadrature on the finite element approximation to the exact solution of elliptic equations with discontinuous coefficients. Due to low global regularity of the solution, it seems difficult to achieve optimal order of convergence with classical finite element methods [Z. Chen, J. Zou, Finite element methods and their convergence for elliptic and parabolic interface problems, Numer. Math. 79 (1998) 175-202]. We derive error estimates in finite element method with quadrature for elliptic interface problems in a two-dimensional convex polygonal domain. Optimal order error estimates in L^2 and H^1 norms are shown to hold even if the regularity of the solution is low on the whole domain. Finally, numerical experiment for two dimensional test problem is presented in support of our theoretical findings.\n",
      "\n",
      "5. id: 5390893e20f70186a0d9367c   score: 0.81888163   abstract: The finite element tearing and interconnecting (FETI) method is an iterative substructuring method using Lagrange multipliers to enforce the continuity of the finite element solution across the subdomain interface. Mortar finite elements are nonconforming finite elements that allow for a geometrically nonconforming decomposition of the computational domain into subregions and, at the same time, for the optimal coupling of different variational approximations in different subregions.We present a numerical study of FETI algorithms for elliptic self-adjoint equations discretized by mortar finite elements. Several preconditioners which have been successful for the case of conforming finite elements are considered. We compare the performance of our algorithms when applied to classical mortar elements and to a new family of biorthogonal mortar elements, and we discuss the differences between e\n",
      "\n",
      "6. id: 53908d6520f70186a0dd0ba2   score: 0.7084144   abstract: In the framework of domain decomposition, we extend the main ideas of the mortar element method to the numerical resolution of Maxwell''s equations (in wave form) by $H(\\curl)$-conforming finite elements. The method we propose turns out to be a new nonconforming nonoverlapping domain decomposition method where nonmatching grids are allowed at the interfaces between subdomains. A model problem is considered, the convergence of the discrete approximation is analyzed and an error estimate is provided. The method is proven to be slightly sub-optimal with a loose of a factor $\\sqrt{|\\ln h|}$ with respect to the degree of polynomials. In order to achieve this convergence result we nevertheless need extra-regularity assumptions on the solution of the continuous problem. TEL:: 0382-505652 EMAIL:: annalisa@dimat.unipv.it\n",
      "\n",
      "7. id: 5390c04b20f70186a0f58077   score: 0.6723317   abstract: In this paper, the mortar element method for non-selfadjoint and indefinite second order elliptic problems is studied. Only under minimal regularity assumption, the existence, uniqueness and uniform convergence of the solution for the mortar element method are proven. Furthermore, an additive Schwarz preconditioning method is proposed and nearly optimal convergence rate for the preconditioned GMRES method is shown under minimal regularity assumption.\n",
      "\n",
      "8. id: 539098dc20f70186a0e0d170   score: 0.6547844   abstract: In this paper, we are concerned with mortar element methods for the numerical solution of the eddy currents equations based on domain decompositions on nonmatching grids using individual subdomain discretizations by the lowest order edge elements of Nédélec's first family. The main results are optimal a priori error estimates of the global discretization error and the Lagrange multipliers that take care of the weak continuity constraints on the tangential traces across interior subdomain boundaries. These estimates are derived under moderate regularity assumptions.\n",
      "\n",
      "9. id: 53908f5b20f70186a0dd8cb3   score: 0.6274948   abstract: The Finite Element Tearing and Interconnecting (FETI) method is an iterative substructuring method using Lagrange multipliers to enforce the continuity of the finite element solution across the subdomain interface. Mortar finite elements are nonconforming finite elements that allow for a geometrically nonconforming decomposition of the computational domain into subregions and, at the same time, for the optimal coupling of different variational approximations in different subregions. We present a numerical study of FETI algorithms for elliptic self-adjoint equations discretized by mortar finite elements. Several preconditioners which have been successful for the case of conforming finite elements are considered. We compare the performance of our algorithms when applied to classical mortar elements and to a new family of biorthogonal mortar elements and discuss the differences between enfo\n",
      "\n",
      "10. id: 5390a1bc20f70186a0e54dba   score: 0.6184924   abstract: In this paper, we consider a semi-discrete mortar finite volume element method for two-dimensional parabolic problems. This method is based on the mortar Crouzeix-Raviart non-conforming finite element space. It is proved that the mortar finite volume element approximations derived are convergent with the optimal order in the H^1- and L^2-norms. Numerical experiments are presented to illustrate the theoretical results.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1716076\n",
      "index                                        55323c5c45cec66b6f9dbe43\n",
      "title               The existence of periodic solutions for couple...\n",
      "authors                             Xinhong Zhang, Wenxue Li, Ke Wang\n",
      "year                                                           2015.0\n",
      "venue                                                  Neurocomputing\n",
      "references          558e96a90cf2c779a6477e21;558fe3610cf28af999b57742\n",
      "abstract            This paper is concerned with the existence of ...\n",
      "id                                                            1716076\n",
      "clustered_labels                                                    2\n",
      "Name: 1716076, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558d6b020cf2e667580500aa   score: 0.9996111   abstract: In this paper, by using the continuation theorem of coincidence degree theory, a criterion of the existence of positive periodic solutions is obtained for a single species model with feedback regulation and distributed time delay.\n",
      "\n",
      "2. id: 5390b04120f70186a0ed739f   score: 0.9995609   abstract: By employing Mawhin continuation theorem and constructing suitable Lyapunov functions, the existence and globally exponential stability of periodic solution for a class of nonautonomous differential system with impulses and time-varying delays are investigated in this paper. Some applications, an illustrative example and numerical simulations are given to show the effectiveness of the main results.\n",
      "\n",
      "3. id: 5390a25820f70186a0e5f20b   score: 0.99948466   abstract: In this paper, by employing the continuation theorem of coincidence degree theory, we establish an easily verifiable criteria for the existence of at least four positive periodic solutions for a discrete time delayed predator-prey system with nonmonotonic functional response and harvesting.\n",
      "\n",
      "4. id: 5390bded20f70186a0f48895   score: 0.99942744   abstract: In this paper, we investigate a class of neural networks with mixed time-varying delays and discontinuous activations, new criteria for the existence and exponential stability of the periodic solution are established by using the Mawhin-like coincidence theorem, M-matrix theory, differential inequality techniques and Lyapunov functional. The obtained results are new and complement previously known results. Finally, we present a numerical example with simulations to demonstrate the effectiveness of the theoretical results.\n",
      "\n",
      "5. id: 5390b5ed20f70186a0f0c1c7   score: 0.99942076   abstract: First, a general theorem on the existence of periodic solutions for equations with small discrete delay is obtained by employing a technique which is based on a result of the existence of inertial manifold for small discrete delay equation, meanwhile, this general theorem is applied to show the existence of periodic solution for a predator-prey system with small discrete delay. Second, this technique is also used to obtain the existence of travelling wave solution for a host-vector disease model with small discrete delay.\n",
      "\n",
      "6. id: 5390b04120f70186a0ed8480   score: 0.99935883   abstract: By using the continuation theorem of Mawhin's coincidence degree theory, some new sufficient conditions are obtained for the existence of periodic solution of higher-order Cohen-Grossberg type neural networks with variable delays and impulses, moreover, the monotonicity and smoothness of activation function are not assumed in this paper.\n",
      "\n",
      "7. id: 53909e8a20f70186a0e2d285   score: 0.9993462   abstract: In this paper, we use the coincidence degree theory to establish new results on the existence of w-periodic solutions for a two-neuron network model.\n",
      "\n",
      "8. id: 5390981d20f70186a0e04a8a   score: 0.9993411   abstract: Sufficient conditions are obtained for the existence of positive periodic solutions of neutral equations with state-dependent delays. The proof is based on the theory of coincidence degree. The existence results are applied to equations of population dynamics.\n",
      "\n",
      "9. id: 53909f2d20f70186a0e3922d   score: 0.9992848   abstract: Sufficient conditions are obtained for the existence and global attractivity of periodic solutions of a class of higher-order Cohen-Grossberg type neural networks with delays. The proof is based on Gaines and Mawhin's continuation theorem of coincidence degree theory, the Lyapunov functional and a nonsingular M-matrix. One example is exploited to illustrate the effectiveness of the proposed criteria.\n",
      "\n",
      "10. id: 5390c04520f70186a0f57e8c   score: 0.99926203   abstract: By using the continuation theorem of coincidence degree theory, the existence conditions of four periodic solutions for the delayed predator-prey system is established.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1667194\n",
      "index                                        55916b8b0cf2e89307ca9b0b\n",
      "title                The Complexity of the Nucleolus in Compact Games\n",
      "authors             Gianluigi Greco, Enrico Malizia, Luigi Palopol...\n",
      "year                                                           2015.0\n",
      "venue                   ACM Transactions on Computation Theory (TOCT)\n",
      "references          5390a25820f70186a0e5fde9;539087e120f70186a0d65...\n",
      "abstract            The nucleolus is a well-known solution concept...\n",
      "id                                                            1667194\n",
      "clustered_labels                                                    3\n",
      "Name: 1667194, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a74f20f70186a0e8c8d5   score: 0.985663   abstract: A significantly complete account of the complexity underlying the computation of relevant solution concepts in compact coalitional games is provided. The starting investigation point is the setting of graph games, about which various long-standing open problems were stated in the literature. The paper gives an answer to most of them, and in addition provides new insights on this setting, by stating a number of complexity results about some relevant generalizations and specializations. The presented results also pave the way towards precisely carving the tractability frontier characterizing computation problems on compact coalitional games.\n",
      "\n",
      "2. id: 5390bae620f70186a0f3c7b4   score: 0.98262435   abstract: In this paper, we study the computational complexity of solution concepts in the context of coalitional games. Firstly, we distinguish two different kinds of core, the undominated core and excess core, and investigate the difference and relationship between them. Secondly, we thoroughly investigate the computational complexity of undominated core and three farsighted solution concepts---farsighted core, farsighted stable set and largest consistent se}.\n",
      "\n",
      "3. id: 5390b56a20f70186a0f04a56   score: 0.9762654   abstract: We show that distinguishing 1/2-satisfiable Unique-Games instances from (3/8 + ε)-satisfiable instances is NP-hard (for all ε 0). A consequence is that we match or improve the best known c vs. s NP-hardness result for Unique-Games for all values of c (except for c very close to 0). For these c, ours is the first hardness result showing that it helps to take the alphabet size larger than 2. Our NP-hardness reductions are quasilinear-size and thus show nearly full exponential time is required, assuming the ETH.\n",
      "\n",
      "4. id: 5390990f20f70186a0e10a14   score: 0.9700135   abstract: We study the algorithmic issues of finding the nucleolus of a flow game. The flow game is a cooperative game defined on a network D = (V, E; ω). The player set is E and the value of a coalition S ⊆ E is defined as the value of the maximum flow from source to sink in the subnetwork induced by S. We show that the nucleolus of the flow game defined on a simple network (ω(e) = 1 for each e ∈ E) can be computed in polynomial time by a linear program duality approach, settling a twenty-three years old conjecture by Kalai and Zemel. In contrast, we prove that both computation and recognition of the nucleolus are NP-hard for flow games with general capacity.\n",
      "\n",
      "5. id: 5390a74f20f70186a0e8bd2b   score: 0.9693822   abstract: Using the ellipsoid method, both Deng et al. [Deng, X., Q. Fang, X. Sun. 2006. Finding nucleolus of flow game. Proc. 17th ACM-SIAM Sympos. Discrete Algorithms. ACM Press, New York, 124--131] and Potters et al. [Potters, J., H. Reijnierse, A. Biswas. 2006. The nucleolus of balanced simple flow networks. Games Econom. Behav.54 205--225] show that the nucleolus of simple flow games (where all edge capacities are equal to one) can be computed in polynomial time. Our main result is a combinatorial method based on eliminating redundant s--t path constraints such that only a polynomial number of constraints remains. This leads to efficient algorithms for computing the core, nucleolus, and nucleon of simple flow games. Deng et al. also prove that computing the nucleolus for (general) flow games is NP-hard. We generalize this by proving that computing the f-nucleolus of flow games is NP-hard for \n",
      "\n",
      "6. id: 5390ae2e20f70186a0ec6e29   score: 0.9605836   abstract: We consider the computational complexity of coalitional solution concepts in scenarios related to load balancing such as anonymous and congestion games. In congestion games, Pareto-optimal Nash and strong equilibria, which are resilient to coalitional deviations, have recently been shown to yield significantly smaller inefficiency. Unfortunately, we show that several problems regarding existence, recognition, and computation of these concepts are hard, even in seemingly special classes of games. In anonymous games with constant number of strategies, we can efficiently recognize a state as Pareto-optimal Nash or strong equilibrium, but deciding existence for a game remains hard. In the case of player-specific singleton congestion games, we show that recognition and computation of both concepts can be done efficiently. In addition, in these games there are always short sequences of coaliti\n",
      "\n",
      "7. id: 5390bb7b20f70186a0f4152a   score: 0.9605836   abstract: We consider the computational complexity of coalitional solution concepts in scenarios related to load balancing such as anonymous and congestion games. In congestion games, Pareto-optimal Nash and strong equilibria, which are resilient to coalitional deviations, have recently been shown to yield significantly smaller inefficiency. Unfortunately, we show that several problems regarding existence, recognition, and computation of these concepts are hard, even in seemingly special classes of games. In anonymous games with constant number of strategies, we can efficiently recognize a state as Pareto-optimal Nash or strong equilibrium, but deciding existence for a game remains hard. In the case of player-specific singleton congestion games, we show that recognition and computation of both concepts can be done efficiently. In addition, in these games there are always short sequences of coaliti\n",
      "\n",
      "8. id: 558f3bff0cf222bc17bc23d9   score: 0.94649786   abstract: This paper describes a fast algorithm to find the nucleolus of any game with a nonempty imputation set. It is based on the algorithm scheme of Maschler et al. Maschler, M., J. Potters, S. Tijs. 1992. The general nucleolus and the reduced game property. Internal. J. Game Theory21 83--106. for the general nucleolus.\n",
      "\n",
      "9. id: 5390b60d20f70186a0f12703   score: 0.9422947   abstract: The computational complexity of relevant core-related questions for coalitional games is addressed from the coalition structure viewpoint, i.e., without assuming that the grand-coalition necessarily forms. In the analysis, games are assumed to be in \"compact\" form, i.e., their worth functions are implicitly given as polynomial-time computable functions over succinct game encodings provided as input. Within this setting, a complete picture of the complexity issues arising with the core, as well as with the related stability concepts of least core and cost of stability, is depicted. In particular, the special cases of superadditive games and of games whose sets of feasible coalitions are restricted over tree-like interaction graphs are also studied.\n",
      "\n",
      "10. id: 5390ad0720f70186a0ebb716   score: 0.93546456   abstract: We analyze the computational complexity of various two-dimensional platform games. We state and prove several meta-theorems that identify a class of these games for which the set of solvable levels is NP-hard, and another class for which the set is even PSPACE-hard. Notably COMMANDERKEEN is shown to be NP-hard, and PRINCE OF PERSIA is shown to be PSPACE-complete. We then analyze the related game Lemmings, where we construct a set of instances which only have exponentially long solutions. This shows that an assumption by Cormode in [3] is false and invalidates the proof that the general version of the LEMMINGS decision problem is in NP. We then augment our construction to only include one entrance, which makes our instances perfectly natural within the context of the original game.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1645921\n",
      "index                                        559158300cf232eb904fbd74\n",
      "title                     General Document Retrieval in Compact Space\n",
      "authors             Gonzalo Navarro, Simon J. Puglisi, Daniel Vale...\n",
      "year                                                           2015.0\n",
      "venue                      Journal of Experimental Algorithmics (JEA)\n",
      "references          539087d420f70186a0d5e42a;5390aa0f20f70186a0ea9...\n",
      "abstract            Given a collection of documents and a query pa...\n",
      "id                                                            1645921\n",
      "clustered_labels                                                    1\n",
      "Name: 1645921, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bded20f70186a0f48229   score: 0.97161794   abstract: Document retrieval is one of the best-established information retrieval activities since the ’60s, pervading all search engines. Its aim is to obtain, from a collection of text documents, those most relevant to a pattern query. Current technology is mostly oriented to “natural language” text collections, where inverted indexes are the preferred solution. As successful as this paradigm has been, it fails to properly handle various East Asian languages and other scenarios where the “natural language” assumptions do not hold. Inthis survey, we cover the recent research in extending the document retrieval techniques to a broader class of sequence collections, which has applications in bioinformatics, data and web mining, chemoinformatics, software engineering, multimedia information retrieval, and many other fields. We focus on the algorithmic aspects of the techniques, uncovering a rich wor\n",
      "\n",
      "2. id: 5390b86b20f70186a0f2a0ee   score: 0.9554855   abstract: We describe recent breakthroughs in the field of compressed data structures, in which the data structure is stored in a compressed representation that still allows fast answers to queries. We focus in particular on compressed data structures to support the important application of pattern matching on massive document collections. Given an arbitrary query pattern in textual form, the job of the data structure is to report all the locations where the pattern appears. Another variant is to report all the documents that contain at least one instance of the pattern. We are particularly interested in reporting only the most relevant documents, using a variety of notions of relevance. We discuss recently developed techniques that support fast search in these contexts as well as under additional positional and temporal constraints.\n",
      "\n",
      "3. id: 5390b29820f70186a0eea9d6   score: 0.95531917   abstract: We describe a data structure that uses O(n)-word space and reports k most relevant documents that contain a query pattern P in optimal O(|P| + k) time. Our construction supports an ample set of important relevance measures, such as the frequency of P in a document and the minimal distance between two occurrences of P in a document. We show how to reduce the space of the data structure from O(n log n) to O(n (log σ + log D + log log n)) bits, where σ is the alphabet size and D is the total number of documents.\n",
      "\n",
      "4. id: 558b904d612c6b62e5e8ba61   score: 0.9510514   abstract: We address the problem of indexing a collection D={T\\\"1,T\\\"2,...,T\\\"D} of D string documents of total length n, so that we can efficiently answer top-k queries: retrieve k documents most relevant to a pattern P of length p given at query time. There exist linear-space data structures, that is, using O(n) words, that answer such queries in optimal O(p+k) time for an ample set of notions of relevance. However, using linear space is not sufficiently good for large text collections. In this paper we explore how far the space/time tradeoff for this problem can be pushed. We obtain three results: (1) When relevance is measured as term frequency (number of times P appears in a document T\\\"i), an index occupying |CSA|+o(n) bits answers the query in time O(t\\\"s\\\"e\\\"a\\\"r\\\"c\\\"h(p)+klg^2klg^@en), where CSA is a compressed suffix array indexing D, t\\\"s\\\"e\\\"a\\\"r\\\"c\\\"h(p) is its time to find the suffix\n",
      "\n",
      "5. id: 53909f8220f70186a0e3c7b3   score: 0.94539934   abstract: In this paper we address the problem of constructing an index for a text document or a collection of documents to answer various questions about the occurrences of a pattern when allowing a constant number of errors. In particular, our index can be built to report all occurrences, all positions, or all documents where a pattern occurs in time linear in the size of the query string and the number of results. This improves over previous work where the look-up time was either not linear or depended upon the size of the document corpus. Our data structure has size O(nlog^dn) on average and with high probability for input size n and queries with up to d errors. Additionally, we present a trade-off between query time and index complexity that achieves worst-case bounded index size and preprocessing time with linear look-up time on average.\n",
      "\n",
      "6. id: 5390b3ae20f70186a0ef35d5   score: 0.9434519   abstract: In this paper we address the problem of constructing an index for a text document or a collection of documents to answer various questions about the occurrences of a pattern when allowing a constant number of errors. In particular, our index can be built to report all occurrences, all positions, or all documents where a pattern occurs in time linear in the size of the query string and the number of results. This improves over previous work where the lookup time is not linear or depends upon the size of the document corpus. Our data structure has size $O\\left(n\\log^k n\\right)$ on average and with high probability for input size n and queries with up to k errors. Additionally, we present a trade-off between query time and index complexity that achieves worst-case bounded index size and preprocessing time with linear lookup time on average.\n",
      "\n",
      "7. id: 558bdca70cf23f2dfc594753   score: 0.94218844   abstract: Representing versioned documents, such as Wikipedia history, web archives, genome databases, backups, is challenging when we want to support searching for an exact substring and retrieve the documents that contain the substring. This problem is called <em>document listing</em>. We present an index for the document listing problem on versioned documents. Our index is the first one based on grammar-compression. This allows for good results on repetitive collections, whereas standard techniques cannot achieve competitive space for solving the same problem. Our index can also be addapted to work in a more standard way, allowing users to search for word-based phrase queries and conjunctive queries at the same time. Finally, we discuss extensions that may be possible in the future, for example, supporting ranking capabilities within the index itself.\n",
      "\n",
      "8. id: 53909e8a20f70186a0e2d65f   score: 0.9395816   abstract: We propose succinct data structures for text retrieval systems supporting document listing queries and ranking queries based on the tf*idf (term frequency times inverse document frequency) scores of documents. Traditional data structures for these problems support queries only for some predetermined keywords. Recently Muthukrishnan proposed a data structure for document listing queries for arbitrary patterns at the cost of data structure size. For computing the tf*idf scores there has been no efficient data structures for arbitrary patterns. Our new data structures support these queries using small space. The space is only 2/@e times the size of compressed documents plus 10n bits for a document collection of length n, for any 0\n",
      "\n",
      "9. id: 5390b78a20f70186a0f23c00   score: 0.9395816   abstract: Supporting top-k document retrieval queries on general text databases, that is, finding the k documents where a given pattern occurs most frequently, has become a topic of interest with practical applications. While the problem has been solved in optimal time and linear space, the actual space usage is a serious concern. In this paper we study various reduced-space structures that support top-k retrieval and propose new alternatives. Our experimental results show that our novel structures and algorithms dominate almost all the space/time tradeoff.\n",
      "\n",
      "10. id: 558be17d0cf20e727d0f35c5   score: 0.9238675   abstract: Document listing is the problem of preprocessing a set of sequences, called documents, so that later, given a short string called the pattern, we retrieve the documents where the pattern appears. While optimal-time and linear-space solutions exist, the current emphasis is in reducing the space requirements. Current document listing solutions build on compressed suffix arrays. This paper is the first attempt to solve the problem using a Lempel-Ziv compressed index of the text collections. We show that the resulting solution is very fast to output most of the resulting documents, taking more time for the final ones. This makes this index particularly useful for interactive scenarios or when listing some documents is sufficient. Yet, it also offers a competitive space/time tradeoff when returning the full answers.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1660395\n",
      "index                                        559149fe0cf232eb904fb9cc\n",
      "title               Moving Beyond e-Health and the Quantified Self...\n",
      "authors             Alan Chamberlain, m.c. schraefel, Erika Poole,...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference Compani...\n",
      "references          558b1cfc612c41e6b9d43ac7;539087a520f70186a0d47...\n",
      "abstract            Abstract What is the role of CSCW as methodolo...\n",
      "id                                                            1660395\n",
      "clustered_labels                                                    3\n",
      "Name: 1660395, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390be6620f70186a0f4d11e   score: 0.950503   abstract: CSCW as a field has been concerned since its early days with healthcare, studying how healthcare work is collaboratively and practically achieved and designing systems to support that work. Reviewing literature from the CSCW Journal and related conferences where CSCW work is published, we reflect on the contributions that have emerged from this work. The analysis illustrates a rich range of concepts and findings towards understanding the work of healthcare but the work on the larger policy level is lacking. We argue that this presents a number of challenges for CSCW research moving forward: in having a greater impact on larger-scale health IT projects; broadening the scope of settings and perspectives that are studied; and reflecting on the relevance of the traditional methods in this field - namely workplace studies - to meet these challenges.\n",
      "\n",
      "2. id: 5390aefc20f70186a0ecd8fd   score: 0.87873113   abstract: This showcase paper describes the research of the Multidisciplinary Design Group, Vienna University of Technology, over the course of more than 20 years. It discusses this research and the contributions to CSCW resulting from it, under four headings: work practices in health care; studies of design practice and development of supporting technologies; coordination work and technologies; and gender studies, identifying the factors that shaped the group's engagement in specific projects and the analytical frameworks they developed in collaboration with international cooperation partners.\n",
      "\n",
      "3. id: 5390a25820f70186a0e5f06b   score: 0.7516481   abstract: Research in CSCW faces challenges for assessing constructs of technology use and cooperation in context. This study examines a system in a healthcare context, and uses activity as the unit of analysis. An innovative approach to measuring technology use and cooperation is applied that has potential for replication across other systems and contexts. The findings provide evidence that technology use and cooperation can be operationalized and examined in context and demonstrate how it can be done reliably. The results show the importance of understanding the participation of different roles within a CSCW context and the factors of technology use impact levels of cooperation.\n",
      "\n",
      "4. id: 5390be6620f70186a0f4c354   score: 0.7379143   abstract: Designing effective CSCW systems in healthcare requires a careful consideration of the entire enterprise. This study uses computational text analysis and network visualization of topical terms and keywords to map the extant knowledge domain of CSCW in healthcare. The results are framed using a multi-level enterprise model, comprised of people, technology, process, and organization. Emerging trends and prominent patterns are identified. The study contributes to a broader understanding of CSCW research in healthcare and demonstrates the value of adapting an enterprise (as a) system lens.\n",
      "\n",
      "5. id: 5390b3ae20f70186a0ef4d2b   score: 0.6550051   abstract: Researchers and practitioners show increasing interest in utilizing patient-generated information on the Web. Although the HCI and CSCW communities have provided many exciting opportunities for exploring new ideas and building broad agenda in health, few venues offer a platform for interdisciplinary and collaborative brainstorming about design challenges and opportunities in this space. The goal of this workshop is to provide participants with opportunities to interact with stakeholders from diverse backgrounds and practices - researchers, practitioners, designers, programmers, and ethnographers - and together generate tangible design outcomes that utilize patient-generated information on the Web. Through small multidisciplinary group work, we will provide participants with new collaboration opportunities, understanding of the state of the art, inspiration for future work, and ideally av\n",
      "\n",
      "6. id: 5390a25820f70186a0e5f066   score: 0.5900117   abstract: In just over two decades, CSCW has grown from a small group of researchers that recognized their shared interests across a variety of disciplines to a community that draws members from around the world and an even wider range of fields and interests. This year's conference continues that expansion, as you will find presentations on gaming, technology in the home, healthcare applications, social networking, and cross-cultural collaboration. The ACM CSCW Conference has a special role in compiling the most recent work in the field and creating a forum to share and build on that work to move the community forward. The premier status of ACM's CSCW Conference is hard-earned, in part because of its rigorous review process. Here is a glimpse into how this review process works. For the first time this year, Full Papers (10 pages) and Notes (4 pages) submissions were reviewed under the same proces\n",
      "\n",
      "7. id: 5390a25820f70186a0e5f02c   score: 0.5877065   abstract: In just over two decades, CSCW has grown from a small group of researchers that recognized their shared interests across a variety of disciplines to a community that draws members from around the world and an even wider range of fields and interests. This year's conference continues that expansion, as you will find presentations on gaming, technology in the home, healthcare applications, social networking, and cross-cultural collaboration. The ACM CSCW Conference has a special role in compiling the most recent work in the field and creating a forum to share and build on that work to move the community forward. The premier status of ACM's CSCW Conference is hard-earned, in part because of its rigorous review process. Here is a glimpse into how this review process works. For the first time this year, Full Papers (10 pages) and Notes (4 pages) submissions were reviewed under the same proces\n",
      "\n",
      "8. id: 539096cb20f70186a0df72d3   score: 0.5708697   abstract: The proposed workshop aims to form a community of individuals interested in using computing technology to promote healthcare and support wellness in the context of homecare. We strive to connect and engage researchers from several distinct fields of scientific inquiry and practice: people with clinical experience, developers of enabling technologies and HCI researchers interested in home healthcare and issues such as aging in place. The focus of this one-day workshop is on establishing common ground in vocabulary, research methods and research framework; understanding the shared needs of people with health challenges, their families and clinicians, and developing a joint framework for future research.\n",
      "\n",
      "9. id: 5390bd1520f70186a0f42fe4   score: 0.55528456   abstract: Much is to be learned from working with health science research teams closely on longer-term projects to evaluate technology-enabled interventions that support health and wellness. The author's participation on a long-term randomized clinical trial, which has required substantial technical development and maintenance efforts, has provided valuable insight into difficult problems of ensuring long-term engagement and making long-term study administration manageable.\n",
      "\n",
      "10. id: 5390b63320f70186a0f16beb   score: 0.53497696   abstract: In an analysis departing from the global health situation, the foundation for a change of paradigm in health informatics based on socially embedded information infrastructures and technologies is identified and discussed. It is shown how an increasing computing and data transmitting capacity can be employed for proactive health computing. As a foundation for ubiquituoshealth promotion and prevention of disease and injury, proactive health systems use data from multiple sources to supply individuals and communities evidence-based information on means to improve their state of health and avoid health risks. The systems are characterised by:*being profusely connected to the world around them, using perceptual interfaces, sensors and actuators; *responding to external stimuli at faster than human speeds; *networked feed-back loops; and *humans remaining in control, while being left outside t\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1662793\n",
      "index                                        55913a630cf232eb904fb5a7\n",
      "title                        Sketching Cuts in Graphs and Hypergraphs\n",
      "authors                              Dmitry Kogan, Robert Krauthgamer\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Conference on Innovati...\n",
      "references          558ae16c612c41e6b9d3c38a;5390a55520f70186a0e79...\n",
      "abstract            Sketching and streaming algorithms are in the ...\n",
      "id                                                            1662793\n",
      "clustered_labels                                                    3\n",
      "Name: 1662793, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390990f20f70186a0e10a53   score: 0.6278371   abstract: Data stream processing has recently received increasing attention as a computational paradigm for dealing with massive data sets. While major progress has been achieved for several fundamental data sketching and statistics problems, there are many problems that seem to be hard in a streaming setting, including most classical graph problems. Relevant examples are graph connectivity and shortest paths, for which linear lower bounds on the \"space X passes\" product are known. Some recent papers have shown that several graph problems can be solved with one or few passes, if the working memory is large enough to contain all the vertices of the graph. A natural question is whether we can reduce the space usage at the price of increasing the number of passes. Surprisingly, no algorithm with both sublinear space and passes is known for natural graph problems in classical streaming models.Motivate\n",
      "\n",
      "2. id: 5390bed320f70186a0f4f40e   score: 0.6172243   abstract: We give a new (1+&epsi;)-approximation for sparsest cut problem on graphs where small sets expand significantly more than the sparsest cut (expansion of sets of size n/r exceeds that of the sparsest cut by a factor □log nlog r, for some small r; this condition holds for many natural graph families). We give two different algorithms. One involves Guruswami-Sinop rounding on the level-r Lasserre relaxation. The other is combinatorial and involves a new notion called Small Set Expander Flows (inspired by the expander flows of [ARV'09] which we show exists in the input graph. Both algorithms run in time 2O(r) poly(n). We also show similar approximation algorithms in graphs with genus g with an analogous local expansion condition. This is the first algorithm we know of that achieves (1+&epsi;)-approximation on such general family of graphs.\n",
      "\n",
      "3. id: 559149a40cf2127aa930c8b9   score: 0.57612467   abstract: We consider the problem of estimating the value of max cut in a graph in the streaming model of computation. At one extreme, there is a trivial 2-approximation for this problem that uses only O(log n) space, namely, count the number of edges and output half of this value as the estimate for max cut value. On the other extreme, if one allows Õ(n) space, then a near-optimal solution to the max cut value can be obtained by storing an Õ(n)-size sparsifier that essentially preserves the max cut. An intriguing question is if poly-logarithmic space suffices to obtain a non-trivial approximation to the max-cut value (that is, beating the factor 2). It was recently shown that the problem of estimating the size of a maximum matching in a graph admits a non-trivial approximation in poly-logarithmic space. Our main result is that any streaming algorithm that breaks the 2-approximation barrier requir\n",
      "\n",
      "4. id: 558b33e2612c41e6b9d4679b   score: 0.43854472   abstract: We present a streaming algorithm that makes one pass over the edges of an unweighted graph presented in random order, and produces a polylogarithmic approximation to the size of the maximum matching in the graph, while using only polylogarithmic space. Prior to this work the only approximations known were a folklore Õ(√n) approximation with polylogarithmic space in an n vertex graph and a constant approximation with Ω(n) space. Our work thus gives the first algorithm where both the space and approximation factors are smaller than any polynomial in n. Our algorithm is obtained by effecting a streaming implementation of a simple \\\"local\\\" algorithm that we design for this problem. The local algorithm produces a O(k · n1/k) approximation to the size of a maximum matching by exploring the radius k neighborhoods of vertices, for any parameter k. We show, somewhat surprisingly, that our local \n",
      "\n",
      "5. id: 5390882420f70186a0d894a1   score: 0.3261641   abstract: We present an improved semidefinite programming based approximation algorithm for the MAX CUT problem in graphs of maximum degree at most 3. The approximation ratio of the new algorithm is at least 0.9326. This improves, and also somewhat simplifies, a result of Feige, Karpinski and Langberg. We also observe that results of Hopkins and Staton and of Bondy and Locke yield a simple combinatorial 4/5-approximation algorithm for the problem. Slightly improved results would appear in the full version of the paper.\n",
      "\n",
      "6. id: 5390b60d20f70186a0f128e7   score: 0.28407922   abstract: We study exact algorithms for the MAX-CUT problem. Introducing a new technique, we present an algorithmic scheme that computes a maximum cut in graphs with bounded maximum degree. Our algorithm runs in time O^*(2^(^1^-^(^2^/^@D^)^)^n). We also describe a MAX-CUT algorithm for general graphs. Its time complexity is O^*(2^m^n^/^(^m^+^n^)). Both algorithms use polynomial space.\n",
      "\n",
      "7. id: 5390995d20f70186a0e15b71   score: 0.26626188   abstract: We study the approximability of the multicut and the (non-bipartite) sparsest cut problems in directed graphs. In the multicut problem, we are a given a graph G along with k source-sink pairs, and the goal is to find a smallest subset of edges whose deletion separates all source-sink pairs. The sparsest cut problem has the same input, but the goal is to find a subset of edges to delete so as to minimize the ratio of deleted edges to the number of source-sink pairs that are separated by this deletion. Study of algorithms for cut problems is intimately connected to the dual notion of flows in networks, and many approximation algorithms for cut problems use a flow solution as a starting point. The best known approximation algorithm for directed multicut is based on this approach and gives an O(√n)-approximation. On the other hand, the gap between the maximum multicommodity flow and the mini\n",
      "\n",
      "8. id: 558eb4c30cf2e6675805196f   score: 0.25367606   abstract: As graphs continue to grow in size, we seek ways to effectively process such data at scale. The model of streaming graph processing, in which a compact summary is maintained as each edge insertion/deletion is observed, is an attractive one. However, few results are known for optimization problems over such dynamic graph streams. In this paper, we introduce a new approach to handling graph streams, by instead seeking solutions for the parameterized versions of these problems. Here, we are given a parameter k and the objective is to decide whether there is a solution bounded by k. By combining kernelization techniques with randomized sketch structures, we obtain the first streaming algorithms for the parameterized versions of Maximal Matching and Vertex Cover. We consider various models for a graph stream on n nodes: the insertion-only model where the edges can only be added, and the dynam\n",
      "\n",
      "9. id: 5390a45520f70186a0e711a4   score: 0.23005375   abstract: We show that the sparsest cut in graphs with n vertices and m edges can be approximated within O(log2 n) factor in Õ(m + n3/2) time using polylogarithmic single commodity max-flow computations. Previous algorithms are based on multicommodity flows that take time Õ(m + n2). Our algorithm iteratively employs max-flow computations to embed an expander flow, thus providing a certificate of expansion. Our technique can also be extended to yield an O(log2 n)-(pseudo-) approximation algorithm for the edge-separator problem with a similar running time.\n",
      "\n",
      "10. id: 5390b29820f70186a0eea9a3   score: 0.20545557   abstract: This work is concerned with approximating constraint satisfaction problems (CSPs) with additional global cardinality constraints. For example, Max Cut is a boolean CSP where the input is a graph G = (V, E) and the goal is to find a cut S ∪ S = V that maximizes the number of crossing edges, |E(S, S)|. The Max Bisection problem is a variant of Max Cut with a global constraint that each side of the cut has exactly half the vertices, i.e., |S| = |V|/2. Several other natural optimization problems like Min Bisection and approximating Graph Expansion can be formulated as CSPs with global cardinality constraints. In this work, we formulate a general approach towards approximating CSPs with global cardinality constraints using SDP hierarchies. To demonstrate the approach we present the following results: • Using the Lasserre hierarchy, we present an algorithm that runs in time O(npoly(1/ε)) that \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717636\n",
      "index                                        559124c40cf232eb904faf7a\n",
      "title               Just-in-time component-wise power and thermal ...\n",
      "authors             Shah Mohammad Faizur Rahman, Qing Yi, Houman H...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 12th ACM International Conf...\n",
      "references          55323cdc45cec66b6f9dd090;5390b0ca20f70186a0edb...\n",
      "abstract            As computer systems increasingly focus on bala...\n",
      "id                                                            1717636\n",
      "clustered_labels                                                    3\n",
      "Name: 1717636, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a1f820f70186a0e5d345   score: 0.91259205   abstract: Significant opportunities for power optimization exist at application design stage and are not yet fully exploited by system and application designers. We describe the challenges developers face in optimizing software for energy efficiency by exploiting application-level knowledge. To address these challenges, we propose the development of automated tools that profile the energy usage of various resource components used by an application and guide the design choices accordingly. We use a preliminary version of a tool we have developed to demonstrate how automated energy profiling helps a developer choose between alternative designs in the energy-performance trade-off space.\n",
      "\n",
      "2. id: 53908b6c20f70186a0dbd8f5   score: 0.82190347   abstract: Managing power consumption while simultaneously delivering acceptable levels of performance is becoming a critical issue in several application domains such as wireless computing. We integrate compiler-assisted techniques with power-aware operating system services and present scheduling techniques to reduce energy consumption of applications that have deadlines. We show by simulation that our dynamic power management schemes dramatically decrease energy consumption.\n",
      "\n",
      "3. id: 5390b72e20f70186a0f21d1c   score: 0.81859183   abstract: Accurately modeling application performance for specific architectures allows us to understand and analyze the impact of various architectural features on performance which will ultimately lead to improved performance and better architecture design choices for efficiency and scalability on future systems. However, with the end of Dennard scaling, processors can no longer maintain constant power per unit area as before and consequently power and energy efficiencies has become arguably an even more important factor than performance for future systems. Unfortunately, performance and power/energ efficiencies are not metrics that can be optimized separately, but are intricately dependent factors. Therefore, in order to design efficient systems that can meet both performance and energy/power requirements that are becoming ever more stringent, an acccurate performance-energy model is required s\n",
      "\n",
      "4. id: 5390893e20f70186a0d9274b   score: 0.77864933   abstract: Power consumption has become an increasingly important factor in the field of computer architecture. It affects issues such as heat dissipation and packaging cost, which in turn affects the design and cost of a mobile terminal. Today, a lot of effort is put into the design of architectures and software implementation to increase performance. However, little is done on a system level to minimize power consumption, which is crucial in mobile systems.We propose an adaptive chip-multiprocessor (CMP) architecture, where the number of active processors is dynamically adjusted to the current workload need in order to save energy while preserving performance. The architecture is suitable in future mobile terminals where we anticipate a bursty and performance demanding workload.We have carried out an evaluation of the performance and power consumption of the proposed architecture using previously\n",
      "\n",
      "5. id: 5390a37f20f70186a0e6c460   score: 0.7764535   abstract: Managing the power consumption of computing platforms is a complicated problem thanks to a multitude of hardware configuration options and characteristics. Much of the academic research is based on unrealistic assumptions, and has, therefore, seen little practical uptake. We provide an overview of the difficulties facing power management schemes when used in real systems. We present Koala, a platform which uses a pre-characterised model at run-time to predict the performance and energy consumption of a piece of software. An arbitrary policy can then be applied in order to dynamically trade performance and energy consumption. We have implemented this system in a recent Linux kernel, and evaluated it by running a variety of benchmarks on a number of different platforms. Under some conditions, we observe energy savings of 26% for a 1% performance loss.\n",
      "\n",
      "6. id: 5390a96e20f70186a0ea3578   score: 0.7745834   abstract: Energy consumption of any component in a system may sometimes constitute just a small percentage of that of the overall system, making it necessary to address the issue of energy efficiency across the entire range of system components, from memory, to the CPU, to peripherals. Presented is a hardware architecture for detecting regions of application execution at runtime, for which there is opportunity to run a device at a slightly lower performance level, by reducing the operating frequency and voltage, to save energy. The proposed architecture, the Power Adaptation Unit (PAU) may be used to control the operating voltage of various system components, ranging from the CPU core, to memory and peripherals. An evaluation of the tradeoffs in performance versus energy savings and hardware cost of the PAU is conducted, along with results on its efficacy for a set of benchmarks. It is shown that \n",
      "\n",
      "7. id: 53909fca20f70186a0e44c55   score: 0.741674   abstract: Power consumption and energy efficiency are important factors in the initial design and day-to-day management of computer systems. Researchers and system designers need benchmarks that characterize energy efficiency to evaluate systems and identify promising new technologies. To predict the effects of new designs and configurations, they also need accurate methods of modeling power consumption.\n",
      "\n",
      "8. id: 5390ada620f70186a0ec2aff   score: 0.73885757   abstract: Performance, power, and temperature are now all first-order design constraints. Balancing power efficiency, thermal constraints, and performance requires some means to convey data about real-time power consumption and temperature to intelligent resource managers. Resource managers can use this information to meet performance goals, maintain power budgets, and obey thermal constraints. Unfortunately, obtaining the required machine introspection is challenging. Most current chips provide no support for per-core power monitoring, and when support exists, it is not exposed to software. We present a methodology for deriving per-core power models using sampled performance counter values and temperature sensor readings. We develop application-independent models for four different (four- to eight-core) platforms, validate their accuracy, and show how they can be used to guide scheduling decision\n",
      "\n",
      "9. id: 5390b4da20f70186a0effd6e   score: 0.68573904   abstract: As an initial step in our Green Software research, this paper investigates whether software optimization at the application level can help achieve higher energy efficiency and better thermal behavior. We use both direct measurements and modeling to quantify power, energy and temperature for a given software method. The infrastructure includes a new power estimator for multicore systems developed by regressing measurements from a custom-designed suite of microbenchmarks. Using our evaluation methodology on a real-life multicore system, we explore two case studies. In the first one, we use software tuning for improving the scalability and energy-efficiency of a parallel application. The second case study explores the effect of temperature optimization on system-level energy consumption.\n",
      "\n",
      "10. id: 5390b86b20f70186a0f29149   score: 0.6791787   abstract: Energy efficiency is a top requirement in embedded system design. Understanding the complex issue of software power consumption in early design phases is of extreme importance to make the right design decisions. Power simulators offer flexibility and allow a detailed view on the sources of power consumption. In this paper we present XEEMU, a fast, cycle-accurate simulator, which aims at the most accurate modeling of the XScale architecture possible. It has been validated using measurements on real hardware and shows a high accuracy for runtime, instantaneous power, and total energy consumption estimation. The average error is as low as 3.0% and 1.6% for runtime and energy consumption estimation, respectively.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691639\n",
      "index                                        5592561b0cf2aff368683c40\n",
      "title               Solving strong controllability of temporal pro...\n",
      "authors              Alessandro Cimatti, Andrea Micheli, Marco Roveri\n",
      "year                                                           2015.0\n",
      "venue                                                     Constraints\n",
      "references          5390880720f70186a0d7a989;53908b1820f70186a0db4...\n",
      "abstract            Temporal Problems (TPs) represent constraints ...\n",
      "id                                                            1691639\n",
      "clustered_labels                                                    2\n",
      "Name: 1691639, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55923ccb612c4fa28ff7a2cd   score: 0.9936365   abstract: The framework of temporal problems with uncertainty (TPU) is useful to express temporal constraints over a set of activities subject to uncertain (and uncontrollable) duration. In this work, we focus on the most general class of TPU, namely disjunctive TPU (DTPU), and consider the case of weak controllability, that allows one to model problems arising in practical scenarios (e.g. on-line scheduling).We first tackle the decision problem, i.e. whether there exists a schedule of the activities that, depending on the uncertainty, satisfies all the constraints. We propose a logical approach, based on the reduction to a problem of Satisfiability Modulo Theories (SMT), in the theory of Linear Real Arithmetic with Quantifiers. This results in the first implemented solver for weak controllability of DTPUs.Then, we tackle the problem of synthesizing control strategies for scheduling the activities\n",
      "\n",
      "2. id: 5390b8d720f70186a0f2bf82   score: 0.98197925   abstract: Many applications, such as scheduling and temporal planning, require the solution of Temporal Problems (TP's) representing constraints over the timing of activities. A TP with uncertainty (TPU) is characterized by activities with uncontrollable duration. Depending on the Boolean structure of the constraints, we have simple (STPU), constraint satisfaction (TCSPU), and disjunctive (DTPU) temporal problems with uncertainty. In this work we tackle the problem of strong controllability, i.e. finding an assignment to all the controllable time points, such that the constraints are fulfilled under any possible assignment of uncontrollable time points. We work in the framework of Satisfiability Modulo Theory (SMT), where uncertainty is expressed by means of universal quantifiers. We obtain the first practical and comprehensive solution for strong controllability: the use of quantifier elimination\n",
      "\n",
      "3. id: 5390a6d920f70186a0e8892e   score: 0.9630224   abstract: Driven by planning problems with both disjunctive constraints and contingency, we define the Disjunctive Temporal Problem with Uncertainty (DTPU), an extension of the DTP that includes contingent events. Generalizing existing work on Simple Temporal Problems with Uncertainty, we divide the time-points into controllable and uncontrollable classes, and propose varying notions of controllability to replace the notion of consistency.\n",
      "\n",
      "4. id: 5390a1e620f70186a0e5a857   score: 0.92892635   abstract: Many temporal problems arising in automated planning and schedulingcan be expressed as Disjunctive Temporal Problems (DTPs). Most of DTP solvers in the literature treat DTPs as Constraint SatisfactionProblems (CSPs) or Satisfiability Problems (SATs), and solve the musing standard CSP (SAT) techniques. Basically DTPs are represented through logically related topological relations between temporal variables, however, unfortunately little work has been done on exploiting the topological information to direct the search for DTP resolving. According to the \"fail-first\" (FF) principle for dynamic variable ordering (DVO) heuristics in CSP literature, this paper proposes a DVO which is based on the topological structure of DTP (which is defined to be Disjunctive Temporal Network). Experimental results reveal that the proposed DVO outperforms Minimal Remaining Values heuristics--a DVO that is wid\n",
      "\n",
      "5. id: 5390a6d920f70186a0e8875a   score: 0.8735388   abstract: Certain planning systems that deal with quantitative time constraints have used an underlying Simple Temporal Problem solver to ensure temporal consistency of plans. However, many applications involve processes of uncertain duration whose timing cannot be controlled by the execution agent. These cases require more complex notions of temporal feasibility. In previous work, various \"controllability\" properties such as Weak, Strong, and Dynamic Controllability have been defined. The most interesting and useful Controllability property, the Dynamic one, has ironically proved to be the most difficult to analyze. In this paper, we resolve the complexity issue for Dynamic Controllability. Unexpectedly, the problem turns out to be tractable. We also show how to efficiently execute networks whose status has been verified.\n",
      "\n",
      "6. id: 5390a05a20f70186a0e4a0bc   score: 0.8489722   abstract: Constraint-based approaches to scheduling have typicallyformulated the problem as one of finding a consistent assignment ofstart times for each goal activity. In contrast, we are pursuing anapproach that operates with a problem formulation more akin toleast-commitment frameworks, where the objective is to postsufficient additional precedence constraints between pairs ofactivities contending for the same resources to ensure feasibilitywith respect to time and resource constraints. One noteworthycharacteristic of this Precedence Constraint Posting (PCP)approach, is that solutions generated in this way generallyencapsulate a set of feasible schedules (i.e., a solution containsthe sets of activity start times that remain consistent with postedsequencing constraints). Such solutions can offer advantages whenthere is temporal uncertainty associated with executingactivities.In this paper, we co\n",
      "\n",
      "7. id: 5390a9a520f70186a0ea5fb0   score: 0.83346015   abstract: Here we give a short overview of the DPLL(T) approach to Satisfiability Modulo Theories (SMT), which is at the basis of current state-of-the-art SMT systems. After that, we provide a documented list of theoretical and practical current challenges related to SMT, including some new ideas to exploit SAT techniques in Constraint Programming.\n",
      "\n",
      "8. id: 5390a96f20f70186a0ea4508   score: 0.8304567   abstract: The Disjunctive Temporal Problem with Uncertainty (DTPU) is an extension of the Disjunctive Temporal Problem (DTP) that accounts for events not under the control of the executing agent. We investigate the semantics of DTPU constraints, refining the existing notion that they are simply disjunctions of STPU constraints. We then develop the first sound and complete algorithm to determine whether Strong Controllability holds for a DTPU. We analyze the complexity of our algorithm with respect to the number of constraints in different classes, showing that, for several common subclasses of DTPUs, determining Strong Controllability has the same complexity as solving DTPs.\n",
      "\n",
      "9. id: 5390893e20f70186a0d94911   score: 0.8251675   abstract: Many temporal applications like planning and schedulingcan be viewed as special cases of the numeric and symbolic temporalconstraint satisfaction problem. Thus we have developed a temporalmodel, TemPro, based on the interval Algebra, to express suchapplications in term of qualitative and quantitative temporalconstraints. TemPro extends the interval algebra relations ofAllen to handle numeric information. To solve a constraint satisfactionproblem, different approaches have been developed. These approachesgenerally use constraint propagation to simplify the originalproblem and backtracking to directly search for possible solutions.The constraint propagation can also be used during the backtrackingto improve the performance of the search. The objective of thispaper is to assess different policies for finding if a TemPronetwork is consistent. The main question we want to answer hereis ’’how \n",
      "\n",
      "10. id: 5390a6b120f70186a0e84f84   score: 0.7854732   abstract: In this paper, we focus on extending the expressive power of constraint-based temporal reasoning formalisms. We begin with the well-known Simple Temporal Problem with Uncertainty, and incorporate three extensions: prior observability, in which the values of uncontrollable events become known prior to their actual occurrence; partial shrinkage, in which an observation event triggers the reduction of a contingent temporal interval; and a generalization of partial shrinkage to requirement links, making it possible to express certain types of uncertainty that may arise even when the time points in a problem are themselves fully controllable. We describe levels of controllability in the resulting formalism, the Generalized STPU, and relate this formalism to related developments in disjunctive temporal reasoning. Throughout, we motivate our approach with simple, real-world examples that illust\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707351\n",
      "index                                        559170800cf2e89307ca9d09\n",
      "title               The Gauntlet: The Design of a Community Challe...\n",
      "authors             Daniel Hawkins, Clarissa Ishak, MaoYang Li, Ja...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390980720f70186a0e02f6e;5390b13020f70186a0edc...\n",
      "abstract            Many people in present culture have a desire t...\n",
      "id                                                            1707351\n",
      "clustered_labels                                                    0\n",
      "Name: 1707351, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539099a220f70186a0e17331   score: 0.14151853   abstract: In this paper, we present interim results from the Communities and Place project, which is exploring methods for understanding communities in a variety of contexts, and how to inform the design of technology to support them. We report on our experience with adapting an existing game-based approach for working with video as a resource in participatory design processes. Our adaptations allow the approach to be used with diverse data arising out of the different communities we are engaged with, and different design traditions we approach the problem from, leading to the formation of common design themes to inform our future work on this project.\n",
      "\n",
      "2. id: 5390b48420f70186a0efc6c9   score: 0.09484524   abstract: This paper describes the design and preliminary evaluation of Aulura, a system designed for motivating people to increase their physical activity. Aulura is an ambient information display that aims to ‘lure' users to interact with it, to review their progress and set personal goals. We present the design of ‘ambient cues' added to a picture frame with the aim to increase user interaction with the device and engagement with a physical activity promotion service. Empirical evaluation in a home simulation laboratory provided positive feedback relating to its potential to further engage participants in an online lifestyle management service.\n",
      "\n",
      "3. id: 5390879920f70186a0d41961   score: 0.07878401   abstract: In recent years system engineers, product designers, and human interface designers have become increasingly interested in developing ways of involving users in the design and evolution of computer-based systems. Some have turned for guidance and inspiration to an approach to systems design pioneered in Scandinavia and often referred to as Participatory Design. In this paper we examine the development of a computer-based design tool, Trillium, which on the surface looked like an example of Participatory Design in that users were directly involved in the development of the technology. Our analysis leads us to conclude, however, that Trillium's development departed in critical ways from our current model of Participatory Design and to suggest that the manner in which users are involved in the development effort plays an important role in the success of the endeavor.\n",
      "\n",
      "4. id: 558ce23b0cf23fdd601e0ef6   score: 0.069668226   abstract: As an initial effort in developing tools that support users' creation of their own behavior-change plans, we conducted a formative user study. We intended to explore people's creation of plans for their own behavioral goals, with minimal support to facilitate their goal-setting, implementation of behavior-change techniques, and self-monitoring. In this paper, we present lessons that we obtained from this initial study, and insights on shifts in our design tools for a follow-up formative study currently underway.\n",
      "\n",
      "5. id: 558aeff7612c41e6b9d3de3e   score: 0.06548521   abstract: While it is commonly claimed that users of participatory design projects reap benefits from their participation, little research exists that shows if this truly occurs in the real world. In this paper, we introduce the method and results of assessing the participants' perception of their personal benefits and the degree of participation in a large project in the healthcare field. Our research shows that a well-executed participatory design project can produce most of the benefits hypothesized in the literature but also highlights the challenges of assessing individual benefits and the PD process.\n",
      "\n",
      "6. id: 5390bf1320f70186a0f515b8   score: 0.056756895   abstract: Participatory design (PD) is mostly conducted face-to-face with the support of physical props. Although this is a valid and very beneficial approach, it is unfortunately not applicable in every project context. It is especially challenging when a project's stakeholders are widely distributed; software tools can address this challenge. A review of existing online annotation tools indicates that they do not meet the essential requirements for distributed PD tools. To address this issue, a prototype called Pdot has been developed and evaluated with some users whose feedback will inform its future development.\n",
      "\n",
      "7. id: 558ae1c9612c41e6b9d3c437   score: 0.046119746   abstract: In this paper, we explore using large digital displays in combination with a personal mobile application to publicly and privately encourage people to make healthy choices. We designed, built, and deployed an experimental system called Lunch Line that promoted healthy eating. Lunch Line includes a public display that enables passersby to view the reported eating behavior of a group of people and take on daily \\\"food challenges,\\\" and a mobile web application that allows users to record personal food choices, report challenge achievement, and compare their choices with other users and with USDA recommendations. Results from a 3-week field evaluation at a company cafeteria showed that our integrated system was effective in drawing public attention, delivering challenges, enabling self-tracking and self-reflection, and providing feedback on personal and group choices. We share lessons on ho\n",
      "\n",
      "8. id: 5390a25820f70186a0e5f89e   score: 0.044931643   abstract: In this paper we present our approach to the evaluation of multimodal applications by using participatory design workshops. Our goal is to obtain user feedback for our design on a fundamental, conceptual level. We propose in this paper the use of design ideas coming from the users, not only by translating them one-to-one into design but also by analyzing them in order to reflect on the design concepts behind the artifacts being constructed. By providing examples of workshops conducted, we show a methodology that helps in exploring the design space and that has the potential of producing more interesting jumps inside the design space, towards more a satisfying user experience.\n",
      "\n",
      "9. id: 539098dc20f70186a0e0d4c2   score: 0.042009056   abstract: This paper provides an overview of approaches to Participatory Design (PD) of computer applications, and briefly outlines how PD overviews are important in tackling PD's current challenges.\n",
      "\n",
      "10. id: 539099a220f70186a0e18f90   score: 0.038466193   abstract: PARTICIPATORY DESIGNParticipatory design (PD) is a diverse collection of principles and practices aimed at making technologies and social institutions more responsive to human needs. A central tenet of PD is the direct involvement of people in the co-design of the systems they use. This tenet is based on the recognition that when people are involved in shaping their social, technological and material environments, the better suited these environments are to everyday realities and requirements and the more people able are to claim authority over their work and leisure lives.The PD Conferences have been held every two years since 1990. The conference brings together a multidisciplinary and international group of researchers, software developers, social scientists, designers, activists, practitioners, users, citizens, cultural workers and managers who adopt distinctively participatory appro\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698647\n",
      "index                                        558b38f8612c41e6b9d4711f\n",
      "title               Randomized and quantum complexity of nonlinear...\n",
      "authors                                                 Maciej Goćwin\n",
      "year                                                           2015.0\n",
      "venue                             Applied Mathematics and Computation\n",
      "references          5390af8920f70186a0ed0a88;5390ad8920f70186a0ec0...\n",
      "abstract            We deal with the complexity of nonlinear BVPs ...\n",
      "id                                                            1698647\n",
      "clustered_labels                                                    2\n",
      "Name: 1698647, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539099a220f70186a0e1765e   score: 0.962462   abstract: We study the problem, initiated by Kacewicz [Randomized and quantum algorithms yield a speed-up for initial-value problems, J. Complexity 20 (2004) 821-834; see also http://arXiv.org/abs/quant-ph/ 0311148], of finding randomized and quantum complexity of initial-value problems. We showed in Kacewicz (2004) that a speed-up in both settings over the worst-case deterministic complexity is possible. In the present paper we prove, by defining new algorithms, that further improvement in upper bounds on the randomized and quantum complexity can be achieved. In the Hölder class of right-hand side functions with r continuous bounded partial derivatives, with rth derivative being a Hölder function with exponent ρ, the ε-complexity is shown to be O ((1/ε)1/(r+ρ+1/3)) in the randomized setting, and O ((1/ε)1/(r+ρ+1/2)) on a quantum computer (up to logarithmic factors). This is an improvement for the\n",
      "\n",
      "2. id: 5390a88c20f70186a0e98cea   score: 0.9437636   abstract: We study the problem, initiated by Kacewicz [Randomized and quantum algorithms yield a speed-up for initial-value problems, J. Complexity 20 (2004) 821-834; see also http://arXiv.org/abs/quant-ph/0311148], of finding randomized and quantum complexity of initial-value problems. We showed in Kacewicz (2004) that a speed-up in both settings over the worst-case deterministic complexity is possible. In the present paper we prove, by defining new algorithms, that further improvement in upper bounds on the randomized and quantum complexity can be achieved. In the Holder class of right-hand side functions with r continuous bounded partial derivatives, with rth derivative being a Holder function with exponent @r, the @?-complexity is shown to be O(1/@?)^1^/^(^r^+^@r^+^1^/^3^) in the randomized setting, and O(1/@?)^1^/^(^r^+^@r^+^1^/^2^) on a quantum computer (up to logarithmic factors). This is a\n",
      "\n",
      "3. id: 5390981d20f70186a0e04a7c   score: 0.8830344   abstract: Quantum algorithms and complexity have recently been studied not only for discrete, but also for some numerical problems. Most attention has been paid so far to the integration and approximation problems, for which a speed-up is shown in many important cases by quantum computers with respect to deterministic and randomized algorithms on a classical computer. In this paper, we deal with the randomized and quantum complexity of initial-value problems. For this nonlinear problem, we show that both randomized and quantum algorithms yield a speed-up over deterministic algorithms. Upper bounds on the complexity in the randomized and quantum settings are shown by constructing algorithms with a suitable cost, where the construction is based on integral information. Lower bounds result from the respective bounds for the integration problem.\n",
      "\n",
      "4. id: 5390ad8920f70186a0ec02d5   score: 0.88161486   abstract: We study the complexity of a two-point boundary value problem. We concentrate on the linear problem of order k with separated boundary conditions. Right-hand side functions are assumed to be r times differentiable with all derivatives bounded by a constant. We consider three models of computation: deterministic with standard and linear information, randomized and quantum. In each setting, we construct an algorithm for solving the problem, which allows us to establish upper complexity bounds. In the deterministic setting, we show that the use of linear information gives us a speed-up of at least one order of magnitude compared with the standard information. For randomized algorithms, we show that the speed-up over standard deterministic algorithms is by 1/2 in the exponent. For quantum algorithms, we can achieve a speed-up by one order of magnitude. We also provide lower complexity bounds\n",
      "\n",
      "5. id: 53908b4920f70186a0dbaa11   score: 0.8159669   abstract: The complexity of quantum computation remains poorly understood. While physicists attempt to find ways to create quantum computers, we still do not have much evidence one way or the other as to how useful these machines will be. The tools of computational complexity theory should come to bear on these important questions.Quantum computing often scares away many potential researchers in computer science because of the apparent background need in quantum mechanics and the alien looking notation used in papers on the topic.This paper will give an overview of quantum computation from the point of view of a complexity theorist. We will see that one can think of BQP as yet another complexity class and study its power without focusing on the physical aspects behind it.\n",
      "\n",
      "6. id: 5390a28020f70186a0e61e7f   score: 0.7233095   abstract: The optimal order of complexity of the approximation of the imbedding from anisotropic Sobolev classes B(Wrp ([0, 1]d)) and H篓older Nikolskii classes B(Hrp([0, 1]d)) into the Lq([0, 1]d) with q ≤ p quantum computation model is obtained up to logarithmic factors. It shows that the quantum algorithms are not significantly better than the classical ones for this type of problems.\n",
      "\n",
      "7. id: 539087e120f70186a0d66b9f   score: 0.6689884   abstract: The quantum model of computation is a model, analogous to the probabilistic Turing machine (PTM), in which the normal laws of chance are replaced by those obeyed by particles on a quantum mechanical scale, rather than the rules familiar to us from the macroscopic world. We present here a problem of distinguishing between two fairly natural classes of functions, which can provably be solved exponentially faster in the quantum model than in the classical probabilistic one, when the function is given as an oracle drawn equiprobably from the uniform distribution on either class. We thus offer compelling evidence that the quantum model may have significantly more complexity theoretic power than the PTM. In fact, drawing on this work, Shor has recently developed remarkable new quantum polynomial-time algorithms for the discrete logarithm and integer factoring problems.\n",
      "\n",
      "8. id: 5390a17720f70186a0e51ae8   score: 0.6399611   abstract: The quantum model of computation is a probabilistic model, similar to the probabilistic Turing Machine, in which the laws of chance are those obeyed by particles on a quantum mechanical scale, rather than the rules familiar to us from the macroscopic world. We present here a problem of distinguishing between two fairly natural classes of function, which can provably be solved exponentially faster in the quantum model than in the classical probabilistic one, when the function is given as an oracle drawn equiprobably from the uniform distribution on either class. We thus offer compelling evidence that the quantum model may have significantly more complexity theoretic power than the probabilistic Turing Machine. In fact, drawing on this work, Shor (1994) has recently developed remarkable new quantum polynomial-time algorithms for the discrete logarithm and integer factoring problems.\n",
      "\n",
      "9. id: 5390c04b20f70186a0f5856a   score: 0.5771379   abstract: We consider the root finding of a real-valued function f defined on the d-dimensional unit cube. We assume that f has r continuous partial derivatives, with all partial derivatives of order r being Holder functions with the exponent @r. We study the @e-complexity of this problem in three settings: deterministic, randomized and quantum. It is known that with the root error criterion the deterministic @e-complexity is infinite, i.e., the problem is unsolvable. We show that the same holds in the randomized and quantum settings. Under the residual error criterion, we show that the deterministic and randomized @e-complexity is of order @e^-^d^/^(^r^+^@r^). In the quantum setting, the @e-complexity is shown to be of order @e^-^d^/^(^2^(^r^+^@r^)^). This means that a quadratic speed-up is achieved on a quantum computer.\n",
      "\n",
      "10. id: 53909e7c20f70186a0e2c21f   score: 0.53038853   abstract: The query complexity of the following numerical problem is studied in the quantum model of computation: consider a general elliptic partial differential equation of order 2m in a smooth, bounded domain Q ⊂ Rd with smooth coefficients and homogeneous boundary conditions. We seek to approximate the solution on a smooth submanifold M ⊆ Q of dimension 0 ≤ d1 ≤ d. With the right-hand side belonging to Cr (Q), and the error being measured in the L∞(M) norm, we prove that the nth minimal quantum error is (up to logarithmic factors) of order n-min((r+2m)/d1,r/d+1). For comparison, in the classical deterministic setting the nth minimal error is known to be of order n-r/d, for all d1, while in the classical randomized setting it is (up to logarithmic factors) n-min((r+2m)/d1,r/d+1/2).\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1704168\n",
      "index                                        55323b2c45cec66b6f9d976a\n",
      "title               OSIC-Based SD MIMO Detection Algorithm for LTE...\n",
      "authors                         Mohamed G. El-Mashed, Sayed El-Rabaie\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references          5390ac1820f70186a0eb374b;5390a88c20f70186a0e99...\n",
      "abstract            This paper presents the performance of a Long ...\n",
      "id                                                            1704168\n",
      "clustered_labels                                                    1\n",
      "Name: 1704168, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bf1320f70186a0f4ff08   score: 0.99193794   abstract: The implementation of Multiple input Multiple output (MIMO) system posts a major challenge to designer of receiver due to the huge processing power required for MIMO detection. To solve this problem with efficient MIMO detection algorithm, in this paper, we consider an adaptive hybrid MIMO detection approach based on channel condition in LTE/LTE Advanced System (downlink) to reduce the complexity with the satisfactory performance. We emphatically analyzed the bit error rate (BER) performance model for different algorithms. Our hybrid approach combines Zero Forcing (ZF), Ordered Successive Interference Cancellation (OSIC) and maximum likelihood (ML) algorithm based on the analysis of real-time SINR and the symbol error rate.We proposed a weight to decide the switching strategy. Simulation shows that the proposed switching strategy achieves better performance with lower complexity in the L\n",
      "\n",
      "2. id: 5390b5c620f70186a0f08885   score: 0.98427594   abstract: In multiple-input multiple-output (MIMO) systems, sphere decoding (SD) can achieve a performance equivalent to a full-search maximum likelihood decoding, with reduced complexity. Several researchers reported techniques that reduce the complexity of SD further. In this paper, a new technique that decreases the computational complexity of SD substantially, without sacrificing performance, is introduced. The reduction is accomplished by deconstructing the decoding metric to decrease the number of computations and by exploiting the structure of a lattice representation. Furthermore, an application of SD employing a proposed smart implementation with very low computational complexity is introduced. This application calculates the soft bit metrics of a bit-interleaved convolutional-coded MIMO system in an efficient manner. On the basis of the reduced complexity SD, the proposed smart implement\n",
      "\n",
      "3. id: 5390a88c20f70186a0e9926e   score: 0.98282325   abstract: An efficient multiple-input multiple-output (MIMO) detection (MD) algorithm includes novel, low-complexity, near-optimal and robust scheme is proposed in wireless communications when imperfect noise estimation is considered. By using MIMO technique, capacity increases proportionally as the number of antennas is increased, but the introduced inter-antenna interference (IAI) degrades system performance. To better mitigate IAI, we propose a two-stage procedure to achieve maximum likelihood (ML) performance while keeping at acceptable level of computational complexity. A novel two-stage procedure is proposed as follows that is suitable for either in an overdetermined or an underdetermined MIMO system. In an overdetermined system, interference cancellation is first processed at Stage-1 using sorted QR decomposition (SQRD) followed by Stage-2 that performs a genetic algorithm (GA). In terms of\n",
      "\n",
      "4. id: 5390aaf920f70186a0ead8ce   score: 0.97767127   abstract: Currently multiple-input multiple-output (MIMO) technology has been widely studied in both industrial and academic areas as it can be utilized to transmit multiple spatially parallel streams. The number of data streams (or layer), supported over MIMO channel, is the maximum number of Tx/Rx antennas, where one encoded block is conveyed over one layer or aggregated several layers. In some cases, the transmission quality on different layers are not the same, which can be utilized to optimize the encoded stream to layer mapping in order to improve the link level performance. In this paper, relying on non-equal importance of information and parity bits in decoding, a new method to map the information bits onto high quality layer, whereas parity bits onto low quality layers is proposed. The performance gain obtained from the optimized mapping scheme is verified via simulation results.\n",
      "\n",
      "5. id: 5390995d20f70186a0e15887   score: 0.975249   abstract: We describe the VLSI implementation of MIMO detectors that exhibit close-to optimum error-rate performance, but still achieve high throughput at low silicon area. In particular, algorithms and VLSI architectures for sphere decoding (SD) and K-best detection are considered, and the corresponding trade-offs between uncoded error-rate performance, silicon area, and throughput are explored. We show that SD with a per-block run-time constraint is best suited for practical implementations.\n",
      "\n",
      "6. id: 5390b48420f70186a0efabf5   score: 0.9732915   abstract: In comparison to single antenna systems, a wireless multiple-input multiple-output (MIMO) system provides higher throughput at no additional cost of bandwidth, but the high complexity of the detection algorithms poses a major challenge to the hardware implementation. Maximum likelihood (ML) MIMO detection guarantees optimal performance but implies huge processing complexity, which makes acceptable this approach only when the number of transmitting antennas is low and the adopted modulation scheme has a small cardinality. Sphere decoding (SD) is an efficient method that significantly reduces the average processing complexity with no performance penalty. Most of known sphere decoders have been implemented as application specific integrated circuits (ASICs), but the need for high degree of flexibility in MIMO detection makes interesting also application specific instruction set processor (A\n",
      "\n",
      "7. id: 5390a55520f70186a0e7b595   score: 0.9730877   abstract: Multiple Input Multiple Output (MIMO) system employs multiple antennas at both the transmitter and the receiver to improve the BER (Bit Error Rate) and system capacity. Different signal detection algorithms have different computational complexity and BER performance. In this paper, we provide a group sphere decoder (SD) in a MIMO wireless communication system. The group sphere decoder can further reduce the computational complexity for the conventional SD method.\n",
      "\n",
      "8. id: 5390a9a520f70186a0ea6c55   score: 0.96820134   abstract: In order to adapt for a high-rate transmission and improve the performance of operation, in this paper, a new low complex sphere decoding (SD) algorithm is proposed in the MIMO-ZP-OFDM systems. It is an approximate optimization algorithm and can be used in the space-time coding and uncoded multiple-antenna systems. ML sequence detection compared with SD algorithm the latter can reduce the complex and keep the performance of systems, especial for high-rate transmission operation and the occasion of transmit antenna beyond receive antenna. Simulation results show that the efficiency and superiority of this algorithm.\n",
      "\n",
      "9. id: 55922f8e0cf244696a09db2d   score: 0.96203625   abstract: In this paper, we propose a scalable and implementation efficient OSIC-based ML algorithm in a quantized space with higher performance for MIMO detection, which can be applied to the LTE-A downlink physical layer system. It is characterized by dividing the overall OSIC detector into small dimension blocks to reduce complexity. The proposed algorithm utilizes ML algorithm in a quantized space to detect the first data streams and overcome error propagation problem. Then, it applies small dimension OSIC block to detect other data streams. The mathematical analysis is illustrated and derived. This paper shows BER performance of the proposed algorithm and compares its performance with other algorithms. This paper also presents the computational complexity to show that it gives lower complexity close to optimal ML algorithm. Simulation results show that the proposed algorithm provides a better\n",
      "\n",
      "10. id: 5390aaf920f70186a0eae2ef   score: 0.95998776   abstract: The maximum likelihood (ML) detection is the optimal detection method for multiple-input multiple-output (MIMO) communication systems. The normal K-best Sphere Decoding Algorithm (SDA) can guarantee a fixed throughput, but it induces a large bit error rate (BER) degradation. In order to achieve close-to-ML performance, the K value needs to be sufficiently large. Thus, it needs large computation with long latency and low throughput. In this paper, a reconfigurable Kbest SDA is proposed by using fixed K values for different layers of K-best detection, which can increase the MIMO detection performance. The architecture of 4×4 MIMO detector is designed for both 16QAM and 64QAM modulation.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675367\n",
      "index                                        55913d070cf232eb904fb663\n",
      "title               A Large-Scale Study of User Image Search Behav...\n",
      "authors             Jaimie Y. Park, Neil O'Hare, Rossano Schifanel...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390a63c20f70186a0e82a48;53908b9320f70186a0dbf...\n",
      "abstract            In this study, we analyze user image search be...\n",
      "id                                                            1675367\n",
      "clustered_labels                                                    0\n",
      "Name: 1675367, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a80f20f70186a0e974f4   score: 0.9954615   abstract: The aim of this paper was to analyze users' behavior during image retrieval exercises. Results revealed that users tend to follow a set search strategy: firstly they input one or two keyword search terms one after another and view the images generated by their initial search and after they navigate their way around the web by using the 'back to home' or 'previous page' buttons. These results are consistent with existing Web research. Many of the actions recorded revealed that subjects behavior differed depending on if the task set was presented as a closed or open task. In contrast no differences were found for the time subjects took to perform a single action or their use of the AND operator.\n",
      "\n",
      "2. id: 5390995d20f70186a0e1599a   score: 0.99544394   abstract: The aim of this paper was to analyze users' behavior during image retrieval exercises. Results revealed that users tend to follow a set search strategy: firstly they input one or two keyword search terms one after another and view the images generated by their initial search and after they navigate their way around the web by using the 'back to home' or 'previous page' buttons. These results are consistent with existing Web research. Many of the actions recorded revealed that subjects behavior differed depending on if the task set was presented as a closed or open task. In contrast no differences were found for the time subjects took to perform a single action or their use of the AND operator.\n",
      "\n",
      "3. id: 5390bae620f70186a0f3c413   score: 0.98571813   abstract: In modern search engines, an increasing number of search result pages (SERPs) are federated from multiple specialized search engines (called verticals, such as Image or Video). As an effective approach to interpret users' click-through behavior as feedback information, most click models were designed to reduce the position bias and improve ranking performance of ordinary search results, which have homogeneous appearances. However, when vertical results are combined with ordinary ones, significant differences in presentation may lead to user behavior biases and thus failure of state-of-the-art click models. With the help of a popular commercial search engine in China, we collected a large scale log data set which contains behavior information on both vertical and ordinary results. We also performed eye-tracking analysis to study user's real-world examining behavior. According these analys\n",
      "\n",
      "4. id: 558ce4220cf23fdd601e0f8d   score: 0.98555213   abstract: There are many existing studies of user behavior in simple tasks (e.g., navigational and informational search) within a short duration of 1--2 queries. However, we know relatively little about user behavior, especially browsing and clicking behavior, for longer search session solving complex search tasks. In this paper, we characterize and compare user behavior in relatively long search sessions (10 minutes; about 5 queries) for search tasks of four different types. The tasks differ in two dimensions: (1) the user is locating facts or is pursuing intellectual understanding of a topic; (2) the user has a specific task goal or has an ill-defined and undeveloped goal. We analyze how search behavior as well as browsing and clicking patterns change during a search session in these different tasks. Our results indicate that user behavior in the four types of tasks differ in various aspects, in\n",
      "\n",
      "5. id: 53908bde20f70186a0dc7f82   score: 0.9812749   abstract: A large number of digital images are stored on the Internet. From an educational perspective the fact that a vast number of images on a number of topics are readily available and are in some cases free is a very helpful thing. With the rapidly expanding nature of the net however, it is becoming increasingly difficult to search out and retrieve relevant images. The aim of this study was to analyze users' behavior during image retrieval exercises. Results revealed that users tend to follow a set search strategy: firstly they input one or two keyword search terms one after another and view the images generated by their initial search and after they navigate their way around the web by using the 'back to home' or 'previous page' buttons. These results are consistent with existing Web research. Many of the actions recorded revealed that subjects' behavior differed depending on if the task set\n",
      "\n",
      "6. id: 539089ab20f70186a0d95162   score: 0.9806178   abstract: A prototype image retrieval system with browse and searchcapabilities was developed to investigate patterns of searching acollection of digital visual images, as well as factors, such asimage size, resolution, and download speed, which affect browsing.The subject populations were art history specialists andnon-specialists. Through focus group interviews, a controlled test,post-test interviews and an online survey, data was gathered tocompare preferences and actual patterns of use in browsing andsearching. While specialists preferred direct search to browsing, andgeneralists used browsing as their preferred mode, both user groupsfound each mode to play a role depending on information need, andfound value in a system combining both browse and direct search.There were no significant differences in performance among the searchmodes of browse, search, and combined browse/search models when th\n",
      "\n",
      "7. id: 53909f2d20f70186a0e38c43   score: 0.97354424   abstract: A large fraction of queries submitted to Web search enginesoccur very infrequently. We describe search log studiesaimed at elucidating behaviors associated with rare andcommon queries. We present several analyses and discussresearch directions.\n",
      "\n",
      "8. id: 5390aefb20f70186a0ecc244   score: 0.9706322   abstract: Many user studies in Web information searching have found the significant effect of task types on search strategies. However, little attention was given to Web image searching strategies, especially the query reformulation activity despite that this is a crucial part in Web image searching. In this study, we investigated the effects of topic domains and task types on user's image searching behavior and query reformulation strategies. Some significant differences in user's tasks specificity and initial concepts were identified among the task domains. Task types are also found to influence participant's result reviewing behavior and query reformulation strategies.\n",
      "\n",
      "9. id: 5390a06e20f70186a0e4cbe3   score: 0.97023994   abstract: Users' past search behaviour provides a rich context that an information retrieval system can use to tailor its search results to suit an individual's or a community's information needs. In this paper, we present an investigation of the variability in search behaviours for the same queries in a close-knit community. By examining web proxy cache logs over a period of nine months, we extracted a set of 135 queries that had been issued by at least ten users. Our analysis indicates that, overall, users clicked on highly ranked and relevant pages, but they tend to click on different sets of pages. Examination of the query reformulation history revealed that users often have different search intents behind the same query. We identify three major causes for the community's interaction behaviour differences: the variance of task, the different intents expressed with the query, and the snippet an\n",
      "\n",
      "10. id: 5390ac1720f70186a0eb2739   score: 0.9552357   abstract: An improved understanding of the relationship between search intent, result quality, and searcher behavior is crucial for improving the effectiveness of web search. While recent progress in user behavior mining has been largely focused on aggregate server-side click logs, we present a new class of search behavior models that also exploit fine-grained user interactions with the search results. We show that mining these interactions, such as mouse movements and scrolling, can enable more effective detection of the user's search goals. Potential applications include automatic search evaluation, improving search ranking, result presentation, and search advertising. We describe extensive experimental evaluation over both controlled user studies, and logs of interaction data collected from hundreds of real users. The results show that our method is more effective than the current state-of-the-\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1713111\n",
      "index                                        55323c2145cec66b6f9db47b\n",
      "title               Dynamically generated commitment protocols in ...\n",
      "authors                     Akın Günay, Michael Winikoff, Pınar Yolum\n",
      "year                                                           2015.0\n",
      "venue                       Autonomous Agents and Multi-Agent Systems\n",
      "references          558e99030cf222bc17bc1495;558f83c70cf2e0e37c4f38d1\n",
      "abstract            Agent interaction is a fundamental part of any...\n",
      "id                                                            1713111\n",
      "clustered_labels                                                    2\n",
      "Name: 1713111, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bae620f70186a0f3c7fd   score: 0.9975274   abstract: Interaction is a fundamental part of multiagent systems, and is usually regulated by protocols. Typically, these protocols are defined at design-time. However, there exist situations where it is desirable to generate protocols at runtime. We develop an algorithm to generate commitment protocols considering the agent's goals, its capabilities and its domain knowledge about the other agents' goals and services. We then provide a method to rank these protocols in terms of their risk and benefit in order to select the one that best suits the agent's current state.\n",
      "\n",
      "2. id: 5390a9a520f70186a0ea591c   score: 0.93732566   abstract: Current approaches to multi-agent interaction involve specifying protocols as sets of possible interactions, and hard-coding decision mechanisms into agent programs in order to decide which path an interaction will take. This leads to several problems, three of which are particularly notable: hard-coding the decisions about interaction within an agent strongly couples the agent and the protocols it uses, which means a change to a protocol involves a changes in any agent that uses such a protocol; agents can use only the protocols that are coded into them at design time; and protocols cannot be composed at runtime to bring about more complex interactions. To achieve the full potential of multi-agent systems, we believe that it is important that multi-agent interaction protocols exist at runtime in systems as entities that can be inspected, referenced, composed, and shared, rather than as \n",
      "\n",
      "3. id: 5390b44620f70186a0ef884b   score: 0.9335521   abstract: Much current research is focussed on developing agent interaction protocols (AIPs) that will ensure seamless interaction amongst agents in multi agent systems. The research covers areas such as desired properties of AIPs, reasoning about interaction types, languages and tools for representing AIPs, and implementing AIPs. However, there has been little work on defining the structural make up of an agent interaction protocol, or defining dedicated approaches for developing agent interaction protocols from a clear problem definition to the final specification. This paper addresses these gaps. We present a dedicated approach for developing agent interaction protocols. Our approach is driven by an analysis of the application domain and our proposed structured agent interaction protocol definition.\n",
      "\n",
      "4. id: 5390a30b20f70186a0e69672   score: 0.92788786   abstract: Many practitioners view agent interaction protocols as rigid specifications that are defined a priori , and hard-code their agents with a set of protocols known at design time -- an unnecessary restriction for intelligent and adaptive agents. To achieve the full potential of multi-agent systems, we believe that it is important that multi-agent interaction protocols are treated as first-class computational entities in systems. That is, they exist at runtime in systems as entities that can be referenced, inspected, composed, invoked and shared, rather than as abstractions that emerge from the behaviour of the participants. Using first-class protocols, a goal-directed agent can assess a library of protocols at runtime to determine which protocols best achieve a particular goal. In this paper, we presented three methods that enable agents to determine if a protocol achieves a specified goal.\n",
      "\n",
      "5. id: 53909fbd20f70186a0e425b8   score: 0.9254997   abstract: Protocols describe interactions among agents and thus underlie the engineering of multiagent systems. However, protocols are enacted by agents in physical systems. In particular, considerations of communication models and how distributed agents are able to make compatible choices would greatly affect whether a protocol may in fact be enacted successfully. The objective of this paper is to study the conceptual underpinnings of protocol enactment in multiagent systems. It seeks to characterize the operationalization of agents so as to determine whether and when agents may be interoperable.\n",
      "\n",
      "6. id: 5390b29820f70186a0ee8fed   score: 0.9253649   abstract: The work achieved in multi-agent interactions design mostly relates to protocols definition, specification, etc. In this paper we tackle a new problem, the dynamic selection of interaction protocols. Generally the protocols and the roles agents play in protocol based interactions are imposed upon the system at design time. This static selection severely limits the openness, the dynamic behaviours agents are expected to exhibit, the integration of new protocols, etc. To address this issue, we developed a method which enables agents to select protocols themselves at runtime when they need to interact with one another. We define the concepts and the mechanisms which enable agents to perform this dynamic selection.\n",
      "\n",
      "7. id: 5390a17720f70186a0e52563   score: 0.91505283   abstract: Many practitioners view agent interaction protocols as rigid specifications that are defined a priori, and hard-code their agents with a set of protocols known at design time --- an unnecessary restriction for intelligent and adaptive agents. To achieve the full potential of multi-agent systems, we believe that it is important that multi-agent interaction protocols are treated as first-class computational entities in systems. That is, they exist at runtime in systems as entities that can be referenced, inspected, composed, invoked and shared, rather than as abstractions that emerge from the behaviour of the participants. Using first-class protocols, a goal-directed agent can assess a library of protocols at runtime to determine which protocols best achieve a particular goal. In this paper, we present three methods for annotating protocols with their outcomes, and matching protocols using\n",
      "\n",
      "8. id: 5390c04520f70186a0f55bcf   score: 0.90681833   abstract: Multi agent systems rely on communication between agents, typically regulated by protocols, to facilitate cooperation and coordination. To create protocols, multi agent system designers spend an extensive amount of time designing and testing to ensure that all requirements of the new protocol have been met. To aid the creation of multi agent protocols, research has shifted from designing low-level protocols, where every interaction must be outlined, to designing higher-level protocol description languages, which allow high-level interactions to be described. Though the focus of research has shifted, the general process of designing new multi agent protocols has remained consistent and still requires that designers describe actions of agents at various stages of a protocol. This dissertation describes an alternative process for creating protocols: an automatic protocol creator that genera\n",
      "\n",
      "9. id: 5390ba0a20f70186a0f32f56   score: 0.9046505   abstract: Interaction protocols play a fundamental role in multiagent systems. In this work, after analyzing the trends that are emerging not only from research on multiagent interaction protocols but also from neighboring fields, like research on workflows and business processes, we propose a novel definition of commitment-based interaction protocols, that is characterized by the decoupling of the constitutive and the regulative specifications and that explicitly foresees a representation of the latter based on constraints among commitments. A clear distinction between the two representations has many advantages, mainly residing in a greater openness of multiagent systems, and an easier reuse of protocols and of action definitions. A language, named 2CL, for writing regulative specifications is also given together with a designer-oriented graphical notation.\n",
      "\n",
      "10. id: 53909a9320f70186a0e23254   score: 0.89912134   abstract: Multiagent systems involve a rich variety of interactions among agents-situated computations that are autonomous in their behavior and heterogeneous in structure. These interactions can be realized unambiguously if they are governed by published protocols, since agents diverse in their structure and behavior can interact as long as they respect the protocols. However, traditional protocol specifications are unduly rigid for application in open settings involving autonomous entities. They represent protocols simply as an ordering of steps and stifle the participants' autonomy due to a lack of flexibility during enactment. Commitments among agents, which are akin to contractual obligations among businesses, are a powerful abstraction for modeling flexible protocols. Commitment-based design enables a more faithful model of the openness of the business world. However, modeling business inter\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1739231\n",
      "index                                        553e6e9f0cf2cadf4a06952e\n",
      "title               Straight slits map and its inverse of bounded ...\n",
      "authors                                                Ali W. Sangawi\n",
      "year                                                           2015.0\n",
      "venue                           Advances in Computational Mathematics\n",
      "references                                   558f9ef90cf23638afbe6b1a\n",
      "abstract            This paper presents a boundary integral equati...\n",
      "id                                                            1739231\n",
      "clustered_labels                                                    2\n",
      "Name: 1739231, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390958920f70186a0deed9d   score: 0.9890554   abstract: We present here a simple method of numerical conformal mappings from bounded multiply connected domains onto the bounded canonical slit domains of Nehari, i.e., a disk with concentric circular slits, and an annulus with concentric circular slits. In this method, we express the mapping functions in terms of a pair of conjugate harmonic functions and approximate them, using the charge simulation method, by a linear combination of complex logarithmic functions. Some numerical examples show the effectiveness of the method.\n",
      "\n",
      "2. id: 5390c04520f70186a0f56ffd   score: 0.98020524   abstract: This paper presents a boundary integral equation method for computing numerical conformal mapping of bounded multiply connected region onto a spiral slits region. The method is an extension of the author's method for computing the circular slits map of bounded multiply connected regions (see A.W.K. Sangawi et al. (2012) [16]). Several numerical examples are given to prove the effectiveness of the proposed methods.\n",
      "\n",
      "3. id: 539087ef20f70186a0d6d529   score: 0.9783019   abstract: A simple numerical method is described for computing the following two conformal maps: (a) from a domain exterior to closed Jordan curves onto a circular slit domain and (b) from a domain exterior to closed Jordan curves onto a radial slit domain. They constitute a dual problem and can be computed in a dual way. The numerical method is based on the charge simulation method or the method of fundamental solutions applied to the Dirichlet problem of Laplace's equation in which a pair of conjugate harmonic functions are approximated by a linear combination of complex logarithmic potentials. The unknown coefficients are determined by the collocation condition imposed on the real part or the imaginary part, the modulus or the argument, of the approximate mapping function. Effectiveness of the method is demonstrated by some typical examples.\n",
      "\n",
      "4. id: 558ff4710cf28fa9103171c2   score: 0.9765354   abstract: A method is presented for constructing approximations to the standard mappings for multiply connected regions given by Nehari [5]. The case of mapping onto a slit annulus is considered in detail, and computational results are presented for several examples.\n",
      "\n",
      "5. id: 5390a72320f70186a0e8b252   score: 0.959001   abstract: We present a unified boundary integral method for approximating the conformal mappings from any bounded or unbounded multiply connected region $G$ onto the five classical canonical slit domains. The method is based on a uniquely solvable boundary integral equation with the generalized Neumann kernel. Using the proposed method, the approximate mapping functions onto the five canonical slit domains can be computed in a unified way by solving linear systems with a common coefficient matrix. The method can be also used for calculating the conformal mappings of simply and doubly connected regions. The performance of the method is illustrated by several examples for regions with smooth boundaries and with piecewise smooth boundaries.\n",
      "\n",
      "6. id: 558feeaa0cf23515427188e5   score: 0.94728357   abstract: A numerical method, based on the integral equation formulation of Symm, is described for computing approximations to the mapping functions which accomplish the following conformal maps: (a) the mapping of a domain interior to a closed Jordan curve onto the interior of the unit disc, (b) the mapping of a domain exterior to a closed Jordan curve onto the exterior of the unit disc, (c) the mapping of a doubly-connected domain bounded by two closed Jordan curves onto a circular annulus. The numerical method is based on approximating the unknown source density by cubic splines and \\\"singular\\\" functions, and is particularly suited for the mapping of difficult domains having sharp corners.\n",
      "\n",
      "7. id: 5390b78a20f70186a0f2468a   score: 0.9326989   abstract: We present a method for numerical computation of conformal mappings from simply or doubly connected domains onto so-called canonical domains, which in our case are rectangles or annuli. The method is based on conjugate harmonic functions and properties of quadrilaterals. Several numerical examples are given.\n",
      "\n",
      "8. id: 5390893e20f70186a0d93710   score: 0.91655886   abstract: A simple algorithm is presented for numerical approximation of conformal mappings for simply connected planar regions. The desired mapping is represented by a normalized polynomial of degree N, determined by the boundary values which it is presumed to take at the Nth roots of unity; then its values at the N intermediate 2Nth roots of unity are used to correct the initial guess. Each iteration, which consists essentially of matrix multiplication and boundary projection, costs O(N log N) arithmetic operations. Numerical results are provided to confirm linear convergence of the algorithm.\n",
      "\n",
      "9. id: 5390a88c20f70186a0e98c34   score: 0.8842393   abstract: We propose a method to map a multiply connected bounded planar region conformally to a bounded region with circular boundaries. The norm of the derivative of such a conformal map satisfies the Laplace equation with a nonlinear Neumann type boundary condition. We analyze the singular behavior at corners of the boundary and separate the major singular part. The remaining smooth part solves a variational problem which is easy to discretize. We use a finite element method and a gradient descent method to find an approximate solution. The conformal map is then constructed from this norm function. We tested our algorithm on a polygonal region and a curvilinear smooth region.\n",
      "\n",
      "10. id: 558fac410cf2b666404671d5   score: 0.86223567   abstract: Gaier [3] has given an account of the different constructive methods of approximating the conformal mapping. Our method relates to the methods of extremal points by Fekete [1] and Leja [6]. These point systems can be used to approximate the conformal mapping. In the case of a domain bounded by a closed analytic Jordan curve, however, the point system we deal with has much better properties of approximating the conformal mapping than the other point systems. (Compare [9], Pommerenke [12].) We define an algorithm to compute our point system and we finally take three curves for which we compute the point system with the help of which we get an approximation of the conformal mapping. The numerical results are rather good.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675669\n",
      "index                                        55915ab50cf2127aa930cd89\n",
      "title               Experiencing Autonomous Vehicles: Crossing the...\n",
      "authors             Alexander Meschtscherjakov, Manfred Tscheligi,...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558af80b612c41e6b9d3efc8;558ae10b612c41e6b9d3c...\n",
      "abstract            Autonomous vehicles have gained attention rece...\n",
      "id                                                            1675669\n",
      "clustered_labels                                                    0\n",
      "Name: 1675669, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558af80b612c41e6b9d3efc8   score: 0.9793935   abstract: Autonomous Driving has gained attention from academia and industry over the last decades. Research organizations and companies have developed (semi-) autonomous vehicles and first in-situ studies have been conducted. This workshop follows last year's first AUI workshop on user experience (UX) and autonomous driving (AD). We would like to widen the conversation on UX and AD based on the results from last year. The focus lies in an in-depth discussion on challenges and potentials for UX and AD among experts and researchers. We will explore various areas such as methodological issues, human factors, entertainment, social driving, and novel user interface approaches. The overall aim of the workshop is to discuss the future landscape for research within and across each these areas.\n",
      "\n",
      "2. id: 558b172f612c41e6b9d42dfc   score: 0.9628831   abstract: As autonomous cars gain popularity, the importance of studying user experience (UX) in autonomous cars becomes increasingly important. Because validated UX measures specific to autonomous driving have not been developed, we identified several factors of interest common to researchers working at the intersection of autonomous driving, driving simulators, and user experience. We have collected corresponding validated questionnaires to create a comprehensive inventory. We based our selection on attributes such as length of the questionnaires, validation, and prevalence of use such that our work may contribute to an easy and fast setup of high quality questionnaires for the study of UX in autonomous cars. In this extended abstract, we recap the factors we have inventoried.\n",
      "\n",
      "3. id: 5390b19020f70186a0edf69d   score: 0.8844391   abstract: This workshop will address two emerging fields within the HCI community: user experience (UX) and the automotive context. It will bring HCI experts together to discuss UX factors for the specific characteristics of car interiors and automotive user interfaces. It targets the development of a better view of UX within the whole car (driver, front seat, backseat area) beyond traditional marketing instruments known within the automotive industry.\n",
      "\n",
      "4. id: 558bd0960cf25dbdbb04db90   score: 0.6796041   abstract: A major challenge in the future of traffic is to understand how \\\"socially-aware vehicles\\\" could be making use of their social habitus, formed by any information that can be inferred from past and present social relations, social interactions, and a driver's social state when exposed to other participants in real, live traffic. The aim of this workshop in recognition of this challenge is to advance on a common understanding of the symbiosis between drivers, cars, and the infrastructure. The central objective of the workshop is to provoke an active debate on the adequacy of the concept of social, natural, and peripheral interaction, addressing questions such as \\\"who can communicate what\\\", \\\"when\\\", \\\"how\\\", and \\\"why\\\"? To tackle these questions, we would like to collect different, radical, innovative, versatile, and engaging works that challenge or re-imagine human interactions in the\n",
      "\n",
      "5. id: 5390a8b220f70186a0e9cb5a   score: 0.6618136   abstract: Human-Robot Interaction researchers are beginning to reach out to fields not traditionally associated with interaction research, such as the performing arts, cartooning, and animation. These collaborations offer the potential for novel insights about how to get robots and people to interact more effectively, but they also involve a number of unique challenges. This full-day workshop will offer a venue for HRI researchers and their collaborators from these diverse fields to report on their work, share insights about the collaboration process, and to help begin to define an exciting new area in HRI.\n",
      "\n",
      "6. id: 558b030c612c41e6b9d4051b   score: 0.6380463   abstract: Automotive interactive technologies represent an exemplar challenge for user experience (UX) designers, as the concerns for aesthetics, functionality and usability add up to the compelling issues of safety and cognitive demand. This extended abstract presents a methodology for the user-centred creation and evaluation of novel in-car applications, involving real users in realistic use settings. As a case study, we present the methodologies of an ideation workshop in a simulated environment and the evaluation of six design idea prototypes for in-vehicle head up display (HUD) applications using a semi-naturalistic drive. Both methods rely on video recordings of real traffic situations that the users are familiar with and/or experienced themselves. The extended abstract presents experiences and results from the evaluation and reflection on our methods.\n",
      "\n",
      "7. id: 5390bf1320f70186a0f508a7   score: 0.6238353   abstract: This paper presents a project in its early stages of development, in which we propose a solution to the problem of human interaction with autonomous vehicles. We have devised a method for design of a user interface that displays sufficient and crucial information to the driver. Our contribution in this work is (i) identifying different modes of driving behavior, (ii) building an expectation model of a driver, and (iii) implementing an interface system.\n",
      "\n",
      "8. id: 5390a30b20f70186a0e6aeca   score: 0.6001298   abstract: In this report we discuss some of the challenges when applying a user-centred design approach in the field of human-robot interaction (HRI). The discussion is based on a one-day workshop at the NordiCHI'08 conference, investigating how methods, techniques and perspectives from the field of Human Computer Interaction (HCI) could contribute to and learn from recent developments in the area of HRI. Emphasis was put on topics that are infrequent in mainstream HCI such as machine movement, autonomy, anthropomorphism, physical interaction, environmental issues and issues concerned more generally with cultural notions of robots.\n",
      "\n",
      "9. id: 5390a8b220f70186a0e9cb5d   score: 0.47142097   abstract: The field of human-robot interaction is new but growing rapidly. While there are now several established researchers in the field, many of the current human-robot interaction practitioners are students or recently graduated. This workshop, to be held in conjunction with the HRI 2010 Conference, aims to bring together graduate students to present their current research to an audience of their peers in a setting that is less formal and more interactive than the main HRI conference, to talk about the important issues in their field, and to hear about what their colleagues are doing. Participants are encouraged to actively engage in and form relationships with others by discussing fundamental topics in HRI and by engaging in hands-on group activities.\n",
      "\n",
      "10. id: 53908b4920f70186a0dbace7   score: 0.4338615   abstract: The driver/rider experience is a major development in mobile user-interface (UI) design worldwide, similar in scale to the first introduction of personal computers to the desktop. Most automobile manufacturers seeking to develop smart cars have relatively little experience with advanced software-based UIs and information visualization (IV). This panel introduces essential issues of vehicle UI design to the CHI community and offers competing views about the most important issues affecting usability, safety, appeal, functionality, information, and entertainment.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1710785\n",
      "index                                        55323be645cec66b6f9dad17\n",
      "title               Dynamic optimization facilitated by the memory...\n",
      "authors                               Tao Zhu, Wenjian Luo, Lihua Yue\n",
      "year                                                           2015.0\n",
      "venue               Soft Computing - A Fusion of Foundations, Meth...\n",
      "references          558ce9540cf2a2c70f68c134;5390bd1520f70186a0f44...\n",
      "abstract            Memorizing the past information for later envi...\n",
      "id                                                            1710785\n",
      "clustered_labels                                                    2\n",
      "Name: 1710785, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba3820f70186a0f34f81   score: 0.82290184   abstract: Modern day real world applications present us challenging instances where the system needs to adapt to a changing environment without any sacrifice in its optimality. This led researchers to lay the foundations of dynamic problems in the field of optimization. Literature shows different approaches undertaken to tackle the problem of dynamic environment including techniques like diversity scheme, memory, multi-population scheme etc. In this paper we have proposed a hybrid scheme by combining k-means clustering technique with modified Artificial Bee Colony (ABC) algorithm as the base optimizer and it is expected that the clusters locate the optima in the problem. Experimental benchmark set that appeared in IEEE CEC 2009 has been used as test-bed and our ClPABC (Clustering Particle ABC) algorithm is compared against 4 state-of-the-art algorithms. The results show the superiority of our ClPA\n",
      "\n",
      "2. id: 5390c04520f70186a0f57e04   score: 0.5345214   abstract: Many real-world optimization problems are dynamic, in which the environment, i.e. the objective function and restrictions, can change over time. In this case, the optimal solution(s) to the problem may change as well. These problems require optimization algorithms to continuously and accurately track the trajectory of the optima (optimum) through the search space. In this paper, we propose a bi-population hybrid collaborative model of Crowding-based Differential Evolution (CDE) and Particle Swarm Optimization (PSO) for Dynamic Optimization Problems (DOPs). In our approach, called CDEPSO, a population of genomes is responsible for locating several promising areas of the search space and keeping diversity throughout the run using CDE. Another population is used to exploit the area around the best found position using the PSO. Several mechanisms are used to increase the efficiency of CDEPSO\n",
      "\n",
      "3. id: 5390baa120f70186a0f37e6a   score: 0.49006784   abstract: The use of memory-based Evolutionary Algorithms (EAs) for dynamic optimization problems (DOPs) has proved to be efficient, namely when past environments reappear later. Memory EAs using associative approaches store the best solution and additional information about the environment. In this paper we propose a new algorithm called Extended Virtual Loser Genetic Algorithm (eVLGA) to deal with the Dynamic Traveling Salesman Problem (DTSP). In this algorithm, a matrix called extended Virtual Loser (eVL) is created and updated during the evolutionary process. This matrix contains information that reflects how much the worst individuals differ from the best, working as environmental information, which can be used to avoid past errors when new individuals are created. The matrix is stored into memory along with the current best individual of the population and, when a change is detected, this in\n",
      "\n",
      "4. id: 5390bd1520f70186a0f44cf1   score: 0.4426666   abstract: Many problems considered in optimization and artificial intelligence research are static: information about the problem is known a priori, and little to no uncertainty about this information is presumed to exist. Most real problems, however, are dynamic: information about the problem is released over time, uncertain events may occur, or the requirements of the problem may change as time passes. One technique for improving optimization and learning in dynamic environments is by using information from the past. By using solutions from previous environments, it is often easier to find promising solutions in a new environment. A common way to maintain and exploit information from the past is the use of memory, where solutions are stored periodically and can be retrieved and refined when the environment changes. Memory can help search respond quickly and efficiently to changes in a dynamic pr\n",
      "\n",
      "5. id: 5390985d20f70186a0e07483   score: 0.40969306   abstract: In recent years there has been a growing interest in studying evolutionary algorithms for dynamic optimization problems due to its importance in real world applications. Several approaches have been developed, such as the memory scheme. This paper investigates the application of the memory scheme for population-based incremental learning (PBIL) algorithms, a class of evolutionary algorithms, for dynamic optimization problems. A PBIL-specific memory scheme is proposed to improve its adaptability in dynamic environments. In this memory scheme the working probability vector is stored together with the best sample it creates in the memory and is used to reactivate old environments when change occurs. Experimental study based on a series of dynamic environments shows the efficiency of the memory scheme for PBILs in dynamic environments. In this paper, the relationship between the memory schem\n",
      "\n",
      "6. id: 5390a55520f70186a0e792fd   score: 0.4066852   abstract: In recent years, there has been an increasing concern from the evolutionary computation community on dynamic optimization problems since many real-world optimization problems are time-varying. In this paper, a triggered memory scheme is introduced into the particle swarm optimization to deal with dynamic environments. The triggered memory scheme enhances traditional memory scheme with a triggered memory generator. Experimental study over a benchmark dynamic problem shows that the triggered memory-based particle swarm optimization algorithm has stronger robustness and adaptability than traditional particle swarm optimization algorithms, both with and without traditional memory scheme, for dynamic optimization problems.\n",
      "\n",
      "7. id: 5390a28020f70186a0e62716   score: 0.39934304   abstract: This work extends the Particle Swarm Optimization (PSO) algorithm for working on dynamic environments. We propose an evaporation mechanism to solve the outdated memory problem. We empirically show that our evaporation mechanism is able to achieve self-adaption without any knowledge on when changes occur.\n",
      "\n",
      "8. id: 5390a79f20f70186a0e92a35   score: 0.3889661   abstract: In the real world, many applications are non-stationary optimization problems. This requires that optimization algorithms need to not only find the global optimal solution but also track the trajectory of the changing global best solution in a dynamic environment. To achieve this, this paper proposes a clustering particle swarm optimizer (CPSO) for dynamic optimization problems. The algorithm employs hierarchical clustering method to track multiple peaks based on a nearest neighbor search strategy. A fast local search method is also proposed to find the near optimal solutions in a local promising region in the search space. Six test problems generated from a generalized dynamic benchmark generator (GDBG) are used to test the performance of the proposed algorithm. The numerical experimental results show the efficiency of the proposed algorithm for locating and tracking multiple optima in \n",
      "\n",
      "9. id: 5390b56a20f70186a0f065e7   score: 0.35925174   abstract: In recent years, interest in studying evolutionary algorithms (EAs) for dynamic optimization problems (DOPs) has grown due to its importance in real-world applications. Several approaches, such as the memory and multiple population schemes, have been developed for EAs to address dynamic problems. This paper investigates the application of the memory scheme for population-based incremental learning (PBIL) algorithms, a class of EAs, for DOPs. A PBIL-specific associative memory scheme, which stores best solutions as well as corresponding environmental information in the memory, is investigated to improve its adaptability in dynamic environments. In this paper, the interactions between the memory scheme and random immigrants, multipopulation, and restart schemes for PBILs in dynamic environments are investigated. In order to better test the performance of memory schemes for PBILs and other \n",
      "\n",
      "10. id: 5390a79f20f70186a0e92a34   score: 0.34466398   abstract: This paper presents the evolutionary programming with an ensemble of memories to deal with optimization problems in dynamic environments. The proposed algorithm modifies a recent version of evolutionary programming by introducing a simulated-annealing-like dynamic strategy parameter as well as applying local search towards the most improving directions. Diversity of the population is enhanced by an ensemble of external archives that serve as short-term and long-term memories. The archive members also act as the basic solutions when environmental changes occur. The algorithm is tested on a set of 6 multimodal problems with a total 49 change instances provided by CEC 2009 Competition on Evolutionary Computation in Dynamic and Uncertain Environments and the results are presented.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1673954\n",
      "index                                        559162a30cf2e89307ca97cc\n",
      "title               Illumination invariant color segmentation meth...\n",
      "authors             Byeongdae Woo, Youngjung Uh, Kwangyong Lim, Ye...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 9th International Conferenc...\n",
      "references          5390979920f70186a0dffe75;5390ae2e20f70186a0ec7...\n",
      "abstract            This paper proposes a color segmentation metho...\n",
      "id                                                            1673954\n",
      "clustered_labels                                                    3\n",
      "Name: 1673954, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a79f20f70186a0e90a41   score: 0.99850464   abstract: An effective method for traffic sign segmentation is proposed. Through the selection of appropriate characteristic operator on the basis of the distinctive color features of traffic sign, it first quickly obtains gray image of chromatic aberration. Then, the Otsu's thresholding algorithm is subsequently applied to locate accurately the candidate regions of the traffic sign. The experimental result shows that the proposed algorithm can extract the traffic sign from the background well up to the technical standards under various natural illuminating conditions, and the segmentation effect is superior to that performed by the generic segmentation with steady threshold, and the performance in shape checking is thus improved. The presented approach is featured in good robustness, high speed, and as a result can be potentially applied to the real-time processing and commercialization.\n",
      "\n",
      "2. id: 5390adfd20f70186a0ec5d91   score: 0.99821776   abstract: Color image segmentation is still a challenging problem. Literature reveals many supervised algorithms wherein the primary input is the number of segments to which the image is to be segmented. Currently researchers are focusing on unsupervised segmentation algorithms. The main advantage of the proposed method is that no a priori information is required to segment the given color image and hence considered as an unsupervised approach. The proposed method is found to be reliable and works satisfactorily on different kinds of color images. Subjective comparison and objective evaluation shows the efficacy of the proposed method over other existing methods.\n",
      "\n",
      "3. id: 53908e0020f70186a0dd407f   score: 0.9980045   abstract: Abstract: We present a system for the whole road signs detection and recognition task. Road sign regions are detected and extracted from real world scenes on the basis of their color and shape features. Color segmentation is performed introducing a dynamic threshold in the pixel aggregation process on the HSV color space. The dynamic threshold allows the reduction of hue instability in real scenes depending on external brightness variation. Experimental results, using real road images in different environment conditions, are also reported.\n",
      "\n",
      "4. id: 5390a79f20f70186a0e90a44   score: 0.9928786   abstract: The drawbacks of commonly applied color treatment based on the HSI space model have been identified as being the additional computation load for color model conversion, which results in compromising the real-time traffic sign detection. The relevant analysis shows that the major factor adversely affecting the segmentation of traffic signs is closely associated with the color complexity. To reduce the computational complexity in the scene image processing, a new solution based on the color standardization, as defined in this work, is proposed. The scene image is mapped to a simple standardized image consisting of standard color of eight categories, among which five colors related to traffic signs are extracted to form the standardized traffic signs region. The experimental results show that the proposed approach simplifies the complex color description of the scene image, and achieves the\n",
      "\n",
      "5. id: 5390ad8920f70186a0ec177a   score: 0.9921233   abstract: Based on the sequential video frame, a method to detect the traffic sign was proposed. The color image was mapped from RGB to improved HSI color space first, and then converted it to binary image according the color of signs. The shape features were used to remove some noise, and the improved Hough transform was used to locate the areas of traffic signs. Compared to the existed detection algorithm, a previous frame was used as a priori information to reduce the treatment region, then to decrease the computation. In order to prevent the undetected region, a full inspection was processed when the number of consecutive frames which detected the traffic signs was more than the threshold. The results show that the method in this paper performs better than the traditional method.\n",
      "\n",
      "6. id: 53909a0320f70186a0e2058f   score: 0.98975134   abstract: Traffic sign recognition usually consists of two stages: detection and classification. In this paper, we describe the classification stage using the ring-partitioned method. The proposed method uses a specified grayscale image in the pre-processing step and ring-partitioned matching in the matching step. The method does not need carefully prepared many samples of traffic sign images for the training process, alternatively only the standard traffic signs are used as the reference images. The experimental results show the effectiveness of the method in the matching of occluded, rotated, and illumination problems of the traffic sign images with the fast computation time.\n",
      "\n",
      "7. id: 5390a1f720f70186a0e5b8f8   score: 0.98609793   abstract: A method for color image segmentation using a competitive learning clustering scheme is examined, and some basic improvements are made. Two important aspects of the color image segmentation problem, namely color space selection and oversegmentation, are discussed in the context of the algorithm, with comments about suitability and effectivenessof choices for various applications. A variety of settings are tested and compared to highlight performance.\n",
      "\n",
      "8. id: 539099ec20f70186a0e1d30d   score: 0.9803186   abstract: The localization and interpretation of traffic signs by means of a wise-artificial system is one of the several applications of the recognizing-image techniques.In this work we present techniques addressed to the correct and efficient localization of traffic signs from real images, obtaining information about their location, colour, shape, size and orientation. Moreover, it is shown that colour models non-dependent of illumination combined with non-dimensional features are able to discriminate shapes and provide good results in the traffic signs detection. Several experimental results concerning the application of previously mentioned techniques are shown later on.\n",
      "\n",
      "9. id: 5390aaf920f70186a0eae9b7   score: 0.97821885   abstract: Image segmentation is an important and difficult task in computer vision applications. Various methods have been introduced in the past to use gray-level histogram in deciding the segmentation threshold for monochrome images. With the reducing price of color cameras, different color spaces have also been considered in color image based segmentations. In this paper, a study of the effect of color spaces is presented and a segmentation strategy is introduced to select the most effective space in which the segmentation result could be improved. Experimental results show that the proposed method can provide robust segmentation outcomes subject to parts with different colors and under different illumination conditions.\n",
      "\n",
      "10. id: 53909f6920f70186a0e3a32d   score: 0.9763105   abstract: Color segmentation takes a great attention because color is an effective and robust visual cue for characterizing an object from the others. However, color segmentation suffers from color variations incurred by irregular illumination changes. We propose a reliable color modeling approach in hue-saturation-intensity (HSI) color space while considering intensity information by adopting the B-spline curve fitting to make a mathematical model for statistical characteristics of a color with respect to intensity. It is based on the fact that color distribution of a single-colored object is not invariant with respect to brightness variations even in the HS (hue-saturation) plane. The statistical characteristics contain the mean and standard deviation of hue and saturation with respect to intensity. They are mathematically expressed as four bar graphs. In order to fit the bar graphs to continuou\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1672597\n",
      "index                                        559171810cf2e89307ca9d84\n",
      "title                         Building helper bone rigs from examples\n",
      "authors                                                Tomohiko Mukai\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 19th Symposium on Interacti...\n",
      "references          558b2427612c41e6b9d44a0a;558bd3fe0cf23f2dfc593...\n",
      "abstract            Helper bone system has been widely used in rea...\n",
      "id                                                            1672597\n",
      "clustered_labels                                                    0\n",
      "Name: 1672597, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b0ca20f70186a0edb59b   score: 0.8694931   abstract: Skeleton-based linear blend skinning (LBS) remains the most popular method for real-time character deformation and animation. The key to its success is its simple implementation and fast execution. However, in addition to the well-studied elbow-collapse and candy-wrapper artifacts, the space of deformations possible with LBS is inherently limited. In particular, blending with only a scalar weight function per bone prohibits properly handling stretching, where bones change length, and twisting, where the shape rotates along the length of the bone. We present a simple modification of the LBS formulation that enables stretching and twisting without changing the existing skeleton rig or bone weights. Our method needs only an extra scalar weight function per bone, which can be painted manually or computed automatically. The resulting formulation significantly enriches the space of possible de\n",
      "\n",
      "2. id: 558b2427612c41e6b9d44a0a   score: 0.82432055   abstract: We introduce an example-based rigging approach to automatically generate linear blend skinning models with skeletal structure. Based on a set of example poses, our approach can output its skeleton, joint positions, linear blend skinning weights, and corresponding bone transformations. The output can be directly used to set up skeleton-based animation in various 3D modeling and animation software as well as game engines. Specifically, we formulate the solving of a linear blend skinning model with a skeleton as an optimization with joint constraints and weight smoothness regularization, and solve it using an iterative rigging algorithm that (i) alternatively updates skinning weights, joint locations, and bone transformations, and (ii) automatically prunes redundant bones that can be generated by an over-estimated bone initialization. Due to the automatic redundant bone pruning, our approac\n",
      "\n",
      "3. id: 558afb29612c41e6b9d3f752   score: 0.7905303   abstract: This paper presents a real-time skinning technique for character animation based on a two-layered deformation model. For each frame, the skin of a generic character is first deformed by using a classic linear blend skinning approach, then the vertex positions are adjusted according to a Position Based Dynamics schema. We define geometric constraints which mimic the flesh behavior and produce interesting effects like volume conservation and secondary animations, in particular passive jiggling behavior, without relying on a predefined training set of poses. Once the whole model is defined, the character animation is synthesized in real-time without suffering of the inherent artefacts of classic interactive skinning techniques, such as the \\\"candy-wrapper\\\" effect or undesired skin bulging.\n",
      "\n",
      "4. id: 53909f2d20f70186a0e3875f   score: 0.7411123   abstract: Enveloping, or the mapping of skeletal controls to the deformations of a surface, is key to driving realistic animated characters. Despite its widespread use, enveloping still relies on slow or inaccurate deformation methods. We propose a method that is both fast, accurate and example-based. Our technique introduces a rotational regression model that captures common skinning deformations such as muscle bulging, twisting, and challenging areas such as the shoulders. Our improved treatment of rotational quantities is made practical by model reduction that ensures real-time solution of least-squares problems, independent of the mesh size. Our method is significantly more accurate than linear blend skinning and almost as fast, suggesting its use as a replacement for linear blend skinning when examples are available.\n",
      "\n",
      "5. id: 5390a17720f70186a0e53c22   score: 0.67319167   abstract: Skinning of skeletally deformable models is extensively used for real-time animation of characters, creatures and similar objects. The standard solution, linear blend skinning, has some serious drawbacks that require artist intervention. Therefore, a number of alternatives have been proposed in recent years. All of them successfully combat some of the artifacts, but none challenge the simplicity and efficiency of linear blend skinning. As a result, linear blend skinning is still the number one choice for the majority of developers. In this article, we present a novel skinning algorithm based on linear combination of dual quaternions. Even though our proposed method is approximate, it does not exhibit any of the artifacts inherent in previous methods and still permits an efficient GPU implementation. Upgrading an existing animation system from linear to dual quaternion skinning is very ea\n",
      "\n",
      "6. id: 5390a8b220f70186a0e9bf41   score: 0.65907604   abstract: We present a new rigging and skinning method which uses a database of partial rigs extracted from a set of source characters. Given a target mesh and a set of joint locations, our system can automatically scan through the database to find the best-fitting body parts, tailor them to match the target mesh, and transfer their skinning information onto the new character. For the cases where our automatic procedure fails, we provide an intuitive set of tools to fix the problems. When used fully automatically, the system can generate results of much higher quality than a standard smooth bind, and with some user interaction, it can create rigs approaching the quality of artist-created manual rigs in a small fraction of the time.\n",
      "\n",
      "7. id: 5390b00c20f70186a0ed61cc   score: 0.65907604   abstract: We present a new rigging and skinning method which uses a database of partial rigs extracted from a set of source characters. Given a target mesh and a set of joint locations, our system can automatically scan through the database to find the best-fitting body parts, tailor them to match the target mesh, and transfer their skinning information onto the new character. For the cases where our automatic procedure fails, we provide an intuitive set of tools to fix the problems. When used fully automatically, the system can generate results of much higher quality than a standard smooth bind, and with some user interaction, it can create rigs approaching the quality of artist-created manual rigs in a small fraction of the time.\n",
      "\n",
      "8. id: 5390bb7b20f70186a0f3ff92   score: 0.6353356   abstract: In this paper, an efficient deformation framework is presented for skeleton-driven polygonal characters. Standard solutions, such as linear blend skinning, focus on primary deformations and require intensive user adjustment. We propose constructing a lattice of cubic cells embracing the input surface mesh. Based on the lattice, our system automatically propagates smooth skinning weights from bones to drive the surface primary deformation, and it rectifies the over-compressed regions by volume preservation. The secondary deformation is, in the meanwhile, generated by the lattice shape matching with dynamic particles. The proposed framework can generate both low- and high-frequency surface motions such as muscle deformation and vibrations with few user interventions. Our results demonstrate that the proposed lattice-based method is liable to GPU computation, and it is adequate to real-time\n",
      "\n",
      "9. id: 5390985d20f70186a0e0857f   score: 0.6099889   abstract: We extend approaches for skinning characters to the general setting of skinning deformable mesh animations. We provide an automatic algorithm for generating progressive skinning approximations, that is particularly efficient for pseudo-articulated motions. Our contributions include the use of nonparametric mean shift clustering of high-dimensional mesh rotation sequences to automatically identify statistically relevant bones, and robust least squares methods to determine bone transformations, bone-vertex influence sets, and vertex weight values. We use a low-rank data reduction model defined in the undeformed mesh configuration to provide progressive convergence with a fixed number of bones. We show that the resulting skinned animations enable efficient hardware rendering, rest pose editing, and deformable collision detection. Finally, we present numerous examples where skins were automa\n",
      "\n",
      "10. id: 5390b78a20f70186a0f23a77   score: 0.5993679   abstract: Current approaches to skeletally-controlled character articulation range from real-time, closed-form skinning methods to offline, physically-based simulation. In this paper, we seek a closed-form skinning method that approximates nonlinear elastic deformations well while remaining very fast. Our contribution is two-fold: (1) we optimize skinning weights for the standard linear and dual quaternion skinning techniques so that the resulting deformations minimize an elastic energy function. We observe that this is not sufficient to match the visual quality of the original elastic deformations and therefore, we develop (2) a new skinning method based on the concept of joint-based deformers. We propose a specific deformer which is visually similar to nonlinear variational deformation methods. Our final algorithm is fully automatic and requires little or no input from the user other than a rest\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1705625\n",
      "index                                        55323b5e45cec66b6f9d9c9e\n",
      "title               Adaptive and Distributed TDMA Scheduling Proto...\n",
      "authors             Ehsan Gholami, Amir Masoud Rahmani, Mehdi Dehg...\n",
      "year                                                           2015.0\n",
      "venue               Wireless Personal Communications: An Internati...\n",
      "references          55323adf45cec66b6f9d8f0f;55323ae045cec66b6f9d8...\n",
      "abstract            The Wireless Sensor Networks are used to monit...\n",
      "id                                                            1705625\n",
      "clustered_labels                                                    1\n",
      "Name: 1705625, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a45620f70186a0e7267d   score: 0.9941347   abstract: Because of the particularities of wireless sensor networks, the design of MAC protocol in wireless sensor networks should be paid main attention to minimize the energy consumption. Many schemes, which have significant contributions in power saving, can be found. However, these schemes concentrate on reducing the end-to-end delay, or on the energy saving. This paper proposes a TDMA-based scheduling scheme that balances energy saving and end-to-end delay. This balance is achieved by an appropriate scheduling of the path, which is assigned by graph coloring method. Our approach consists of two phases: in the first phase, we use Genetic Algorithm to get the graph coloring strategy, and we use this scheme to color the each path of the networks; then, we use the solution of the edge-coloring to schedule. This proposed scheme achieves the reduction of the end-to-end delay caused by sleep state \n",
      "\n",
      "2. id: 5390a25820f70186a0e5ea7d   score: 0.9832145   abstract: Power saving is a very critical issue in energy-constrained wireless sensor networks. Many schemes can be found in the literature, which have significant contributions in energy conservation. However, these schemes do not concentrate on reducing the end-to-end packet delay while at the same time retaining the energy-saving capability. Since a long delay can be harmful for either large or small wireless sensor networks, this paper proposes a TDMA-based scheduling scheme that balances energy-saving and end-to-end delay. This balance is achieved by an appropriate scheduling of the wakeup intervals, to allow data packets to be delayed by only one sleep interval for the end-to-end transmission from the sensors to the gateway. The proposed scheme achieves the reduction of the end-to-end delay caused by the sleep mode operation while at the same time it maximizes the energy savings.\n",
      "\n",
      "3. id: 53909f8220f70186a0e3cb68   score: 0.9763557   abstract: Energy consumption is one of the most crucial design issues in wireless sensor networks since prolonging the network lifetime depends on the efficient management of sensing node energy resource. In this research study, a new TDMA based MAC protocol, which is not only energy aware but also delay sensitive, is introduced for wireless sensor networks. In the proposed MAC, to achieve energy conservation, sensing nodes employing the proposed MAC sleeps periodically to reduce duty cycle and minimize idle listening. In addition, to provide lower message delay, any time critical sensing node requests extra time slots form the central node when its queue size exceeds the upper threshold value. Unlike common wireless sensor network models with a multi-hop topology, the proposed WSN architecture has a centralized structure especially for energy efficiency and fulfillment of the delay requirement of\n",
      "\n",
      "4. id: 5390ad8920f70186a0ec1077   score: 0.97246706   abstract: This work presents a distributed time slot assignment algorithm which adopts TDMA as Medium Access Control, specially suited to support applications with strict delay, jitter and throughput requirements characterized by convergecast traffic patterns in sensor networks. (E.g. wireless video surveillance sensor networks). Our algorithm has three characteristics: (1) Every node is guaranteed a path to the base station for its data delivery. In the path, sufficient resource is reserved and weighted fairness can be achieved. (2) It uses cascading time slot assignment and jitter minimization algorithm in each node to minimize jitter and end to end delay (3) Nodes are only active during their scheduled slots and sleep otherwise. This offers energy saving by reducing idle listening and avoiding overhearing. The performance of the proposed algorithm is evaluated over simulations and analyzed theo\n",
      "\n",
      "5. id: 53909f8220f70186a0e3e314   score: 0.9694401   abstract: This paper presents a distributed algorithm for efficient Time Division Multiple Access (TDMA) slot allocation in Wireless Sensor Networks. Unlike most of the existent proposals, which try to guarantee interference-free allocation based on the n-hop criterion (which can only work with regular topologies), the presented algorithm bases its decisions on the received signal strength, allowing it to operate independently of the WSN topology. The performance of the proposed slot allocation algorithm was compared with centralized depth-first slot allocation using computer simulation. The results show that the proposed scheme is significantly faster, only paying the price of extra energy consumption during the initial network setup phase.\n",
      "\n",
      "6. id: 558ae389612c41e6b9d3c7cd   score: 0.9687381   abstract: Various sleep scheduling methods have been proposed to achieve energy efficiency in wireless sensor networks. Although these methods can make most nodes sleep to save energy, they do not give adequate consideration to the amount of data originating in specific areas of the network. The authors proposed a sleep scheduling method that divided a sensor network into an adjustable number of areas to ensure that more than one node remained active in each area, but which required all nodes to be able to access the sink node. However, the conditions of simulation experiments displayed a problem in that all the active sensor nodes sent packets to the sink node at the same time. This paper reports how the problem was solved in fresh evaluations of the proposed method and shows that the proposed method also extends the network lifetime.\n",
      "\n",
      "7. id: 5390b29820f70186a0ee9bf7   score: 0.9644556   abstract: In wireless sensor networks, time division multiple access (TDMA) -based MAC can eliminate collisions, hence save energy and guarantee a bounded delay. However, the slot scheduling problem in TDMA is an NP problem. To minimized the total slots needed by a set of data collection tasks and saving the energy consumed on switching between the active and sleep states, a novel particle swarm optimization (PSO)-based scheduling algorithm called PSOSA is proposed in TDMA sensor networks. This algorithm can take full advantage of the searching ability of PSO, which is powerful for solving NP problems. Simulation results show that PSOSA requires less slots and energy to finish a set of data collection tasks. Moreover, compare with coloring algorithms, PSOSA have more flexibility to deal with a multi-objective optimization problem.\n",
      "\n",
      "8. id: 5390b04120f70186a0ed683b   score: 0.9608783   abstract: This work presents a distributed time-slot assignment algorithm which adopts TDMA as Medium Access Control, specially suited to support applications with strict delay, jitter, and throughput requirements characterized by convergecast traffic pattern in sensor networks. (e.g. wireless video surveillance sensor networks). The proposed algorithm has three characteristics: (1) every node is guaranteed a path to the base station for its data delivery. In the path, sufficient resource is reserved and weighted fairness can be achieved. (2) It uses cascading time-slot assignment and jitter minimization algorithm in each node to minimize jitter and end-to-end delay. (3) Nodes are only active during their scheduled slots and sleep otherwise. This offers energy saving by reducing idle listening and avoiding overhearing. The performance of the proposed algorithm is evaluated over simulations and ana\n",
      "\n",
      "9. id: 53909f6a20f70186a0e3bf68   score: 0.9502266   abstract: This paper presents a Self-Reorganizing Slot Allocation (SRSA) mechanism for TDMA based Medium Access Control (MAC) protocols in wireless sensor networks. With TDMA, a node can achieve significant energy savings by remaining active only during allocated slots for transmissions and receptions. In multi-cluster networks, it is often necessary for nodes to use either CDMA or FDMA for preventing interference across neighbor clusters. The goal of this paper is to provide an alternative design that can reduce inter-cluster TDMA interference without having to use spectrum expensive CDMA or FDMA. The primary contribution of this paper is to demonstrate that with adaptive slot allocation, it is possible to reduce such interference under low loading conditions, which is often the case for sensor networks with monitoring applications. The second contribution is to design a feedback based adaptive a\n",
      "\n",
      "10. id: 5390a55520f70186a0e7b5f8   score: 0.94834626   abstract: TDMA has been proposed as a MAC protocol for wireless sensor networks (WSNs) due to its efficiency in high WSN load. However, TDMA is plagued with shortcomings; we present modifications to TDMA that will allow for the same efficiency of TDMA, while allowing the network to conserve energy during times of low load (when there is no activity being detected). Recognizing that aggregation plays an essential role in WSNs, TDMA-ASAP adds to TDMA: (a) transmission parallelism based on a level-by-level localized graph-coloring, (b) appropriate sleeping between transmissions (\"napping\"), (c) judicious and controlled TDMA slot stealing to avoid empty slots to be unused and (d) intelligent scheduling/ordering transmissions. Our results show that TDMA-ASAP's unique combination of TDMA, slot-stealing, napping, and message aggregation significantly outperforms other hybrid WSN MAC algorithms and has a \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1701356\n",
      "index                                        55323ad845cec66b6f9d8e4b\n",
      "title               Simulation systems of small and medium batch p...\n",
      "authors                                                   Yu. A. Zack\n",
      "year                                                           2015.0\n",
      "venue                                   Automation and Remote Control\n",
      "references                                   5390882820f70186a0d8b7d4\n",
      "abstract            This paper formulates the tasks that can be so...\n",
      "id                                                            1701356\n",
      "clustered_labels                                                    0\n",
      "Name: 1701356, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908bad20f70186a0dc24e6   score: 0.6317079   abstract: Providing management control of the multi-stage manufacturing processes found in industry today requires the engineer to apply more sophisticated methods of process definition and analysis. These statistical methods may be collectively called operations research of which simulation is a specific technique. The following paper describes the use of simulation to solve multi-stage process problems in terms of problem definition and analysis of the simulator output.\n",
      "\n",
      "2. id: 5390af8920f70186a0ed1162   score: 0.62738067   abstract: The present paper faces the problem of simplifying simulation tools management in their industrial applications. An approach to implement efficiently and effectively simulation models in manufacturing systems, as decision support system, is deployed. The framework proposed is very flexible and easy to use because of the building block architecture and the automatic model generation. This model is focused on operational decisions as those concerning with scheduling problems.\n",
      "\n",
      "3. id: 5390ad0720f70186a0ebade0   score: 0.6017106   abstract: The control and engineering tasks accomplished with the help of automated process simulation systems are described. The practical solution examples are cited. The UniSim Design system, providing modeling of all stages of production cycle, is described.\n",
      "\n",
      "4. id: 558b396a612c41e6b9d47210   score: 0.5805899   abstract: The purpose of this paper is to describe a detailed and sequential methodology in industrial processes improvement using simulation models. In order to strengthen this academic approach, its application is explained through a case study developed in a medium size company that manufactures windows, doors and facades established in Bucaramanga, Colombia. The article presents the description of the industrial process, the development of the simulation model and the experimental results obtained with its conclusions. The main benefits of this research include: First, the adjustment of basic industrial engineering terms of efficiency and capacity to the specialized simulation software ARENA®, second, the performance analysis of a complex manufacturing process, and finally, the techniques used for the improvement of the performance measures avoiding trial and error techniques.\n",
      "\n",
      "5. id: 5390aaf920f70186a0ead03c   score: 0.53682894   abstract: In this work we present an analysis of simulation tools and modelling technologies for production systems. The use of these tools within a collaborative environment will be a mainstay for distributed manufacturing companies which require the integration of design, manufacturing resources and processes across the product lifecycle.\n",
      "\n",
      "6. id: 539087f820f70186a0d725bc   score: 0.5306014   abstract: This paper discusses how simulation is used to design and analyze manufacturing or warehousing systems. Topics discussed include: manufacturing issues investigated by simulation, techniques for building valid and credible models, manufacturing simulation software, statistical considerations, and simulation pitfalls. A case study is included.\n",
      "\n",
      "7. id: 539087f820f70186a0d727c0   score: 0.5306014   abstract: This paper discusses how simulation is used to design and analyze manufacturing or warehousing systems. Topics discussed include: manufacturing issues investigated by simulation, techniques for building valid and credible models, manufacturing simulation software, statistical considerations, and simulation pitfalls. A case study is included.\n",
      "\n",
      "8. id: 5390b19020f70186a0ee09ac   score: 0.49656874   abstract: Manufacturing systems can be very complex and are often costly to develop and operate. Simulation technology has been shown to be an effective tool for optimizing manufacturing system design, operations, and maintenance procedures. However, each manufacturing simulation is usually developed to address a specific set of industrial issues, and may only apply to a small portion of a complex manufacturing system. To enable manufacturers to more easily use simulation technology to solve complex manufacturing issues, this paper defines a reference architecture for component-based simulation (RACS). With the architecture, complex manufacturing systems are functionally partitioned into smaller interacting subsystems, and simulations of those subsystems are combined to form a federated simulation of the overall manufacturing system. This enables the simultaneous analysis of different aspects of e\n",
      "\n",
      "9. id: 5390a2e920f70186a0e67d33   score: 0.43542135   abstract: The objective of this paper is to present what is at least the authors' general assessment of the state-of-the-art of modeling and simulation in the process industries, which in this context is taken to include the chemical, petrochemical, pulp and paper, metals, waste and water treatment industries but excluding the manufacturing industries such as the automobile industry. Since a number of texts are available on this topic for those readers interested in a more technical treatment, this discussion will tend to be more general, emphasizing such aspects as economic justification, importance of experimental and/or plant data, etc.\n",
      "\n",
      "10. id: 5390a37f20f70186a0e6be05   score: 0.4248892   abstract: We present aspects of a simulated based system for analyzing and designing production control systems. The core of the system is a simulation of a manufacturing system operating with the Production Authorization Card system. The simulation model is fast and flexible, making it attractive for generating large datasets for use in developing simulation metamodels of expected performance for a wide variety of production configurations. Details of the simulation system are provided, along with a discussion of the issues to be considered when using it to design production control systems.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1690280\n",
      "index                                        55924d650cf26384af04a339\n",
      "title                        Use of web mining in studying innovation\n",
      "authors                 Abdullah Gök, Alec Waterworth, Philip Shapira\n",
      "year                                                           2015.0\n",
      "venue                                                  Scientometrics\n",
      "references          5390aca920f70186a0eb9359;5390bb1d20f70186a0f3e...\n",
      "abstract            As enterprises expand and post increasing info...\n",
      "id                                                            1690280\n",
      "clustered_labels                                                    2\n",
      "Name: 1690280, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909f8220f70186a0e3d353   score: 0.99620515   abstract: Web experiments generate insights and promote innovation.\n",
      "\n",
      "2. id: 53908b9320f70186a0dc0e97   score: 0.94122344   abstract: This study reports the results of an online census of the number and character of the publicly accessible Web sites of Fortune 500 companies. The results indicate that approximately 25% of the companies have Web sites. In accord with popular opinion, the sites are used primarily as the electronic equivalent of an annual report (first level effect). The census also indicates that virtually none of the sites offer new ways or forms of doing business (second level effects).\n",
      "\n",
      "3. id: 539089ab20f70186a0d96e82   score: 0.93463415   abstract: During the last year, a large research study was funded by the European Commission; its aim was to determine on-line company characteristics and their Web business strategies while assessing their Web site efficiency. A comparative comparison of North American company practices and comparable ones in Europe was also made. Some of the interesting results are presented here.\n",
      "\n",
      "4. id: 5390979920f70186a0e00f42   score: 0.9213576   abstract: Innovations abound as the traditional Web-based \"search\" moves into uncharted territory\n",
      "\n",
      "5. id: 5390980720f70186a0e02c9d   score: 0.91730267   abstract: In this report, we summarize the contents and outcomes of the recent WebKDD 2004 workshop that was held in conjunction with the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2004), August 22-25, 2004, in Seattle, Washington. We also reflect on the trend in participation levels in the WebKDD series of workshops over the last six years, and indicate new directions in Web mining research as reflected in the latest workshop.\n",
      "\n",
      "6. id: 53908a9720f70186a0da640c   score: 0.8782099   abstract: In this paper, we discuss mining with respect to web data referred here as web data mining. In particular, our focus is on web data mining research in context of our web warehousing project called WHOWEDA (Warehouse of Web Data). We have categorized web data mining into threes areas; web content mining, web structure mining and web usage mining. We have highlighted and discussed various research issues involved in each of these web data mining category. We believe that web data mining will be the topic of exploratory research in near future.\n",
      "\n",
      "7. id: 5390962020f70186a0df4e06   score: 0.8645393   abstract: The evolution of the World Wide Web has brought usenormous amounts of information for business and researchuse. Design and implementation of an automatedsystem for web data mining has become important for companieswishing to utilize useful information from the web.This paper is an attempt at describing confidence on approximatequeries on large datasets which is done in thecontext of an automated system for web data mining. Thesystem has been designed to identify, extract, filter, andanalyze data from web resources. An approach to evaluatingthe quality of extracted web data is also discussed. Thiswork is an exploratory study of web data retrieval and webdata analysis.\n",
      "\n",
      "8. id: 5390a1e620f70186a0e5b64e   score: 0.86373675   abstract: With the rapid growth in business size, today’s businesses orient towards electronic technologies. Unfortunately the enormous size and hugely unstructured data on the web. Extracting valuable information from such an ever-increasing data is an extremely tedious task towards the success of businesses. Web content mining can play a major role in solving these issues and application of web content mining can be very encouraging in some areas. In this paper we present a review of some very interesting, efficient yet implementable techniques from the field of web content mining and study their impact in the area specific to business user needs focusing both on the customer as well as the producer. These techniques have been analyzed and compared on the basis of their execution time and relevance of the result they produced against a particular search.\n",
      "\n",
      "9. id: 5390b4da20f70186a0f00e11   score: 0.852199   abstract: As editors of the Special Issue on a Decade of Mining the Web, we provide a brief overview of how Web mining evolved from the first Web mining workshop (WEBKDD'99) till today. We then introduce the papers of the special issue. Each of them is in a domain of Web mining research; it contains a survey of the past and a vision for the future.\n",
      "\n",
      "10. id: 53909f8220f70186a0e3e6f5   score: 0.83894384   abstract: The paper is engaged in a discussion over applications of Web mining to the intelligent search engine, customer relationship management, personalized service and commercial credit evaluation in e-business. And analysis and reasoning of the mass of information in e-business are made by the technology of Web mining, which can dig out potential modes and predict customers' action, to help enterprises' decision-makers adjust their marketing strategy, reduce the risk, make right decisions and get competitive advantage.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1696300\n",
      "index                                        55922d3e0cf244696a09d95d\n",
      "title               A Novel Finite-Time Stability Criterion for Li...\n",
      "authors             Manfeng Hu, Jinde Cao, Aihua Hu, Yongqing Yang...\n",
      "year                                                           2015.0\n",
      "venue                        Circuits, Systems, and Signal Processing\n",
      "references          5390a9a420f70186a0ea4b2d;5390b60d20f70186a0f13...\n",
      "abstract            This paper is concerned with the finite-time s...\n",
      "id                                                            1696300\n",
      "clustered_labels                                                    2\n",
      "Name: 1696300, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a40520f70186a0e70133   score: 0.98866826   abstract: The consensus problem in a multi-agent system with general nonlinear coupling is investigated in this paper. It is demonstrated that, under suitable conditions on communication, all agents approach a prescribed value if a small fraction of them are controlled by simple feedback control. The stability analysis is based on a blend of graph-theoretic and system-theoretic tools where the contraction analysis and multiple Lyapunov functions play central roles. Numerous numerical examples, which support the analytical results very well, are also included.\n",
      "\n",
      "2. id: 5390a93b20f70186a0ea01d3   score: 0.9851003   abstract: This paper revisits the problem of stability analysis for linear discrete-time systems with time-varying delay in the state. By utilizing the delay partitioning idea, new stability criteria are proposed in terms of linear matrix inequalities (LMIs). These conditions are developed based on a novel Lyapunov functional. In addition to delay dependence, the obtained conditions are also dependent on the partitioning size. We have also established that the conservatism of the conditions is a non-increasing function of the number of partitions. Numerical examples are given to illustrate the effectiveness and advantage of the proposed methods.\n",
      "\n",
      "3. id: 5390a28020f70186a0e61e20   score: 0.98302   abstract: Consensus of multiagent networks with time delay is studied. Based on two prior models, a modified network model in which each communication receiver has the same time delay is presented. Then a criterion is proposed for choosing the time delay in solving the multiagent consensus problem.Simulations show that each communication receiver with the same time delay determined by the proposed criterion can be asymptotically synchronized with a desired speed.\n",
      "\n",
      "4. id: 5390bf1320f70186a0f51cc5   score: 0.9784259   abstract: We study in this paper the consensus problem for multi-agent systems with agents characterized by high-order linear systems with time delays in both the communication network and inputs. Provided that the open-loop dynamics of the agents is not exponentially unstable, but may be polynomially unstable, and the communication topology contains a directed spanning tree, a truncated predictor feedback approach is established to solve the consensus problem. It is shown that, if the delays are constant and exactly known, the consensus problems can be solved by both full state feedback and observer based output feedback protocols for arbitrarily large yet bounded delays. If it is further assumed that the open-loop dynamics of the agents only contains zero eigenvalues, the delays are allowed to be time-varying and unknown. Numerical examples are worked out to illustrate the effectiveness of the p\n",
      "\n",
      "5. id: 558b4875612c41e6b9d4817c   score: 0.9748691   abstract: Consensus problem is investigated for heterogeneous multi-agent systems composed of first-order agents and second-order agents in this paper. Leader-following consensus protocol is adopted to solve consensus problem of heterogeneous multi-agent systems with time-varying communication and input delays. By constructing Lyapunov-Krasovkii functional, sufficient consensus conditions in linear matrix inequality (LMI) form are obtained for the system under fixed interconnection topology. Moreover, consensus conditions are also obtained for the heterogeneous systems under switching topologies with time delays. Simulation examples are given to illustrate effectiveness of the results.\n",
      "\n",
      "6. id: 5390a80e20f70186a0e95ca4   score: 0.9730877   abstract: This paper studies the problem of stability analysis for discrete-time delay systems. By using a delay decomposition approach and the discrete Jensen inequality, a new stability criterion is presented in terms of linear matrix inequalities (LMIs) and proved to be less conservative than the existing ones. A numerical example is given to illustrate the effectiveness and advantages of the proposed method.\n",
      "\n",
      "7. id: 5390af8920f70186a0ed020d   score: 0.9714559   abstract: In this paper, the problem of stability analysis of discrete-time delay systems with two additive time-varying delays is considered. A new stability result is derived for a general class of delay systems which has practical application background in networked control systems. The stability criterion is expressed in the form of linear matrix inequalities (LMIs), which can be readily solved by using standard numerical software. An illustrative example is provided to show the advantage of the proposed stability condition.\n",
      "\n",
      "8. id: 5390a28020f70186a0e61e1f   score: 0.97046465   abstract: Consensus of multiagent networks with time delay is studied. Based on two prior models, a modified network model in which each communication receiver has the same time delay is presented. Then a criterion is proposed for choosing the time delay in solving the multiagent consensus problem. Simulations show that each communication receiver with the same time delay determined by the proposed criterion can be asymptotically synchronized with a desired speed.\n",
      "\n",
      "9. id: 558b8f62612c6b62e5e8b8db   score: 0.9700135   abstract: Recently proposed conditions on finite-time stability in time-delay systems are revisited and it is shown that they are incorrect. General comments on possibility of finite-time convergence in time-delay systems and a necessary condition are given.\n",
      "\n",
      "10. id: 5390a5b020f70186a0e7ddc2   score: 0.96697646   abstract: This paper studies the consensus problem of multi-agent systems with nonuniform time-delays and dynamically changing topologies. A linear consensus protocol is introduced to realize local control strategies for these second-order discrete-time agents. By model transformations and applying the properties of nonnegative matrices, sufficient conditions are derived for state consensus of the systems. It is shown that arbitrary bounded time-delays can safely be tolerated, even though the communication structures between agents dynamically change over time and the corresponding directed graphs may not have spanning trees. Finally, a numerical example is included to illustrate the obtained results.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707981\n",
      "index                                        55323b9a45cec66b6f9da3b8\n",
      "title               Direct private query in location-based service...\n",
      "authors                                    Charles Asanya, Ratan Guha\n",
      "year                                                           2015.0\n",
      "venue                                   The Journal of Supercomputing\n",
      "references          558b8b25612c6b62e5e8b289;5390a8b220f70186a0e9b...\n",
      "abstract            Private query in location-based service allows...\n",
      "id                                                            1707981\n",
      "clustered_labels                                                    1\n",
      "Name: 1707981, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b7775612c6b62e5e8937c   score: 0.9871788   abstract: In this paper we present a solution to one of the location-based query problems. This problem is defined as follows: (i) a user wants to query a database of location data, known as Points Of Interest (POIs), and does not want to reveal his/her location to the server due to privacy concerns; (ii) the owner of the location data, that is, the location server, does not want to simply distribute its data to all users. The location server desires to have some control over its data, since the data is its asset. We propose a major enhancement upon previous solutions by introducing a two stage approach, where the first step is based on Oblivious Transfer and the second step is based on Private Information Retrieval, to achieve a secure solution for both parties. The solution we present is efficient and practical in many scenarios. We implement our solution on a desktop machine and a mobile device\n",
      "\n",
      "2. id: 5390af8920f70186a0ecf85d   score: 0.9764905   abstract: With many location-based services, it is implicitly assumed that the location server receives actual users locations to respond to their spatial queries. Consequently, information customized to their locations, such as nearest points of interest can be provided. However, there is a major privacy concern over sharing such sensitive information with potentially malicious servers, jeopardizing users’ private information. The anonymity- and cloaking-based approaches proposed to address this problem cannot provide stringent privacy guarantees without incurring costly computation and communication overhead. Furthermore, they require a trusted intermediate anonymizer to protect user locations during query processing. This paper proposes a fundamental approach based on private information retrieval to process range and K-nearest neighbor queries, the prevalent queries used in many location-based\n",
      "\n",
      "3. id: 539099ec20f70186a0e1cb41   score: 0.9678993   abstract: This paper tackles a major privacy concern in current location-based services where users have to continuously report their locations to the database server in order to obtain the service. For example, a user asking about the nearest gas station has to report her exact location. With untrusted servers, reporting the location information may lead to several privacy threats. In this paper, we present Casper1; a new framework in which mobile and stationary users can entertain location-based services without revealing their location information. Casper consists of two main components, the location anonymizer and the privacy-aware query processor. The location anonymizer blurs the users' exact location information into cloaked spatial regions based on user-specified privacy requirements. The privacy-aware query processor is embedded inside the location-based database server in order to deal w\n",
      "\n",
      "4. id: 5390a45620f70186a0e7304d   score: 0.96485513   abstract: Protecting users' location information in location-based services, also termed location privacy, has recently garnered significant attention due to its importance in satisfying users' privacy concerns when using location-aware services. Several approaches proposed in the literature blur the user's location in a region by increasing its spatial extent or anonymizing the user among several other users. Such approaches in nature require users to communicate through a trusted anonymizer for all of their queries which can impose unrealistic overall communication/computation overhead between the server and the anonymizer for users with more stringent privacy requirements. We revisit the location privacy problem with the objective of providing significantly more stringent privacy guarantees and propose SPIRAL, a Scalable Private Information Retrieval Approach to Location privacy, which is to th\n",
      "\n",
      "5. id: 5390be6620f70186a0f4cbca   score: 0.9541374   abstract: In this paper we propose a fundamental approach to perform the class of Range and Nearest Neighbor (NN) queries, the core class of spatial queries used in location-based services, without revealing any location information about the query in order to preserve users' private location information. The idea behind our approach is to utilize the power of one-way transformations to map the space of all objects and queries to another space and resolve spatial queries blindly in the transformed space. Traditional encryption based techniques, solutions based on the theory of private information retrieval, or the recently proposed anonymity and cloaking based approaches cannot provide stringent privacy guarantees without incurring costly computation and/or communication overhead. In contrast, we propose efficient algorithms to evaluate KNN and range queries privately in the Hilbert transformed sp\n",
      "\n",
      "6. id: 5390a6b120f70186a0e83dec   score: 0.9503189   abstract: In this article, we present a new privacy-aware query processing framework, Capser&ast;, in which mobile and stationary users can obtain snapshot and/or continuous location-based services without revealing their private location information. In particular, we propose a privacy-aware query processor embedded inside a location-based database server to deal with snapshot and continuous queries based on the knowledge of the user's cloaked location rather than the exact location. Our proposed privacy-aware query processor is completely independent of how we compute the user's cloaked location. In other words, any existing location anonymization algorithms that blur the user's private location into cloaked rectilinear areas can be employed to protect the user's location privacy. We first propose a privacy-aware query processor that not only supports three new privacy-aware query types, but als\n",
      "\n",
      "7. id: 5390ad0720f70186a0ebc542   score: 0.9470882   abstract: Mobile smartphone users frequently need to search for nearby points of interest from a location based service, but in a way that preserves the privacy of the users' locations. We present a technique for private information retrieval that allows a user to retrieve information from a database server without revealing what is actually being retrieved from the server. We perform the retrieval operation in a computationally efficient manner to make it practical for resource-constrained hardware such as smartphones, which have limited processing power, memory, and wireless bandwidth. In particular, our algorithm makes use of a variable-sized cloaking region that increases the location privacy of the user at the cost of additional computation, but maintains the same traffic cost. Our proposal does not require the use of a trusted third-party component, and ensures that we find a good compromise\n",
      "\n",
      "8. id: 5390a45620f70186a0e72c01   score: 0.9462003   abstract: With the proliferation of mobile devices (e.g., PDAs, cell phones, etc.), location-based services have become more and more popular in recent years. However, users have to reveal their location information to access location-based services with existing service infrastructures. It is possible that adversaries could collect the location information, which in turn invades user's privacy. There are existing solutions for query processing on spatial networks and mobile user privacy protection in Euclidean space. However there is no solution for solving queries on spatial networks with privacy protection. Therefore, we aim to provide network distance spatial query solutions which can preserve user privacy by utilizing K-anonymity mechanisms. In this paper, we present two novel query algorithms, PSNN and PSRQ, for answering nearest neighbor queries and range queries on spatial networks without\n",
      "\n",
      "9. id: 5390b19020f70186a0ee03f9   score: 0.93891287   abstract: Location-Based Services (LBSs) have been gaining popularity due to a wide range of interesting and important applications being developed. However, the users availing such services are concerned about their location privacy, in that they are forced to reveal their sensitive location information to untrusted third-parties. In this paper, we propose a new privacy-preserving approach, Cover Locations, which allows a user to access an LBS without revealing his/her actual location. Based on its current location, the user's device queries for a few specifically chosen surrounding locations and constructs the results corresponding to its location from the results obtained for each queried location. Since the user location does not leave the user's device - as either a latitude and longitude pair, or as an obfuscated region - the user is guaranteed very high level of privacy. The Cover Locations\n",
      "\n",
      "10. id: 5390979920f70186a0e00ecd   score: 0.90000373   abstract: Location-based queries are network services that provide users with local information based on their current geographic locations. For example, these services could list nearby restaurants and help users navigate unknown areas. However, these services cause privacy concerns, because they require that users transmit their current positions to an external service provider. Location information collected from GPS or wireless LAN can be so precise that, even if the user issues a query without explicit user identification, an adversary can learn the user's identity by linking the position to publicly available, identified location records. Simply coarsening location information, for example to zip codes, may unnecessarily degrade service quality. I propose location privacy mechanisms that address this tradeoff between privacy and service quality. For a given population density around a user, \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674942\n",
      "index                                        559258400cf205530abc9866\n",
      "title               TextAlive: Integrated Design Environment for K...\n",
      "authors                      Jun Kato, Tomoyasu Nakano, Masataka Goto\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          558b3012612c41e6b9d46203;53908bcc20f70186a0dc6...\n",
      "abstract            This paper presents TextAlive, a graphical too...\n",
      "id                                                            1674942\n",
      "clustered_labels                                                    0\n",
      "Name: 1674942, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909a0220f70186a0e1f496   score: 0.7181004   abstract: Animated text, commonly called kinetic typography, is any attractive visual expression used in films, TV programs, video games, etc. Previous studies have developed tools that support the authoring and rendering of kinetic typography. However, authoring kinetic typography is not easy because its methodology is still at an early stage. Hence, we systematize expression elements in kinetic typography and propose an automatic composer that converts raw text into kinetic typography data.\n",
      "\n",
      "2. id: 539099a220f70186a0e1736b   score: 0.6981237   abstract: This paper describes an effort to design and implement an interactive system involving two independent processors - one a display processor and the other a music processor. The goal of the work was to develop tools and techniques for the artist interested in working with the media of animation and music in an integrated fashion. Since the purpose of our project was purely artistic, its success and significance can be evaluated only in terms of the uniqueness and viability of the products which our tools and techniques have yielded thus far. Our present results are unique to the extent that they would be extremely difficult, if not impossible, to reproduce on other existing animation systems. As far as aesthetic criteria are concerned, all we can say is that we find our results viable enough to warrant public exposure.The basic principle of our system is that both animation and music are \n",
      "\n",
      "3. id: 53908bfb20f70186a0dc9a97   score: 0.46402106   abstract: The utility of an interactive tool can be measured by how pervasively it is embedded into a user's workflow. Tools for artists additionally must provide an appropriate level of control over expressive aspects of their work while suppressing unwanted intrusions due to details that are, for the moment, unnecessary. Our focus is on tools that target editing the expressive aspects of character motion. These tools allow animators to work in a way that is more expedient than modifying low-level details, and offers finer control than high level, directorial approaches. To illustrate this approach, we present three such tools, one for varying timing (succession), and two for varying motion shape (amplitude and extent). Succession editing allows the animator to vary the activation times of the joints in the motion. Amplitude editing allows the animator to vary the joint ranges covered during a mo\n",
      "\n",
      "4. id: 5390b5c620f70186a0f07b90   score: 0.45301983   abstract: This project explores issues of typographic design within multimedia contexts, which include interactive, sonic and animated components.\n",
      "\n",
      "5. id: 539088b820f70186a0d8fb73   score: 0.37605008   abstract: Kinetic typography --- text that uses movement or other temporal change --- has recently emerged as a new form of communication. As we hope to illustrate in this paper, kinetic typography can be seen as bringing some of the expressive power of film --- such as its ability to convey emotion, portray compelling characters, and visually direct attention --- to the strong communicative properties of text. Although kinetic typography offers substantial promise for expressive communications, it has not been widely exploited outside a few limited application areas (most notably in TV advertising). One of the reasons for this has been the lack of tools directly supporting it, and the accompanying difficulty in creating dynamic text. This paper presents a first step in remedying this situation --- an extensible and robust system for animating text in a wide variety of forms. By supporting an appr\n",
      "\n",
      "6. id: 558b0834612c41e6b9d40da9   score: 0.35532737   abstract: We describe an audiovisual live coding performance created using Gibber, a creative coding environment that runs in the browser. The performance takes advantage of novel affordances for rapidly creating music, shaders, and mappings that tie together audio and visual modalities.\n",
      "\n",
      "7. id: 539089bb20f70186a0d989bc   score: 0.28776783   abstract: We propose a prototype authoring tool to synchronize among computer graphics animation, narration, and background music through the simple and interactive manipulation of a graphical interface. It is meant to be of assistance to those who lack special knowledge and/or skill in editing such media. The tool provides the following two features. First, users can visually grasp the contents of a background music track and a narration along a single timeline, and can also display the scenario of a computer graphics animation sharing the same timeline. Second, this tool provides a graphical user interface for interactively editing (reducing and/or expanding) both background music and narration without any deterioration. That is, the tool can synchronize the playback time of the background music with the corresponding portion of the computer graphics animation specified by the users by means of \n",
      "\n",
      "8. id: 53908a4020f70186a0d9dbb4   score: 0.28180072   abstract: Kinetic (dynamic) typography has demonstrated the ability to add significant emotive content and appeal to expressive text, allowing some of the qualities normally found in film and the spoken word to be added to static text. Kinetic typography has been widely and successfully used in film title sequences as well as television and computer-based advertising. However, its communicative abilities have not been widely studied, and its potential has rarely been exploited outside these areas. This is partly due to the difficulty in creating kinetic typography with current tools, often requiring hours of work to animate a single sentence.In this paper, we present the Kinedit system, a basic authoring tool that takes initial steps toward remedying this situation and hence promoting exploration of the communicative potential of kinetic typography for personal communication. Kinedit is informed b\n",
      "\n",
      "9. id: 539098b820f70186a0e0b4db   score: 0.2694217   abstract: This paper begins by evaluating various systems in terms of factors for building interactive audiovisual environments. The main issues for flexibility and expressiveness in the generation of dynamic sounds and images are then isolated. The design and development of an audiovisual system prototype is described at the end.\n",
      "\n",
      "10. id: 53908cde20f70186a0dcde05   score: 0.22507577   abstract: Abstract We propose that video/audio animation be considered as a first-class object on the World Wide Web. Animation is a very \"bandwidth-efficient\" alternative to using video streams, especially for presentations involving mathematical objects and interactions. We present an object-oriented model that supports drawing-based and frame-based animation. Based on that model, we describe an extension of the HyperText Markup Language to support these capabilities. BU-NCSA Mosanim, a modified version of the NCSA Mosaic for X(v2.5), was developed and is available for distribution via anonymous FTP to demonstrate the concepts and potentials of animation in presentations and interactive game playing over the web.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1675228\n",
      "index                                        55914bdc0cf232eb904fba46\n",
      "title               Providing Adaptive Support in an Interactive S...\n",
      "authors                                 Samad Kardan, Cristina Conati\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          53909fbd20f70186a0e42224;5390a7f520f70186a0e93...\n",
      "abstract            Recent rise of Massive Open Online Courses (MO...\n",
      "id                                                            1675228\n",
      "clustered_labels                                                    0\n",
      "Name: 1675228, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390bf1320f70186a0f514a0   score: 0.95473194   abstract: Assessing the rapidly evolving realm of massive open online courses.\n",
      "\n",
      "2. id: 558b095f612c41e6b9d40f93   score: 0.9184803   abstract: Simulations can be powerful learning tools that allow students to explore and understand concepts in ways that are not possible in typical classroom settings. However, research is lacking as to how to use simulations most effectively in different types of learning environments. To address this need, we designed a study to examine the impact of using online interactive simulations on the learning and motivation of 109 undergraduate architecture students from two large public universities. The simulation tool allowed students to create models of spatial structures and analyze the effects of loads on structural member forces and deflections. The authors incorporated the simulations into our instructional design using an inquiry approach because it was consistent with our goals of teaching students concepts and the process of deriving the concepts. They documented that online interactive sim\n",
      "\n",
      "3. id: 5390a4d020f70186a0e7639b   score: 0.9097468   abstract: We explored the impact on learning of interactive simulations that were coordinated with AutoTutor, a learning environment that helps students by holding a conversation in natural language. We randomly assigned 132 college students to one of three conditions: AutoTutor without simulations, AutoTutor with simulations, and a Monte Carlo AutoTutor that randomly generated dialogue moves. A pretest-posttest design was used to measure learning gains, as measured by objective multiple choice questions. All versions of AutoTutor were successful in promoting learning. The Monte Carlo AutoTutor produced significantly lower gains than the interactive simulation version for higher knowledge learners, and the direction of the three means were in the predicted direction. Improved simulation dialogues, modeling of good simulation manipulation strategies, and faster display of simulations are expected t\n",
      "\n",
      "4. id: 53908d6620f70186a0dd2bf9   score: 0.7190878   abstract: Abstract: In this paper I present my efforts in exploring the integration of physical and computational media for the design of interactive simulations to support learning about complex domains. My effort involves the design of interactive learning environments to integrate systems supporting alternative ways of interaction with simulations with an emphasis upon support for shared interaction to mediate social aspects of learning, knowledge construction, reflection and design. Some examples from the projects I am conducting in Sweden are presented.\n",
      "\n",
      "5. id: 5390bed320f70186a0f4eaeb   score: 0.6556668   abstract: Due to the recent emergence of massive open online courses (MOOCs), students and teachers are gaining unprecedented access to high-quality educational content. However, many questions remain on how best to utilize that content in a classroom environment. In this small-scale, exploratory study, we compared two ways of using a recorded video lecture. In the online learning condition, students viewed the video on a personal computer, and also viewed a follow-up tutorial (a quiz review) on the computer. In the blended learning condition, students viewed the video as a group in a classroom, and received the follow-up tutorial from a live lecturer. We randomly assigned 102 students to these conditions, and assessed learning outcomes via a series of quizzes. While we saw significant learning gains after each session conducted, we did not observe any significant differences between the online an\n",
      "\n",
      "6. id: 5390bed320f70186a0f4eae5   score: 0.6406359   abstract: Massive open online courses (MOOCs) provide learning materials and automated assessments for large numbers of virtual users. Because every interaction is recorded, we can longitudinally model performance over the course of the class. We create a panel model of achievement in an early MOOC to estimate within- and between-user differences. In this study, we hope to contribute to HCI literature by, first, applying quasi-experimental methods to identify behaviors that may support student learning in a virtual environment, and, second, by using a panel model that takes into account the longitudinal, dynamic nature of a multiple-week class.\n",
      "\n",
      "7. id: 53909f8220f70186a0e3d3cd   score: 0.611672   abstract: For successful classroom use, simulations must be integrated with other educational materials. The examples and methods described here have all been used in large introductory courses.\n",
      "\n",
      "8. id: 558b7a25612c6b62e5e897f5   score: 0.5283205   abstract: Massive open online courses (MOOCs) have launched a scale shift in higher education, with several individual MOOCs now boasting tens or hundreds of thousands of participants worldwide. Our MOOC on the principles of functional programming has more than 100,000 registered students to date, and boasts one of the highest rates of completion (19.2%) for its size. In this paper, we describe our experience organizing this popular MOOC, and demonstrate how providing innovative supporting tools (IDE plugins, testing frameworks, interactive build tools, automated cloud-based graders, style checkers) and considering key human-computer interaction factors potentially contributed to this markedly high completion rate. We collect an unprecedented volume of course statistics and survey results and have made them available, along with scripts for generating interactive web-based visualizations, as an op\n",
      "\n",
      "9. id: 5390bfa220f70186a0f54ee0   score: 0.52644926   abstract: In this article the author reflects on the experiences of three Massive Open Online Courses (MOOCs): Edfuture (CHFE), Learning Design for 21st Century Curriculum (OLDS-MOOC), and Open Education (H817). Discussion draws on the perceived differences between OERs and MOOCs and questions the definitions of 'success', 'engagement', 'completion', and 'drop out' in a MOOC. Some lessons learnt as a participant are also discussed.\n",
      "\n",
      "10. id: 5390bded20f70186a0f483d7   score: 0.5203745   abstract: Massive Open Online Courses or MOOCs, both in their major approaches xMOOC and cMOOC, are attracting a very interesting debate about their influence in the higher education future. MOOC have both great defenders and detractors. As an emerging trend, MOOCs have a hype repercussion in technological and pedagogical areas, but they should demonstrate their real value in specific implementation and within institutional strategies. Independently, MOOCs have different issues such as high dropout rates and low number of cooperative activities among participants. This paper presents an adaptive proposal to be applied in MOOC definition and development, with a special attention to cMOOC, that may be useful to tackle the mentioned MOOC problems.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1660662\n",
      "index                                        559128fb0cf232eb904fb09d\n",
      "title               \\\"Local Remote\\\" Collaboration: Applying Remot...\n",
      "authors             Stacey D. Scott, T.C. Nicholas Graham, James R...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference Compani...\n",
      "references          5390995d20f70186a0e15db8;539088b820f70186a0d8f...\n",
      "abstract            Co-located environments have long been conside...\n",
      "id                                                            1660662\n",
      "clustered_labels                                                    3\n",
      "Name: 1660662, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53909a0220f70186a0e1fbbc   score: 0.989791   abstract: Despite the availability of awareness servers and casual interaction systems, distributed groups still cannot maintain artifact awareness -- the easy awareness of the documents, objects, and tools that other people are using -- that is a natural part of co-located work environments. To address this deficiency, we designed an awareness tool that uses screen sharing to provide information about other people's artifacts. People see others' screens in miniature at the edge of their display, can selectively raise a larger view of that screen to get more detail, and can engage in remote pointing if desired. Initial experiences show that people use our tool for several purposes: to maintain awareness of what others are doing, to project a certain image of themselves, to monitor progress and coordinate joint tasks, to help determine when another person can be interrupted, and to engage in serend\n",
      "\n",
      "2. id: 5390b3ae20f70186a0ef4d83   score: 0.9778412   abstract: The primary contribution of my work is a methodology for the study of teamwork and taskwork during mixed-focus collaboration. I am currently completing the third experimental study in a series of three that investigate the role of shared and personal displays in supporting collaboration. The first study compared single- and multi-display configurations, and found that single display systems supported coordination, whereas multi-display systems improved collaborative outcomes. The second study compared different shared display types and identified how they support group synchronization, monitoring, and grounding behaviours. The final study investigates information sharing when groups are supported by interactive tabletops and tablets.\n",
      "\n",
      "3. id: 53909fbc20f70186a0e41a2f   score: 0.95606345   abstract: We present an exploratory lab study that provides observations and measures about the usage of interaction devices in co-located cooperative work situations at a tabletop display. We designed our experiment with the aim of providing a context for the collaboration that shares as many characteristics of real life as possible. Twenty-two participants were instructed to perform a shared goal task. They worked in co-located pairs on solving three sets of two jigsaw puzzles concurrently. They were allowed to use any combination of direct and indirect input device, i.e., touch and mouse, to achieve the goal. Additionally, a hidden task was imposed on the participants in the second and third puzzle task: They had to discover that pieces were mixed up between the two displayed puzzles. The role of the hidden task was to trigger spontaneous transitions from individual to collaborative work. Our o\n",
      "\n",
      "4. id: 539095ba20f70186a0df179a   score: 0.9558166   abstract: From the multitude of workplace studies that we have seen during the last decades, it has been shown that a common environment to a large extent supports coordination of work. The use of common artifacts and awareness of the co-workers activities effortlessly afford communication of the current state of work. Inevitably, a question arises: how can we get similar support for distributed groups? One idea has been to use a continuously open video and/or audio link, i.e. a media space, to support the informal coordination possibilities that are lacking in a distributed setting. In this paper, two cases from air traffic control are presented, where the long-term use of video and audio links plays an important role for communicating real-time updates of the state of work. The possibility to overhear and oversee what the colleagues are doing in remote positions reduces to a large extent the amo\n",
      "\n",
      "5. id: 5390979920f70186a0dffbe1   score: 0.9323302   abstract: Based on a naturalistic study of industrial designers engaged in collocated collaborative design work in a technologically unsophisticated environment, we observed a number of interactions that lead to a number of insights, namely, (1) seating and the shape and orientation of the working surface has an effect on line of sight and eye-contact behaviors, (2) being able to reach objects on the working surface effects an individual collaborator's ability to become the focus of attention, (3) in collaborative work, people may work on the same document or divide labors to work on different documents simultaneously, (4) supporting the use of conventional artifacts that users are familiar with is as important as supporting the use of informational devices, (5) large workspaces with different privacy levels support both the needs of sharing information and the needs of keeping information private\n",
      "\n",
      "6. id: 5390b20120f70186a0ee56e9   score: 0.92944044   abstract: We conducted an empirical study to investigate the use of personal and shared displays during group work. The collaborative environments under study consisted of personal workspaces, in the form of laptops, and a shared virtual workspace displayed on a nearby wall. Our study compared the use of the large shared display under two different interface content conditions; a status display that provided an overview of the group's current task performance, and a replicated view of the shared workspace that allowed task work to occur on the shared display. The study results suggest that while participants used their personal displays primarily to perform the task, the shared display facilitated several key teamwork mechanisms. In particular, the provided status display best facilitated monitoring of group progress, whereas the replicated content display best facilitated conversational grounding\n",
      "\n",
      "7. id: 558ae8c0612c41e6b9d3d1c6   score: 0.9174507   abstract: Many studies suggest that tangibles and digital tabletops have potential to support collaborative interaction. However, previous findings show that users often work in parallel with such systems. One design strategy that may encourage collaboration rather than parallel use involves creating a system that responds to co-dependent access points in which more than one action is required to create a successful system response. To better understand how co-dependent access points support collaboration, we designed a comparative study with 12 young adults using the same application with a co-dependent and an independent access point design. We collected and analyzed categories of both verbal and behavioural data in the two conditions. Our results show support for the co-dependent strategy and suggest ways that the co-dependent design can be used to support flexible collaboration on tangible tab\n",
      "\n",
      "8. id: 5390979920f70186a0dffbbb   score: 0.896614   abstract: The status quo for co-located groupware is to assume that \"social protocols\" (standards of polite behavior) are sufficient to coordinate the actions of a group of users; however, prior studies of groupware use as well as our own observations of groups using a shared tabletop display suggest potential for improving groupware interfaces by incorporating coordination policies - direct manipulation mechanisms for avoiding and resolving conflicts. We discuss our observations of group tabletop usage and present our coordination framework. We conclude with example usage scenarios and discuss future research suggested by this framework.\n",
      "\n",
      "9. id: 5390b86b20f70186a0f2a09e   score: 0.89048487   abstract: One of the most popular scenarios for advertising interactive surfaces in the home is their support for solving co-located collaborative tasks. Examples include joint planning of events (e.g., holidays) or deciding on a shared purchase (e.g., a present for a common friend). However, this usually implies that all interactions with information happen on the common display. This is in contrast to the current practices to use personal devices and further, most people's behavior to constantly switch between individual and group phases because people have differing search strategies, preferences, etc. We therefore investigated how the combination of personal devices and a simple way of exchanging information between these devices and an interactive surface changes the way people solve collaborative tasks compared to an existing approach of using personal devices. Our study results clearly indi\n",
      "\n",
      "10. id: 53909ed120f70186a0e30a26   score: 0.8576489   abstract: This work-in-progress discusses qualitative findings about the impact of portable technologies in collocated collaboration. Laptops, cell phones, and other handheld devices are both a distraction during face-to-face meetings, and at the same time allow spontaneous access to needed information. Interviews with fifteen professionals were conducted to elicit why and how these technologies are used in meeting settings. Responses across participants varied strongly and indicate that this emerging research area must look at the notion of context in new ways to support both individual and group needs.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1662852\n",
      "index                                        55913edf0cf232eb904fb6fc\n",
      "title               Lightweight Java Profiling with Partial Safepo...\n",
      "authors                Peter Hofer, David Gnedt, Hanspeter Mössenböck\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 6th ACM/SPEC International ...\n",
      "references          558bd23b0cf23f2dfc593b5f;5390ad0720f70186a0ebb...\n",
      "abstract            Sampling profilers are popular because of thei...\n",
      "id                                                            1662852\n",
      "clustered_labels                                                    3\n",
      "Name: 1662852, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558bd23b0cf23f2dfc593b5f   score: 0.94757545   abstract: Sampling is a popular approach to profiling because it typically has only a small impact on performance and does not modify the profiled application. Common sampling profilers collect data about an application by pausing the application threads, walking the stacks to create stack traces, and then adding the traces to their profile. Waiting threads are often sampled as well, even when they have not been active since their last sample. Sampling profilers for Java commonly rely on safepoints, which are locations in the Java code where a thread can pause to be sampled. However, restricting profiling to these locations affects the accuracy of the profile, and the safepoint mechanism itself imposes significant pause times on the application. We present stack fragment sampling, a new approach for Java applications that minimizes pause times and eliminates redundant samples altogether. It interr\n",
      "\n",
      "2. id: 5390a6d920f70186a0e880e9   score: 0.891811   abstract: This paper describes our sampling-based profiler that exploits a processor's HPM (Hardware Performance Monitor) to collect information on running Java applications for use by the Java VM. Our profiler provides two novel features: Java-level event profiling and lightweight context-sensitive event profiling. For Java events, we propose new techniques to leverage the sampling facility of the HPM to generate object creation profiles and lock activity profiles. The HPM sampling is the key to achieve a smaller overhead compared to profilers that do not rely on hardware helps. To sample the object creations with the HPM, which can only sample hardware events such as executed instructions or cache misses, we correlate the object creations with the store instructions for Java object headers. For the lock activity profile, we introduce an instrumentation-based technique, called ProbeNOP, which use\n",
      "\n",
      "3. id: 5390a93b20f70186a0e9f9f2   score: 0.79390603   abstract: As data centers and end users become increasingly reliant on virtualization technology, more efficient and accurate methods of profiling such systems are needed. However, under virtualization the virtual machine and OS each try to manage the same resources independently, the underlying hardware is now multiplexed between many streams of execution, and non-trivial interference can be caused by seemingly unrelated resources. While sampling techniques are effective at gathering average behaviors over long runs, understanding the time-varying behavior of programs under virtualization, the correlation between events at the level of program phases, or the transient effects of rare events requires a new way of profiling virtualized applications. To this end we present VrtProf, a low overhead profiling tool that automates the collection of hardware and software events spanning the vertical execu\n",
      "\n",
      "4. id: 53909eef20f70186a0e36871   score: 0.7444706   abstract: Existing profilers for Java applications typically rely on custom instrumentation in the Java virtual machine, and measure only limited types of resource consumption. Garbage collection and multi-threading pose additional challenges to profiler design and implementation. In this paper we discuss a general-purpose, portable, and extensible approach for obtaining comprehensive profiling information from the Java virtual machine. Profilers based on this framework can uncover CPU usage hot spots, heavy memory allocation sites, unnecessary object retention, contended monitors, and thread deadlocks. In addition, we discuss a novel algorithm for thread-aware statistical CPU time profiling, a heap profiling technique independent of the garbage collection implementation, and support for interactive profiling with minimum overhead.\n",
      "\n",
      "5. id: 5390bf1320f70186a0f5086a   score: 0.66519314   abstract: Sampling is a popular approach to collecting data for profiling and monitoring, because it has a small impact on performance and does not modify the observed application. When sampling stack traces, they can be merged into a calling context tree that shows where the application spends its time and where performance problems lie. However, Java VM implementations usually rely on safepoints for sampling stack traces. Safepoints can cause inaccuracies and have a considerable performance impact. We present a new approach that does not use safepoints, but instead relies on the operating system to take snapshots of the stack at arbitrary points. These snapshots are then asynchronously decoded to call traces, which are merged into a calling context tree. We show that we are able to decode over 90% of the snapshots, and that our approach has very small impact on performance even at high sampling \n",
      "\n",
      "6. id: 5390995d20f70186a0e16037   score: 0.6534587   abstract: Calling context profiles are used in many inter-procedural code optimizations and in overall program understanding. Unfortunately, the collection of profile information is highly intrusive due to the high frequency of method calls in most applications. Previously proposed calling-context profiling mechanisms consequently suffer from either low accuracy, high overhead, or both. We have developed a new approach for building the calling context tree at runtime, called adaptive bursting. By selectively inhibiting redundant profiling, this approach dramatically reduces overhead while preserving profile accuracy. We first demonstrate the drawbacks of previously proposed calling context profiling mechanisms. We show that a low-overhead solution using sampled stack-walking alone is less than 50% accurate, based on degree of overlap with a complete calling-context tree. We also show that a static\n",
      "\n",
      "7. id: 5390b2d620f70186a0eeba24   score: 0.6475754   abstract: Prevailing profilers for Java, which rely on standard, native-code profiling interfaces, are not portable, give imprecise results due to serious measurement perturbation, and cause excessive overheads. In contrast, program transformations allow to generate reproducible profiles in a fully portable way with significantly less overhead. This paper presents a profiling framework that instruments Java programs at the bytecode level to build context-sensitive execution profiles at runtime. The profiling framework includes an exact profiler as well as a sampling profiler. User-defined profiling agents can be written in pure Java, too, in order to customize the runtime processing of profiling data.\n",
      "\n",
      "8. id: 5390990f20f70186a0e106c0   score: 0.5266167   abstract: This paper presents innovative program transformations for the efficient and accurate profiling of Java programs. The profiling is based on a deterministic sampling mechanism that exploits the number of executed JVM bytecode instructions to trigger a user-defined profiling agent in order to process samples of the call stack. The instrumentation is entirely portable, profiles are reproducible, and the sampling rate can be dynamically tuned. Moderate overhead and high profile accuracy make the profiling framework attractive for developers of complex systems, such as application servers.\n",
      "\n",
      "9. id: 5390995d20f70186a0e15ce0   score: 0.519628   abstract: This article presents a novel framework for the sampling-based profiling of Java programs, which relies on program transformation techniques. We exploit bytecode instruction counting to regularly activate a user-defined profiling agent, which processes the current call stack. This approach has several advantages, such as making the instrumentation entirely portable, generating reproducible profiles, and enabling a fine-grained adjustment of the sampling rate. Our framework offers a flexible API to write portable profiling agents in pure Java. While the overhead due to our profiling scheme is comparable to the overhead caused by prevailing, timing-based profilers, the resulting profiles are much more accurate. Copyright © 2006 John Wiley & Sons, Ltd.\n",
      "\n",
      "10. id: 5390a5b020f70186a0e7e459   score: 0.4824291   abstract: The Calling Context Tree (CCT) is a prevailing datastructure for calling context profiling. As generating a complete CCT reflecting every method call is expensive, recent research has focused on efficiently approximating the CCT with sampling techniques. However, for tasks such as debugging, testing, and reverse engineering, complete and accurate CCTs are often needed. In this paper, we introduce the ParCCT, a novel approach to parallelizing application code and CCT generation on multicores. Each thread maintains a shadow stack and generates \"packets\" of method calls and returns that correspond to partial CCTs. Each packet includes a copy of the shadow stack, indicating the calling context of the first method call in the packet. Hence, packets are independent of each other and can be processed out-of-order and in parallel in order to update the CCT. Our portable and extensible implementa\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698733\n",
      "index                                        558ad9b8612c41e6b9d3b82a\n",
      "title               The Linux FAT32 allocator and file creation or...\n",
      "authors                                               Wicher Minnaard\n",
      "year                                                           2015.0\n",
      "venue               Digital Investigation: The International Journ...\n",
      "references          5390980720f70186a0e03df3;5390b61e20f70186a0f14...\n",
      "abstract            The allocation algorithm of the Linux FAT32 fi...\n",
      "id                                                            1698733\n",
      "clustered_labels                                                    1\n",
      "Name: 1698733, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a74f20f70186a0e8ca20   score: 0.98138225   abstract: Estimates are made of the amount of searching required for the exact location of a record in several types of storage systems, including the index-table method of addressing and the sorted-file method. Detailed data and formulas for access time are given for an \"open\" system which offers high flexibility and speed of access. Experimental results are given for actual record files.\n",
      "\n",
      "2. id: 5390b20120f70186a0ee3dd3   score: 0.9769787   abstract: Increasing the visibility and access to underlying file structure on consumer devices can vastly improve the user experience.\n",
      "\n",
      "3. id: 53909eef20f70186a0e3697b   score: 0.9758546   abstract: The 4.4BSD file system includes a new algorithm for allocating disk blocks to files. The goal of this algorithm is to improve file clustering, increasing the amount of sequential I/O when reading or writing files, thereby improving file system performance. In this paper we study the effectiveness of this algorithm at reducing file system fragmentation. We have created a program that artificially ages a file system by replaying a workload similar to that experienced by a real file system. We used this program to evaluate the effectiveness of the new disk allocation algorithm by replaying ten months of activity on two file systems that differed only in the disk allocation algorithms that they used. At the end of the ten month simulation, the file system using the new allocation algorithm had approximately half the fragmentation of a similarly aged file system that used the traditional disk\n",
      "\n",
      "4. id: 5390b61e20f70186a0f14c26   score: 0.96329963   abstract: TomTom navigation systems are widely used, but unfortunately these devices do not store any information that can be used to link time and position on any non-volatile media. However, this kind of information can be found in the volatile Random Access Memory (RAM) of these devices. There are two methods to extract the content stored in RAM. The first method uses the available JTAG signals on the device. For the second method, a small Linux distribution is loaded into the device. Analysis of the extracted content shows some information linking time and position of the device. Although the quantity of information found at this point is limited, the possibility to link the position and time of the device can be invaluable in some cases.\n",
      "\n",
      "5. id: 5390bda020f70186a0f47f0a   score: 0.9323302   abstract: File systems that manage magnetic disks have long recognized the importance of sequential allocation and large transfer sizes for file data. Fast random access has dominated metadata lookup data structures with increasing use of B-trees on-disk. Yet our experiments with workloads dominated by metadata and small file access indicate that even sophisticated local disk file systems like Ext4, XFS and Btrfs leave a lot of opportunity for performance improvement in workloads dominated by metadata and small files. In this paper we present a stacked file system, TABLEFS, which uses another local file system as an object store. TABLEFS organizes all metadata into a single sparse table backed on disk using a Log-Structured Merge (LSM) tree, LevelDB in our experiments. By stacking, TABLEFS asks only for efficient large file allocation and access from the underlying local file system. By using an L\n",
      "\n",
      "6. id: 5390b60d20f70186a0f11497   score: 0.9300781   abstract: Flash memory has become the most important storage media in the mobile multimedia products such as MP3 players, mobile phones, and digital cameras. Most mobile multimedia devices, however, use one of the conventional FAT file systems slightly modified for flash memory environments. Our analysis shows that theses file systems have some restriction in recording a live multimedia stream stably because they have irregular write response times. In this paper, we have considered the problems of the existing FAT file system and propose two new techniques to solve the problems. The first technique called sector reservation method reduces internal overhead effectively. And the other method called ACPA avoids the periodic cluster allocation of the conventional FAT file system and removes the frequent modifications on the file allocation table in the FAT file system. To evaluate our new techniques,\n",
      "\n",
      "7. id: 53909eef20f70186a0e3680b   score: 0.9240048   abstract: For five years, we collected annual snapshots of filesystem metadata from over 60,000 Windows PC file systems in a large corporation. In this paper, we use these snapshots to study temporal changes in file size, file age, file-type frequency, directory size, namespace structure, file-system population, storage capacity and consumption, and degree of file modification. We present a generative model that explains the namespace structure and the distribution of directory sizes. We find significant temporal trends relating to the popularity of certain file types, the origin of file content, the way the namespace is used, and the degree of variation among file systems, as well as more pedestrian changes in sizes and capacities. We give examples of consequent lessons for designers of file systems and related software.\n",
      "\n",
      "8. id: 53909f6920f70186a0e3adf7   score: 0.9094255   abstract: For five years, we collected annual snapshots of file-system metadata from over 60,000 Windows PC file systems in a large corporation. In this article, we use these snapshots to study temporal changes in file size, file age, file-type frequency, directory size, namespace structure, file-system population, storage capacity and consumption, and degree of file modification. We present a generative model that explains the namespace structure and the distribution of directory sizes. We find significant temporal trends relating to the popularity of certain file types, the origin of file content, the way the namespace is used, and the degree of variation among file systems, as well as more pedestrian changes in size and capacities. We give examples of consequent lessons for designers of file systems and related software.\n",
      "\n",
      "9. id: 53908f5b20f70186a0dd9a1b   score: 0.8978745   abstract: In this report, we describe the collection of file system traces from three different environments. By using the auditing system to collect traces on client machines, we are able to get detailed traces with minimal kernel changes. We then present results of traffic analysis on the traces, contrasting them with those from previous studies. Based on these results, we argue that file systems must optimize disk layout for good read performance.\n",
      "\n",
      "10. id: 5390ac5720f70186a0eb5a99   score: 0.89423656   abstract: Today’s file systems typically need multiple disk accesses for a single read operation of a file. In the worst case, when none of the needed data is already in the cache, the metadata for each component of the file path has to be read in. Once the metadata of the file has been obtained, an additional disk access is needed to read the actual file data. For a target scenario consisting almost exclusively of reading small files, which is typical in many Web 2.0 scenarios, this behavior severely impacts read performance. In this paper, we propose a new file system approach, which computes the expected location of a file using a hash function on the file path. Additionally, file metadata is stored together with the actual file data. Together, these characteristics allow a file to be read in with only a single disk access. The introduced approach is implemented extending the ext2 file system a\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1730909\n",
      "index                                        5534d8ba45cedae85c379568\n",
      "title               Dynamic reconfigurable puncturing for secure w...\n",
      "authors             Liang Tang, Jude Angelo Ambrose, Akash Kumar, ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references                                   558b5398612c41e6b9d494aa\n",
      "abstract            The ubiquity of wireless devices has created s...\n",
      "id                                                            1730909\n",
      "clustered_labels                                                    0\n",
      "Name: 1730909, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b2fc20f70186a0eef138   score: 0.9691495   abstract: Because of the wireless media, wireless communication is inherently more susceptible to attack. Because of the popularized use of wireless technology not only communication but business, commerce and all other transactions has also been shifted from wired to wireless paradigm. So it is important to secure wireless communication not only at higher layers but also at physical layer. No one can access neither the data nor the transmission signals to compromise the security by any means. This paper discuss one of the way through which security at the physical layer can be achieved by sending data plus noise so as to hide data behind the noise. By doing this we can fool the hacker by the noise rather than having access to the data.\n",
      "\n",
      "2. id: 5537da0b0cf23ee1cc767a26   score: 0.9606575   abstract: Security is challenging for wireless networks, in part because signals radiate outwards and can be intercepted easily and unobtrusively. Because of this, security cannot be provided simply by protecting the hardware, and must be supported by the communication protocol and implementation.\n",
      "\n",
      "3. id: 558b5398612c41e6b9d494aa   score: 0.9308365   abstract: Wireless communication is an indispensable tool in our daily life. Due to the open nature of wireless channels, wireless communication is more vulnerable to attacks than wired communication. Security is paramount in wireless communication to overcome these attacks. A reconfigurable convolutional encoder/decoder based physical layer security mechanism, named ReConv, is proposed in this paper. ReConv provides an extra level of security at the physical level by dynamically updating base band convolution parameters. ReConv can interleave secure packets along with normal unsecure packets to hide the secure information. An eavesdropper will see the packets with changed convolution parameters as packets containing errors and will drop them. However, the rightful receiver will be able to decode the secure packets without error. Since the secure packets are dropped at the eavesdropper's physical \n",
      "\n",
      "4. id: 5390a6d920f70186a0e87b4a   score: 0.915356   abstract: Wireless communication systems have found their applications in almost every field of life. Most popular of these applications being web browsing, cellular telephony, SMS, MMS and audio & videoconferencing, just to name a few. There are however a few inherent problems in the communication over a wireless channel. The first one is that it is inherently prone to transmission errors and the second one is the security of data transmitted over the channel. Both the problems are researched separately by researchers, giving separate solutions. Forward error correction mechanisms are under research for error correction at the receiver side and cryptography has been proposed as a mechanism for data security. However these mechanisms have been dealt separately so far. We hereby propose a novel scheme that combines the two mechanisms together i.e. in our scheme we use symmetric cryptography in the \n",
      "\n",
      "5. id: 539099ec20f70186a0e1c205   score: 0.8912444   abstract: Although conventional cryptographic security mechanisms are essential to the overall problem of securing wireless networks, these techniques do not directly leverage the unique properties of the wireless domain to address security threats. The properties of the wireless medium are a powerful source of domain-specific information that can complement and enhance traditional security mechanisms. In this paper, we propose to utilize the fact that the radio channel decorre-lates rapidly in space, time and frequency in order to to establish new forms of authentication and confidentiality that operate at the physical layer and can be used to facilitate cross-layer security paradigms. Specifically, for authentication services, we illustrate two channel probing techniques that can be used to verify the authenticity of a transmitter. Similarly, for confidentiality, we examine several strategies fo\n",
      "\n",
      "6. id: 5390ab8820f70186a0eb1c62   score: 0.8474635   abstract: Wireless communication is a fast-growing technology to provide the flexibility and mobility for the users. But it is less secure than wired networks, and then wireless security is becoming an important in product research and development area. In this paper, we provide a hardware-software co-design of robust security for WLAN. The design objective is to provide an efficient architecture that satisfies the constraints such as processing delay and data throughput required in standards. We have implemented the design supporting IEEE 802.11n. Finally, we show its results of implementation and evaluate performance.\n",
      "\n",
      "7. id: 5390adfd20f70186a0ec5508   score: 0.84479356   abstract: While conventional cryptographic security mechanisms are essential to the overall problem of securing wireless networks, they do not directly leverage the unique properties of the wireless domain to address security threats. The wireless medium is a powerful source of domain-specific information that can complement and enhance traditional security mechanisms. In this article we argue that new security paradigms which exploit physical layer properties of the wireless medium, such as the rapid spatial, spectral, and temporal decorrelation properties of the radio channel, can enhance confidentiality and authentication services. We outline some basic constructions for these services, and then provide a case study for how such strategies can be integrated into a broader security framework for a wireless network.\n",
      "\n",
      "8. id: 5390a2be20f70186a0e64497   score: 0.76646847   abstract: This talk explores the question of whether wireless communication can achieve security - confidentiality (unicast and broadcast), authentication, non-repudiation, anonymity - without assuming any shared secrets and using only low-cost computation primitives. The key idea is to exploit physical characteristics of the network medium to develop a basis of physical primitives that suffice for rethinking a security protocol suite.\n",
      "\n",
      "9. id: 539098b820f70186a0e0c1fc   score: 0.75346667   abstract: Motivated by the tradeoff between security and efficiency performance parameters that has been imposed on all modern wireless security protocols, we designed a novel security system that gained in both parameters. Our system is based on stream ciphers for their speed, but maintaining a much more solid and proven security. Such security strength stems from the novel deployment of permutation vectors and the data records in the regeneration of the secret key. Moreover, the involvement of the former results in an adaptive and efficient data integrity mechanism that relies on error propagations in the data stream. Simulation results show that our security protocol is much faster than peer mechanisms such as WEP and CCMP. Hence, we anticipate a great opportunity to deploy our system in environments with scarce bandwidth, which are the most vulnerable; specifically the wireless domain.\n",
      "\n",
      "10. id: 558af8aa612c41e6b9d3f199   score: 0.75346667   abstract: Physical layer security, which is an alternative to traditional cryptographic methods, has emerged as a promising candidate to protect wireless transmissions from an eavesdropper. An important measure of physical layer security is the secrecy outage: the event when the instantaneous secrecy capacity, which for the considered scenario is the difference between the capacity of the Alice (the transmitter) to Bob (the legitimate receiver) channel and the capacity of the Alice to Eve (the eavesdropper) channel, is below a target secrecy rate for which the system has been designed. In this paper, we argue that design under such an outage definition on a wireless channel is inadequate in some scenarios, as it treats very different error events with equal weight. In response, we propose that two conditions are met: the instantaneous capacity between the source and destination is above a target r\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1716196\n",
      "index                                        55323c6045cec66b6f9dbeea\n",
      "title               An adaptive gradient descent-based local searc...\n",
      "authors                                  Aliasghar Arab, Alireza Alfi\n",
      "year                                                           2015.0\n",
      "venue                  Information Sciences: an International Journal\n",
      "references                                   558fdc09612c29c89cd7b993\n",
      "abstract            Memetic Algorithm (MA) is a combination of Evo...\n",
      "id                                                            1716196\n",
      "clustered_labels                                                    1\n",
      "Name: 1716196, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539099a220f70186a0e184a7   score: 0.97826034   abstract: Memetic algorithms are evolutionary algorithms incorporating local search to increase exploitation. This hybridization has been fruitful in countless applications. However, theory on memetic algorithms is still in its infancy.Here, we introduce a simple memetic algorithm, the (1+1) Memetic Algorithm (1+1(MA)), working with a population size of 1 and no crossover. We compare it with the well-known (1+1) EA and randomized local search and show that these algorithms can outperform each other drastically.On problems like, e.g., long path problems it is essential to limit the duration of local search. We investigate the (1+1) MA with a fixed maximal local search duration and define a class of fitness functions where a small variation of the local search duration has a large impact on the performance of the (1+1) MA.All results are proved rigorously without assumptions.\n",
      "\n",
      "2. id: 5390bfa220f70186a0f54e75   score: 0.90582335   abstract: Memetic Algorithms (MAs) are computational intelligence structures combining multiple and various operators in order to address optimization problems. The combination and interaction amongst operators evolves and promotes the diffusion of the most successful units and generates an algorithmic behavior which can handle complex objective functions and hard fitness landscapes. Handbook of Memetic Algorithms organizes, in a structured way, all the the most important results in the field of MAs since their earliest definition until now. A broad review including various algorithmic solutions as well as successful applications is included in this book. Each class of optimization problems, such as constrained optimization, multi-objective optimization, continuous vs combinatorial problems, uncertainties, are analysed separately and, for each problem, memetic recipes for tackling the difficulties\n",
      "\n",
      "3. id: 5390b2d720f70186a0eec73d   score: 0.90582335   abstract: Memetic Algorithms (MAs) are computational intelligence structures combining multiple and various operators in order to address optimization problems. The combination and interaction amongst operators evolves and promotes the diffusion of the most successful units and generates an algorithmic behavior which can handle complex objective functions and hard fitness landscapes. Handbook of Memetic Algorithms organizes, in a structured way, all the the most important results in the field of MAs since their earliest definition until now. A broad review including various algorithmic solutions as well as successful applications is included in this book. Each class of optimization problems, such as constrained optimization, multi-objective optimization, continuous vs combinatorial problems, uncertainties, are analysed separately and, for each problem, memetic recipes for tackling the difficulties\n",
      "\n",
      "4. id: 5390b04120f70186a0ed6afa   score: 0.86223567   abstract: The performance of a memetic algorithm (MA) largely depends on the synergy between its global and local search counterparts. The amount of global exploration and local exploitation to be carried out, for optimal performance, varies with problem type. Therefore, an algorithm should intelligently allocate its computational efforts between genetic search and local search. In this work we propose an adaptive local search method that adjusts the effort for local tuning of individuals, taking feedback from the search. We implemented an MA hybridizing this adaptive local search method with differential evolution algorithm. Experimenting with a standard benchmark suite it was found that the proposed MA can utilize its global and local search components adaptively. The proposed algorithm also exhibited very competitive performance with other existing algorithms.\n",
      "\n",
      "5. id: 53909f8220f70186a0e3e012   score: 0.8601343   abstract: A differential evolutionary (DE) algorithm modified by initialization and Local searching is proposed. In the new algorithm, the stochastic properties of chaotic system is used to spread the individuals in search spaces as much as possible, the simplex search method is employed to speed up the local exploiting and the DE operators help the algorithm to jump to a better point. Numerical experiments on benchmark examples including 13 high dimensional functions demonstrate that the new method achieved an improved success rate and final solution with less computational effort.\n",
      "\n",
      "6. id: 5390a79f20f70186a0e90627   score: 0.8308688   abstract: In order to improve differential evolution (DE) algorithm’s global search ability during later periods, annealing idea was merged into the strategy of selecting DE’s amplification factor. Based on the good micro search ability of chaos, the population started adaptive chaotic mutation when it failed into premature. Through comparing this proposed hybrid strategy with original and another improved strategy of DE respectively based sixteen benchmark functions, simulation results show that the proposed hybrid algorithm has not only better ability of departing from local extremes in later periods but also has improved performance of DE in other important aspects simultaneously.\n",
      "\n",
      "7. id: 5390bf1320f70186a0f5172f   score: 0.82741016   abstract: Memetic algorithms have been devised to rectify the absence of a local search mechanism in evolutionary algorithms. This paper proposes a differential memetic algorithm (DMA). To this end, first we propose a differential bidirectional random search as a local search algorithm. Then, a randomized blending crossover (RBleX) is proposed which aimed to scatter the new born offspring more diversely in the whole search space. We devise our proposed DMA, by using the RBleX crossover in the GA, and including the DBRS local search algorithm. A comparison of the performance of the DMA and those of seven other evolutionary/memetic or hybrid algorithms reported in two different papers on numerous bechmark functions demonstrates better performance of proposed DMA algorithm in most of the cases.\n",
      "\n",
      "8. id: 5390ba3820f70186a0f34f8c   score: 0.8236123   abstract: This paper provides a novel approach to design an adaptive memetic algorithm by utilizing the composite benefits of Differential Evolution for global search and Q-learning for local refinement. The performance of the proposed adaptive memetic algorithm has been studied on a real-time multi-robot path-planning problem. Experimental results obtained for both simulation and real frameworks indicate that the proposed algorithm based path-planning scheme outperforms real coded Genetic Algorithm, Particle Swarm Optimization and Differential Evolution, particularly its currently best version with respect to two standard metrics defined in the literature.\n",
      "\n",
      "9. id: 5390b4c320f70186a0efdd18   score: 0.8042522   abstract: Memetic Algorithm is a metaheuristic search method. It is based on both the natural evolution and individual learning by transmitting unit of information among them. In the present paper, Genetic Algorithm due to its good exploration capability is used for exploration and Particle Swarm Optimization (PSO) does local search. The memetic process is realized using the fitness information from the individual having best fitness value and searching around it locally with PSO. The proposed algorithm (PSO based memetic algorithm -pMA) is tested on 13 standard benchmark functions having unimodal and multimodal property. When results are compared, the proposed memetic algorithm shows better performance than GA and PSO. The performance of the discussed memetic algorithm is better in terms of convergence speed and quality of solutions.\n",
      "\n",
      "10. id: 5390b5c620f70186a0f078ec   score: 0.7937462   abstract: Adaptation of parameters and operators represents one of the recent most important and promising areas of research in evolutionary computations; it is a form of designing self-configuring algorithms that acclimatize to suit the problem in hand. Here, our interests are on a recent breed of hybrid evolutionary algorithms typically known as adaptive memetic algorithms (MAs). One unique feature of adaptive MAs is the choice of local search methods or memes and recent studies have shown that this choice significantly affects the performances of problem searches. In this paper, we present a classification of memes adaptation in adaptive MAs on the basis of the mechanism used and the level of historical knowledge on the memes employed. Then the asymptotic convergence properties of the adaptive MAs considered are analyzed according to the classification. Subsequently, empirical studies on repres\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1720793\n",
      "index                                        55323cae45cec66b6f9dca57\n",
      "title               Systematic errors of mapping functions which a...\n",
      "authors             Florian Zus, Galina Dick, Jan Dousa, Jens Wickert\n",
      "year                                                           2015.0\n",
      "venue                                                   GPS Solutions\n",
      "references                                   558b394f612c41e6b9d471c0\n",
      "abstract            Precise global navigation satellite system (GN...\n",
      "id                                                            1720793\n",
      "clustered_labels                                                    2\n",
      "Name: 1720793, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558b394f612c41e6b9d471c0   score: 0.97671366   abstract: The troposphere delay is an important source of error for precise GNSS positioning due to its high correlation with the station height parameter. It has been demonstrated that errors in mapping functions can cause sub-annual biases as well as affect the repeatability of GNSS solutions, which is a particular concern for geophysical studies. Three-dimensional ray-tracing through numerical weather models (NWM) is an excellent approach for capturing the directional and daily variation of the tropospheric delay. Due to computational complexity, its use for positioning purposes is limited, but it is an excellent tool for evaluating current state-of-the-art mapping functions used for geodetic positioning. Many mapping functions have been recommended in the past such as the Niell Mapping Function (NMF), Vienna Mapping Function 1 (VMF1), and the Global Mapping Function (GMF), which have been adop\n",
      "\n",
      "2. id: 5390adfd20f70186a0ec5c9c   score: 0.8299059   abstract: The Global Positioning System (GPS) observations from the EUREF Permanent Network (EPN) are routinely analyzed by the EPN analysis centers using a tropospheric delay modeling based on standard pressure values, the Niell Mapping Functions (NMF), a cutoff angle of 3° and down-weighting of low elevation observations. We investigate the impact on EPN station heights and Zenith Total Delay (ZTD) estimates when changing to improved models recommended in the updated 2003 International Earth Rotation and Reference Systems Service (IERS) Conventions, which are the Vienna Mapping Functions 1 (VMF1) and zenith hydrostatic delays derived from numerical weather models, or the empirical Global Mapping Functions (GMF) and the empirical Global Pressure and Temperature (GPT) model. A 1-year Global Positioning System (GPS) data set of 50 regionally distributed EPN/IGS (International GNSS Service) stations\n",
      "\n",
      "3. id: 5390b9d520f70186a0f3042f   score: 0.563438   abstract: For GPS single frequency users, the ionospheric contribution to the error budget is estimated by the well-known Klobuchar algorithm. For Galileo, it will be mitigated by a global algorithm based on the NeQuick model. This algorithm relies on the adaptation of the model to slant Total Electron Content (sTEC) measurements. Although the performance specifications of these algorithms are expressed in terms of delay and TEC, the users might be more interested in their impact on positioning. Therefore, we assessed the ability of the algorithms to improve the positioning accuracy using globally distributed permanent stations for the year 2002 marked by a high level of solar activity. We present uncorrected and corrected performances, interpret these and identify potential causes for Galileo correction discrepancies. We show vertical errors dropping by 56---64 % due to the analyzed ionospheric c\n",
      "\n",
      "4. id: 5390af8920f70186a0ed016f   score: 0.47555965   abstract: Ionospheric delays can be efficiently eliminated from single-frequency data using a combination of carrier phases and code ranges. Unfortunately, GPS and GLONASS ranges are relatively noisy which can limit the use of the positioning method. Nevertheless, position standard deviations are in the range of 6---8 cm (horizontal) and 7---9 cm (3d) obtained from diurnal data batches from selected IGS reference stations can be further reduced to 2---3 cm (3d) for weekly smoothed averages. GPS data sets collected in Ghana (Africa) reveal a typical level of 10 cm of deviation that must be anticipated under average conditions. Looking at the future of GNSS, the European Galileo system will, in contrast to GPS, provide the broadband signal E5 that is by far less affected by multipath thus providing rather precise range measurements. Simulated processing runs featuring both high ionospheric and tropo\n",
      "\n",
      "5. id: 5390b36120f70186a0ef1382   score: 0.47014356   abstract: Atmospheric delays are contributors to the GNSS error budget in precise GNSS positioning that can reduce positioning accuracy considerably if not compensated appropriately. Both ionospheric and tropospheric delay corrections can be determined with help of reference stations in active GNSS networks. One approach to interpolate these error terms to the user's location that is employed in Germany's SAPOS network is the determination of area correction parameters (ACP, German: \"Flächenkorrekturparameter--FKP\"). A 2D interpolation scheme using data from at least 3 reference stations surrounding the rover is employed. A modification of this method was developed which only makes use of as few as 2 reference stations and provides 1D linear correction parameters along a \"corridor\" in which the user's rover is moving. We present the results of a feasibility study portraying results from use of cor\n",
      "\n",
      "6. id: 55323c1c45cec66b6f9db355   score: 0.45904583   abstract: A new method for positioning a signal receiver of the global navigation satellite system is proposed within the ray theory and under the assumption that the atmosphere is spherically stratified and the dependence of the speed of the radio signal sent by each navigation satellite on the distance from the center of the Earth is known. A new basic system of equations is presented. For the minimum number of four navigation satellites, it consists of eight equations in eight unknowns, four of which are accessory ray parameters. The correctness of the new basic system is proved, and an algorithm for its solution is described. The purpose of the new method is a significant improvement in the accuracy of the absolute positioning of the signal receiver of the global navigation satellite system through the proper account of irregularities of the ionosphere and troposphere.\n",
      "\n",
      "7. id: 5390af8920f70186a0ed0175   score: 0.42536652   abstract: Precise GPS positioning requires the processing of carrier-phase observations and fixing integer ambiguities. With increasing distance between receivers, ambiguity fixing becomes more difficult because ionospheric and tropospheric effects do not cancel sufficiently in double differencing. A popular procedure in static positioning is to increase the length of the observing session and/or to apply atmospheric (ionospheric) models and corrections. We investigate the methodology for GPS rapid static positioning that requires just a few minutes of dual-frequency GPS observations for medium-length baselines. Ionospheric corrections are not required, but the ionospheric delays are treated as pseudo-observations having a priori values and respective weights. The tropospheric delays are reduced by using well-established troposphere models, and satellite orbital and clock errors are eliminated by \n",
      "\n",
      "8. id: 5390a06e20f70186a0e4c68a   score: 0.41312194   abstract: Global navigation satellite system (GNSS) is designed to serve both civilian and military applications. However, the GNSS performance suffers from several errors, such as ionosphere delay, troposphere delay, ephemeris error, and receiver noise and multipath. Among these errors, the multipath is one of the most unpredictable error sources in high-accuracy navigation. This paper applies a modified adaptive filter to reduce code and carrier multipath errors in GPS. The filter employs a tap-delay line with an Adaline network to estimate the direction and the delayed-signal parameters. Then, the multipath effect is mitigated by subtracting the estimated multipath effects from the processed correlation function. The hardware complexity of the method is also compared with other existing methods. Simulation results show that the proposed method using field data has a significant reduction in mul\n",
      "\n",
      "9. id: 5390bfa220f70186a0f53606   score: 0.36999774   abstract: An increasing number of GNSS reference stations are installed around the world to provide real-time precise positioning services. In most of the current services, a full network solution is required for the precise determination of biases. Such a network solution is time consuming and difficult to achieve for very large regions such as Europe or China. Therefore, we developed a multi-layer processing scheme for precise point positioning (PPP) regional augmentation to avoid processing large networks. Furthermore, we use L1 and L2 raw observations and estimate atmospheric delays, which were properly constrained to the atmospheric corrections derived from the reference stations. Therefore, inaccurate representation of atmospheric delays due to temporal and/or spatial atmospheric fluctuations in the processing can be compensated. The proposed scheme of PPP regional augmentation was implement\n",
      "\n",
      "10. id: 5390b19020f70186a0edebd6   score: 0.33906123   abstract: In precise point positioning (PPP), the ionospheric delay is corrected in a first-order approximation from GPS dual-frequency observations, which should eliminate almost completely the ionosphere as a source of error. However, sudden plasma density variations can adversely affect the GPS signal, degrading accuracy and reliability of positioning techniques. The occurrence of plasma density irregularities is frequent at equatorial latitudes and is reflected in large total electron content (TEC) variations. We study the relation between large changes in the rate of TEC (ROT) and positioning errors in single-epoch PPP. At equatorial latitudes and during post-sunset hours, the estimated altitudes contain errors of several meters for a single-epoch position determination, and latitude and longitude estimates are also degraded. These results have been corroborated by the online CSRS-PPP (NRCan)\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1703233\n",
      "index                                        55914e760cf232eb904fbb0a\n",
      "title                  Probabilistic diagnosability of hybrid systems\n",
      "authors              Yi Deng, A. Agung Julius, Alessandro D'Innocenzo\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th International Conferen...\n",
      "references          539087cb20f70186a0d593fc;53908b6c20f70186a0dbd...\n",
      "abstract            The model-based fault diagnosability analysis ...\n",
      "id                                                            1703233\n",
      "clustered_labels                                                    2\n",
      "Name: 1703233, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac1720f70186a0eb24bb   score: 0.9570375   abstract: Fault diagnosis is crucial for ensuring the safe operation of complex engineering systems. Many present-day systems combine physical and computational processes, and are best modeled as hybrid systems, where the dynamic behavior combines continuous evolution interspersed with discrete configuration changes. Due to the complexity of such modern engineering systems, formal methods are required for reliable and correct design, analysis, and implementation of hybrid system diagnosers. This dissertation presents a systematic, model-based approach to event-based diagnosis of hybrid systems based on qualitative abstractions of deviations from nominal behavior. The primary contributions of this work center on (i) incorporating relative measurement orderings into fault isolation for continuous and hybrid systems, which describe predicted temporal orderings of measurement deviations, (ii) providin\n",
      "\n",
      "2. id: 53908ae020f70186a0dade27   score: 0.9503189   abstract: Efficient algorithms exist for fault detection and isolation of physical systems based on functional redundancy. In a qualitative approach, this redundancy can be captured by a temporal causal graph (TCG), a directed graph that may include temporal information. However, in a detailed continuous model, time constants may be present that are beyond the bandwidth of the data acquisition system, which leads to incorrect fault isolation because of a difference in observed and modeled behavior. To solve this, the modeled time constants can be taken to be infinitely small, which results in a model with mixed continuous/discrete, hybrid behavior that is difficult to analyze because the causality of the directed graph may change. In this paper, to avoid the combinatorial explosion when using a bank of TCGs in parallel, causal paths are parametrized by the state of local switches. The result is a \n",
      "\n",
      "3. id: 53908ae020f70186a0dadc4b   score: 0.9361684   abstract: In this paper, we present a rigorous design of a Fault Diagnosis and Isolation algorithm. The system is modelled as a hybrid system with a network of parallel components. The requirement is specified in Duration Calculus, a dense time temporal logic. We use traditional program logic, suitably extended, to verify the discrete component and subsequently derive a number of properties of the system. Finally, the requirement is shown to be satisfied by proving that it can be deduced from the system properties.\n",
      "\n",
      "4. id: 5390a80f20f70186a0e967db   score: 0.91891783   abstract: We present a model based approach to diagnosability analysis for interacting finite state systems where fault isolation is deferred until the system comes to a standstill. Local abstractions of the system model are used to alleviate the state space explosion. Pairs of closely coupled automata are merged and replaced by a single automaton with an equivalently behavior as seen from the rest of the system; interaction between the merged automata is internalized and the new equivalent automaton is subsequently abstracted from internal behavior irrelevant to fault isolation. In moderately concurrent systems these steps can often be iterated until the system consists of a single automaton providing a compact encoding of all possible fault scenarios of the original model. We illustrate how the resulting abstraction can be used as a basis for post mortem diagnosability analysis.\n",
      "\n",
      "5. id: 5390aca920f70186a0eb985b   score: 0.9147487   abstract: This paper shows how to efficiently diagnose systems by making use of observations. In particular, we present two theorems concerning the effect of observations on the complexity of Model-Based Diagnosis. The first theorem shows how the presence of certain observations allows us to decompose a diagnostic reasoning task into independent reasoning tasks on sub-systems. The second theorem shows how the absence of certain observations allows us to ignore parts of a system during diagnostic reasoning. Another main contribution of this paper is an application of these theorems to diagnosing discrete-event systems. In particular, we identify observability and modularity characteristics of discrete-event systems that make them amenable to the presented theorems and, hence, to any diagnostic approach that employs these theorems effectively. This also explains why a particular approach that we hav\n",
      "\n",
      "6. id: 5390962020f70186a0df3c87   score: 0.85427785   abstract: The diagnosis of “intermittent” faults in dynamic systems modeled as discrete event systems is considered. In many systems, faulty behavior often occurs intermittently, with fault events followed by corresponding “reset” events for these faults, followed by new occurrences of fault events, and so forth. Since these events are usually unobservable, it is necessary to develop diagnostic methodologies for intermittent faults. Prior methodologies for detection and isolation of permanent faults are no longer adequate in the context of intermittent faults, since they do not account explicitly for the dynamic behavior of these faults. This paper addresses this issue by: (i) proposing a modeling methodology for discrete event systems with intermittent faults; (ii) introducing new notions of diagnosability associated with fault and reset events; and (iii) developing necessary and sufficient condi\n",
      "\n",
      "7. id: 5390a06e20f70186a0e4d356   score: 0.852199   abstract: This paper addresses the problem of fault detection and isolation for a particular class of discrete event dynamical systems called hierarchical finite state machines (HFSMs). A new version of the property of diagnosability for discrete event systems tailored to HFSMs is introduced. This notion, called L1-diagnosability, captures the possibility of detecting an unobservable fault event using only high level observations of the behavior of an HFSM. Algorithms for testing L1-diagnosability are presented. In addition, new methodologies are presented for studying the diagnosability properties of HFSMs that are not L1-diagnosable. These methodologies avoid the complete expansion of an HFSM into its corresponding flat automaton by focusing the expansion on problematic indeterminate cycles only in the associated extended diagnoser.\n",
      "\n",
      "8. id: 5390b5c620f70186a0f078d1   score: 0.8480936   abstract: Many networked embedded sensing and control systems can be modeled as hybrid systems with interacting continuous and discrete dynamics. These systems present significant challenges for monitoring and diagnosis. Many existing model-based approaches focus on diagnostic reasoning assuming appropriate fault signatures have been generated. However, an important missing piece is the integration of model-based techniques with the acquisition and processing of sensor signals and the modeling of faults to support diagnostic reasoning. This paper addresses key modeling and computational problems at the interface between model-based diagnosis techniques and signature analysis to enable the efficient detection and isolation of incipient and abrupt faults in hybrid systems. A hybrid automata model that parameterizes abrupt and incipient faults is introduced. Based on this model, an approach for diagn\n",
      "\n",
      "9. id: 5390a6b120f70186a0e84e9f   score: 0.8478418   abstract: Diagnosability of systems is an essential property that determines how accurate any diagnostic reasoning can be on a system given any sequence of observations. Generally, in the literature of dynamic event-driven systems, diagnosability analysis is performed by algorithms that consider a system as a whole and their response is either a positive answer or a counter example. In this paper, we present an original framework for diagnosability checking. The diagnosability problem is solved in a distributed way in order to take into account the distributed nature of realistic problems. As opposed to all other approaches, our algorithm also provides an exhaustive and synthetic view of the reasons why the system is not diagnosable. Finally, the presented algorithm is scalable in practice: it provides an approximate and useful solution if the computational resources are not sufficient.\n",
      "\n",
      "10. id: 5390956e20f70186a0deda88   score: 0.83615357   abstract: In this paper, we introduce a new model for diagnosable systems called (t,k)-diagnosable system which guarantees that at least k faulty units (processors) in a system are detected provided that the number of faulty units does not exceed t. This system includes classical one-step diagnosable systems and sequentially diagnosable systems. We prove a necessary and sufficient condition for (t,k)-diagnosable system, and discuss a lower bound for diagnosability. Finally, we deal with a relation between (t,k)-diagnosability and diagnosability of classical basic models.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1660532\n",
      "index                                        55915c800cf232eb904fbea8\n",
      "title               Personalized Mobile App Recommendation: Reconc...\n",
      "authors             Bin Liu, Deguang Kong, Lei Cen, Neil Zhenqiang...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Eighth ACM International Co...\n",
      "references          558ae302612c41e6b9d3c66d;5390985d20f70186a0e07...\n",
      "abstract            Recent years have witnessed a rapid adoption o...\n",
      "id                                                            1660532\n",
      "clustered_labels                                                    2\n",
      "Name: 1660532, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 559144040cf232eb904fb857   score: 0.9512329   abstract: The aim of this research was to understand what affects people's privacy preferences in smartphone apps. We ran a four-week study in the wild with 34 participants. Participants were asked to answer questions, which were used to gather their personal context and to measure their privacy preferences by varying app name and purpose of data collection. Our results show that participants shared the most when no information about data access or purpose was given, and shared the least when both of these details were specified. When just one of either purpose or the requesting app was shown, participants shared less when just the purpose was specified than when just the app name was given. We found that the purpose for data access was the predominant factor affecting users' choices. In our study the purpose condition vary from being not specified, to vague to be very specific. Participants were \n",
      "\n",
      "2. id: 558af565612c41e6b9d3ea54   score: 0.94218844   abstract: With the dramatic growing of mobile application markets, users can find apps with any function they desire in these markets. However, the huge amounts of apps make it quite a challenge for users to discover good applications efficiently. Previous studies recommend applications based on the download history, user ratings or app usage records. Most of these studies fail to capture users' personal interests in mobile applications precisely. In this paper, we leverage apps as features for describing user's personal interests and propose a novel approach to do personalized recommendation. We introduce a Small-Crowd model to distinguish apps at reflecting users' personal interests, and design a weighting method to rank the installed apps for users by combining the global download information with fine-grained app usage records. The extensive experiments validate the effectiveness of our approa\n",
      "\n",
      "3. id: 5390b9d520f70186a0f31ec7   score: 0.89533985   abstract: Due to the huge and still rapidly growing number of mobile applications (apps), it becomes necessary to provide users an app recommendation service. Different from conventional item recommendation where the user interest is the primary factor, app recommendation also needs to consider factors that invoke a user to replace an old app (if she already has one) with a new app. In this work we propose an Actual- Tempting model that captures such factors in the decision process of mobile app adoption. The model assumes that each owned app has an actual satisfactory value and a new app under consideration has a tempting value. The former stands for the real satisfactory value the owned app brings to the user while the latter represents the estimated value the new app may seemingly have. We argue that the process of app adoption therefore is a contest between the owned apps' actual values and th\n",
      "\n",
      "4. id: 5390a54620f70186a0e77fa7   score: 0.89533985   abstract: A number of mobile applications have emerged that allow users to locate one another. However, people have expressed concerns about the privacy implications associated with this class of software, suggesting that broad adoption may only happen to the extent that these concerns are adequately addressed. In this article, we report on our work on PeopleFinder, an application that enables cell phone and laptop users to selectively share their locations with others (e.g. friends, family, and colleagues). The objective of our work has been to better understand people's attitudes and behaviors towards privacy as they interact with such an application, and to explore technologies that empower users to more effectively and efficiently specify their privacy preferences (or \"policies\"). These technologies include user interfaces for specifying rules and auditing disclosures, as well as machine learn\n",
      "\n",
      "5. id: 558acae6612c41e6b9d3a449   score: 0.8818186   abstract: We present a prototype recommendation system for mobile applications that exploits a rather general description of the user's context. One of the main features of the proposed solution is the proactive and completely automated procedure of querying the apps marketplace, able to retrieve a set of apps and to rank them on the basis of the current situation of the user. We also present a first experimental evaluation that confirms the effectiveness of the general design and implementation choices and sheds some light on the peculiar features and critical issues of recommendation systems for mobile applications.\n",
      "\n",
      "6. id: 5390bae620f70186a0f3c412   score: 0.8344069   abstract: As a tremendous number of mobile applications (apps) are readily available, users have difficulty in identifying apps that are relevant to their interests. Recommender systems that depend on previous user ratings (i.e., collaborative filtering, or CF) can address this problem for apps that have sufficient ratings from past users. But for apps that are newly released, CF does not have any user ratings to base recommendations on, which leads to the cold-start problem. In this paper, we describe a method that accounts for nascent information culled from Twitter to provide relevant recommendation in such cold-start situations. We use Twitter handles to access an app's Twitter account and extract the IDs of their Twitter-followers. We create pseudo-documents that contain the IDs of Twitter users interested in an app and then apply latent Dirichlet allocation to generate latent groups. At test\n",
      "\n",
      "7. id: 5390bb7b20f70186a0f41217   score: 0.82544905   abstract: In this paper, we propose a personalized recommendation system for mobile application software (app) to mobile user using semantic relations of apps consumed by users. To do that, we define semantic relations between apps consumed by a specific member and his/her social members using Ontology. Based on the relations, we identify the most similar social members from the reasoning process. The reasoning is explored from measuring the common attributes between apps consumed by the target member and his/her social members. The more attributes shared by them, the more similar is their preference for consuming apps. We also develop a prototype of our system using OWL (Ontology Web Language) by defining ontology-based semantic relations among 50 mobile apps. Using the prototype, we showed the feasibility of our algorithm that our recommendation algorithm can be practical in the real field and u\n",
      "\n",
      "8. id: 5390bed320f70186a0f4ef46   score: 0.81449586   abstract: As the smartphone is popular recently, mobile applications have been more and more attractive to users. With the exponential growth of applications' number, many works have been done to recommender the interesting applications for users. To check the effectiveness of these methods, the researchers are all tend to choose the traditional accuracy metrics to evaluate their system. However, considering the particularity of the mobile application recommendation, the accuracy evaluation metrics are not enough. Aiming at this problem, the goal of this paper is towards propose a new metric to evaluate the results of the mobile recommender systems and present an associated recommendation methodology. Base on the real usage pattern of mobile applications, we suggest a new evaluation metric called Cooperation. That is, when users use one application, they would enjoy using another one, we say those\n",
      "\n",
      "9. id: 5390b86b20f70186a0f2a75c   score: 0.80240077   abstract: The explosive growth of the mobile application (app) market has made it difficult for users to find the most interesting and relevant apps from the hundreds of thousands that exist today. Context is key in the mobile space and so too are proactive services that ease user input and facilitate effective interaction. We believe that to enable truly novel mobile app recommendation and discovery, we need to support real context-aware recommendation that utilizes the diverse range of implicit mobile data available in a fast and scalable manner. In this paper we introduce the Djinn model, a novel context-aware collaborative filtering algorithm for implicit feedback data that is based on tensor factorization. We evaluate our approach using a dataset from an Android mobile app recommendation service called appazaar. Our results show that our approach compares favorably with state-of-the-art colla\n",
      "\n",
      "10. id: 558ae302612c41e6b9d3c66d   score: 0.79117644   abstract: With the rapid prevalence of smart mobile devices, the number of mobile Apps available has exploded over the past few years. To facilitate the choice of mobile Apps, existing mobile App recommender systems typically recommend popular mobile Apps to mobile users. However, mobile Apps are highly varied and often poorly understood, particularly for their activities and functions related to privacy and security. Therefore, more and more mobile users are reluctant to adopt mobile Apps due to the risk of privacy invasion and other security concerns. To fill this crucial void, in this paper, we propose to develop a mobile App recommender system with privacy and security awareness. The design goal is to equip the recommender system with the functionality which allows to automatically detect and evaluate the security risk of mobile Apps. Then, the recommender system can provide App recommendation\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1698590\n",
      "index                                        558b49a8612c41e6b9d4846e\n",
      "title               Design and analysis of an innovative light tra...\n",
      "authors             Masihollah Mahmoudpour, Abolghassem Zabihollah...\n",
      "year                                                           2015.0\n",
      "venue                                     Microelectronic Engineering\n",
      "references          5390a77d20f70186a0e8e09a;5390a0b720f70186a0e4f774\n",
      "abstract            Graphical abstractDisplay Omitted We focus on ...\n",
      "id                                                            1698590\n",
      "clustered_labels                                                    0\n",
      "Name: 1698590, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539095ba20f70186a0df0c70   score: 0.97462875   abstract: Lead zirconate titanate (PbZrxTi1-xO3; PZT) (x=0.52) films using photosensitive PZT solution were formed on platinum(111)-coated Si substrate by sol-gel method. A relative thick self-patternable PZT film using photosensitive stock solution with 15% of excess Pb and UV (365 nm) irradiation has reached an advanced state in the application area of micro-detection system. Well-developed physical and electrical properties were observed after final anneal treatment due to the adaptation of optimized intermediate anneal treatment to remove organic groups in the film without the formation of pyrochlore phase. The 510 nm thick PZT film with smooth surface showed remnant polarization (Pr) of 25 µC/cm2, coercive field (Ec) of 45 kV/cm, and piezoelectric coefficient of d33 = 183 pC/N. The film showed a little leaky behavior when comparing with conventional PZT films prepared by sol-gel procedure and\n",
      "\n",
      "2. id: 53908b9320f70186a0dc03ed   score: 0.967163   abstract: Lead zirconate titanate Pb(Zr0.53Ti0.47)O3 (PZT) ferroelectric thin films of various thicknesses were grown on Pt(111)/Ti/SiO2/Si(100) substrates by a simple sol-gel process, without reflux or high temperature distillation to remove water. The thin films were annealed at 550-600 °C in an oxygen atmosphere by rapid thermal annealing (RTA), and highly (111)-oriented PZT thin films have been obtained. The microstructure and surface morphologies of the thin films have studied by X-ray diffraction (XRD) and atomic force microscopy (AFM). AFM images shown that the highly (111) oriented PZT thin films on Pt/Ti/SiO2/Si substrates with grain sizes of 0.2-0.3 and 2-3 µm, and root mean square roughnesses of 0.92 and 34 nm, for PZT films with thicknesses of 0.3 and 0.56 µm, respectively. The remanent polarization (Pr) and coercive electric field (Ec) values were 32.2 µC/cm2 and 79.9 kV/cm, 27.7 µC/c\n",
      "\n",
      "3. id: 5390b3da20f70186a0ef5bfe   score: 0.96182173   abstract: For developing freestanding piezoelectric microcantilevers with low resonant frequency, some critical mechanical considerations, especially cantilever bending, were given in this study. Two strategies, using piezoelectric thick films and adding a stress compensation layer, were calculationally analyzed for mitigating the cantilever bending, and then was applied for the fabrication of PZT freestanding microcantilevers. (100) oriented PZT thick films with the thickness of 6.93 μm were grown on the Pt/SiO2/Si substrate by chemical solution deposition (CSD), and the SiO2 layer with the thickness of 1.0 μm was kept under the PZT layer as a stress compensation layer of the freestanding microcantilevers. The freestanding microcantilevers fabricated with the micromachining process possessed the resonant frequency of 466.1 Hz, and demonstrated no obvious cantilever bending.\n",
      "\n",
      "4. id: 5390b29820f70186a0eea478   score: 0.9614615   abstract: For developing freestanding piezoelectric microcantilevers with low resonant frequency, some critical mechanical considerations, especially cantilever bending, were given in this study. Two strategies, using piezoelectric thick films and adding a stress compensation layer, were calculationally analyzed for mitigating the cantilever bending, and then was applied for the fabrication of PZT freestanding microcantilevers. (100) oriented PZT thick films with the thickness of 6.93 μm were grown on the Pt/SiO2/Si substrate by chemical solution deposition (CSD), and the SiO2 layer with the thickness of 1.0 μm was kept under the PZT layer as a stress compensation layer of the freestanding microcantilevers. The freestanding microcantilevers fabricated with the micromachining process possessed the resonant frequency of 466.1 Hz, and demonstrated no obvious cantilever bending.\n",
      "\n",
      "5. id: 5390a5b020f70186a0e7cdc2   score: 0.9574373   abstract: SiN-based micro cantilever actuators with lengths of the order of 1mm and NdFeB/Ta thin films for actuation were designed, fabricated and characterized. Multilayered thin films of NdFeB and Ta, with total thicknesses of 13µm and 3µm, were deposited on 3µm-thick SiN cantilevers by magnetron sputtering. The remnant flux density and coercive force of the thin film were as high as those obtained for bulk NdFeB magnets. The SiN based magnetic actuator showed good strength even when undergoing considerable bending. Large bidirectional out-of-plane displacements (up to 700µm) were generated using an air cored coil placed beneath the cantilevers. Static and dynamic evaluations were carried out using a CCD camera and an optical fiber displacement sensor, respectively. The experimentally measured displacements were in agreement with simulated values.\n",
      "\n",
      "6. id: 5539291b0cf26c551af543c4   score: 0.95606345   abstract: A new lead-zirconium-titan ate (PZT) actuated scanning light source design is proposed. An Aerosol deposition process will be used to construct a high quality 5 μm thick film of PZT. The PZT actuator will induce non-linear vibrations to scan an area with a cantilever waveguide. The fabrication procedure for the design is presented. Fabrication results are discussed. A finite element analysis of the actuator pad is also presented. Fabrication steps needed for a comparison of the amplitude of actuation, resolution, and frequency of scan with previously developed Si based devices are also discussed.\n",
      "\n",
      "7. id: 5390962020f70186a0df5b55   score: 0.95531917   abstract: The electrical properties of lanthanum doped lead zirconate titanate (PLZT) thin films prepared by photochemical metal-organic deposition (PMOD) using photosensitive starting precursors have been characterized. PLZT films with various La concentration were prepared by PMOD on Si(1 0 0) for observing the image of self-patterned PLZT film or on Pt(1 1 1)/Ti/SiO2/Si(1 0 0) for ferroelectric properties measurement. Even though PLZT film with 0 mol% of La, strictly PZT, showed an asymmetric behavior in polarization-voltage (P-E) relation, PLZT film by doping La showed symmetric behavior in P-E relation. The amelioration of electric and ferroelectric properties with increased substitution of La in PLZT film was observed, especially with 3 mol% La doped PLZT film, the most characteristic P-E hysteresis loop in the point of imprint property was obtained with comparatively large remnant polarizat\n",
      "\n",
      "8. id: 5390a0b720f70186a0e4f774   score: 0.9526623   abstract: This paper reports the formation of a new-layered film structure and the highly improved photovoltaic output of the lead lanthanum zirconate titanate (PLZT) employed. The new structural design is described using an upper top transparent indium tin oxide (ITO) electrode. The PLZT film structure exhibited V and μA output. The photovoltaic current of the PLZT film per unit width was more than 102 times larger than that of bulk PLZT, while the photovoltaic voltage per unit thickness in the layered film structure was almost the same as that in bulk ceramics and single crystals. These differences are due to the characteristics of the film structure and configuration of the electrode. The PLZT film also has the advantage of easily controllable parameters: film thickness, illuminated area, and illumination intensity. A simple model is used for the phenomenological explanation of the improved pho\n",
      "\n",
      "9. id: 53908e0020f70186a0dd5ac2   score: 0.9520419   abstract: The effect of ceramic composition on the electromechanical displacement degradation of RAINBOW (Reduced and Internally Biased Oxide Wafer) actuators was investigated. RAINBOWs were fabricated from commercially available PZT-5H and PZT-5A piezoelectric disks as well as from tape cast PLZT piezoelectric 7/65/35 and electrostrictive 9/65/35 compositions. Displacement properties were measured at low electric fields (10 to 13 kV/cm) under loads of 0 to 500 g, and displacement degradation as a function of time was observed over 10 /super7 cycles. The PZT-5A and PLZT 9/65/35 compositions exhibited minimal decrease in displacement when load was applied. Furthermore, these compositions retained approximately 65 percent of their initial displacement after 10 /super7 cycles under a load of 300 g. PZT-5H and PLZT 7/65/35 degraded completely under these conditions.\n",
      "\n",
      "10. id: 55323cf345cec66b6f9dd3ad   score: 0.95150405   abstract: Graphical abstractDisplay Omitted Highlights¿ Lorentz-force actuated silicon microcantilevers fabrication process description. ¿ Thermal noise characterization and investigation of actuation efficiency. ¿ Resonance frequencies found to depend on static bending. ¿ Applications envisioned as cantilever sensors, parallel AFM, and nanomanipulation. Here we present a microcantilever system with a combination of robust Lorentz force-based electromagnetic actuation and highly sensitive optical beam deflection (OBD) method. We present the microcantilevers' architecture, manufacturing process, and wide characteristics including spring constants, static and dynamic response and a study of coupling between them. Highest measured static electromagnetic and thermal actuation responsivities are 1.82nm/µAT and 1.47í¿10-4nm/µA2 (or 1.34nm/µW), respectively, while force resolution is 12pN for a 35mN/m mi\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1728758\n",
      "index                                        55323d6c45cec66b6f9de459\n",
      "title               Testing automation of context-oriented program...\n",
      "authors                                          Mohamed A. El-Zawawy\n",
      "year                                                           2015.0\n",
      "venue               Applied Computational Intelligence and Soft Co...\n",
      "references          5390b5df20f70186a0f0b4db;5390882c20f70186a0d8c...\n",
      "abstract            A new approach for programming that enables sw...\n",
      "id                                                            1728758\n",
      "clustered_labels                                                    2\n",
      "Name: 1728758, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390a4d020f70186a0e7619a   score: 0.99611557   abstract: Context-oriented programming (COP) is a new programming approach whereby the context in which expressions evaluate can be adapted as a program runs. COP provides a degree of flexibility beyond object-oriented programming, while arguably retaining more modularity and structure than aspect-oriented programming. Although many languages exploring the context-oriented approach exist, to our knowledge no formal type-sound dynamic semantics of these languages exists. We address this shortcoming by providing a concise syntax-based formal semantics for context-oriented programming with layers, as witnessed by ContextL, ContextJ*, and other languages. Our language is based on Featherweight Java extended with layers and scoped layer activation and deactivation. As layers may introduce methods not appearing in classes, we also give a static type system that ensures that no program gets stuck (i.e., \n",
      "\n",
      "2. id: 5390a01420f70186a0e47534   score: 0.9223426   abstract: While many software systems today have to be aware of the context in which they are executing, there is still little support for structuring a program with respect to context. A first step towards better context-orientation was the introduction of method layers. This paper proposes two additional language concepts, namely the implicit activation of method layers, and the introduction of dynamic variables.\n",
      "\n",
      "3. id: 5390ba3820f70186a0f36658   score: 0.91964257   abstract: Context-oriented Programming (COP) has been proposed as a new promising paradigm for programming context-aware applications in pervasive environments. However, the expressive power of the layer abstraction in current COP extensions is limited, as layers are only suitable for expressing context with boolean or nominal values and lack support for describing temporal property of context. Besides, partial methods of COP are defined only on top of each single layer but not on a combination of layers, and the behavior of an invoked method is unpredictable as it is determined by the layer activation order. In this paper, we enriches COP by replacing layers with well-structured context entries. Each context entry specifies a piece of context information as well as its temporal property, and a context is represented as a set of many context entries. Many new operations are introduced to manipulat\n",
      "\n",
      "4. id: 5390b2fc20f70186a0eee2da   score: 0.8967949   abstract: Today’s programming platforms do not provide sufficient constructs that allow a program’s behavior to depend on the context in which it is executing. This paper presents the design and implementation of programming language extensions that explicitly support our vision of Context-oriented Programming. In this model, programs can be partitioned into layers that can be dynamically activated and deactivated depending on their execution context. Layers are sets of partial program definitions that can be composed in any order. Context-oriented Programming encourages rich, dynamic modifications of program behavior at runtime, requiring an efficient implementation. We present a dynamic representation of layers that yields competitive performance characteristics for both layer activation/deactivation and overall program execution. We illustrate the performance of our implementation by providing \n",
      "\n",
      "5. id: 558b793b612c6b62e5e89641   score: 0.89606965   abstract: Context-oriented programming (COP) provides a very intuitive way to handle run-time behavior varying in several dimensions. However, COP usually requires major language extensions and implies a considerable performance loss. To avoid language extensions we propose to specify program execution environments as contextual values in separate units. A tool translates such specifications into C++ classes usable in the rest of the program. Without the need of multiple dispatch, the performance can largely profit from simple caching. Furthermore, it is easy to support debugging and store contextual values in configuration files.\n",
      "\n",
      "6. id: 5390b20120f70186a0ee50a6   score: 0.87048715   abstract: Context-oriented programming is an emerging paradigm addressing at the language level the issue of dynamic software adaptation and modularization of context-specific concerns. In this paper we propose JavaCtx, a tool which employs coding conventions to generate the context-aware semantics for Java programs and subsequently weave it into the application. The contribution of JavaCtx is twofold: the design of a set of coding conventions which allow to write context-oriented software in plain Java and the concept of context-oriented semantics injection, which allows to introduce the context-aware semantics through standard aspect-oriented programming. Both of these points allow to seamless integrate JavaCtx in the existing industrial-strength appliances and so ease the development of context-oriented software in consolidated industrial settings.\n",
      "\n",
      "7. id: 5390a25820f70186a0e5f6c5   score: 0.86350673   abstract: Context-oriented Programming, or COP, provides programmers with dedicated abstractions and mechanisms to concisely represent behavioral variations that depend on execution context. By treating context explicitly, and by directly supporting dynamic composition, COP allows programmers to better express software entities that adapt their behavior late-bound at run-time. Our paper illustrates COP constructs, their application, and their implementation by developing a sample scenario, using ContextS in the Squeak/Smalltalk programming environment.\n",
      "\n",
      "8. id: 5390bb1d20f70186a0f3d7c7   score: 0.8610715   abstract: With the increase of research interest in context-oriented programming (COP), several COP languages with different characteristics have been proposed. Although they share common language features to modularize context-dependent variations of behavior, they take quite different ways to realize them. Because of such differences, each language cannot solely cover all use cases of implementing context-dependent behavioral variations. In this paper, we propose a new COP language Javanese that unifies several COP mechanisms into general linguistic constructs. Specifically, it provides context declarations to identify context and its specification of the range of execution sequences where this context is active, activate declarations to define the relation between contexts and layers, and context group declarations that modularize these declarations and specify the set of instances where they a\n",
      "\n",
      "9. id: 5390aefc20f70186a0ecde5b   score: 0.84556025   abstract: We develop a minimal core calculus called ContextFJ to model language mechanisms for context-oriented programming (COP). Unlike other formal models of COP, ContextFJ has a direct operational semantics that can serve as a precise description of the core of COP languages. We also discuss a simple type system that helps to prevent undefined methods from being accessed via proceed.\n",
      "\n",
      "10. id: 5390a4d020f70186a0e76196   score: 0.81420064   abstract: Context-oriented programming (COP) extensions have been implemented for several languages. Each concrete language design and implementation comes with different variations of the features of the COP paradigm. In this paper, we provide a comparison of eleven COP implementations, discuss their designs, and evaluate their performance.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1650897\n",
      "index                                        55912e6a0cf232eb904fb220\n",
      "title                   PaperChains: Dynamic Sketch+Voice Annotations\n",
      "authors                  Jennifer Pearson, Simon Robinson, Matt Jones\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 18th ACM Conference on Comp...\n",
      "references          5390880d20f70186a0d7bfb6;5390a88c20f70186a0e99...\n",
      "abstract            In this paper we present a novel interface for...\n",
      "id                                                            1650897\n",
      "clustered_labels                                                    3\n",
      "Name: 1650897, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b78a20f70186a0f24b0f   score: 0.44083032   abstract: In this paper we consider the current and future use of cameraphones in the context of rural South Africa, where many people do not have access to the latest models and ICT infrastructure is poor. We report a new study of cameraphone use in this setting, and the design and testing of a novel application for creating rich multimedia narratives and materials. We argue for better creative media applications on mobile platforms in this region, and greater attention to their local use.\n",
      "\n",
      "2. id: 5390b4c320f70186a0efdb9f   score: 0.39484245   abstract: This paper describes the interface component of a visual collaborative storytelling system that enables participatory narration by means of tangible user interaction. The essential feature of the interface is to incorporate participants' creative actions by embedding metaphorical schemes through its mechanics and from the visualisation of self-organising content to support collaborative narrative comprehension.\n",
      "\n",
      "3. id: 5390bded20f70186a0f49579   score: 0.30829436   abstract: Mobile devices are encountering increasing popularity as systems for interactive storytelling. Novels and comics can be enriched and made interactive by means of touchable interfaces and multimedia contents, so that the usual linear narration can be broken up in many paths and customized, giving rise to alternative developments.\n",
      "\n",
      "4. id: 539089d320f70186a0d9b8ea   score: 0.20817892   abstract: Desktop computers are not designed for multi-person face-to-face conversation in a social setting. We describe the design of a novel user interface for multi-user interactive informal storytelling. Our design is guided by principles of experience sharing, the disappearing computer, visual navigation, and implicit query formulation.\n",
      "\n",
      "5. id: 5390a9a420f70186a0ea4c3a   score: 0.20465964   abstract: It is widely recognised that paper remains a pervasive resource for collaboration and yet there has been uncertain progress in developing technologies that aim to enhance paper documents with computational capabilities. In this article, we discuss the design of a technology that interweaves developments in hardware and materials, electronics and software, and seeks to create new affinities between digital content and paper. The design of the technology drew from findings from naturalistic studies of the uses of paper, particularly when considering how `users' might `interact' with the augmented technology. We briefly review these studies and discuss the results of an evaluation of the emerging technology. Analysis of the fine details of the conduct of participants in these assessments suggest how, even when we design simple forms of interaction with a device, these can be shaped and tran\n",
      "\n",
      "6. id: 5390aca920f70186a0eb8e42   score: 0.200871   abstract: Tangible user interfaces have been promoted and discussed in the Ubicomp and HCI communities for 15 years. In TUIs physical objects are used for the control and representation of digital information, similarly to how icons are used in graphical user interfaces for the same purpose. Most reported TUI systems have the nature of research prototypes, available in laboratories or museums. This paper reports an attempt to understand the impact of TUIs in users' everyday environments through 2 low-cost simple set-up tangible interfaces for music that can be freely downloaded from a website. The systems are based on computer vision, printed paper and audio output. A few hundreds of users downloaded them and played with them. We logged users interaction with the interfaces and analysed content posted by them on our own and other web sites to observe and evaluate how they relate to such novel syst\n",
      "\n",
      "7. id: 5390ac5720f70186a0eb6078   score: 0.10818896   abstract: We describe and reflect on a method we used to evaluate usability and give insights on situated use of a mobile digital storytelling prototype. We report on rich data we gained by implementing this method and argue that we were able to learn more about our prototype, users, their needs, and their context, than we would have through other evaluation methods. We look at the usability problems we uncovered and discuss how our flexibility in field-testing allowed us to observe unanticipated usage, from which we were able to motivate future design directions. Finally, we reflect on the importance of spending time in-situ during all stages of design, especially when designing across cultures.\n",
      "\n",
      "8. id: 5390a93b20f70186a0ea0a2d   score: 0.10557884   abstract: We reflect on activities to design a mobile application to enable rural people in South Africa's Eastern Cape to record and share their stories, which have implications for 'cross-cultural design,' and the wider use of stories in design. We based our initial concept for generating stories with audio and photos on cell-phones on a scenario informed by abstracting from digital storytelling projects globally and our personal experience. But insights from ethnography, and technology experiments involving storytelling, in a rural village led us to query our grounding assumptions and usability criteria. So, we implemented a method using cell-phones to localise storytelling, involve rural users and probe ways to incorporate visual and audio media. Products from this method helped us to generate design ideas for our current prototype which offers great flexibility. Thus we present a new way to d\n",
      "\n",
      "9. id: 5390be6620f70186a0f4c4cc   score: 0.086169556   abstract: In this paper we present a novel interaction technique that helps to make textual information more accessible to those with low or no textual literacy skills. AudioCanvas allows cameraphone users to interact directly with their own photos of printed media to receive audio feedback or narration. The use of a remote telephone-based service also allows our design to be used over a standard phone line, removing the need for data connections, which can be problematic in developing regions. We show the value of the technique via user evaluations in both a rural Indian village and a South African township.\n",
      "\n",
      "10. id: 5390995c20f70186a0e14359   score: 0.0658447   abstract: The PhotoArcs interface aims to enable easy and fun creation and manipulation of photo-narratives to encourage sharing and interaction. PhotoArcs leverages the benefits of existing sharing habits both online and face-to-face. We describe our design of the PhotoArcs interface, report on the results of an exploratory low-fidelity usability study with five participants, and outline future directions.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1700506\n",
      "index                                        5592379b612c4fa28ff79fc8\n",
      "title                      Termination proofs for linear simple loops\n",
      "authors              Hong Yi Chen, Shaked Flur, Supratik Mukhopadhyay\n",
      "year                                                           2015.0\n",
      "venue               International Journal on Software Tools for Te...\n",
      "references          5390b3da20f70186a0ef5d5b;53909a9320f70186a0e22...\n",
      "abstract            Analysis of termination and other liveness pro...\n",
      "id                                                            1700506\n",
      "clustered_labels                                                    1\n",
      "Name: 1700506, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b95420f70186a0f2e01a   score: 0.99632156   abstract: Analysis of termination and other liveness properties of an imperative program can be reduced to termination proof synthesis for simple loops, i.e., loops with only variable updates in the loop body. Among simple loops, the subset of Linear Simple Loops (LSLs) is particular interesting because it is common in practice and expressive in theory. Existing techniques can successfully synthesize a linear ranking function for an LSL if there exists one. However, when a terminating LSL does not have a linear ranking function, these techniques fail. In this paper we describe an automatic method that generates proofs of universal termination for LSLs based on the synthesis of disjunctive ranking relations. The method repeatedly finds linear ranking functions on parts of the state space and checks whether the transitive closure of the transition relation is included in the union of the ranking rel\n",
      "\n",
      "2. id: 5390b44620f70186a0ef9e52   score: 0.8671483   abstract: We present a new method for proving liveness and termination properties for fair concurrent programs, which does not rely on finding a ranking function or on computing the transitive closure of the transition relation. The set of states from which termination or some liveness property is guaranteed is computed by a backwards reachability analysis. A central technique for handling concurrency is a check for certain commutativity properties. The method is not complete. However, it can be seen as a complement to other methods for proving termination, in that it transforms a termination problem into a simpler one with a larger set of terminated states. We show the usefulness of our method by applying it to existing programs from the literature. We have also implemented it in the framework of Regular Model Checking, and used it to automatically verify non-starvation for parameterized algorith\n",
      "\n",
      "3. id: 53908b1820f70186a0db3960   score: 0.8474635   abstract: We present two algorithms to prove termination of programs by synthesizing linear ranking functions. The first uses an invariant generator based on iterative forward propagation with widening and extracts rankingf unctions from the generated invariants by manipulating polyhedral cones. It is capable of finding subtle ranking functions which are linear combinations of many program variables, but is limited to programs with few variables.The second, more heuristic, algorithm targets the class of structured programs with single-variable ranking functions. Its invariant generator uses a heuristic extrapolation operator to avoid iterative forward propagation over program loops. For the programs we have considered, this approach converges faster and the invariants it discovers are sufficiently strong to imply the existence of ranking functions.\n",
      "\n",
      "4. id: 5390bb7b20f70186a0f40cc2   score: 0.6827853   abstract: Program termination is a hot research topic in program analysis. The last few years have witnessed the development of termination analyzers for programming languages such as C and Java with remarkable precision and performance. These systems are largely based on techniques and tools coming from the field of declarative constraint programming. In this paper, we first recall an algorithm based on Farkas' Lemma for discovering linear ranking functions proving termination of a certain class of loops. Then we propose an extension of this method for showing the existence of eventual linear ranking functions, i.e., linear functions that become ranking functions after a finite unrolling of the loop. We show correctness and completeness of this algorithm.\n",
      "\n",
      "5. id: 5390a05a20f70186a0e4aa65   score: 0.65201986   abstract: Establishing the termination of programs is a fundamental problem in the field of software verification. For transformational programs, termination is used to extend partial correctness to total correctness. For reactive systems, termination reasoning is used to establish liveness properties. In the context of theorem proving, termination is used to establish the consistency of definitional axioms and to automate proofs by induction. Of course, termination is an undecidable problem, as Turing himself proved. However, the question remains: how automatic can a general termination analysis be in practice? In this dissertation, we develop two new general frameworks for reasoning about termination and demonstrate their effectiveness in automating the task of proving termination in the domain of applicative first-order functional languages. The foundation of the first framework is the developm\n",
      "\n",
      "6. id: 5390b52620f70186a0f03f41   score: 0.64500815   abstract: The classical technique for proving termination of a generic sequential computer program involves the synthesis of a ranking function for each loop of the program. Linear ranking functions are particularly interesting because many terminating loops admit one and algorithms exist to automatically synthesize it. In this paper we present two such algorithms: one based on work dated 1991 by Sohn and Van Gelder; the other, due to Podelski and Rybalchenko, dated 2004. Remarkably, while the two algorithms will synthesize a linear ranking function under exactly the same set of conditions, the former is mostly unknown to the community of termination analysis and its general applicability has never been put forward before the present paper. In this paper we thoroughly justify both algorithms, we prove their correctness, we compare their worst-case complexity and experimentally evaluate their effic\n",
      "\n",
      "7. id: 5390a1d420f70186a0e57d2b   score: 0.61630094   abstract: We present a new approach for termination proofs that uses polynomial interpretations (with possibly negative coefficients) together with the \"maximum\" function. To obtain a powerful automatic method, we solve two main challenges: (1) We show how to adapt the latest developments in the dependency pair framework to our setting. (2) We show how to automate the search for such interpretations by integrating \" max \" into recent SAT-based methods for polynomial interpretations. Experimental results support our approach.\n",
      "\n",
      "8. id: 5390a72220f70186a0e89b5e   score: 0.599192   abstract: One approach to mechanizing proofs of termination is to require that each new recursive function definition be accompanied by an ordering relation. The main disadvantage of this methodology is to determine an ordering relation first and prove the termination based on it. This paper describes an alternative approach that does not require a separate order relation. The proof of termination is constructed as a proof by induction that mirrors the recursion structure in the axioms. We generate for each new function being defined a termination condition (TC), and from each axiom of the function's definition we generate a corresponding termination axiom. Using the resulting set of termination axioms, we construct a proof of the TC. Both TC and termination axioms are generated automatically by applying a simple tool called TCGen. If the termination is proved, another tool called CodeGen can be a\n",
      "\n",
      "9. id: 53908ae020f70186a0dad87c   score: 0.5899526   abstract: Proofs of termination typically proceed by mapping program states to a well founded domain and showing that successive states of the computation are mapped to elements decreasing in size. Automated termination analysers for logic programs achieve this by measuring and comparing the sizes of successive calls to recursive predicates. The size of the call is measured by a level mapping that in turn is based on a norm on the arguments of the call. A norm maps a term to a natural number. The choice of the norm is crucial for the ability to prove termination. For some programs a fairly complex norm is required. The automated selection of an appropriate norm is a missing link in this research domain and is addressed in this paper. Our proposal is to use the type of a predicate to generate a number of simple norms and to try them in turn for proving the termination of the predicate. Given a term\n",
      "\n",
      "10. id: 5390b00c20f70186a0ed3ec6   score: 0.57212526   abstract: Originally, the concepts of transition invariants and transition predicate abstraction were used to formulate a proof rule and an abstraction-based algorithm for the verification of liveness properties of concurrent programs under fairness assumptions. This note applies the two concepts for proving termination of sequential programs. We believe that the specialized setting exhibits the underlying principles in a more direct way.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1668538\n",
      "index                                        559256a80cf28b1a968ffca8\n",
      "title               Mojim: A Reliable and Highly-Available Non-Vol...\n",
      "authors             Yiying Zhang, Jian Yang, Amirsaman Memaripour,...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Twentieth International Con...\n",
      "references          5390981d20f70186a0e05868;53908bad20f70186a0dc2...\n",
      "abstract            Next-generation non-volatile memories (NVMs) p...\n",
      "id                                                            1668538\n",
      "clustered_labels                                                    3\n",
      "Name: 1668538, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558ad9e4612c41e6b9d3b87c   score: 0.9193534   abstract: Currently, new storage technologies which unite the latency and byte-addressability of DRAM with the persistence of disks are being developed. This non-volatile memory (NVRAM) may start a software revolution. Traditionally, software was developed for two levels of storage and NVRAM reduces the hierarchy to a single-level store. Current research projects are already exploring the potential of NVRAM, but they face a challenge when they want to evaluate the performance: The new hardware is not yet available. In this paper, we discuss why benchmark results which are gained on existing DRAM are insufficient for a prediction of the performance on NVRAM. Either existing instructions have to be changed or new ones have to be introduced. We further show that the bochs emulator can be used to build systems which resemble NVRAM, to predict the NVRAM's consequences, and it even allows a comparison o\n",
      "\n",
      "2. id: 5390bb1d20f70186a0f3e8bb   score: 0.86960393   abstract: Emerging non-volatile memory (NVM) technologies have DRAM-like latency with storage-like density, offering unique capability to analyze large data sets significantly faster than flash or disk storage. However, the hybrid nature of these NVM technologies such as phase-change memory (PCM) make it difficult to use them to best advantage in the memory-storage hierarchy. These NVMs lack the fast write latency required of DRAM and are thus not suitable as DRAM equivalent on the memory bus, yet their low latency even in random access patterns is not easily exploited over an I/O bus. In this work, we describe an FPGA-based system to execute application-specific operations in the NVM controller and evaluate its performance on two micro benchmarks and a key valuestore. Our system Minerva extends the conventional solid-state drive (SSD) architecture to offload data or I/O intensive application code\n",
      "\n",
      "3. id: 553a799a0cf288c3e22746e2   score: 0.8590736   abstract: As high performance NVM storage such as PCM and STT-RAM emerge, legacy software layers optimized for HDDs should be revisited. Specifically, as storage performance approaches DRAM performance, existing I/O mechanisms and software configurations should be reassessed. This paper explores the challenges and implications of using NVM storage with a broad range of experiments. We measure the performance of a system with NVM storage emulated by DRAM with proper timing parameters and compare it with that of HDD storage environments under various configurations. Our experimental results show that even with storage as fast as DRAM, the performance gain is not large for read operations as current I/O mechanisms do a good job of hiding the slow performance of HDD. To assess the potential benefit of fast storage media, we change various I/O configurations and perform experiments to quantify the effe\n",
      "\n",
      "4. id: 5390aefc20f70186a0ecde3a   score: 0.6575384   abstract: The predicted shift to non-volatile, byte-addressable memory (e.g., Phase Change Memory and Memristor), the growth of \"big data\", and the subsequent emergence of frameworks such as memcached and NoSQL systems require us to rethink the design of data stores. To derive the maximum performance from these new memory technologies, this paper proposes the use of single-level data stores. For these systems, where no distinction is made between a volatile and a persistent copy of data, we present Consistent and Durable Data Structures (CDDSs) that, on current hardware, allows programmers to safely exploit the low-latency and non-volatile aspects of new memory technologies. CDDSs use versioning to allow atomic updates without requiring logging. The same versioning scheme also enables rollback for failure recovery. When compared to a memory-backed Berkeley DB B-Tree, our prototype-based results sh\n",
      "\n",
      "5. id: 5390a4cc20f70186a0e7401c   score: 0.61745495   abstract: This paper explores the feasibility of a cost-efficient storage architecture that offers the reliability and access performance characteristics of a high-end system. This architecture exploits two opportunities: First, scavenging idle storage from LAN-connected desktops not only offers a low-cost storage space, but also high I/O throughput by aggregating the I/O channels of the participating nodes. Second, the two components of data reliability - durability and availability - can be decoupled to control overall system cost. To capitalize on these opportunities, we integrate two types of components: volatile, scavenged storage and dedicated, yet low-bandwidth durable storage. On the one hand, the durable storage forms a low-cost back-end that enables the system to restore the data the volatile nodes may lose. On the other hand, the volatile nodes provide a high-throughput front-end. While\n",
      "\n",
      "6. id: 5390bded20f70186a0f4905a   score: 0.61543465   abstract: Emerging non-volatile memory (NVM) technologies have gained a lot of attention recently. The byte-addressability and high density of NVM enable computer architects to build large-scale main memory systems. NVM has also been shown to be a promising alternative to conventional persistent store. With NVM, programmers can persistently retain in-memory data structures without writing them to disk. Therefore, one can envision that in the future, NVM will play the role of both working memory and persistent store at the same time. Persistent store demands consistency and durability guarantees, thereby imposing new design constraints on the memory system. Consistency is achieved at the expense of serializing multiple write operations. Durability requires memory cells to guarantee non-volatility and thus reduces the write speed. Therefore, a unified architecture oblivious to these two use cases wo\n",
      "\n",
      "7. id: 558af0c0612c41e6b9d3e069   score: 0.5941392   abstract: While non-volatile memory (NVRAM) devices have the potential to alleviate the trade-off between performance, scalability, and energy in storage and memory subsystems, a block interface and storage subsystems designed for slow I/O devices make it difficult to efficiently exploit NVRAMs in a portable and extensible way. We propose an object-based storage model as a way of addressing the shortfalls of the current interfaces. Through the design of Muninn, an object-based versioning key-value store, we demonstrate that an in-device NVRAM management layer can be as efficient as that of NVRAM-aware key-value stores while not requiring host resources or host changes, and enabling tightly-coupled optimizations. Muninn is also designed to show that versioning can be added to a file system transparently with minimal host-side changes. As a flash key-value store, it achieves better life-time and low\n",
      "\n",
      "8. id: 53908b4920f70186a0dbb9bb   score: 0.57612467   abstract: File systems and databases usually make several synchronous disk write accesses in order to make sure that the disk always has a consistent view of their data, and that data can be recovered in the case of a system crash. Since synchronous disk operations are slow, some systems choose to employ asynchronous disk write operations, at the cost of low reliability: in case of a system crash all data that have not yet been written to disk are lost.In this paper we describe a software-based approach into using the network memory in a workstation cluster as a layer of Non-Volatile memory (NVRAM). Our approach takes a set of volatile main memories residing in independent workstations and transforms it into a fault-tolerant memory - much like RAIDS do with magnetic disks. This layer of NVRAM allows us to create systems that combine the reliability of synchronous disk accesses with the cost of asy\n",
      "\n",
      "9. id: 5390baa120f70186a0f37d08   score: 0.57403666   abstract: Today's commercial cloud service providers require the availability with an annual uptime percentage at least 99.95\\%. While memory errors become norms instead of exceptions with the increasing memory's density and capacity in cloud applications. Thus, uncorrected errors from DRAM can be a significant source of system downtime. To address this increasingly important concern, both hardware and software memory mirroring technologies are studied nowadays to provide memory high availability. However, hardware solutions like mirror memory, which uses doubled chip, need dedicated and costly peripheral hardware. While existing software approaches, i.e., virtual machine's checkpoint technology, reduce the expense but incur the high overhead in practical usage. In this paper, we present a novel system called \\emph{k}Memvisor to provide system-wide high availability memory mirroring. It is a softw\n",
      "\n",
      "10. id: 5390ada620f70186a0ec211f   score: 0.5614553   abstract: Data replication has been widely used as a mean of increasing the data availability of large-scale storage systems where failures are normal. Aiming to provide cost-effective availability, and improve performance and load-balancing of large-scale storage cluster, this paper presents a dynamic replication management scheme referred to as DRM. A model is developed to express availability as function of replica number. Based on this model, minimal replica number to satisfy availability requirement can be determined. DRM further places these replicas among Object-Based Storage Devices (OSD) in a balance way, taking into account different capacity and blocking probability of each OSD in heterogeneous environment. Proposed DRM can dynamically redistribute workloads among OSD cluster by adjusting replica number and location according to workload changing and OSD capacity. Our experiment results\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1691446\n",
      "index                                        5592553a0cf2aff368683bc0\n",
      "title               A posteriori error control and adaptivity for ...\n",
      "authors                              Theodoros Katsaounis, Irene Kyza\n",
      "year                                                           2015.0\n",
      "venue                                           Numerische Mathematik\n",
      "references          5390893e20f70186a0d93174;5390972920f70186a0dfb...\n",
      "abstract            We derive optimal order a posteriori error est...\n",
      "id                                                            1691446\n",
      "clustered_labels                                                    2\n",
      "Name: 1691446, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390958920f70186a0deec1d   score: 0.91550726   abstract: It is known that the energy technique for a posteriori error analysis of finite element discretizations of parabolic problems yields suboptimal rates in the norm $L^\\infty (0,T; L^2 (\\Omega)).$ In this paper, we combine energy techniques with an appropriate pointwise representation of the error based on an elliptic reconstruction operator which restores the optimal order (and regularity for piecewise polynomials of degree higher than one). This technique may be regarded as the \"dual a posteriori\" counterpart of Wheeler's elliptic projection method in the a priori error analysis.\n",
      "\n",
      "2. id: 558b1a2d612c41e6b9d43590   score: 0.8951567   abstract: In this paper, we study linearized Crank---Nicolson Galerkin FEMs for a generalized nonlinear Schrödinger equation. We present the optimal $$L^2$$ L 2 error estimate without any time-step restrictions, while previous works always require certain conditions on time stepsize. A key to our analysis is an error splitting, in terms of the corresponding time-discrete system, with which the error is split into two parts, the temporal error and the spatial error. Since the spatial error is $$\\\\tau $$ ¿ -independent, the numerical solution can be bounded in $$L^{\\\\infty }$$ L ¿ -norm by an inverse inequality unconditionally. Then, the optimal $$L^2$$ L 2 error estimate can be obtained by a routine method. To confirm our theoretical analysis, numerical results in both two and three dimensional spaces are presented.\n",
      "\n",
      "3. id: 5390b24420f70186a0ee7105   score: 0.87810534   abstract: We prove a posteriori error estimates for time discrete approximations, for semilinear parabolic equations with solutions that might blow up in finite time. In particular we consider the backward Euler and the Crank-Nicolson methods. The main tools that are used in the analysis are the reconstruction technique and energy methods combined with appropriate fixed point arguments. The final estimates we derive are conditional and lead to error control near the blow up time.\n",
      "\n",
      "4. id: 55323c1445cec66b6f9db234   score: 0.86499614   abstract: A second-order singularly perturbed parabolic equation in one space dimension is considered. For this equation, we give computable a posteriori error estimates in the maximum norm for two semidiscretisations in time and a full discretisation using P 1 FEM in space. Both the Backward-Euler method and the Crank-Nicolson method are considered. Certain critical details of the implementation are addressed. Based on numerical results we discuss various aspects of the error estimators in particular their effectiveness.\n",
      "\n",
      "5. id: 5390b71120f70186a0f1e4cf   score: 0.84695786   abstract: In this paper, we introduce a general approach for proving optimal $L^2$ error estimates for the semidiscrete local discontinuous Galerkin (LDG) methods solving linear high order wave equations. The optimal order of error estimates holds not only for the solution itself but also for the auxiliary variables in the LDG method approximating the various order derivatives of the solution. Examples including the one-dimensional third order wave equation, one-dimensional fifth order wave equation, and multidimensional Schrödinger equation are explored to demonstrate this approach. The main idea is to derive energy stability for the various auxiliary variables in the LDG discretization by using the scheme and its time derivatives with different test functions. Special projections are utilized to eliminate the jump terms at the cell boundaries in the error estimate in order to achieve the optimal\n",
      "\n",
      "6. id: 539087f320f70186a0d6ecd2   score: 0.8446654   abstract: The finite volume element method (FVE) is a discretization technique for partial differential equations. This paper develops discretization energy error estimates for general self-adjoint elliptic boundary value problems with FVE based on triangulations, on which there exist linear finite element spaces, and a very general type of control volumes (covolumes).The energy error estimates of this paper are also optimal but the restriction conditions for the covolumes given in [R. E. Bank and D. J. Rose, SIAM J. Numer. Anal., 24 (1987), pp. 777--787], [Z. Q. Cai, Numer. Math., 58 (1991), pp. 713--735] are removed. The authors finally provide a counterexample to show that an expected L2-error estimate does not exist in the usual sense. It is conjectured that the optimal order of $\\|u-u_h\\|_{0,\\Omega}$ should be O(h) for the general case.\n",
      "\n",
      "7. id: 53909f2d20f70186a0e37c4c   score: 0.8053262   abstract: In this paper we derive a posteriori error estimates for space-time finite element discretizations of parabolic optimization problems. The provided error estimates assess the discretization error with respect to a given quantity of interest and separate the influences of different parts of the discretization (time, space, and control discretization). This allows us to set up an efficient adaptive algorithm which successively improves the accuracy of the computed solution by construction of locally refined meshes for time and space discretizations.\n",
      "\n",
      "8. id: 5390a72320f70186a0e8b2bc   score: 0.77696157   abstract: We derive a posteriori error estimates in the $L_\\infty((0,T];L_\\infty(\\Omega))$ norm for approximations of solutions to linear parabolic equations. Using the elliptic reconstruction technique introduced by Makridakis and Nochetto and heat kernel estimates for linear parabolic problems, we first prove a posteriori bounds in the maximum norm for semidiscrete finite element approximations. We then establish a posteriori bounds for a fully discrete backward Euler finite element approximation. The elliptic reconstruction technique greatly simplifies our development by allowing the straightforward combination of heat kernel estimates with existing elliptic maximum norm error estimators.\n",
      "\n",
      "9. id: 5390a17720f70186a0e51d24   score: 0.77218723   abstract: This paper addresses the theoretical analysis of a fully discrete scheme for the one-dimensional time-dependent Schrodinger equation on unbounded domain. We first reduce the original problem into an initial-boundary value problem in a bounded domain by introducing a transparent boundary condition, then fully discretize this reduced problem by applying Crank-Nicolson scheme in time and linear or quadratic finite element approximation in space. By a rigorous analysis, this scheme has been proved to be unconditionally stable and convergent, its convergence order has also be obtained. Finally, two numerical examples are performed to show the accuracy of the scheme.\n",
      "\n",
      "10. id: 539087dd20f70186a0d64abb   score: 0.7718435   abstract: The problem of obtaining /posteriori/ estimates of the discretization error when one uses finite element methods to approximate problems with an incompressibility constraint is discussed. A general approach to the treatment of the constraint condition and to the (possible) non-self-adjointness of the associated momentum equations is presented. A posteriori error estimates are derived for adaptive h, p, and h-p type finite element schemes. Key features are that the local error residual problems are not subject to an incompressibility constraint thereby avoiding the need for special finite element schemes and that the analysis is valid for essentially any discretization scheme, including continuous and discontinuous pressure spaces. The estimator bounds the actual error measured in an energy-like norm.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1668769\n",
      "index                                        559164ce0cf2e89307ca988a\n",
      "title               Pipelined SHA-3 Implementations on FPGA: Archi...\n",
      "authors             Harris E. Michail, Lenos Ioannou, Artemios G. ...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the Second Workshop on Cryptogr...\n",
      "references          5390ae2e20f70186a0ec7cf6;5390b4c420f70186a0efe...\n",
      "abstract            Efficient and high-throughput designs of hash ...\n",
      "id                                                            1668769\n",
      "clustered_labels                                                    3\n",
      "Name: 1668769, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b2fc20f70186a0eef82d   score: 0.92482364   abstract: Cryptographic hash functions are at heart of many information security applications like digital signatures, message authentication codes (MACs), and other forms of authentication. In consequence of recent innovations in cryptanalysis of commonly used hash algorithms, NIST USA announced a publicly open competition for selection of new standard Secure Hash Algorithm called SHA-3. An essential part of this contest is hardware performance evaluation of the candidates. In this work we present a high throughput efficient hardware implementation of one of the final round candidate of SHA-3: BLAKE. We implemented and investigated the performance of BLAKE on latest Xilinx FPGAs. We show our results in form of chip area consumption, throughput and throughput per area. We compare and contrasted these results with most recently reported implementations of BLAKE. Our design ranked highest in terms o\n",
      "\n",
      "2. id: 5390981d20f70186a0e05496   score: 0.9238675   abstract: The hash functions MD5, RIPEMD-160, and SHA-1/224/256/384/512 were implemented by using a 0.13-µm CMOS standard cell library with two synthesis options, area and speed optimizations, and their performances were evaluated. The smallest circuit of 8.0 Kgates with a throughput of 929 Mbps, and the highest throughput of 2.9 Gbps with 27.3 Kgates were obtained for SHA-1 and SHA-384/512 respectively. In terms of overall performance with consideration of the security levels, we conclude that SHA-256 is the best algorithm, with compact circuits of 11.5~15.3 Kgates and high throughputs of 1.1 ~ 2.4 Gbps. Our implementations also showed the highest throughputs for all of the hash functions in comparison with the state of the art.\n",
      "\n",
      "3. id: 53909e8b20f70186a0e2ecee   score: 0.9049869   abstract: The hash functions MD5, RIPEMD-160, and SHA-1/224/256/384/512 were implemented by using a 0.13-µm CMOS standard cell library with two synthesis options, area and speed optimizations, and their performances were evaluated. The smallest circuit of 8.0 Kgates with a throughput of 934 Mbps, and the highest throughput of 2.9 Gbps with 27.3 Kgates were obtained for SHA-1 and SHA- 384/512, respectively. In terms of overall performance with consideration of the security levels, we conclude that SHA-256 is the best algorithm, with compact circuits of 11.5-15.3 Kgates and high throughputs of 1.1-2.4 Gbps. Our implementations also showed the highest throughputs for all of the hash functions in comparison with the state of the art. These high performance hardware implementations can also be used to break hash functions. Therefore, we evaluated the hardware cost to break the most popular hash functio\n",
      "\n",
      "4. id: 5390af8920f70186a0ed11c3   score: 0.88018054   abstract: The main applications of the hash functions are met in the fields of communication integrity and signature authentication. A hash function is utilized in the security layer of every communication protocol. However, as protocols evolve and new high-performance applications appear, the throughput of most hash functions seems to reach to a limit. Furthermore, due to the tendency of the market to minimize devices' size and increase their autonomy to make them portable, power issues have also to be considered. In this work a new technique is presented for increasing frequency and throughput of all widely used hash functions - and those that will be used in the future- hash functions such as MD-5, SHA-1, RIPEMD (all versions), SHA-256, SHA-384, and SHA-512 etc. Comparing to conventional pipelined implementations of hash functions the proposed parallelism technique leads to a 33%-50% higher thr\n",
      "\n",
      "5. id: 5390ae2e20f70186a0ec7cf6   score: 0.83196384   abstract: The second round of the NIST-run public competition is underway to find a new hash algorithm(s) for inclusion in the NIST Secure Hash Standard (SHA-3). This paper presents the full implementations of all of the second round candidates in hardware with all of their variants. In order to determine their computational efficiency, an important aspect in NIST's round two evaluation criteria, this paper gives an area/speed comparison of each design both with and without a hardware interface, thereby giving an overall impression of their performance in resource constrained and resource abundant environments. The implementation results are provided for a Virtex-5 FPGA device. The efficiency of the architectures for the hash functions are compared in terms of throughput per unit area.\n",
      "\n",
      "6. id: 5390a01420f70186a0e488e2   score: 0.8010035   abstract: Hash functions are forming a special family of cryptographic algorithms, which are applied wherever message integrity and authentication issues are critical. Implementations of these functions are cryptographic primitives to the most widely used cryptographic schemes and security protocols such as Secure Electronic Transactions (SET), Public Key Infrastructure (PKI), IPSec and Virtual Private Networks (VPNs). As time passes it seems that all these applications call for higher throughput due to their rapid acceptance by the market especially to the corresponding servers of these applications. In this work a new technique is presented for increasing frequency and throughput of the currently most used hash function, which is SHA-1. This technique involves the application of spatial and temporal precomputation. Comparing to conventional pipelined implementations of hash functions, the propos\n",
      "\n",
      "7. id: 5390b3ae20f70186a0ef3238   score: 0.78365755   abstract: High-throughput and area-efficient designs of hash functions and corresponding mechanisms for Message Authentication Codes (MACs) are in high demand due to new security protocols that have arisen and call for security services in every transmitted data packet. For instance, IPv6 incorporates the IPSec protocol for secure data transmission. However, the IPSec's performance bottleneck is the HMAC mechanism which is responsible for authenticating the transmitted data. HMAC's performance bottleneck in its turn is the underlying hash function. In this article a high-throughput and small-size SHA-256 hash function FPGA design and the corresponding HMAC FPGA design is presented. Advanced optimization techniques have been deployed leading to a SHA-256 hashing core which performs more than 30&percnt; better, compared to the next better design. This improvement is achieved both in terms of through\n",
      "\n",
      "8. id: 5390b3ae20f70186a0ef3236   score: 0.78365755   abstract: High-throughput and area-efficient designs of hash functions and corresponding mechanisms for Message Authentication Codes (MACs) are in high demand due to new security protocols that have arisen and call for security services in every transmitted data packet. For instance, IPv6 incorporates the IPSec protocol for secure data transmission. However, the IPSec's performance bottleneck is the HMAC mechanism which is responsible for authenticating the transmitted data. HMAC's performance bottleneck in its turn is the underlying hash function. In this article a high-throughput and small-size SHA-256 hash function FPGA design and the corresponding HMAC FPGA design is presented. Advanced optimization techniques have been deployed leading to a SHA-256 hashing core which performs more than 30&percnt; better, compared to the next better design. This improvement is achieved both in terms of through\n",
      "\n",
      "9. id: 5390b29820f70186a0ee8d0a   score: 0.7816641   abstract: Secure cryptographic hash functions are core components in many applications like challenge-response authentication systems or digital signature schemes. Many of these applications are used in cost-sensitive markets and thus slow budget implementations of such components are very important. In the present paper, we focus on the new SHA-3 competition, started by the National Institute of Standards and Technology (NIST), which searches for a new hash function in response to security concerns regarding the previous hash functions SHA-1 and the SHA-2 family. This work adds new valuable data to the competition, by providing an evaluation of area-efficient implementations of all finalists. Our results show, that it is possible to implement all candidates reasonably small. We focus on area-efficiency and therefore we do not rank the candidates by absolute throughput, but rather by the area and \n",
      "\n",
      "10. id: 5390a5dc20f70186a0e80410   score: 0.76100606   abstract: Today, security is a topic which attacks the greatinterest of researchers. Many encryption algorithms have beeninvestigated, and developed in the last years. Hash functionsare important security primitives used for authentication anddata integrity. The reconfigurable cryptographic chip is anintegrated circuit that is designed by means of the method ofreconfigurable architecture, and is used for encryption anddecryption. It can implement many different cipher algorithmsflexibly and quickly, and be used in many fields. This work isrelated to hash functions FPGA implementation. Five differenthash functions SHA-1, SHA-224, SHA-256, SHA-384 andSHA-512 are studied. A reconfigurable architecture isproposed for the implementation of all of them in the samehardware module. Finally, it gives the implementation resultsbased on the FPGA of the family of Stratix II of AlteraCorporation. The proposed \n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1674123\n",
      "index                                        559152c20cf232eb904fbc04\n",
      "title               A new hybrid algorithm for software fault loca...\n",
      "authors                        Jeongho Kim, Jonghee Park, Eunseok Lee\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 9th International Conferenc...\n",
      "references          5390a28120f70186a0e63413;5390b04120f70186a0ed6...\n",
      "abstract            We previously presented a spectrum-based fault...\n",
      "id                                                            1674123\n",
      "clustered_labels                                                    3\n",
      "Name: 1674123, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 55323cb345cec66b6f9dcb23   score: 0.959001   abstract: HighlightsWe propose a new technique for fault localization, coined dynamic code coverage.DCC initially uses a coarser instrumentation to analyze the program traces.Instrumentation is increased for faulty components, until a finer detail is reached.A topology model to decide whether to use DCC or other techniques is also proposed.We validate our approach using real-world, large applications. Spectrum-based fault localization is amongst the most effective techniques for automatic fault localization. However, abstractions of program execution traces, one of the required inputs for this technique, require instrumentation of the software under test at a statement level of granularity in order to compute a list of potential faulty statements. This introduces a considerable overhead in the fault localization process, which can even become prohibitive in, e.g., resource constrained environments\n",
      "\n",
      "2. id: 55323cb445cec66b6f9dcb2f   score: 0.9281487   abstract: HighlightsA novel fault localization approach (HSS) based on compounding full slices and execution slices.A maximal risk evaluation formula which is theoretically proved, for calculating suspiciousness based on a hybrid spectrum of full slices and execution slices.A prototype tool HSFal (hybrid slice spectrum fault locator) to implement our proposed approach.An empirical evaluation of proposed approach comparing with coverage-based approaches and slice-based approaches. Most of the existing fault localization approaches use execution coverage of test cases to isolate the suspicious codes that likely contain faults. Program slicing can extract the dependencies of program entities with respect to a specific criterion. Therefore this technique is expected to have a beneficial effect on fault localization. In this paper, we propose a novel approach using a hybrid spectrum of full slices and \n",
      "\n",
      "3. id: 5390be6620f70186a0f4b106   score: 0.9043131   abstract: Spectrum-based fault localization refers to the process of identifying program units that are buggy from two sets of execution traces: normal traces and faulty traces. These approaches use statistical formulas to measure the suspiciousness of program units based on the execution traces. There have been many spectrum-based fault localization approaches proposing various formulas in the literature. Two of the best performing and well-known ones are Tarantula and Ochiai. Recently, Xie et al. find that theoretically, under certain assumptions, two families of spectrum-based fault localization formulas outperform all other formulas including those of Tarantula and Ochiai. In this work, we empirically validate Xie et al.'s findings by comparing the performance of the theoretically best formulas against popular approaches on a dataset containing 199 buggy versions of 10 programs. Our empirical \n",
      "\n",
      "4. id: 5390af8920f70186a0ecfbac   score: 0.8484707   abstract: Abstract: Fault localization is a major activity in program debugging. To automate this time-consuming task, many existing fault-localization techniques compare passed executions and failed executions, and suggest suspicious program elements, such as predicates or statements, to facilitate the identification of faults. To do that, these techniques propose statistical models and use hypothesis testing methods to test the similarity or dissimilarity of proposed program features between passed and failed executions. Furthermore, when applying their models, these techniques presume that the feature spectra come from populations with specific distributions. The accuracy of using a model to describe feature spectra is related to and may be affected by the underlying distribution of the feature spectra, and the use of a (sound) model on inapplicable circumstances to describe real-life feature s\n",
      "\n",
      "5. id: 5390a01420f70186a0e468ef   score: 0.82261705   abstract: To a given test case, fault localization has to be proceeded when its output is wrong. A novel method is presented to localize a fault. Firstly, by analyzing the relation between testing requirement and test cases that satisfying it, some assistant test cases are selected out. Then, program sliceis introduced to reduce the searching domain based on priority, which has been evaluated according to the occurrences in the selected slices. Two procedures, refining and augmenting, are followed here to fault localization: in the refining phase, the most suspicious codes are checked step by step; in the augmenting phase, more codes will be gradually considered on the basis of direct data dependency. At last, experimental studies are performed to illustrate the effectiveness of the technique.\n",
      "\n",
      "6. id: 5390b4c420f70186a0eff12d   score: 0.7999117   abstract: Many spectrum-based fault localization measures have been proposed in the literature. However, no single fault localization measure completely outperforms others: a measure which is more accurate in localizing some bugs in some programs is less accurate in localizing other bugs in other programs. This paper proposes to compose existing spectrum-based fault localization measures into an improved measure. We model the composition of various measures as an optimization problem and present a search-based approach to explore the space of many possible compositions and output a heuristically near optimal composite measure. We employ two search-based strategies including genetic algorithm and simulated annealing to look for optimal solutions and compare the effectiveness of the resulting composite measures on benchmark software systems. Compared to individual spectrum-based fault localization t\n",
      "\n",
      "7. id: 5390a40520f70186a0e6eded   score: 0.79787195   abstract: Spectrum-based fault localization is a statistical technique that aims at helping software developers to find faults quickly by analyzing abstractions of program traces to create a ranking of most probable faulty components (e.g., program statements). Although spectrum-based fault localization has been shown to be effective, its diagnostic accuracy is inherently limited, since the semantics of components are not considered. In particular, components that exhibit identical execution patterns cannot be distinguished. To enhance its diagnostic quality, in this paper, we combine spectrum-based fault localization with a model-based debugging approach based on abstract interpretation within a framework coined Deputo. The model-based approach is used to refine the ranking obtained from the spectrum-based method by filtering out those components that do not explain the observed failures when the\n",
      "\n",
      "8. id: 5390bfa220f70186a0f5357f   score: 0.7867867   abstract: Most of the existing fault localization approaches use execution coverage of test cases to isolate the suspicious codes that likely contain faults. Program slicing can extract the dependencies of program entities with respect to a specific criterion. Therefore this technique is expected to have a beneficial effect on fault localization. In this paper, we propose a novel approach using a hybrid spectrum of full slices and execution slices to improve the effectiveness of fault localization. In particular, our approach firstly computes full slices of failed test cases and execution slices of passed test cases respectively. Secondly it constructs the hybrid spectrum by intersecting full slices and execution slices. Finally it computes the suspiciousness of each statement in the hybrid slice spectrum and generates a fault location report with descending suspiciousness of each statement. We al\n",
      "\n",
      "9. id: 5390b1d220f70186a0ee343a   score: 0.74316806   abstract: Spectrum-based Fault Localization (SBFL) is one of the most popular approaches for locating software faults, and has received much attention because of its simplicity and effectiveness. It utilizes the execution result of each test case (failure or pass) and the corresponding coverage information to evaluate the likelihood of each program entity (e.g., a statement or a predicate) being faulty. Different formulas for computing such likelihood have been proposed based on different intuitions. All existing SBFL techniques have assumed the existence of a testing oracle, that is, a mechanism which can determine whether the execution of a test case fails or passes. However, such an assumption does not always hold. Recently, metamorphic testing has been proposed to alleviate the oracle problem. Thus, it is a natural extension to investigate how it can help SBFL techniques to locate faults even \n",
      "\n",
      "10. id: 5390bded20f70186a0f49eac   score: 0.6984323   abstract: Spectra-based fault localization (SFL) is an automatic fault-localization technique which has received a lot of attention due to its simplicity and effectiveness. SFL uses ranking metric (RM) to rank the risk of fault existence in each program entity after dynamically collecting the necessary information. The evaluation of RMs for SFL has recently become a research focus. To evaluate the average performance of RMs for SFL with different single-fault types, an If-While-If (IWI) model-based approach is presented in this paper. Firstly, through investigating rankings of statements in the IWI model, this paper takes an optimal RM known as an example to analyze its localization effectiveness for five types of single-fault. Secondly, a generic hierarchical method is given in the IWI model to precisely calculate the average performance of RMs. Two experiments, that calculate the average perform\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1729860\n",
      "index                                        55323d7f45cec66b6f9de84c\n",
      "title               A fast parallel sparse solver for SPICE-based ...\n",
      "authors                         Xiaoming Chen, Yu Wang, Huazhong Yang\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 2015 Design, Automation & T...\n",
      "references                                   558be1150cf20e727d0f3535\n",
      "abstract            The sparse solver is a serious bottleneck in S...\n",
      "id                                                            1729860\n",
      "clustered_labels                                                    0\n",
      "Name: 1729860, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558be1150cf20e727d0f3535   score: 0.83128005   abstract: The sparse matrix solver has become a bottleneck in simulation program with integrated circuit emphasis (SPICE)-like circuit simulators. It is difficult to parallelize the solver because of the high data dependency during the numeric LU factorization and the irregular structure of circuit matrices. This paper proposes an adaptive sparse matrix solver called NICSLU, which uses a multithreaded parallel LU factorization algorithm on shared-memory computers with multicore/multisocket central processing units to accelerate circuit simulation. The solver can be used in all the SPICE-like circuit simulators. A simple method is proposed to predict whether a matrix is suitable for parallel factorization, such that each matrix can achieve optimal performance. The experimental results on 35 matrices reveal that NICSLU achieves speedups of $2.08\\\\times\\\\sim 8.57\\\\times~({\\\\rm on~the~geometric~mean})\n",
      "\n",
      "2. id: 5390b72e20f70186a0f21c55   score: 0.70232594   abstract: SPICE is widely used for transistor-level circuit simulation. However, with the growing complexity of the VLSI at nano-scale, the traditional SPICE simulator has become inefficient to provide accurate verifications. This thesis tries to accelerate transistor-level simulation on multi/many-core systems, and we will solve 3 problems: 1) develop a parallel sparse LU factorization algorithm for circuit simulation, 2) implement the matrix solver on GPU to further accelerate the solver, 3) develop a circuit partitioning based parallel simulation approach on distributed machines to obtain better scalability. The experimental results show that the proposed parallel LU factorization algorithm effectively accelerates the matrix solver for circuit simulation on both CPU and GPU.\n",
      "\n",
      "3. id: 5390bded20f70186a0f4819c   score: 0.5841526   abstract: The sparse matrix solver is a critical component in circuit simulators. Some researches have developed GPU-based LU factorization approaches to accelerate the sparse solver. But the performance of these solvers is constrained by the irregularities of sparse matrices. This work investigates the nonzero patterns and memory access patterns in sparse LU factorization, and explores the common features to give guidelines on the improvements of the GPU solvers. We further propose a crisscross blocked implementation on GPUs. The proposed method attains average speedups of 1.68× compared with the unblocked method and 2.2× compared with 4-threaded PARDISO, for circuit matrices.\n",
      "\n",
      "4. id: 5390881720f70186a0d8093d   score: 0.45519865   abstract: This paper provides a comprehensive study and comparison of two state-of-the-art direct solvers for large sparse sets of linear equations on large-scale distributed-memory computers. One is a multifrontal solver called MUMPS, the other is a supernodal solver called superLU. We describe the main algorithmic features of the two solvers and compare their performance characteristics with respect to uniprocessor speed, interprocessor communication, and memory requirements. For both solvers, preorderings for numerical stability and sparsity play an important role in achieving high parallel efficiency. We analyse the results with various ordering algorithms. Our performance analysis is based on data obtained from runs on a 512-processor Cray T3E using a set of matrices from real applications. We also use regular 3D grid problems to study the scalability of the two solvers.\n",
      "\n",
      "5. id: 5390b5c620f70186a0f0806a   score: 0.44326904   abstract: Sparse solver has become the bottleneck of SPICE simulators. There has been few work on GPU-based sparse solver because of the high data-dependency. The strong data-dependency determines that parallel sparse LU factorization runs efficiently on shared-memory computing devices. But the number of CPU cores sharing the same memory is often limited. The state of the art Graphic Processing Units (GPU) naturally have numerous cores sharing the device memory, and provide a possible solution to the problem. In this paper, we propose a GPU-based sparse LU solver for circuit simulation. We optimize the work partitioning, the number of active thread groups, and the memory access pattern, based on GPU architecture. On matrices whose factorization involves many floating-point operations, our GPU-based sparse LU factorization achieves 7.90x speedup over 1-core CPU and 1.49x speedup over 8-core CPU. We\n",
      "\n",
      "6. id: 5390a80f20f70186a0e96cb5   score: 0.4082178   abstract: For the solution of sparse linear systems from circuit simulation whose coefficient matrices include a few dense rows and columns, a parallel iterative algorithm with distributed Schur complement preconditioning is presented. The parallel efficiency of the solver is increased by transforming the equation system into a problem without dense rows and columns as well as by exploitation of parallel graph partitioning methods. The costs of local, incomplete LU decompositions are decreased by fill-in reducing reordering methods of the matrix and a threshold strategy for the factorization. The efficiency of the parallel solver is demonstrated with real circuit simulation problems on PC clusters.\n",
      "\n",
      "7. id: 539099b320f70186a0e19586   score: 0.4082178   abstract: For the solution of sparse linear systems from circuit simulation whose coefficient matrices include a few dense rows and columns, a parallel iterative algorithm with distributed Schur complement preconditioning is presented. The parallel efficiency of the solver is increased by transforming the equation system into a problem without dense rows and columns as well as by exploitation of parallel graph partitioning methods. The costs of local, incomplete LU decompositions are decreased by fill-in reducing reordering methods of the matrix and a threshold strategy for the factorization. The efficiency of the parallel solver is demonstrated with real circuit simulation problems on PC clusters.\n",
      "\n",
      "8. id: 5390a79f20f70186a0e9227c   score: 0.40627292   abstract: We describe a parallel computing approach for large-scale SPICE-accurate circuit simulation, which is based on a new strategy for the parallel preconditioned iterative solution of circuit matrices. This strategy consists of several steps, including singleton removal, block triangular form (BTF) reordering, hypergraph partitioning, and a block Jacobi pre-conditioner. Our parallel implementation makes use of a mixed load balance, employing a different parallel partition for the matrix load and solve. Based on message-passing, our circuit simulation code was originally designed for large parallel computers, but for the purposes of this paper we demonstrate that it also gives good parallel speedup in modern multi-core environments. We show that our new parallel solver outperforms a serial direct solver, a parallel direct solver and an alternative iterative solver on a set of circuit test pro\n",
      "\n",
      "9. id: 5390b3ae20f70186a0ef450f   score: 0.39018536   abstract: In this paper, we describe a parallel direct solver for general sparse systems of linear equations that has recently been included in the Watson Sparse Matrix Package (WSMP) [7]. This solver utilizes both shared- and distributed- memory parallelism in the same program and is designed for a hierarchical parallel computer with network-interconnected SMP nodes. We compare the WSMP solver with two similar well known solvers: MUMPS [2] and Super_LUDist [10]. We show that the WSMP solver achieves significantly better performance than both these solvers based on traditional algorithms and is more numerically robust than Super_LUDist. We had earlier shown [8] that MUMPS and Super_LUDist are amongst the fastest distributed-memory general sparse solvers available.\n",
      "\n",
      "10. id: 5390ad0720f70186a0ebb3b9   score: 0.34059483   abstract: In this paper, we present a fully parallel transistor level full-chip circuit simulation tool with SPICE-accuracy for general circuit designs. The proposed overlapping domain decomposition approach partitions the circuit into a linear subdomain and multiple non-linear subdomains based on circuit non-linearity and connectivity. Parallel iterative matrix solver is used to solve the linear domain while non-linear subdomains are parallelly distributed into different processors topologically and solved by direct solver. To achieve maximum parallelism, device model evaluation is done parallelly. Parallel domain decomposition technique is used to iteratively solve the different partitions of the circuit and ensure convergence. Orders of magnitude speedup over SPICE is observed for sets of large-scale circuit designs on up to 64 processors.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707211\n",
      "index                                        55323b8c45cec66b6f9da1fd\n",
      "title               Effect of Multiscale PCA De-noising in ECG Bea...\n",
      "authors                            Emina Alickovic, Abdulhamit Subasi\n",
      "year                                                           2015.0\n",
      "venue                        Circuits, Systems, and Signal Processing\n",
      "references          558b5340612c41e6b9d493e2;5390a79f20f70186a0e91...\n",
      "abstract            Current trends in clinical applications demand...\n",
      "id                                                            1707211\n",
      "clustered_labels                                                    2\n",
      "Name: 1707211, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b8d620f70186a0f2ac86   score: 0.9703525   abstract: The objective of this study is to develop an algorithm to detect and classify six types of electrocardiogram (ECG) signal beats including normal beats (N), atrial pre-mature beats (A), right bundle branch block beats (R), left bundle branch block beats (L), paced beats (P), and pre-mature ventricular contraction beats (PVC or V) using a neural network classifier. In order to prepare an appropriate input vector for the neural classifier several pre-processing stages have been applied. Initially, a signal filtering method is used to remove the ECG signal baseline wandering. Continuous wavelet transform is then applied in order to extract features of the ECG signal. Next, principal component analysis is used to reduce the size of the data. A well-known neural network architecture called the multi-layered perceptron neural network is then utilized as the final classifier to classify each ECG\n",
      "\n",
      "2. id: 5390b3ae20f70186a0ef519e   score: 0.9666631   abstract: It is well known that the electrocardiogram (ECG) signal has crucial information to detect heart disease. However, development of automated heart disease diagnosis system is known as nontrivial problem since the ECG signal is different from patient to patient, measured time and environmental conditions. To overcome this problem, context-aware computing based adaptable heart disease diagnosis algorithm is proposed. Before diagnosis step, patient signal type recognition module groups various types of signal characteristics for patients and genetic algorithm (GA) finds optimal set of preprocessing, feature extraction and classifier for each group of signal types. Evaluation results using MIT-BIH database showed that various types of signal type require different preprocessing, feature extraction method and classifier and the test results showed 98.36% of classification accuracy for best opt\n",
      "\n",
      "3. id: 5390ab8820f70186a0eb1736   score: 0.95869267   abstract: Automatic classification of electrocardiogram (ECG) signals is vital for clinical diagnosis of heart disease. This paper investigates the design of an efficient system for recognition of the premature ventricular contraction from the normal beats and other heart diseases. This system includes three main modules: denoising module, feature extraction module and classifier module. In the denoising module, it is proposed the stationary wavelet transform for noise reduction of the electrocardiogram signals. In the feature extraction module a proper combination of the morphological-based features and timing interval-based features are proposed. As the classifier, several supervised classifiers are investigated; they are: a number of multi-layer perceptron neural networks with different number of layers and training algorithms, support vector machines with different kernel types, radial basis f\n",
      "\n",
      "4. id: 53909ed120f70186a0e2f7a4   score: 0.9449947   abstract: A switchable scheme is proposed to discriminate different types of electrocardiogram (ECG) beats based on independent component analysis (ICA). The RR-interval serves as an indicator for the scheme to select between the longer (1.0s) and the shorter (0.556s) data samples for the following processing. Six ECG beat types, including 13900 samples extracted from 25 records in the MIT-BIH database, are employed in this study. Three conventional statistical classifiers are employed to testify the discrimination power of this method. The result shows a promising accuracy of over 99%, with equally well recognition rates throughout all types of ECG beats. Only 27 ICA features are needed to attain this high accuracy, which is substantially smaller in quantity than that in the other methods. The results prove the capability of the proposed scheme in characterizing heart diseases based on ECG signal\n",
      "\n",
      "5. id: 5390b36120f70186a0ef1026   score: 0.937669   abstract: In order to improve the classification results on electrocardiogram (ECG) signals, Optimal Discrimination Plane (ODP) approach is introduced. Features are extracted from time-series data using the ODP that is developed by Fisher’s criterion method. ECG patterns are projected onto two orthogonal vectors, and the two-dimensional feature vectors are used as features to represent the ECG segments. Two types of ECG signals are obtained from MIT-BIH database, namely normal sinus rhythm and premature ventricular contraction. A quadratic discriminant function based classifier and a threshold vector based classifier are employed to classify these ECG beats, respectively. The results show the proposed technique can achieve better classification results compared to that of some recently published on arrhythmia classification.\n",
      "\n",
      "6. id: 5390b56a20f70186a0f06bd6   score: 0.93318766   abstract: The aim of this paper is twofold. First, we present a thorough experimental study to show the superiority of the generalization capability of the support vector machine (SVM) approach in the automatic classification of electrocardiogram (ECG) beats. Second, we propose a novel classification system based on particle swarm optimization (PSO) to improve the generalization performance of the SVM classifier. For this purpose, we have optimized the SVM classifier design by searching for the best value of the parameters that tune its discriminant function, and upstream by looking for the best subset of features that feed the classifier. The experiments were conducted on the basis of ECG data from the Massachusetts Institute of Technology-Beth Israel Hospital (MIT-BIH) arrhythmia database to classify five kinds of abnormal waveforms and normal beats. In particular, they were organized so as to t\n",
      "\n",
      "7. id: 5390a25820f70186a0e60a47   score: 0.93045825   abstract: A novel method is proposed in this paper for the feature extraction of electrocardiogram (ECG). Different with other algorithms, the proposed method utilizes Independent Component Analysis (ICA) and wavelet transform to get an ensemble feature composed of ICA-based features and the QRS complex width feature. The QRS complex is the most characteristic waveform of an ECG signal and its width has been a diagnostic criterion of cardiac arrhythmia. Therefore, our ensemble feature consisting of QRS complex width would provide much more information on cardiac diseases than other methods. The formed ensemble feature is fed into an artificial neural networks classifier. To validate the proposed method, we applied it to the MIT-BIH arrhythmia database. The experimental results have shown the effectiveness of the proposed method.\n",
      "\n",
      "8. id: 5390ba0a20f70186a0f33f24   score: 0.9265699   abstract: An important tool for the heart disease diagnosis is the analysis of electrocardiogram (ECG) signals, since the non-invasive nature and simplicity of the ECG exam. According to the application, ECG data analysis consists of steps such as preprocessing, segmentation, feature extraction and classification aiming to detect cardiac arrhythmias (i.e., cardiac rhythm abnormalities). Aiming to made a fast and accurate cardiac arrhythmia signal classification process, we apply and analyze a recent and robust supervised graph-based pattern recognition technique, the optimum-path forest (OPF) classifier. To the best of our knowledge, it is the first time that OPF classifier is used to the ECG heartbeat signal classification task. We then compare the performance (in terms of training and testing time, accuracy, specificity, and sensitivity) of the OPF classifier to the ones of other three well-know\n",
      "\n",
      "9. id: 5390b24420f70186a0ee7ef1   score: 0.9242787   abstract: This study proposes a simple and effective method, termed Principal Component Analysis (PCA) method, to analyze ECG signals for effectively determining the heartbeat case. This method is easily performed and does not require complex mathematic computations. The average time required for processing a 30-minute long of ECG data is less than 1 minute, and the required maximum memory is only about 10 MB. The ECG records available in the MIT-BIH arrhythmia database are utilized to illustrate the effectiveness of the proposed method. The experiment results show the total classification accuracy was approximately 90.85%.\n",
      "\n",
      "10. id: 539098dc20f70186a0e0db78   score: 0.9168571   abstract: In this paper, we present a new system for the classification of electrocardiogram (ECG) beats by using a fast least square support vector machine (LSSVM). Five feature extraction methods are comparatively examined in the 15-dimensional feature space. The dimension of the each feature set is reduced by using dynamic programming based on divergence analysis. After the preprocessing of ECG data, six types of ECG beats obtained from the MIT-BIH database are classified with an accuracy of 95.2% by the proposed fast LSSVM algorithm together with discrete cosine transform. Experimental results show that not only the fast LSSVM is faster than the standard LSSVM algorithm, but also it gives better classification performance than the standard backpropagation multilayer perceptron network.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1690477\n",
      "index                                        559251ec0cf28b1a968ffb0e\n",
      "title               Blind Students' Challenges in Social Media Com...\n",
      "authors                                                   Rakesh Babu\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Online Pedagogy and C...\n",
      "references          5390bd1520f70186a0f4314e;53909f8220f70186a0e3c...\n",
      "abstract            Social Networking Sites (SNS) are increasingly...\n",
      "id                                                            1690477\n",
      "clustered_labels                                                    0\n",
      "Name: 1690477, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ac1820f70186a0eb3437   score: 0.95013416   abstract: Jay Cross's latest book, Working Smarter: Informal Learning in the Cloud, is his second title about informal learning. Acquiring knowledge through informal contacts and casual conversation, as Cross indicates, is as old as the campfire. However, the rise of social networking technology and Web 2.0 tools have worked as a force multiplier, greatly enhancing the power of people to acquire knowledge.\n",
      "\n",
      "2. id: 5390b72e20f70186a0f215ac   score: 0.9291838   abstract: Learning through Social Network Sites, like Facebook, is a trending topic nowadays, especially since many studies report that students spend more time online in this type of sites. In this article, a case study of a collaborative learning activity with university professors is presented in the context of an online course. From a CSCL perspective, we present results of interaction comparing Social Networks Sites to traditional online forums within a Learning Management System, and the perception of participants in using Facebook for learning activities.\n",
      "\n",
      "3. id: 5390a7f520f70186a0e934a6   score: 0.9280184   abstract: Social networking tools have been enthusiastically heralded as a means to support different learning types and innovative pedagogical practices. They have also been recognized as potential tools to promote informal learning. In this paper we describe work carried out using the synergy of social web tools, learning models and innovative pedagogical practices across a Masters Degree Course. Findings suggest that the use of these tools as a means to distribute an open and flexible learning environment fosters informal interactions and such interactions are perceived by students to have a significant impact over their formal learning outcomes.\n",
      "\n",
      "4. id: 5390baa120f70186a0f3928c   score: 0.8923751   abstract: This paper explores the role of Social Networking Sites (SNS) in e-learning by investigating the attitudes, behaviors, knowledge and views of computing students towards the use of SNS in e-learning. Data was collected from an online survey and interviews, and analyzed to discover the practices, tendencies and the current status of the use of SNS in e-learning as well as how these can be improved. Major factors that facilitate the usage of SNS in e-learning were identified as collaboration, communication, resource sharing, social influence, usefulness and ease of use. Facebook was identified as the most popular SNS. The role of SNS in e-learning is supportive and important. Although the participants were computing students with a high level of IT literacy, and were interested in using SNS for e-learning, only a few were frequently using SNS for e-learning. Reasons for this minimal utiliza\n",
      "\n",
      "5. id: 5390c04520f70186a0f565f5   score: 0.82796735   abstract: In this paper, the authors reviewed the empirical studies on social networking sites SNSs, especially those focused on adopting SNSs for students' learning, published in SSCI journals from 2004 to 2013. It was found that the number of articles has significantly increased, particularly after 2009. Among the 76 published papers, most studies were conducted in higher education, as well as in education domain. Furthermore, the qualitative research method was used more in SNSs and e-learning research. The findings in this study may provide reference and directions for future research, and help teachers to consider using SNSs in their online courses.\n",
      "\n",
      "6. id: 5390bda020f70186a0f473bd   score: 0.7676898   abstract: The goal of this study was to explore the benefits associated with incorporating social networking functionalities within an adult's learning experience. Which social networking sites do they use most frequently? What are the intended purposes of their use? What kind of experience do adult learners have when utilizing social networking services? Why or why not are adult learners using the services provided by the social networking sites? This paper reports a survey conducted among adult learners to identify the answers for the above questions. We believe that the findings from this survey will contribute to understanding future learning design expectations and arrangements.\n",
      "\n",
      "7. id: 5390ba0a20f70186a0f32876   score: 0.7345006   abstract: This study examines the usability challenges and emotional reactions of blind college students in their attempts to access online educational materials and to communicate with colleagues through online technologies. A case study approach was adopted. Five students were interviewed regarding their online learning experiences using Blackboard, a popular Course Management System. Analysis of the interviews revealed that Blackboard was poorly accessible to the blind students, which affected achieving their academic goals. The study also showed that the blind students were motivated and optimistic of their successes despite their frustrations and feelings of marginalization. The study suggests that academic administrators and database designers work jointly with adaptive software developers in developing enhanced user interfaces to ensure universal access and usability of online technologies \n",
      "\n",
      "8. id: 5390ba0a20f70186a0f334c2   score: 0.6940954   abstract: Despite growing interest in knowledge sharing processes in informal spaces, there is a paucity of research on technology-mediated learning in these spaces. Yet the surge in student use of Social Media-enabled phones presents tremendous opportunities for augmenting learning in privileged, authoritative spaces. This study investigated the potential of Facebook-enabled mobiles to leverage learning in informal learning environments. Third Space Theory illuminated understanding of how students draw on potentially contradictory, multiple \"funds of knowledge\" in their meaning making and discourses. Twenty six students were interviewed to explore how they exchanged learning resources and collaborated on academic matters. Findings suggest that student appropriation of Facebook-enhanced phones enhances social learning, hones digital literacies, and affords the co-production of knowledge in learnin\n",
      "\n",
      "9. id: 5390ba0a20f70186a0f34c9b   score: 0.6694208   abstract: With over 800 million users worldwide, the global importance of Facebook as a social-networking platform is beyond doubt. This popularity, particularly among university-students, has encouraged research to explore ways in which social networking can be adapted into virtual learning environments. In particular, this study uses the think-aloud technique to explore university-students' use of and interaction with Facebook. Twenty-six Teesside University students who were also Facebook users took part in a think-aloud study. Seven major categories of experience emerged during the coding and categorisation process of the think-aloud data. Further analysis revealed that six fundamental psychological needs were each related to particular themes of user-experience. Overall, the results demonstrate that psychological needs are particular qualities of students' experience that are important in onl\n",
      "\n",
      "10. id: 5390a74f20f70186a0e8c008   score: 0.63578796   abstract: Although considerable attention in the CSCL community has been on distributed-, Web-, or distance-learning applications, there is evidence suggesting that much of learning, particularly in open-ended problem-solving activities based on tacit information, does not occur in isolation but in face-to-face settings. This has led our research to explore ways to develop technologies and media that enhance participation, collaboration, and learning in face-to-face, copresent settings. This paper explores the history of our research on developing such technologies in the context of our Envisionment and Discovery Collaboratory at the Center for LifeLong Learning & Design at the University of Colorado at Boulder, and discusses my research on interface design to support learning and participation in collaborative settings.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1703874\n",
      "index                                        55323b1f45cec66b6f9d9604\n",
      "title               A variable-gain superregenerative amplifier co...\n",
      "authors             Fernando Rangel De Sousa, Roddy Alexander Rome...\n",
      "year                                                           2015.0\n",
      "venue                Analog Integrated Circuits and Signal Processing\n",
      "references          5390a88c20f70186a0e99cbb;5390a74f20f70186a0e8bbdf\n",
      "abstract            In this paper, a variable-gain amplifier (VGA)...\n",
      "id                                                            1703874\n",
      "clustered_labels                                                    2\n",
      "Name: 1703874, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 53908bde20f70186a0dc6fdd   score: 0.858244   abstract: Abstract A variable gain amplifier with a gain range of 15dB to ,4OdB and 80MHz bandwidth is describe bed here. The circuit is built in a standard CMOS process. A MOS device, operated in the linear region, is used as a shunt feedback element to vary the gain. The circuit embodies an inverse relationship between the gain and the control voltage. The circuit dissipates about 3OmW of power. This circuit has been intended to be used in a PRML read channel chip but can be adapted for other purposes also.\n",
      "\n",
      "2. id: 539087ef20f70186a0d6c2c0   score: 0.8362874   abstract: A variable gain amplifier with linear gain control hasbeen implemented in a commercially available 8 GHz 1.2 µmBiCMOS process. The gain adjustment linearization is based onforcing a linearly controllable current through diode-connectedtransistors, thus generating an internal logarithmic controlvoltage for the Gilbert-type variable gain cell. A Cherry-Hoppertype gain stage is used to provide most of the available gain.Thus, the maximum differential gain is 10 dB with over 1 GHzbandwidth and −6.9 dBm input −-1 dBcompression power. Gain adjustment range of 50 dB at 200 MHzand 38 dB at 960 MHz is reported. The chip area is 1.15 ×2.15 mm and it consumes 40 mA from a 5 V supply.\n",
      "\n",
      "3. id: 5390893e20f70186a0d93f68   score: 0.6794978   abstract: A new methodology to develop variable gain amplifiers is developed. The methodology is based on a feedback loop to generate the exponential characteristic, which is required for VGA circuits. The proposed idea is very suitable for applications that require very low power consumption, and as an application, a new current mode variable gain amplifier will be shown. The gain is adapted via a current signal ranges from −7.5 μA to &plus;6.5 μA. Pspice simulations based on Mietec 0.5 μm CMOS technology show that the gain can be varied over a range of 29.5 dB, with bandwidth of 3 MHz at maximum gain value. The circuit operates between ±1.5 V and consumes an average amount of power less than 495 μW.\n",
      "\n",
      "4. id: 5390c04520f70186a0f57b8a   score: 0.6570985   abstract: A variable-gain amplifier with very low power consumption and wide tuning range is presented. The operational principle of this unique structure is discussed, its most important formulas are derived and its outstanding performance is verified by simulation in TSMC 0.18-μm N-well CMOS fabrication process. Owing to the novel zero-pole repositioning technique, the proposed circuit demonstrates very high frequency bandwidth of 79 MHz while drawing only 0.52 mA from 1.8 V power supply. The interesting results such as a very small core area of about 0.0025 mm2 as well as a wide linear-in-dB and constant-bandwidth tuning range of 68.2 dB along with a very low power consumption of 0.95 mW are achieved utilizing standard CMOS technology. The stability of the proposed VGA is verified through transient sinusoidal response analysis. Full process, voltage and temperature (PVT) variation analysis of t\n",
      "\n",
      "5. id: 539095ba20f70186a0df0ea5   score: 0.6149723   abstract: A variable gain amplifier (VGA) is designed for a GSM subsampling receiver. The VGA is implemented in a 0.35-µm CMOS process and approximately occupies 0.64 mm2. It operates at an IF frequency of 246 MHz. The VGA provides a 60-dB digitally controlled gain range in 2-dB steps. The overall gain accuracy is less than 0.3 dB. The current is 9 mA at 3 V supply. The noise figure at maximum gain is 8.7 dB. The IIP3 is --4 dBm at minimum gain, while the OIP3 is -- 1 dBm at maximum gain. The group delay is 1.5 ns across 5-MHz bandwidth.\n",
      "\n",
      "6. id: 5390bb7b20f70186a0f4104e   score: 0.59854686   abstract: A new circuit architecture for broadband digitally controlled variable gain amplifier (VGA) is introduced in this paper. The gain of the VGA is controlled precisely by using a resistor ladder attenuator and a closed-loop fine gain control block together. The bandwidth of the VGA is extended by applying a compensation technique in the fine gain control block. Implemented in 0.13-μm CMOS technology, the proposed VGA demonstrates a decibel-linear gain range of 24 dB (0---24 dB) with a gain step of 0.1 dB, a gain error\n",
      "\n",
      "7. id: 5390be6620f70186a0f4c943   score: 0.59425694   abstract: Communication systems require a wide gain range. For example the code-division multiple access system (CDMA) requires more than 80 dB of gain range so that, many variable gain amplifiers (VGAs) must be used, resulting in high power consumption and low linearity because of VGA non-linearity factors. In this paper, a one-stage VGA in 0.18 μm technology is presented. The VGA based on the class AB power amplifier is designed and simulated for a high linearity and an 80 dB tuning range. For the linear-in-decibel tuning range, transistors in sub-threshold region is used. The current control circuit of the VGA changes gain continuously from 驴68 to 18 dB at 0.5 GHz and 驴60 to 20 dB at 1 GHz with gain error of less than 2 dB. The power consumption enjoys a highest value about 13.5 mW in the maximum gain and P1dB is also about 驴3.4 dBm at 0.5 GHz and 2.2 dBm at 1 GHz.\n",
      "\n",
      "8. id: 53909e8b20f70186a0e2e9fc   score: 0.5294763   abstract: A CMOS intermediate-frequency (IF) variable-gain amplifier (VGA) is presented in this paper. A transconductance linearization scheme is proposed for the VGA core based on a signal-subtracting structure to achieve low distortion. Temperature-independent decibel-linear gain control characteristic is achieved by an exponential voltage generator based on transfer characteristics of differential pair. The whole VGA, including a highly-linear output stage, is fabricated in 0.25 驴m CMOS technology. Measurements show that the VGA provides a total gain control range of 43 dB with less than 1.2 dB error over 0---80°C, and a constant 3-dB bandwidth of 100 MHz. The third-order intermodulation (IM3) distortion at differential output of 2 VPP is better than 驴55 dB. The VGA dissipates 22.6 mA averagely from 3.3 V supply, and occupies approximately 0.53 mm2.\n",
      "\n",
      "9. id: 539098dc20f70186a0e0d4e4   score: 0.5200698   abstract: An intermediate frequency (IF) variable gain amplifier (VGA) with exponential gain control for a radio receiver is fabricated in 0.25-μm CMOS technology. The techniques to improve the bandwidth and to reduce temperature dependence of gain are described. The complete VGA is composed of two stages of linearized transconductance VGA and three stages of fixed gain amplifier (FGA). The complete VGA provides a continuous 10 dB to 76.5 dB gain control range, an IIP3 of -11.5 dBm and an NF of 15 dB at 40 MHz.\n",
      "\n",
      "10. id: 5390b3da20f70186a0ef6ea1   score: 0.4857522   abstract: In this paper, a low power Variable Gain Amplifier (VGA) circuit with an approximation to exponential gain characteristic is presented. It is achieved using current mirrors to generate appropriate current signals to bias the input stage of the VGA circuit working in triode region, and the output stage working in saturation region, respectively. The VGA circuit presented herein comes with a 549@mW maximum power consumption given a 1.8V supply. Most important of all, it has a linear-in-dB 48-dB dynamic gain range per stage. The effect of the input trasconductance and the output resistance on the linearity of gain control is also discussed. This circuit is fabricated using a 0.18@mm standard CMOS process with a core area of 0.0045mm^2.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1728679\n",
      "index                                        55323d6d45cec66b6f9de47e\n",
      "title               Rigid body interaction for large-scale real-ti...\n",
      "authors                                                Timo Kellomäki\n",
      "year                                                           2015.0\n",
      "venue               International Journal of Computer Games Techno...\n",
      "references          558b2b3d612c41e6b9d457ad;53908bfb20f70186a0dc9...\n",
      "abstract            Simulating large amounts of water in real time...\n",
      "id                                                            1728679\n",
      "clustered_labels                                                    1\n",
      "Name: 1728679, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 539099a220f70186a0e17ea5   score: 0.91505283   abstract: We present a new method for the efficient simulation of large bodies of water, especially effective when three-dimensional surface effects are important. Similar to a traditional two-dimensional height field approach, most of the water volume is represented by tall cells which are assumed to have linear pressure profiles. In order to avoid the limitations typically associated with a height field approach, we simulate the entire top surface of the water volume with a state of the art, fully three-dimensional Navier-Stokes free surface solver. Our philosophy is to use the best available method near the interface (in the three-dimensional region) and to coarsen the mesh away from the interface for efficiency. We coarsen with tall, thin cells (as opposed to octrees or AMR), because they maintain good resolution horizontally allowing for accurate representation of bottom topography.\n",
      "\n",
      "2. id: 558b08fe612c41e6b9d40ead   score: 0.73545164   abstract: Although much progress was made in computer graphics for water simulation, large-scale complex water simulation is still a great challenge. This may be due to that the complicated physical mechanism, such as water dynamics and the complex interactions between water and objects, hinders researchers from efficient and realistic modeling and rendering with fine details.\n",
      "\n",
      "3. id: 53909fca20f70186a0e4438f   score: 0.68626493   abstract: We present a real-time method to simulate and render the turbulent flow and splash of stream water over irregular terrain and with dynamic rigid objects. In order to achieve real-time frame rates, we adopt a GPU-based particle system to perform the simulation, which can simulate interaction between water and dynamic rigid object in real time. We use 2D metaballs with billboards to represent the 3D water surface. Our billboard rendering method does not possess the traditional clipping artifact. Although our proposed method is not a direct physical simulation, the results are empirically plausible and efficient, and especially suitable for interactive graphics application such as computer games.\n",
      "\n",
      "4. id: 53909a9320f70186a0e22d2d   score: 0.54598176   abstract: As computer gets more faster, developers apply more complex physics theory to games. But water simulation in games is limited to render realistically. Water in games flows periodically and Floating object doesn't move depending on flow direction. Because it takes much time to simulate water, many physic methods cannot be applied to a game. This paper suggests a real time water simulation method by precomputed algorithm. This suggestion ensures real time and can interact floating objects realistically.\n",
      "\n",
      "5. id: 5390b19020f70186a0ee0bf4   score: 0.5352199   abstract: This dissertation describes the wave particles technique for simulating water surface waves and two way fluid-object interactions for real-time applications, such as video games.Water exists in various different forms in our environment and it is important to develop necessary technologies to be able to incorporate all these forms in real-time virtual environments. Handling the behavior of large bodies of water, such as an ocean, lake, or pool, has been computationally expensive with traditional techniques even for offline graphics applications, because of the high resolution requirements of these simulations.A significant portion of water behavior for large bodies of water is the surface wave phenomenon. This dissertation discusses how water surface waves can be simulated efficiently and effectively at real-time frame rates using a simple particle system that we call \"wave particles.\" T\n",
      "\n",
      "6. id: 53909a0220f70186a0e1eef2   score: 0.491639   abstract: We present an optimization of the water column-based height-field approach of water simulation by reducing memory footprint and promoting parallel implementation. The simulation still provides three-dimensional fluid animation suitable for water flowing on irregular terrains, intended for interactive applications. Our approach avoids the creation and storage of redundant virtual pipes between columns of water, and removes output dependency for the parallel implementation. We show a GPU implementation of the proposed method that runs at near interactive frame rates with rich lighting effects on the water surface, making it efficient for water animation on natural terrains for Computer Graphics.\n",
      "\n",
      "7. id: 5390b7fe20f70186a0f26efc   score: 0.4402286   abstract: We present a real-time technique to render realistic water volumes. Water volumes are represented as the space enclosed between a ground heightfield and an animable water surface heightfield. This representation allows the application of recent GPU-based heightfield rendering algorithms. Our method is a simplified raytracing approach which correctly handles reflections and refractions and allows us to render complex effects such as light absorption, refracted shadows and refracted caustics. It runs at high framerates by exploiting the power of the latest graphic cards, and could be used in real-time applications like video games, or interactive simulation.\n",
      "\n",
      "8. id: 5390b36120f70186a0ef0f87   score: 0.41667795   abstract: Real-time water rendering has always posed a challenge to developers. Most algorithms concentrate on rendering small bodies of water such as pools and rivers. In this paper, we proposed a real-time rendering method for large water surfaces, such as oceans. This algorithm harnesses both the PC and GPU’s processing power to deliver improved computing efficiency while, at the same time, realistically and efficiently simulating a large body of water. The frustum-based algorithm accomplishes this by representing a smooth water surface as a height value of the viewer, since surface size can be fluidly calculated given the camera frustum position. This algorithm has numerous potential applications in both the gaming and the movie industry. Experimental results show a marked improvement in computing power and increased realism in large surface areas.\n",
      "\n",
      "9. id: 5390adfd20f70186a0ec53b3   score: 0.35309353   abstract: We present a hybrid water simulation method that combines grid based and particles based approaches. Our specialized shallow water solver can handle arbitrary underlying terrain slopes, arbitrary water depth and supports wet-dry regions tracking. To treat open water scenes we introduce a method for handling non-reflecting boundary conditions. Regions of liquid that cannot be represented by the height field including breaking waves, water falls and splashing due to rigid and soft bodies interaction are automatically turned into spray, splash and foam particles. The particles are treated as simple non-interacting point masses and they exchange mass and momentum with the height field fluid. We also present a method for procedurally adding small scale waves that are advected with the water flow. We demonstrate the effectiveness of our method in various test scene including a large flowing ri\n",
      "\n",
      "10. id: 5390b7fe20f70186a0f26ffa   score: 0.33014715   abstract: Large bodies of water are an integral part of nature and, thus, are of high interest for interactive 3D applications, e.g., computer games and virtual environments. We present a new scheme for real-time wave simulation in large-scale water environments with physics-based object interaction. In addition to a fast and realistic liquid representation, our method focuses on the creation of plausible detailed waves caused by moving boats. We expand the well-known wave equation by applying it to moving grids to simulate an apparently limitless body of water. Additionally, we present a fast, particle-based boat simulation, which is coupled to water simulation. Importantly, most parts of our method can be implemented efficiently on GPUs. We demonstrate the visual realism and performance of our approach with several experiments using different boats and other floating objects, achieving high fram\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1708204\n",
      "index                                        55323b9a45cec66b6f9da3d4\n",
      "title                         Information spreading in dynamic graphs\n",
      "authors             Andrea Clementi, Riccardo Silvestri, Luca Trev...\n",
      "year                                                           2015.0\n",
      "venue                                           Distributed Computing\n",
      "references          5390a1d420f70186a0e57de9;539087fe20f70186a0d74...\n",
      "abstract            We present a general approach to study the flo...\n",
      "id                                                            1708204\n",
      "clustered_labels                                                    3\n",
      "Name: 1708204, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390b68720f70186a0f1c4f6   score: 0.99813974   abstract: We present a general approach to study the flooding time (a measure of how fast information spreads) in dynamic graphs (graphs whose topology changes with time according to a random process). We consider arbitrary ergodic Markovian dynamic graph process, that is, processes in which the topology of the graph at time t depends only on its topology at time t-1 and which have a unique stationary distribution. The most well studied models of dynamic graphs are all Markovian and ergodic. Under general conditions, we bound the flooding time in terms of the mixing time of the dynamic graph process. We recover, as special cases of our result, bounds on the flooding time for the random trip model and the random path models; previous analysis techniques provided bounds only in restricted settings for such models. Our result also provides the first bound for the random waypoint model (which is tight\n",
      "\n",
      "2. id: 5390b9d520f70186a0f30dfb   score: 0.9600627   abstract: 1. Introduction. The simplest communication mechanism that implements the broadcast operation is the flooding protocol, according to which the source node is initially informed, and, when a not informed node has an informed neighbor, then it becomes informed at the next time step. In this paper we study the flooding completion time in the case of dynamic graphs with arbitrary degree sequence, which are a special case of random evolving graphs. A random evolving graph is a sequence of graphs (Gt)t≥0 with the same set of nodes, in which, at each time step t, the graph Gt is chosen randomly according to a probability distribution over a specified family of graphs. A special case of random evolving graph is the edge-Markovian model (see the definition below), for which tight upper bounds on the flooding completion time have been obtained by using a so-called reduction lemma, which intuitivel\n",
      "\n",
      "3. id: 5390c04520f70186a0f56f02   score: 0.9532751   abstract: This paper addresses the flooding problem in dynamic graphs, where flooding is the basic mechanism in which every node becoming aware of a piece of information at step t forwards this information to all its neighbors at all forthcoming steps t^'t. We show that a technique developed in a previous paper, for analyzing flooding in a Markovian sequence of Erdos-Renyi graphs, is robust enough to be used also in different contexts. We establish this fact by analyzing flooding in a sequence of graphs drawn independently at random according to a model of random graphs with given expected degree sequence. In the prominent case of power-law degree distributions, we prove that flooding takes almost surely O(logn) steps even if, almost surely, none of the graphs in the sequence is connected. In the general case of graphs with an arbitrary degree sequence, we prove several upper bounds on the floodin\n",
      "\n",
      "4. id: 5390a55520f70186a0e796fe   score: 0.9512329   abstract: We consider Mobile Ad-hoc NETworks (MANETs) formed by n nodes that move independently at random over a finite square region of the plane. Nodes exchange data if they are at distance at most r within each other, where r 0 is the node transmission radius . The flooding time is the number of time steps required to broadcast a message from a source node to every node of the network. Flooding time is an important measure of the speed of information spreading in dynamic networks. We derive a nearly-tight upper bound on the flooding time which is a decreasing function of the maximal velocity of the nodes. It turns out that, when the node velocity is \"sufficiently\" high, even if the node transmission radius r is far below the connectivity threshold , the flooding time does not asymptotically depend on r . So, flooding can be very fast even though every snapshot (i.e. the static random geometric \n",
      "\n",
      "5. id: 5390a55520f70186a0e7aef3   score: 0.9361684   abstract: An edge-Markovian process with birth-rate p and death-rate q generates sequences of graphs (G0,G1,G2,…) with the same node set [n] such that Gt is obtained from Gt−1 as follows: if e ∉ E(Gt−1) then e ∈ E(Gt) with probability p, and if e ∈ E(Gt−1) then e ∉ E(Gt) with probability q. Clementi et al. (PODC 2008) analyzed thoroughly information dissemination in such dynamic graphs, by establishing bounds on their flooding time--flooding is the basic mechanism in which every node becoming aware of an information at step t forwards this information to all its neighbors at all forthcoming steps t∦ t. In this paper, we establish tight bounds on the complexity of flooding for all possible birth rates and death rates, completing the previous results by Clementi et al. Moreover, we note that despite its many advantages in term of simplicity and robustness, flooding suffers from its high bandwidth co\n",
      "\n",
      "6. id: 5390a5b020f70186a0e7c524   score: 0.91920847   abstract: Markovian evolving graphs [2] are dynamic-graph models where the links among a fixed set of nodes change during time according to an arbitrary Markovian rule. They are extremely general and they can well describe important dynamic-network scenarios.\n",
      "\n",
      "7. id: 5390b0ca20f70186a0ed9a01   score: 0.8889517   abstract: Markovian evolving graphs are dynamic-graph models where the links among a fixed set of nodes change during time according to an arbitrary Markovian rule. They are extremely general and they can well describe important dynamic-network scenarios. We study the speed of information spreading in the stationary phase by analyzing the completion time of the flooding mechanism. We prove a general theorem that establishes an upper bound on flooding time in any stationary Markovian evolving graph in terms of its node-expansion properties. We apply our theorem in two natural and relevant cases of such dynamic graphs. Geometric Markovian evolving graphs where the Markovian behaviour is yielded by n mobile radio stations, with fixed transmission radius, that perform independent random walks over a square region of the plane. Edge-Markovian evolving graphs where the probability of existence of any ed\n",
      "\n",
      "8. id: 558afc62612c41e6b9d3f9c2   score: 0.87234735   abstract: We define the spreading time in the SIS process as the average time between the start of the outbreak and the time that the number of infected nodes first reaches the average number of infected nodes in the metastable state. We show that the spreading time can be computed using a uniformised embedded Markov chain and give numerical results for the complete graph and the star graph. For the complete graph we derive, using the same method, an analytical expression for the spreading time starting from a single infected node. We show that the spreading time is only significantly larger for a single initially infected than when a few nodes are infected, and scales logarithmically as a function of the network size for a fixed fraction of infected nodes in the metastable state. We also show that mean-field methods predict that the spreading time in regular graphs is independent of the degree. F\n",
      "\n",
      "9. id: 5390ac1720f70186a0eb2849   score: 0.8499712   abstract: We consider a Mobile Ad-hoc NETwork (MANET) formed by n agents that move at speed V according to the Manhattan Random-Way Point model over a square region of side length L. The resulting stationary (agent) spatial probability distribution is far to be uniform: the average density over the \"central zone\" is asymptotically higher than that over the \"suburb\". Agents exchange data iff they are at distance at most R within each other. We study the flooding time of this MANET: the number of time steps required to broadcast a message from one source agent to all agents of the network in the stationary phase. We prove the first asymptotical upper bound on the flooding time. This bound holds with high probability, it is a decreasing function of R and V, and it is tight for a wide and relevant range of the network parameters (i.e. L, R and V). A consequence of our result is that flooding over the \n",
      "\n",
      "10. id: 55922c9d0cf2e74f816e3689   score: 0.8271311   abstract: We study the epidemic process yielded by the k-Flooding Protocol in geometric Mobile Ad-Hoc Networks. We consider n agents on a square performing independent random walks. At any time step, every active agent informs every non-informed agent which is within distance R from it. An informed agent is active only for k time steps. Initially, a source agent is informed and we look at the completion time of the protocol. We prove optimal bounds on the completion time of the process. Our method of analysis provides a clear picture of the geometric shape of the information spreading over the time. We analyze parsimonious-flooding on MANETs yielded by random-walks.We provide analytical bounds on its completion time. Such bounds are optimal for a wide range of the inputs.Departing from previous analysis, our technique determines the dynamic shape of the information-spreading process.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1717314\n",
      "index                                        55323c7245cec66b6f9dc1e7\n",
      "title               Privacy-aware access control for message excha...\n",
      "authors                Sushama Karumanchi, Anna Squicciarini, Dan Lin\n",
      "year                                                           2015.0\n",
      "venue                                      Telecommunications Systems\n",
      "references          558cb20b0cf2a44995718e8f;558fe4d4612c29c89cd7b...\n",
      "abstract            Vehicular ad hoc networks are a promising and ...\n",
      "id                                                            1717314\n",
      "clustered_labels                                                    0\n",
      "Name: 1717314, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 5390ba0a20f70186a0f32bee   score: 0.99929035   abstract: Vehicular Ad-hoc Networks are a promising and increasingly important paradigm. Their applications range from safety enhancement to mobile entertainment services. However, their deployment requires several security issues to be resolved, particularly, since they rely on insecure wireless communication. In this paper, we propose a cryptographic-based access control framework for vehicles to securely exchange messages in a controlled fashion by integrating moving object modeling techniques with cryptographic policies.\n",
      "\n",
      "2. id: 5390a55520f70186a0e7b77e   score: 0.99679   abstract: Vehicular Ad hoc Networks (VANETs) are promising approach for facilitating road safety, traffic management,and infotainment dissemination for drivers and passengers.However, it is subject to various malicious abuses and security attacks which hinder it from practical implementation.Effective and robust solutions for addressing security and privacy issues are critical for the wide-spread adoption of VANETs. In this paper, we propose an Elliptic Curve Digital Signature Algorithm (ECDSA) based message authentication in a VANET. The operation sequence of the proposed scheme is as follows: 1) Source vehicle generates private key and public key. 2) Public key is made available to all the vehicles in the VANET. 3) Source vehicle creates a hash of the message using secured hash algorithm. 4) Secured has his encrypted using private key in the source vehicle ands ends it to the destination vehicle\n",
      "\n",
      "3. id: 5390b1d220f70186a0ee22bb   score: 0.9926542   abstract: Vehicular Adhoc network (VANET) is a new form of Mobile Adhoc Network (MANET). It integrates mobile connectivity protocols to expedite data transfer between vehicles as well as between roadside equipment and available traffic in network. In VANET, Wireless device sends information to nearby vehicles, and messages can be transmit from one vehicle to another vehicle. Therefore, using VANET can increase safety and traffic optimization. Similar to other technology, in VANET there are some important and noticeable issues. One of the most important of them is Security. Since the network is open and accessible from everywhere in the VANET radio range, it is expected to be an easy target for malicious users. In this paper, I try to discuss security issues as one of the most important problems in Vehicular Adhoc network.\n",
      "\n",
      "4. id: 5390b5c620f70186a0f0819b   score: 0.9878528   abstract: Vehicular Ad Hoc Network (VANET) is an emerging type of network which facilitates vehicles on roads to communicate for driving safety. It requires a mechanism to help authenticate messages, identify valid vehicles, and remove malevolent vehicles which do not obey the rules. Most existing solutions either do not have an effective message verification scheme, or use the public key infrastructure (PKI). In this network, vehicles are able to broadcast messages to other vehicles and a group of known vehicles can also communicate securely among themselves. So group communication is necessary for the network. However, most existing solutions either do not consider this or use pairing operation to realize this. They are either not secure or not effective. In this paper, we provide a more comprehensive set of secure schemes with Hash-based Message Authentication Code (HMAC) in VANETs to overcome \n",
      "\n",
      "5. id: 5390b36120f70186a0ef06cf   score: 0.9856076   abstract: Vehicular ad hoc networks (VANETs), formed by computers embedded in vehicles and the traffic infrastructure, are expected to develop in the near future to improve traffic safety and efficiency. To this end, VANETs should be designed to be resistant against various abuses and attacks. In this paper, we first review the existing proposals to provide security, privacy, and data aggregation in vehicle-to-vehicle communication. We then address the fundamental issue of achieving these conflicting properties in a unified solution, having observed that separate efforts cannot fulfill the VANET design objectives. A set of new mechanisms are suggested for efficiently managing identities and securely compressing cryptographic witnesses, which are among the major obstacles to the deployment of strong security mechanisms in VANETs.\n",
      "\n",
      "6. id: 5390aca920f70186a0eb902c   score: 0.98486924   abstract: Vehicular ad hoc network (VANET) is an emerging type of networks which facilitates vehicles on roads to communicate for driving safety. The basic idea is to allow arbitrary vehicles to broadcast ad hoc messages (e.g. traffic accidents) to other vehicles. However, this raises the concern of security and privacy. Messages should be signed and verified before they are trusted while the real identity of vehicles should not be revealed, but traceable by authorized party. Existing solutions either rely heavily on a tamper-proof hardware device, or cannot satisfy the privacy requirement and do not have an effective message verification scheme. In this paper, we provide a software-based solution which makes use of only two shared secrets to satisfy the privacy requirement (with security analysis) and gives lower message overhead and at least 45% higher successful rate than previous solutions in \n",
      "\n",
      "7. id: 5390aeba20f70186a0ecb23f   score: 0.98445636   abstract: Vehicular Adhoc Networks these days are attracting much attention as they are expected to greatly influence and improve road safety as well as driving conditions. But along with all the benefits that it offers, there is more chance of giving way to frequent and severe malicious attacks. Due to this reason much attention is being given to the security and privacy issues in VANETs. A lot of research work of being performed to improve the standards of this network. In this paper we present an overview on some of the important research works proposed on the security in VANETs. In this paper we review recent advances on some of the security proposals for VANETs proposed by researchers.\n",
      "\n",
      "8. id: 5390995d20f70186a0e1562b   score: 0.9830851   abstract: Vehicular ad hoc networks (VANETs) have the potential to increase road safety and comfort. Especially because of the road safety functions, there is a strong demand for security in VANETs. After defining three application categories the paper outlines main security and privacy requirements in VANETs. Next, a security architecture for VANETs (SAV) is proposed that strives to satisfy the requirements. To find mechanisms applicable in the architecture a survey of existing mechanisms is given.\n",
      "\n",
      "9. id: 539099ec20f70186a0e1c143   score: 0.98020524   abstract: We present a security framework for Vehicular Ad hoc Networks (VANETs), using identity-based cryptography, to provide authentication, confidentiality, non-repudiation and message integrity. Additionally it provides scalable security and privacy using short-lived, authenticated and unforgeable, pseudonyms. This feature can be used by VANET applications that require quantifiable trust and privacy to provide differentiated service based on various levels of trust and privacy thresholds.\n",
      "\n",
      "10. id: 5390a0b720f70186a0e4f28f   score: 0.9780936   abstract: Vehicular networks are very likely to become the most pervasive and applicable of mobile ad hoc networks in this decade. Vehicular Ad hoc NETwork (VANET) has become a hot emerging research subject, but few academic publications describing its security infrastructure. In this paper, we review the secure infrastructure of VANET, some potential applications and interesting security challenges. To cope with these security challenges, we propose a novel secure scheme for vehicular communication on VANETs. The proposed scheme not only protects the privacy but also maintains the liability in the secure communications by using session keys. We also analyze the robustness of the proposed scheme.\n",
      "\n",
      "\n",
      "\n",
      "Original Document: Unnamed: 0                                                    1707491\n",
      "index                                        558eb49b0cf222bc17bc19d3\n",
      "title               'Close the Loop': An iBeacon App to Foster Rec...\n",
      "authors             Diego Casado-Mansilla, Derek Foster, Shaun Law...\n",
      "year                                                           2015.0\n",
      "venue               Proceedings of the 33rd Annual ACM Conference ...\n",
      "references          5390bf1320f70186a0f51547;5390b20120f70186a0ee5...\n",
      "abstract            Contemporary micro-location technologies such ...\n",
      "id                                                            1707491\n",
      "clustered_labels                                                    0\n",
      "Name: 1707491, dtype: object\n",
      "Reranked Documents:\n",
      "1. id: 558c9dde0cf296c8c28cbc20   score: 0.35947654   abstract: In this forum we highlight innovative thought, design, and research in the area of interaction design and sustainability, illustrating the diversity of approaches across HCI communities. ---Lisa Nathan and Samuel Mann, Editors\n",
      "\n",
      "2. id: 558da0b70cf2af9ee80e9ce9   score: 0.315947   abstract: In this forum we highlight innovative thought, design, and research in the area of interaction design and sustainability, illustrating the diversity of approaches across HCI communities. --- Lisa Nathan and Samuel Mann, Editors\n",
      "\n",
      "3. id: 559247d50cf26384af04a04f   score: 0.315947   abstract: In this forum we highlight innovative thought, design, and research in the area of interaction design and sustainability, illustrating the diversity of approaches across HCI communities. --- Lisa Nathan and Samuel Mann, Editors\n",
      "\n",
      "4. id: 5390b52620f70186a0f03409   score: 0.18535663   abstract: Motivated by a recent surge of research related to energy and sustainability, this paper presents a review of energy-related work within HCI as well as from literature outside of HCI. Our review of energy-related HCI research identifies a central cluster of work focused on electricity consumption feedback (ECF). Our review of literature outside of HCI highlights a number of emerging energy systems trends of strong relevance to HCI and interaction design, including smart grid, demand response, and distributed generation technologies. We conclude by outlining a range of opportunities for HCI to engage with the experiential, behavioral, social, and cultural aspects of these emerging systems, including highlighting new areas for ECF research that move beyond our field's current focus on energy feedback displays to increase awareness and motivate individual conservation behavior.\n",
      "\n",
      "5. id: 5390be6620f70186a0f4c4b3   score: 0.15584777   abstract: To date, research in sustainable HCI has dealt with eco-feedback, usage and recycling of appliances within the home, and longevity of portable electronics such as mobile phones. However, there seems to be less awareness of the energy and greenhouse emissions impacts of domestic consumer electronics and information technology. Such awareness is needed to inform HCI sustainability researchers on how best to prioritise efforts around digital media and IT. Grounded in inventories, interview and plug energy data from 33 undergraduate student participants, our findings provide the context for assessing approaches to reducing the energy and carbon emissions of media and IT in the home. In the paper, we use the findings to discuss and inform more fruitful directions that sustainable HCI research might take, and we quantify how various strategies might have modified the energy and emissions impac\n",
      "\n",
      "6. id: 5390bb7b20f70186a0f3fd3f   score: 0.114368536   abstract: Sustainability is a significant topic in HCI and often framed in terms of energy consumption or sustainable food consumption. However, the sustainable issue of wasted food by consumers is a design arena yet to receive more attention. To understand how the passage from food into waste occurs in everyday life, and if, how and where technology can intervene, fieldwork in 17 households has been carried out. The fieldwork and its implications afford inspirations and reveal stimuli where and how technology could potentially intervene. Selected stimuli are explored with two technology probes and a community platform to inform design.\n",
      "\n",
      "7. id: 5390a01420f70186a0e4878a   score: 0.09721884   abstract: In this panel we explore: (1) the burgeoning discourse on sustainability concerns within HCI, (2) the material and behavioral challenges of sustainability in relation to interaction design, (3) the benefits and risks involved in labeling a project or product as environmentally sustainable, and (4) implications of taking on (or ignoring) sustainability as a research, design, and teaching topic for HCI.\n",
      "\n",
      "8. id: 5390bded20f70186a0f496db   score: 0.08195955   abstract: This forum presents innovative thought, design, and research in the area of interaction design and environmental sustainability. We explore how HCI can contribute to the complex, interdisciplinary efforts to address sustainability challenges. ---Elaine M. Huang, Editor\n",
      "\n",
      "9. id: 5390a01420f70186a0e482b5   score: 0.05281402   abstract: This paper describes the design and interprets the results of a survey of 435 undergraduate students concerning the attitudes of this mainly millennial population towards sustainability apropos of the material effects of information technologies. This survey follows from earlier work on notions of Sustainable Interaction Design (SID)---that is the perspective that sustainability can and should be a central focus within HCI. In so doing it advances to some degree the empirical resources needed to scaffold an understanding of the theory and principles of SID. The interpretations offered yield key insights about understanding different notions of what it means to be successful in a material sense to this population and specific design principles for creating interactive designs differently such that more sustainable behaviors are palatable to individuals of varying attitudes.\n",
      "\n",
      "10. id: 5390a01420f70186a0e48886   score: 0.039342504   abstract: In this paper, I build on perspectives in Human-Computer Interaction (HCI) and design literature to develop a theoretical lens to conduct personal inventories of human-product relationships within the home. I describe an ongoing empirical study examining participants' attitudes toward and relationships with interactive technology, and frame this research within the nascent and growing literature in HCI on environmental sustainability. I will present early findings from this study and discuss how these implications can inform potential future design practice within the HCI community.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def limit_characters(text, max_chars=900):\n",
    "    \"\"\"Limit a text to a specified maximum number of characters.\"\"\"\n",
    "    return text[:max_chars] if len(text) > max_chars else text\n",
    "\n",
    "# Initialize the reranked results list\n",
    "reranked_results = []\n",
    "\n",
    "# Loop through each document in the results\n",
    "for result in results:\n",
    "    row = result['document']  # The original document\n",
    "    closest_documents = result['closest_documents']  # The closest documents found\n",
    "\n",
    "    # Prepare matches for reranking\n",
    "    matches_with_details = []\n",
    "    for i, match in enumerate(closest_documents):\n",
    "        doc_id = match['id']\n",
    "        score = match['score']\n",
    "        \n",
    "        # Retrieve the abstract or any other relevant metadata\n",
    "        abstract = match['metadata'].get('abstract', \"\")  # Use get to avoid KeyError\n",
    "        # abstract_tokens = abstract.split()  # Split the abstract into tokens\n",
    "        # limited_abstract = ' '.join(abstract_tokens[:400])  # Join only the first 400 tokens\n",
    "        limited_abstract = limit_characters(abstract, max_chars=900)  # Apply character limit\n",
    "\n",
    "        \n",
    "\n",
    "        # Prepare match info for reranking\n",
    "        match_info = {\n",
    "            'id': doc_id,\n",
    "            'text': limited_abstract\n",
    "        }\n",
    "        matches_with_details.append(match_info)\n",
    "    \n",
    "    # print(row['abstract'])\n",
    "    \n",
    "    # Rerank the documents using the BGE reranker\n",
    "    reranked_documents = pc.inference.rerank(\n",
    "        model=\"bge-reranker-v2-m3\",\n",
    "        query=row['abstract'],  # The original query for reranking\n",
    "        documents=matches_with_details,  # Documents to rerank\n",
    "        top_n=TOP_N,  # Number of top documents to return\n",
    "        return_documents=True  # Specify whether to return documents\n",
    "    )\n",
    "\n",
    "\n",
    "    # Save the reranked results\n",
    "    reranked_results.append({\n",
    "        'document': row,\n",
    "        'closest_documents': reranked_documents\n",
    "    })\n",
    "\n",
    "# print out the reranked results for each document\n",
    "for reranked_result in reranked_results:\n",
    "    print(\"Original Document:\", reranked_result['document'])\n",
    "    print(\"Reranked Documents:\")\n",
    "    for i, doc in enumerate(reranked_result['closest_documents'].data):\n",
    "        print(str(i+1)+'. id: '+doc['document']['id']+'   score: '+str(doc['score'])+'   abstract: '+doc['document']['text']+'\\n')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate retrieved top k documents **after reranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0843\n",
      "Recall: 0.0699\n",
      "F1 Score: 0.0764\n",
      "MAP: 0.2150\n",
      "MRR: 0.2388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_reranker(reranked_results):\n",
    "    # Initialize lists to hold true and predicted labels for evaluation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Loop through each document in the reranked results\n",
    "    for reranked_result in reranked_results:\n",
    "        # Get the actual references (as a set) from the original document\n",
    "        actual_references = set(reranked_result['document']['references'].split(';'))\n",
    "        \n",
    "        # Get the reranked document IDs from the closest documents data\n",
    "        predicted_ids = {doc['document']['id'] for doc in reranked_result['closest_documents'].data}\n",
    "\n",
    "        # Create a combined set of all document IDs (true and predicted)\n",
    "        all_ids = actual_references.union(predicted_ids)\n",
    "\n",
    "        # Populate y_true and y_pred for metrics calculation\n",
    "        for doc_id in all_ids:\n",
    "            y_true.append(1 if doc_id in actual_references else 0)\n",
    "            y_pred.append(1 if doc_id in predicted_ids else 0)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Sample code to calculate MRR\n",
    "def mean_reciprocal_rank(results):\n",
    "    reciprocal_ranks = []\n",
    "    for result in results:\n",
    "        # Find the rank of the first relevant document in reranked results\n",
    "        for i, doc in enumerate(result['closest_documents'].data):\n",
    "            if doc['document']['id'] in result['document']['references']:\n",
    "                reciprocal_ranks.append(1 / (i + 1))\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)  # No relevant doc found\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "# Sample code to calculate MAP\n",
    "def mean_average_precision(results):\n",
    "    average_precisions = []\n",
    "    for result in results:\n",
    "        num_relevant = 0\n",
    "        score_sum = 0\n",
    "        for i, doc in enumerate(result['closest_documents'].data):\n",
    "            if doc['document']['id'] in result['document']['references']:\n",
    "                num_relevant += 1\n",
    "                precision_at_k = num_relevant / (i + 1)\n",
    "                score_sum += precision_at_k\n",
    "        if num_relevant > 0:\n",
    "            average_precisions.append(score_sum / num_relevant)\n",
    "        else:\n",
    "            average_precisions.append(0)\n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "\n",
    "# Assuming reranked_results contains the results from the reranker\n",
    "precision, recall, f1 = evaluate_reranker(reranked_results)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "mrr = mean_reciprocal_rank(reranked_results)\n",
    "map_score = mean_average_precision(reranked_results)\n",
    "print(f'MAP: {map_score:.4f}')\n",
    "print(f'MRR: {mrr:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try to filter by year versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #added year filtering\n",
    "\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# def evaluate_results(results):\n",
    "#     # Create lists to hold true labels and predicted labels\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     average_precisions = []\n",
    "#     reciprocal_ranks = []\n",
    "\n",
    "#     for result in results:\n",
    "#         # Get the actual references (ensure it's a set) and closest document IDs\n",
    "#         actual_references = set(result['document']['references'].split(';'))  # Actual references\n",
    "#         predicted_ids = [match['id'] for match in result['closest_documents']]  # Predicted IDs (list for ranking)\n",
    "#         main_document_year = result['document']['year']  # Extracting the document's year\n",
    "\n",
    "\n",
    "#         # Create a combined set of all document IDs (true and predicted)\n",
    "#         all_ids = actual_references.union(predicted_ids)\n",
    "\n",
    "#         # Append true positives and false negatives for precision, recall, F1 calculations\n",
    "#         for doc_id in all_ids:\n",
    "#             is_append=True\n",
    "#             doc_year_array= network_data.loc[network_data['index'] == doc_id, 'year'].values\n",
    "#             if doc_year_array.size > 0:\n",
    "#                 doc_year = doc_year_array[0]  # Extract the first element\n",
    "#                 if doc_year > main_document_year: #ensureto consider evaluation only for documents that have year smaller than main doc\n",
    "#                     is_append=False\n",
    "#             if is_append==True:\n",
    "#                 y_true.append(1 if doc_id in actual_references else 0)\n",
    "#                 y_pred.append(1 if doc_id in predicted_ids else 0)\n",
    "\n",
    "#         # Calculate Average Precision (AP) for MAP\n",
    "#         relevant_docs_retrieved = 0\n",
    "#         score_sum = 0\n",
    "#         rank=0\n",
    "#         for doc_id in predicted_ids:\n",
    "#             is_append=True\n",
    "#             doc_year_array= network_data.loc[network_data['index'] == doc_id, 'year'].values\n",
    "#             if doc_year_array.size > 0:\n",
    "#                 doc_year = doc_year_array[0]  # Extract the first element\n",
    "#                 if doc_year > main_document_year: #ensure to consider evaluation only for documents that have year smaller than main doc\n",
    "#                     is_append=False\n",
    "\n",
    "#             if is_append:\n",
    "#                 if doc_id in actual_references:\n",
    "#                     relevant_docs_retrieved += 1\n",
    "#                     precision_at_k = relevant_docs_retrieved / (rank + 1)\n",
    "#                     score_sum += precision_at_k\n",
    "#                 rank+=1\n",
    "            \n",
    "#         if relevant_docs_retrieved > 0:\n",
    "#             average_precisions.append(score_sum / relevant_docs_retrieved)\n",
    "#         else:\n",
    "#             average_precisions.append(0)\n",
    "#         rank=0\n",
    "#         # Calculate Reciprocal Rank (RR) for MRR\n",
    "#         for doc_id in predicted_ids:\n",
    "#             is_append=True\n",
    "#             doc_year_array= network_data.loc[network_data['index'] == doc_id, 'year'].values\n",
    "#             if doc_year_array.size > 0:\n",
    "#                 doc_year = doc_year_array[0]  # Extract the first element\n",
    "#                 if doc_year > main_document_year: #ensureto consider evaluation only for documents that have year smaller than main doc\n",
    "#                     is_append=False\n",
    "#             if is_append:\n",
    "#                 if doc_id in actual_references:\n",
    "#                     reciprocal_ranks.append(1 / (rank + 1))\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     rank+=1\n",
    "#         else:\n",
    "#             reciprocal_ranks.append(0)  # No relevant doc found\n",
    "\n",
    "#     # Debugging: Print sizes and samples of y_true and y_pred\n",
    "#     print(f'Size of y_true: {len(y_true)}, Size of y_pred: {len(y_pred)}')\n",
    "#     print(f'y_true samples: {y_true[:10]}')  # Print first 10 elements for verification\n",
    "#     print(f'y_pred samples: {y_pred[:10]}')  # Print first 10 elements for verification\n",
    "\n",
    "#     # Ensure y_true and y_pred are the same size before calculating metrics\n",
    "#     if len(y_true) != len(y_pred):\n",
    "#         print(\"Error: y_true and y_pred are not the same length.\")\n",
    "#         return None, None, None, None, None\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "#     recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "#     f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "#     map_score = sum(average_precisions) / len(average_precisions)\n",
    "#     mrr_score = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "#     return precision, recall, f1, map_score, mrr_score\n",
    "\n",
    "# # Assuming results is your list of results from previous queries\n",
    "# precision, recall, f1, map_score, mrr_score = evaluate_results(results)\n",
    "\n",
    "# # Print evaluation metrics if valid\n",
    "# if precision is not None:\n",
    "#     print(f'Precision: {precision:.4f}')\n",
    "#     print(f'Recall: {recall:.4f}')\n",
    "#     print(f'F1 Score: {f1:.4f}')\n",
    "#     print(f'MAP: {map_score:.4f}')\n",
    "#     print(f'MRR: {mrr_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #added year filtering \n",
    "# import numpy as np\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# def evaluate_reranker(reranked_results):\n",
    "#     # Initialize lists to hold true and predicted labels for evaluation\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "\n",
    "#     # Loop through each document in the reranked results\n",
    "#     for reranked_result in reranked_results:\n",
    "#         # Get the actual references (as a set) from the original document\n",
    "#         actual_references = set(reranked_result['document']['references'].split(';'))\n",
    "        \n",
    "#         # Get the reranked document IDs from the closest documents data\n",
    "#         predicted_ids = {doc['document']['id'] for doc in reranked_result['closest_documents'].data}\n",
    "\n",
    "#         # Create a combined set of all document IDs (true and predicted)\n",
    "#         all_ids = actual_references.union(predicted_ids)\n",
    "\n",
    "#         main_document_year = reranked_result['document']['year']  # Extracting the document's year\n",
    "\n",
    "\n",
    "#         # Populate y_true and y_pred for metrics calculation\n",
    "#         for doc_id in all_ids:\n",
    "#             is_append=True\n",
    "#             doc_year_array= network_data.loc[network_data['index'] == doc_id, 'year'].values\n",
    "#             if doc_year_array.size > 0:\n",
    "#                 doc_year = doc_year_array[0]  # Extract the first element\n",
    "#                 if doc_year > main_document_year: #ensureto consider evaluation only for documents that have year smaller than main doc\n",
    "#                     is_append=False\n",
    "#             if is_append==True:\n",
    "#                 y_true.append(1 if doc_id in actual_references else 0)\n",
    "#                 y_pred.append(1 if doc_id in predicted_ids else 0)\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "#     recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "#     f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "#     return precision, recall, f1\n",
    "\n",
    "# # Sample code to calculate MRR\n",
    "# def mean_reciprocal_rank(results):\n",
    "#     rank=0\n",
    "#     reciprocal_ranks = []\n",
    "#     for result in results:\n",
    "#         main_document_year = result['document']['year']  # Extracting the document's year\n",
    "#         # Find the rank of the first relevant document in reranked results\n",
    "#         for doc in result['closest_documents'].data:\n",
    "#             is_append=True\n",
    "#             doc_year_array= network_data.loc[network_data['index'] == doc['document']['id'], 'year'].values\n",
    "#             if doc_year_array.size > 0:\n",
    "#                 doc_year = doc_year_array[0]  # Extract the first element\n",
    "#                 if doc_year > main_document_year: #ensureto consider evaluation only for documents that have year smaller than main doc\n",
    "#                     is_append=False\n",
    "#             if is_append:\n",
    "#                 if doc['document']['id'] in result['document']['references']:\n",
    "#                     reciprocal_ranks.append(1 / (rank + 1))\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     rank+=1\n",
    "#         else:\n",
    "#             reciprocal_ranks.append(0)  # No relevant doc found\n",
    "#     return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "# # Sample code to calculate MAP\n",
    "# def mean_average_precision(results):\n",
    "#     rank=0\n",
    "#     average_precisions = []\n",
    "#     for result in results:\n",
    "#         num_relevant = 0\n",
    "#         score_sum = 0\n",
    "#         main_document_year = result['document']['year']  # Extracting the document's year\n",
    "#         #print(f'main doc year {main_document_year}')\n",
    "#         for doc in result['closest_documents'].data:\n",
    "#             is_append=True\n",
    "#             doc_year_array= network_data.loc[network_data['index'] == doc['document']['id'], 'year'].values\n",
    "#             if doc_year_array.size > 0:\n",
    "#                 doc_year = doc_year_array[0]  # Extract the first element\n",
    "#                 #print(f'reranked doc year {doc_year}')\n",
    "#                 if doc_year > main_document_year: #ensure to consider evaluation only for documents that have year smaller than main doc\n",
    "#                     is_append=False\n",
    "\n",
    "#             if is_append:\n",
    "#                 if doc['document']['id'] in result['document']['references']:\n",
    "#                     num_relevant += 1\n",
    "#                     precision_at_k = num_relevant / (rank + 1)\n",
    "#                     score_sum += precision_at_k\n",
    "#                 rank+=1\n",
    "#         if num_relevant > 0:\n",
    "#             average_precisions.append(score_sum / num_relevant)\n",
    "#         else:\n",
    "#             average_precisions.append(0)\n",
    "#     return np.mean(average_precisions)\n",
    "\n",
    "\n",
    "# # Assuming reranked_results contains the results from the reranker\n",
    "# precision, recall, f1 = evaluate_reranker(reranked_results)\n",
    "\n",
    "# # Print evaluation metrics\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# #mrr = mean_reciprocal_rank(reranked_results)\n",
    "# map_score = mean_average_precision(reranked_results)\n",
    "# print(f'MRR: {mrr:.4f}')\n",
    "# print(f'MAP: {map_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
